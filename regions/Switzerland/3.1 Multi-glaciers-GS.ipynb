{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import massbalancemachine as mbm\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, LineString, Point\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import GroupKFold, KFold, train_test_split, GroupShuffleSplit\n",
    "from calendar import month_abbr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "from oggm import cfg, utils, workflow, tasks\n",
    "import logging\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "\n",
    "import config\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.xgb_helpers import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(config.SEED)\n",
    "\n",
    "# Specify the short names of the climate variables available in the dataset\n",
    "vois_climate = ['t2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str']\n",
    "vois_topographical = ['aspect', 'slope', 'dis_from_border']\n",
    "\n",
    "# in case no memory\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "cmap = cm.devon\n",
    "color_palette_glaciers = sns.color_palette(get_cmap_hex(cmap, 15))\n",
    "\n",
    "# For bars and lines:\n",
    "# color_diff_xgb = '#878787'\n",
    "color_diff_xgb = '#4d4d4d'\n",
    "\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_xgb = colors[0]\n",
    "color_xgb_winter = colors[1]\n",
    "\n",
    "color_tim = '#c51b7d'\n",
    "\n",
    "# Violin and boxplots:\n",
    "colors_temp_freq = sns.color_palette(get_cmap_hex(cm.devon, 8))\n",
    "boxplot_style = {\n",
    "    \"width\": .6,\n",
    "    \"showcaps\": False,\n",
    "    \"palette\": colors_temp_freq,\n",
    "    \"flierprops\": {\n",
    "        \"marker\": \"x\"\n",
    "    },\n",
    "    \"showmeans\": True,\n",
    "    \"meanprops\": {\n",
    "        \"markerfacecolor\": \"white\"\n",
    "    }\n",
    "}\n",
    "\n",
    "marker_tim = 's'\n",
    "marker_xgb = 'o'\n",
    "marker_std = '_'\n",
    "\n",
    "custom_working_dir = '../../../data/OGGM/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "path_rgi = '../../../data/GLAMOS/CH_glacier_ids_long.csv'\n",
    "rgi_df = pd.read_csv(path_rgi, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stakes data over all glaciers:\n",
    "data_glamos = pd.read_csv(path_PMB_GLAMOS_csv + 'CH_wgms_dataset.csv')\n",
    "data_glamos.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of mean mass balance per glacier:\n",
    "# Get the mean mass balance per glacier\n",
    "mean_mb_per_glacier = data_glamos.groupby(['GLACIER',\n",
    "                                           'YEAR'])['POINT_BALANCE'].mean()\n",
    "matrix = pd.DataFrame(mean_mb_per_glacier).reset_index().pivot(\n",
    "    index='GLACIER', columns='YEAR',\n",
    "    values='POINT_BALANCE').sort_values(by='GLACIER')\n",
    "\n",
    "# get elevation of glaciers:\n",
    "gl_per_el = data_glamos.groupby(['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "\n",
    "matrix = matrix.loc[gl_per_el.sort_values(ascending=True).index]\n",
    "\n",
    "# make index categorical\n",
    "matrix.index = pd.Categorical(matrix.index,\n",
    "                              categories=matrix.index,\n",
    "                              ordered=True)\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "sns.heatmap(data=matrix,\n",
    "            center=0,\n",
    "            cmap=cm.vik_r,\n",
    "            cbar_kws={'label': '[m w.e. $a^{-1}$]'},\n",
    "            ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot elevation:\n",
    "fig = plt.figure(figsize=(10, 2))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "sns.lineplot(gl_per_el.sort_values(ascending=True),\n",
    "             ax=ax,\n",
    "             color='gray',\n",
    "             marker='v')\n",
    "ax.set_xticklabels('', rotation=90)\n",
    "ax.set_ylabel('')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gl = data_glamos.groupby(['GLACIER']).size().sort_values()\n",
    "num_gl.plot(kind='bar', figsize=(15, 5), cmap=cmap)\n",
    "plt.title('Number of total measurements per glacier since 1961')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_glamos.GLACIER.unique()), data_glamos.GLACIER.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = True\n",
    "if RUN:\n",
    "    # Filter data_glamos\n",
    "    print('Running on {} glaciers'.format(len(data_glamos.GLACIER.unique())))\n",
    "    print('Glaciers:', data_glamos.GLACIER.unique())\n",
    "    # Create dataloader:\n",
    "    dataset_gl = mbm.Dataset(data=data_glamos,\n",
    "                             region_name='CH',\n",
    "                             data_path=path_PMB_GLAMOS_csv)\n",
    "    print('Number of winter and annual samples:', len(data_glamos))\n",
    "    print('Number of annual samples:',\n",
    "          len(data_glamos[data_glamos.PERIOD == 'annual']))\n",
    "    print('Number of winter samples:',\n",
    "          len(data_glamos[data_glamos.PERIOD == 'winter']))\n",
    "\n",
    "    # Add climate data:\n",
    "    # Specify the files of the climate data, that will be matched with the coordinates of the stake data\n",
    "    era5_climate_data = path_ERA5_raw + 'era5_monthly_averaged_data.nc'\n",
    "    geopotential_data = path_ERA5_raw + 'era5_geopotential_pressure.nc'\n",
    "\n",
    "    # Match the climate features, from the ERA5Land netCDF file, for each of the stake measurement dataset\n",
    "    dataset_gl.get_climate_features(climate_data=era5_climate_data,\n",
    "                                    geopotential_data=geopotential_data,\n",
    "                                    change_units=True)\n",
    "\n",
    "    print('Converting to monthly resolution')\n",
    "    # For each record, convert to a monthly time resolution\n",
    "    dataset_gl.convert_to_monthly(meta_data_columns=config.META_DATA,\n",
    "                                  vois_climate=vois_climate,\n",
    "                                  vois_topographical=vois_topographical)\n",
    "\n",
    "    # Create a new DataLoader object with the monthly stake data measurements.\n",
    "    dataloader_gl = mbm.DataLoader(data=dataset_gl.data,\n",
    "                                   random_seed=config.SEED,\n",
    "                                   meta_data_columns=config.META_DATA)\n",
    "\n",
    "    print('Number of monthly rows:', len(dataloader_gl.data))\n",
    "    print('Columns in the dataset:', dataloader_gl.data.columns)\n",
    "\n",
    "    # save the data\n",
    "    dataloader_gl.data.to_csv(path_PMB_GLAMOS_csv +\n",
    "                              'CH_wgms_dataset_monthly.csv',\n",
    "                              index=False)\n",
    "else:\n",
    "    # read data\n",
    "    data_monthly = pd.read_csv(path_PMB_GLAMOS_csv +\n",
    "                               'CH_wgms_dataset_monthly.csv')\n",
    "    dataloader_gl = mbm.DataLoader(data=data_monthly,\n",
    "                                   random_seed=config.SEED,\n",
    "                                   meta_data_columns=config.META_DATA)\n",
    "print('Number of monthly rows:', len(dataloader_gl.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check of variables:\n",
    "df = dataloader_gl.data\n",
    "var_to_plot = ['POINT_BALANCE'] + vois_climate\n",
    "df = df[(df.GLACIER == 'aletsch') & (df.YEAR == 1961)].groupby(\n",
    "    ['MONTHS'])[var_to_plot].mean().reset_index()\n",
    "df['month_nb'] = df.MONTHS.apply(\n",
    "    lambda x: list(month_abbr).index(x.capitalize()))\n",
    "df.sort_values(by='month_nb', inplace=True)\n",
    "fig, ax = plt.subplots(2, 4, figsize=(12, 5))\n",
    "\n",
    "for i, ax in enumerate(ax.flatten()):\n",
    "    df.plot(x='MONTHS', y=var_to_plot[i], marker='o', ax=ax)\n",
    "    ax.set_title(var_to_plot[i])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split on glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of mean mass balance per glacier:\n",
    "# Get the mean mass balance per glacier\n",
    "mean_mb_per_glacier = data_glamos.groupby(['GLACIER',\n",
    "                                           'YEAR'])['POINT_BALANCE'].mean()\n",
    "matrix = pd.DataFrame(mean_mb_per_glacier).reset_index().pivot(\n",
    "    index='GLACIER', columns='YEAR',\n",
    "    values='POINT_BALANCE').sort_values(by='GLACIER')\n",
    "\n",
    "# get elevation of glaciers:\n",
    "gl_per_el = data_glamos.groupby(['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "\n",
    "matrix = matrix.loc[gl_per_el.sort_values(ascending=True).index]\n",
    "\n",
    "# make index categorical\n",
    "matrix.index = pd.Categorical(matrix.index,\n",
    "                              categories=matrix.index,\n",
    "                              ordered=True)\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "sns.heatmap(data=matrix,\n",
    "            center=0,\n",
    "            cmap=cm.vik_r,\n",
    "            cbar_kws={'label': '[m w.e. $a^{-1}$]'},\n",
    "            ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_glaciers = [\n",
    "    'tortin', 'limmern', 'taelliboden', 'sanktanna', 'schwarzberg', 'hohlaub',\n",
    "    'rhone'\n",
    "]\n",
    "train_glaciers = [\n",
    "    i for i in data_glamos.GLACIER.unique() if i not in test_glaciers\n",
    "]\n",
    "\n",
    "data_test = data_glamos[data_glamos.GLACIER.isin(test_glaciers)]\n",
    "data_train = data_glamos[data_glamos.GLACIER.isin(train_glaciers)]\n",
    "\n",
    "test_perc = (len(data_test) / len(data_train)) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits, test_set, train_set = getCVSplits(dataloader_gl,\n",
    "                                          test_split_on='GLACIER',\n",
    "                                          test_splits=test_glaciers)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))\n",
    "visualiseSplits(test_set['y'], train_set['y'], splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 10, figsize=(16, 6), sharey='row', sharex='col')\n",
    "train_set['df_X']['POINT_BALANCE'].plot.hist(ax=ax[0, 0],\n",
    "                                             color=color_xgb,\n",
    "                                             alpha=0.6,\n",
    "                                             density=False)\n",
    "ax[0, 0].set_title('PMB')\n",
    "ax[0, 0].set_ylabel('Frequency (train)')\n",
    "train_set['df_X']['POINT_ELEVATION'].plot.hist(ax=ax[0, 1],\n",
    "                                               color=color_xgb,\n",
    "                                               alpha=0.6,\n",
    "                                               density=False)\n",
    "ax[0, 1].set_title('ELV')\n",
    "train_set['df_X']['YEAR'].plot.hist(ax=ax[0, 2],\n",
    "                                    color=color_xgb,\n",
    "                                    alpha=0.6,\n",
    "                                    density=False)\n",
    "ax[0, 2].set_title('YEARS')\n",
    "train_set['df_X']['t2m'].plot.hist(ax=ax[0, 3],\n",
    "                                   color=color_xgb,\n",
    "                                   alpha=0.6,\n",
    "                                   density=False)\n",
    "\n",
    "for i, voi_clim in enumerate(vois_climate):\n",
    "    ax[0, 3 + i].set_title(voi_clim)\n",
    "    train_set['df_X'][voi_clim].plot.hist(ax=ax[0, 3 + i],\n",
    "                                          color=color_xgb,\n",
    "                                          alpha=0.6,\n",
    "                                          density=False)\n",
    "\n",
    "test_set['df_X']['POINT_BALANCE'].plot.hist(ax=ax[1, 0],\n",
    "                                            color=color_tim,\n",
    "                                            alpha=0.6,\n",
    "                                            density=False)\n",
    "ax[1, 0].set_ylabel('Frequency (test)')\n",
    "test_set['df_X']['POINT_ELEVATION'].plot.hist(bins=50,\n",
    "                                              ax=ax[1, 1],\n",
    "                                              color=color_tim,\n",
    "                                              alpha=0.6,\n",
    "                                              density=False)\n",
    "test_set['df_X']['YEAR'].plot.hist(ax=ax[1, 2],\n",
    "                                   color=color_tim,\n",
    "                                   alpha=0.6,\n",
    "                                   density=False)\n",
    "\n",
    "for i, voi_clim in enumerate(vois_climate):\n",
    "    test_set['df_X'][voi_clim].plot.hist(ax=ax[1, 3 + i],\n",
    "                                         color=color_tim,\n",
    "                                         alpha=0.6,\n",
    "                                         density=False)\n",
    "# rotate xticks\n",
    "for ax in ax.flatten():\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.set_xlabel('')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Grid search\n",
    "# For each of the XGBoost parameter, define the grid range\n",
    "# param_grid = {\n",
    "#     'max_depth': [\n",
    "#         3,\n",
    "#         4,\n",
    "#         5,\n",
    "#         6,\n",
    "#     ],\n",
    "#     'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "#     'n_estimators': [100, 200, 300],\n",
    "#     'gamma': [0, 1]\n",
    "# }\n",
    "# param_grid = {\n",
    "#     'learning_rate': np.arange(0.01, 0.3, 0.01),\n",
    "#     'n_estimators': np.arange(50, 400, 15),\n",
    "#     'max_depth': np.arange(3, 10, 1),\n",
    "# }\n",
    "\n",
    "param_ranges = {\n",
    "    'max_depth': [2, 3, 4, 5, 6, 7, 8],\n",
    "    'n_estimators':\n",
    "    [50, 100, 200, 300, 400,\n",
    "     500],  # number of trees (too many = overfitting, too few = underfitting)\n",
    "    'learning_rate': [0.01, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "}\n",
    "\n",
    "param_init = {}\n",
    "param_init['device'] = 'cuda:0'\n",
    "param_init['tree_method'] = 'hist'\n",
    "param_init[\"random_state\"] = config.SEED\n",
    "param_init[\"n_jobs\"] = config.NUM_JOBS\n",
    "\n",
    "# Feature columns:\n",
    "feature_columns = [\n",
    "    'ELEVATION_DIFFERENCE', 'POINT_ELEVATION', 'ALTITUDE_CLIMATE'\n",
    "] + list(vois_climate) + list(vois_topographical)\n",
    "all_columns = feature_columns + config.META_DATA + config.NOT_METADATA_NOT_FEATURES\n",
    "df_X_train_subset = train_set['df_X'][all_columns]\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of testing dataset:', test_set['df_X'][all_columns].shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "RUN = True\n",
    "if RUN:\n",
    "    # Create a CustomXGBoostRegressor instance\n",
    "    custom_xgboost = mbm.models.CustomXGBoostRegressor(**param_init)\n",
    "    # custom_xgboost.randomsearch(\n",
    "    #     parameters=param_grid,\n",
    "    #     n_iter=45,\n",
    "    #     splits=splits,\n",
    "    #     features=df_X_train_subset,\n",
    "    #     targets=train_set['y'],\n",
    "    # )\n",
    "    custom_xgboost.gridsearch(\n",
    "        parameters=param_grid,\n",
    "        splits=splits,\n",
    "        features=df_X_train_subset,\n",
    "        targets=train_set['y'],\n",
    "    )\n",
    "    # save best model\n",
    "    custom_xgboost.save_model(f'xgb_gl_split_GS.pkl')\n",
    "else:\n",
    "    # read model\n",
    "    custom_xgboost = mbm.models.CustomXGBoostRegressor()\n",
    "    custom_xgboost.load_model(f'xgb_gl_split_GS.pkl')\n",
    "\n",
    "# Get best parameters and estimator\n",
    "best_params = custom_xgboost.param_search.best_params_\n",
    "best_estimator = custom_xgboost.param_search.best_estimator_\n",
    "print(\"Best parameters:\\n\", best_params)\n",
    "print(\"Best score:\\n\", custom_xgboost.param_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualiseValPreds(best_estimator, splits, train_set, feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotGridSearchScore(custom_xgboost)\n",
    "plotGridSearchParams(custom_xgboost, param_grid,\n",
    "                     custom_xgboost.param_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to CPU for predictions:\n",
    "xgb = best_estimator.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on test\n",
    "features_test, metadata_test = xgb._create_features_metadata(\n",
    "    test_set['df_X'][all_columns], config.META_DATA)\n",
    "y_pred = xgb.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = xgb.aggrPredict(metadata_test, config.META_DATA, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = xgb.score(test_set['df_X'][all_columns], test_set['y'])  # negative\n",
    "mse, rmse, mae, pearson_corr = xgb.evalMetrics(metadata_test, y_pred,\n",
    "                                               test_set['y'])\n",
    "\n",
    "# Aggregate predictions to annual or winter:\n",
    "df_pred = test_set['df_X'][all_columns].copy()\n",
    "df_pred['target'] = test_set['y']\n",
    "grouped_ids = df_pred.groupby('ID').agg({\n",
    "    'target': 'mean',\n",
    "    'YEAR': 'first',\n",
    "    'POINT_ID': 'first'\n",
    "})\n",
    "grouped_ids['pred'] = y_pred_agg\n",
    "grouped_ids['PERIOD'] = test_set['df_X'][\n",
    "    feature_columns + config.META_DATA +\n",
    "    config.NOT_METADATA_NOT_FEATURES].groupby('ID')['PERIOD'].first()\n",
    "grouped_ids['GLACIER'] = grouped_ids['POINT_ID'].apply(\n",
    "    lambda x: x.split('_')[0])\n",
    "\n",
    "grouped_ids = grouped_ids[grouped_ids.YEAR <= 2021]\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "grouped_ids_annual = grouped_ids[grouped_ids.PERIOD == 'annual']\n",
    "predVSTruth(ax, grouped_ids_annual, mae, rmse, pearson_corr)\n",
    "ax.set_title('Annual MB', fontsize=24)\n",
    "\n",
    "grouped_ids_annual.sort_values(by='YEAR', inplace=True)\n",
    "ax = plt.subplot(2, 2, 2)\n",
    "plotMeanPred(grouped_ids_annual, ax)\n",
    "\n",
    "if 'winter' in grouped_ids.PERIOD.unique():\n",
    "    grouped_ids_winter = grouped_ids[grouped_ids.PERIOD == 'winter']\n",
    "    ax = plt.subplot(2, 2, 3)\n",
    "    predVSTruth(ax, grouped_ids_winter, mae, rmse, pearson_corr)\n",
    "    ax.set_title('Winter MB', fontsize=24)\n",
    "\n",
    "    ax = plt.subplot(2, 2, 4)\n",
    "    grouped_ids_winter.sort_values(by='YEAR', inplace=True)\n",
    "    plotMeanPred(grouped_ids_winter, ax)\n",
    "\n",
    "# ax.set_title('Mean yearly target and prediction')\n",
    "plt.suptitle(f'XGBoost tested on {test_glaciers}', fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIPlot(best_estimator, feature_columns, vois_climate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
