{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from calendar import month_abbr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "from cartopy import crs as ccrs, feature as cfeature\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.enums import Resampling as RResampling\n",
    "import numpy as np\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import joypy\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import geopandas as gpd\n",
    "from matplotlib.patches import Wedge, Patch\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import pickle \n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import WeightedRandomSampler, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'\n",
    "\n",
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', \n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\",  # SGI\n",
    "    \"slope_sgi\",  # SGI\n",
    "    \"hugonnet_dhdt\",  # OGGM\n",
    "    \"consensus_ice_thickness\",  # OGGM\n",
    "    \"millan_v\",  # OGGM\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = pd.read_csv(cfg.dataPath + path_PMB_GLAMOS_csv +\n",
    "                          'CH_wgms_dataset_all.csv')\n",
    "\n",
    "# Capitalize glacier names:\n",
    "glacierCap = {}\n",
    "for gl in data_glamos['GLACIER'].unique():\n",
    "    if isinstance(gl, str):  # Ensure the glacier name is a string\n",
    "        if gl.lower() == 'claridenu':\n",
    "            glacierCap[gl] = 'Clariden_U'\n",
    "        elif gl.lower() == 'claridenl':\n",
    "            glacierCap[gl] = 'Clariden_L'\n",
    "        else:\n",
    "            glacierCap[gl] = gl.capitalize()\n",
    "    else:\n",
    "        print(f\"Warning: Non-string glacier name encountered: {gl}\")\n",
    "\n",
    "# Cut to glaciers with pcsr: \n",
    "glacier_list = ['adler',\n",
    " 'albigna',\n",
    " 'aletsch',\n",
    " 'allalin',\n",
    " 'basodino',\n",
    " 'clariden',\n",
    " 'corbassiere',\n",
    " 'corvatsch',\n",
    " 'findelen',\n",
    " 'forno',\n",
    " 'gietro',\n",
    " 'gorner',\n",
    " 'gries',\n",
    " 'hohlaub',\n",
    " 'joeri',\n",
    " 'limmern',\n",
    " 'morteratsch',\n",
    " 'murtel',\n",
    " 'oberaar',\n",
    " 'otemma',\n",
    " 'pizol',\n",
    " 'plattalva',\n",
    " 'rhone',\n",
    " 'sanktanna',\n",
    " 'schwarzbach',\n",
    " 'schwarzberg',\n",
    " 'sexrouge',\n",
    " 'silvretta',\n",
    " 'tortin',\n",
    " 'tsanfleuron']\n",
    "\n",
    "data_glamos = data_glamos[data_glamos['GLACIER'].isin(glacier_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glacier outlines:\n",
    "glacier_outline_sgi = gpd.read_file(\n",
    "    os.path.join(cfg.dataPath, path_SGI_topo, 'inventory_sgi2016_r2020',\n",
    "                 'SGI_2016_glaciers_copy.shp'))  # Load the shapefile\n",
    "glacier_outline_rgi = gpd.read_file(cfg.dataPath + path_rgi_outlines)\n",
    "\n",
    "# get number of measurements per glacier:\n",
    "glacier_info = data_glamos.groupby('GLACIER').size().sort_values(\n",
    "    ascending=False).reset_index()\n",
    "glacier_info.rename(columns={0: 'Nb. measurements'}, inplace=True)\n",
    "glacier_info.set_index('GLACIER', inplace=True)\n",
    "\n",
    "glacier_loc = data_glamos.groupby('GLACIER')[['POINT_LAT', 'POINT_LON']].mean()\n",
    "\n",
    "glacier_info = glacier_loc.merge(glacier_info, on='GLACIER')\n",
    "\n",
    "glacier_period = data_glamos.groupby(['GLACIER', 'PERIOD'\n",
    "                                      ]).size().unstack().fillna(0).astype(int)\n",
    "\n",
    "glacier_info = glacier_info.merge(glacier_period, on='GLACIER')\n",
    "\n",
    "glacier_info['Train/Test glacier'] = glacier_info.apply(\n",
    "    lambda x: 'Test' if x.name in TEST_GLACIERS else 'Train', axis=1)\n",
    "glacier_info.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign glaciers to river basin names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load RGI glacier IDs ===\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids)\n",
    "rgi_df.columns = rgi_df.columns.str.strip()\n",
    "rgi_df = rgi_df.sort_values(by='short_name').set_index('short_name')\n",
    "\n",
    "# === Load SGI region geometries ===\n",
    "SGI_regions = gpd.read_file(\n",
    "    os.path.join(cfg.dataPath, path_SGI_topo, 'inventory_sgi2016_r2020',\n",
    "                 'sgi_regions.geojson'))\n",
    "\n",
    "# Clean object columns\n",
    "SGI_regions[SGI_regions.select_dtypes(include='object').columns] = \\\n",
    "    SGI_regions.select_dtypes(include='object').apply(lambda col: col.str.strip())\n",
    "\n",
    "SGI_regions = SGI_regions.drop_duplicates().dropna()\n",
    "SGI_regions = SGI_regions.set_index('pk_sgi_region')\n",
    "\n",
    "# === Map to Level 0 river basins ===\n",
    "catchment_lv0 = {\n",
    "    'A': 'Rhine',\n",
    "    'B': 'Rhone',\n",
    "    'C': 'Po',\n",
    "    'D': 'Adige',\n",
    "    'E': 'Danube'\n",
    "}\n",
    "rgi_df['rvr_lv0'] = rgi_df['sgi-id'].str[0].map(catchment_lv0)\n",
    "\n",
    "\n",
    "# === Map to Level 1 river basins using SGI regions ===\n",
    "def get_river_basin(sgi_id):\n",
    "    key = sgi_id.split('-')[0]\n",
    "    if key not in SGI_regions.index:\n",
    "        return None\n",
    "    basin = SGI_regions.loc[key, 'river_basin_name']\n",
    "    if isinstance(basin, pd.Series):\n",
    "        return basin.dropna().unique()[0] if not basin.dropna().empty else None\n",
    "    return basin if pd.notna(basin) else None\n",
    "\n",
    "\n",
    "rgi_df['rvr_lv1'] = rgi_df['sgi-id'].apply(get_river_basin)\n",
    "\n",
    "# Final formatting\n",
    "rgi_df = rgi_df.reset_index().rename(columns={\n",
    "    'short_name': 'GLACIER'\n",
    "}).set_index('GLACIER')\n",
    "rgi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_info = glacier_info.merge(rgi_df[['rvr_lv0', 'rvr_lv1']],\n",
    "                                  on='GLACIER',\n",
    "                                  how='left')\n",
    "glacier_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro & methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geoplots:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sqrt scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the original raster\n",
    "tif_name = \"landesforstinventar-vegetationshoehenmodell_relief_sentinel_2024_2056.tif\"\n",
    "tif_path = os.path.join(cfg.dataPath, 'GLAMOS/RGI/', tif_name)\n",
    "\n",
    "# Desired output resolution (in degrees)\n",
    "# Approx. 100 m in degrees: ~0.0009 deg\n",
    "target_res = 0.0009\n",
    "output_crs = \"EPSG:4326\"  # WGS84\n",
    "\n",
    "with rasterio.open(tif_path) as src:\n",
    "    # Calculate transform and shape with coarser resolution\n",
    "    transform, width, height = calculate_default_transform(\n",
    "        src.crs,\n",
    "        output_crs,\n",
    "        src.width,\n",
    "        src.height,\n",
    "        *src.bounds,\n",
    "        resolution=target_res)\n",
    "\n",
    "    # Set up destination array and metadata\n",
    "    kwargs = src.meta.copy()\n",
    "    kwargs.update({\n",
    "        'crs': output_crs,\n",
    "        'transform': transform,\n",
    "        'width': width,\n",
    "        'height': height\n",
    "    })\n",
    "\n",
    "    # Prepare empty destination array\n",
    "    destination = np.empty((height, width), dtype=src.dtypes[0])\n",
    "\n",
    "    # Reproject with coarsening\n",
    "    reproject(\n",
    "        source=rasterio.band(src, 1),\n",
    "        destination=destination,\n",
    "        src_transform=src.transform,\n",
    "        src_crs=src.crs,\n",
    "        dst_transform=transform,\n",
    "        dst_crs=output_crs,\n",
    "        resampling=Resampling.\n",
    "        average  # average to reduce noise when downsampling\n",
    "    )\n",
    "\n",
    "    extent = [\n",
    "        transform[2], transform[2] + transform[0] * width,\n",
    "        transform[5] + transform[4] * height, transform[5]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1. Preprocessing ----\n",
    "# Square-root scaling of number of measurements\n",
    "glacier_info['sqrt_size'] = np.sqrt(glacier_info['Nb. measurements'])\n",
    "\n",
    "# Cache dataset-wide min and max\n",
    "sqrt_min = glacier_info['sqrt_size'].min()\n",
    "sqrt_max = glacier_info['sqrt_size'].max()\n",
    "\n",
    "# Define the desired marker size range in points^2\n",
    "sizes = (30, 1500)  # min and max scatter size\n",
    "\n",
    "\n",
    "# Function to scale individual values consistently\n",
    "def scaled_size(val, min_out=sizes[0], max_out=sizes[1]):\n",
    "    sqrt_val = np.sqrt(val)\n",
    "    if sqrt_max == sqrt_min:\n",
    "        return (min_out + max_out) / 2\n",
    "    return min_out + (max_out - min_out) * ((sqrt_val - sqrt_min) /\n",
    "                                            (sqrt_max - sqrt_min))\n",
    "\n",
    "\n",
    "# Apply scaling to full dataset for the actual plot\n",
    "glacier_info['scaled_size'] = glacier_info['Nb. measurements'].apply(\n",
    "    scaled_size)\n",
    "\n",
    "# ---- 2. Create figure and base map ----\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "\n",
    "#latN, latS = 48, 45.8\n",
    "latN, latS = 47.1, 45.8\n",
    "lonW, lonE = 5.8, 10.5\n",
    "projPC = ccrs.PlateCarree()\n",
    "ax2 = plt.axes(projection=projPC)\n",
    "ax2.set_extent([lonW, lonE, latS, latN], crs=ccrs.Geodetic())\n",
    "\n",
    "ax2.add_feature(cfeature.COASTLINE)\n",
    "ax2.add_feature(cfeature.LAKES)\n",
    "ax2.add_feature(cfeature.RIVERS)\n",
    "# ax2.add_feature(cfeature.BORDERS, linestyle='-', linewidth=1)\n",
    "\n",
    "# Add the image to the cartopy map\n",
    "\n",
    "masked_destination = np.ma.masked_where(destination == 0, destination)\n",
    "cmap = plt.cm.gray\n",
    "cmap.set_bad(color='white')  # Set masked (bad) values to white\n",
    "ax2.imshow(\n",
    "    masked_destination,\n",
    "    origin='upper',\n",
    "    extent=extent,\n",
    "    transform=ccrs.PlateCarree(),  # Assuming raster is in WGS84\n",
    "    cmap=cmap,  # or any other colormap\n",
    "    alpha=0.6,  # transparency\n",
    "    zorder=0)\n",
    "\n",
    "# Glacier outlines\n",
    "glacier_outline_sgi.plot(ax=ax2, transform=projPC, color='black')\n",
    "\n",
    "# ---- 3. Scatterplot ----\n",
    "custom_palette = {'Train': color_dark_blue, 'Test': '#b2182b'}\n",
    "\n",
    "g = sns.scatterplot(\n",
    "    data=glacier_info,\n",
    "    x='POINT_LON',\n",
    "    y='POINT_LAT',\n",
    "    size='scaled_size',\n",
    "    hue='Train/Test glacier',\n",
    "    sizes=sizes,\n",
    "    alpha=0.6,\n",
    "    palette=custom_palette,\n",
    "    transform=projPC,\n",
    "    ax=ax2,\n",
    "    zorder=10,\n",
    "    legend=True  # custom legend added below\n",
    ")\n",
    "\n",
    "# ---- 4. Gridlines ----\n",
    "gl = ax2.gridlines(draw_labels=True,\n",
    "                   linewidth=1,\n",
    "                   color='gray',\n",
    "                   alpha=0.5,\n",
    "                   linestyle='--')\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "gl.xlabel_style = {'size': 16, 'color': 'black'}\n",
    "gl.ylabel_style = {'size': 16, 'color': 'black'}\n",
    "gl.top_labels = gl.right_labels = False\n",
    "\n",
    "# ---- 5. Custom Combined Legend ----\n",
    "\n",
    "# Hue legend handles\n",
    "handles, labels = g.get_legend_handles_labels()\n",
    "expected_labels = list(custom_palette.keys())\n",
    "hue_entries = [(h, l) for h, l in zip(handles, labels) if l in expected_labels]\n",
    "\n",
    "# Size legend values and handles\n",
    "size_values = [30, 100, 1000, 6000]\n",
    "size_handles = [\n",
    "    Line2D(\n",
    "        [],\n",
    "        [],\n",
    "        marker='o',\n",
    "        linestyle='None',\n",
    "        markersize=np.sqrt(scaled_size(val)),  # matplotlib uses radius\n",
    "        markerfacecolor='gray',\n",
    "        alpha=0.6,\n",
    "        label=f'{val}') for val in size_values\n",
    "]\n",
    "\n",
    "# Separator label\n",
    "separator_handle = Patch(facecolor='none',\n",
    "                         edgecolor='none',\n",
    "                         label='Nb. measurements')\n",
    "\n",
    "# Combine all legend entries\n",
    "# combined_handles = [h for h, _ in hue_entries] + [separator_handle] + size_handles\n",
    "# combined_labels = [l for _, l in hue_entries] + ['Nb. measurements'] + [str(v) for v in size_values]\n",
    "\n",
    "# same but without separator\n",
    "combined_handles = [h for h, _ in hue_entries] + size_handles\n",
    "combined_labels = [l for _, l in hue_entries] + [str(v) for v in size_values]\n",
    "\n",
    "# Final legend\n",
    "ax2.legend(combined_handles,\n",
    "           combined_labels,\n",
    "           title='Nb. measurements',\n",
    "           loc='lower right',\n",
    "           frameon=True,\n",
    "           fontsize=18,\n",
    "           title_fontsize=18,\n",
    "           borderpad=1.2,\n",
    "           labelspacing=1.2,\n",
    "           ncol=3)\n",
    "# ax2.set_title('Glacier measurement locations', fontsize = 25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of measurements per year:\n",
    "data_glamos.groupby(['YEAR', 'PERIOD']).count()['POINT_ID'].unstack().plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    figsize=(20, 5),\n",
    "    color=[color_dark_blue, '#abd9e9'])\n",
    "# plt.title('Number of measurements per year for all glaciers', fontsize = 25)\n",
    "# get legend\n",
    "plt.legend(title='Period', fontsize=18, title_fontsize=20, ncol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meas_period = data_glamos.groupby(['YEAR', 'PERIOD']).count()['POINT_ID'].unstack()\n",
    "meas_period.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils.build_head_tail_pads_from_monthly_df(data_monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in TEST_GLACIERS if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in TEST_GLACIERS]\n",
    "\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(TEST_GLACIERS)]\n",
    "print('Size of test data:', len(data_test))\n",
    "\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of train data:', len(data_train))\n",
    "\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "# Number of annual versus winter measurements:\n",
    "print('Train:')\n",
    "print('Number of winter and annual samples:', len(data_train))\n",
    "print('Number of annual samples:',\n",
    "      len(data_train[data_train.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_train[data_train.PERIOD == 'winter']))\n",
    "\n",
    "# Same for test\n",
    "data_test_annual = data_test[data_test.PERIOD == 'annual']\n",
    "data_test_winter = data_test[data_test.PERIOD == 'winter']\n",
    "\n",
    "print('Test:')\n",
    "print('Number of winter and annual samples:', len(data_test))\n",
    "print('Number of annual samples:', len(data_test_annual))\n",
    "print('Number of winter samples:', len(data_test_winter))\n",
    "\n",
    "print('Total:')\n",
    "print('Number of monthly rows:', len(dataloader_gl.data))\n",
    "print('Number of annual rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'annual']))\n",
    "print('Number of winter rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'winter']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap annual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHeatmap(TEST_GLACIERS, data_glamos, glacierCap, period='annual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_annual = data_glamos[data_glamos.PERIOD == 'annual']\n",
    "num_years = data_annual.groupby(['GLACIER']).nunique().YEAR.sort_values()\n",
    "len(num_years[num_years > 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on a copy to avoid chained-assignment warnings\n",
    "data_annual = data_glamos.loc[data_glamos.PERIOD == 'annual'].copy()\n",
    "\n",
    "# Parse FROM/TO into proper datetimes\n",
    "data_annual['FROM_date'] = pd.to_datetime(data_annual['FROM_DATE'].astype(str),\n",
    "                                          format='%Y%m%d',\n",
    "                                          errors='coerce')\n",
    "\n",
    "# Work on a copy to avoid chained-assignment warnings\n",
    "data_winter = data_glamos.loc[data_glamos.PERIOD == 'winter'].copy()\n",
    "\n",
    "# Parse FROM/TO into proper datetimes\n",
    "data_winter['TO_date'] = pd.to_datetime(data_winter['TO_DATE'].astype(str),\n",
    "                                          format='%Y%m%d',\n",
    "                                          errors='coerce')\n",
    "\n",
    "# Helper: compute mean date and std (in days) for a date Series, using a dummy year (2000)\n",
    "def mean_date_and_std(date_series: pd.Series, circular: bool = False):\n",
    "    # Drop NaT\n",
    "    s = date_series.dropna()\n",
    "    if s.empty:\n",
    "        return pd.NaT, np.nan\n",
    "\n",
    "    # Map to a dummy, fixed year so we can compute day-of-year\n",
    "    dummy = pd.to_datetime({\n",
    "        'year': 2000,\n",
    "        'month': s.dt.month,\n",
    "        'day': s.dt.day\n",
    "    },\n",
    "                           errors='coerce').dropna()\n",
    "\n",
    "    doy = dummy.dt.dayofyear.astype(float)\n",
    "\n",
    "    if not circular:\n",
    "        mean_doy = doy.mean()\n",
    "        std_doy = doy.std()\n",
    "    else:\n",
    "        # Circular mean/std over the year (useful if dates wrap around New Year)\n",
    "        theta = 2 * np.pi * (doy - 1) / 365.0\n",
    "        C = np.mean(np.cos(theta))\n",
    "        S = np.mean(np.sin(theta))\n",
    "        mean_ang = np.arctan2(S, C)\n",
    "        if mean_ang < 0:\n",
    "            mean_ang += 2 * np.pi\n",
    "        mean_doy = (mean_ang / (2 * np.pi)) * 365.0 + 1\n",
    "        R = np.sqrt(C**2 + S**2)\n",
    "        # Convert circular std (radians) to days\n",
    "        std_ang = np.sqrt(-2 * np.log(max(R, 1e-12)))\n",
    "        std_doy = std_ang * 365.0 / (2 * np.pi)\n",
    "\n",
    "    mean_date = pd.Timestamp('2000-01-01') + pd.to_timedelta(mean_doy - 1,\n",
    "                                                             unit='D')\n",
    "    return mean_date, std_doy\n",
    "\n",
    "\n",
    "# Compute stats for FROM and TO\n",
    "mean_from_annual, std_from_annual = mean_date_and_std(data_annual['FROM_date'],\n",
    "                                                      circular=False)\n",
    "mean_from_winter, std_from_winter = mean_date_and_std(data_winter['TO_date'],\n",
    "                                                      circular=False)\n",
    "\n",
    "print(\n",
    "    f\"ANNUAL FROM_DATE -> mean: {mean_from_annual.strftime('%m-%d')} | std: {std_from_annual:.2f} days\"\n",
    ")\n",
    "print(\n",
    "    f\"WINTER TO_DATE   -> mean: {mean_from_winter.strftime('%m-%d')} | std: {std_from_winter:.2f} days\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get elevation of glaciers:\n",
    "gl_per_el = data_glamos[data_glamos.PERIOD == 'annual'].groupby(\n",
    "    ['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "\n",
    "# Plot elevation:\n",
    "fig = plt.figure(figsize=(10, 2))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "sns.lineplot(gl_per_el.sort_values(ascending=True),\n",
    "             ax=ax,\n",
    "             color='gray',\n",
    "             marker='v')\n",
    "ax.set_xticklabels('', rotation=90)\n",
    "ax.set_ylabel('')\n",
    "ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_annual.groupby('GLACIER').nunique().YEAR.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(gl_per_el)\n",
    "\n",
    "median = np.median(gl_per_el.values)\n",
    "upper_quartile = np.percentile(gl_per_el.values, 75)\n",
    "lower_quartile = np.percentile(gl_per_el.values, 25)\n",
    "\n",
    "iqr = upper_quartile - lower_quartile\n",
    "upper_whisker = gl_per_el.values[gl_per_el.values <= upper_quartile +\n",
    "                                 1.5 * iqr].max()\n",
    "lower_whisker = gl_per_el.values[gl_per_el.values >= lower_quartile -\n",
    "                                 1.5 * iqr].min()\n",
    "\n",
    "print(f\"Median elevation: {median:.2f} m\")\n",
    "print(f\"Upper quartile elevation: {upper_quartile:.2f} m\")\n",
    "print(f\"Lower quartile elevation: {lower_quartile:.2f} m\")\n",
    "print(f\"Upper whisker elevation: {upper_whisker:.2f} m\")\n",
    "print(f\"Lower whisker elevation: {lower_whisker:.2f} m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
