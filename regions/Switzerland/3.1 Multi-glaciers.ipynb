{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import massbalancemachine as mbm\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, LineString, Point\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import GroupKFold, KFold, train_test_split, GroupShuffleSplit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "from oggm import cfg, utils, workflow, tasks\n",
    "import logging\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "\n",
    "import config\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.xgb_helpers import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(config.SEED)\n",
    "\n",
    "# Specify the short names of the climate variables available in the dataset\n",
    "vois_climate = ['t2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str']\n",
    "vois_topographical = ['aspect', 'slope', 'dis_from_border']\n",
    "\n",
    "# in case no memory\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "cmap = cm.devon\n",
    "color_palette_glaciers = sns.color_palette(get_cmap_hex(cmap, 15))\n",
    "\n",
    "# For bars and lines:\n",
    "# color_diff_xgb = '#878787'\n",
    "color_diff_xgb = '#4d4d4d'\n",
    "\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_xgb = colors[0]\n",
    "color_xgb_winter = colors[1]\n",
    "\n",
    "color_tim = '#c51b7d'\n",
    "\n",
    "# Violin and boxplots:\n",
    "colors_temp_freq = sns.color_palette(get_cmap_hex(cm.devon, 8))\n",
    "boxplot_style = {\n",
    "    \"width\": .6,\n",
    "    \"showcaps\": False,\n",
    "    \"palette\": colors_temp_freq,\n",
    "    \"flierprops\": {\n",
    "        \"marker\": \"x\"\n",
    "    },\n",
    "    \"showmeans\": True,\n",
    "    \"meanprops\": {\n",
    "        \"markerfacecolor\": \"white\"\n",
    "    }\n",
    "}\n",
    "\n",
    "marker_tim = 's'\n",
    "marker_xgb = 'o'\n",
    "marker_std = '_'\n",
    "\n",
    "custom_working_dir = '../../../data/OGGM/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "path_rgi = '../../../data/GLAMOS/CH_glacier_ids_long.csv'\n",
    "rgi_df = pd.read_csv(path_rgi, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stakes data over all glaciers:\n",
    "data_glamos = pd.read_csv(path_PMB_GLAMOS_csv + 'CH_wgms_dataset.csv')\n",
    "data_glamos.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of mean mass balance per glacier:\n",
    "# Get the mean mass balance per glacier\n",
    "mean_mb_per_glacier = data_glamos.groupby(['GLACIER',\n",
    "                                           'YEAR'])['POINT_BALANCE'].mean()\n",
    "matrix = pd.DataFrame(mean_mb_per_glacier).reset_index().pivot(\n",
    "    index='GLACIER', columns='YEAR',\n",
    "    values='POINT_BALANCE').sort_values(by='GLACIER')\n",
    "\n",
    "# get elevation of glaciers:\n",
    "gl_per_el = data_glamos.groupby(['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "\n",
    "matrix = matrix.loc[gl_per_el.sort_values(ascending=True).index]\n",
    "\n",
    "# make index categorical\n",
    "matrix.index = pd.Categorical(matrix.index,\n",
    "                              categories=matrix.index,\n",
    "                              ordered=True)\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "sns.heatmap(data=matrix,\n",
    "            center=0,\n",
    "            cmap=cm.vik_r,\n",
    "            cbar_kws={'label': '[m w.e. $a^{-1}$]'},\n",
    "            ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot elevation:\n",
    "fig = plt.figure(figsize=(10, 2))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "sns.lineplot(gl_per_el.sort_values(ascending=True),\n",
    "             ax=ax,\n",
    "             color='gray',\n",
    "             marker='v')\n",
    "ax.set_xticklabels('', rotation=90)\n",
    "ax.set_ylabel('')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gl = data_glamos.groupby(['GLACIER']).size().sort_values()\n",
    "num_gl.plot(kind='bar', figsize=(15, 5), cmap=cmap)\n",
    "plt.title('Number of total measurements per glacier since 1961')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_glamos.GLACIER.unique()), data_glamos.GLACIER.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glaciers with data of potential clear sky radiation\n",
    "glDirect = os.listdir(path_direct)  \n",
    "glDirect.remove('clariden')\n",
    "glDirect.append('claridenU')\n",
    "glDirect.append('claridenL')\n",
    "glDirect.remove('stanna')\n",
    "glDirect.append('sanktanna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restgl = Diff(list(glDirect), list(data_glamos.GLACIER.unique()))\n",
    "restgl.sort()\n",
    "restgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gl[restgl][num_gl[restgl]>20].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADD_POT = True\n",
    "\n",
    "if ADD_POT:\n",
    "      # Filter data_glamos\n",
    "      data_glamos = data_glamos[data_glamos.GLACIER.isin(glDirect)]\n",
    "      print('Running on {} glaciers'.format(len(data_glamos.GLACIER.unique())))\n",
    "      print('Glaciers:', data_glamos.GLACIER.unique())\n",
    "      # Create dataloader:\n",
    "      dataset_gl = mbm.Dataset(data=data_glamos,\n",
    "                              region_name='CH',\n",
    "                              data_path=path_PMB_GLAMOS_csv)\n",
    "      print('Number of winter and annual samples:', len(data_glamos))\n",
    "      print('Number of annual samples:',\n",
    "            len(data_glamos[data_glamos.PERIOD == 'annual']))\n",
    "      print('Number of winter samples:',\n",
    "            len(data_glamos[data_glamos.PERIOD == 'winter']))\n",
    "\n",
    "      # Add climate data:\n",
    "      # Specify the files of the climate data, that will be matched with the coordinates of the stake data\n",
    "      era5_climate_data = path_ERA5_raw + 'era5_monthly_averaged_data.nc'\n",
    "      geopotential_data = path_ERA5_raw + 'era5_geopotential_pressure.nc'\n",
    "\n",
    "      # Match the climate features, from the ERA5Land netCDF file, for each of the stake measurement dataset\n",
    "      dataset_gl.get_climate_features(climate_data=era5_climate_data,\n",
    "                                    geopotential_data=geopotential_data,\n",
    "                                    change_units=True)\n",
    "\n",
    "      # Add potential clear sky radiation:\n",
    "      dataset_gl.get_potential_rad(path_direct_save)\n",
    "\n",
    "      # For each record, convert to a monthly time resolution\n",
    "      dataset_gl.convert_to_monthly(meta_data_columns=config.META_DATA,\n",
    "                                    vois_climate=vois_climate,\n",
    "                                    vois_topographical=vois_topographical+['pcsr'])\n",
    "else:\n",
    "      print('Running on {} glaciers'.format(len(data_glamos.GLACIER.unique())))\n",
    "      print('Glaciers:', data_glamos.GLACIER.unique())      \n",
    "      \n",
    "      # Create dataloader:\n",
    "      dataset_gl = mbm.Dataset(data=data_glamos,\n",
    "                              region_name='CH',\n",
    "                              data_path=path_PMB_GLAMOS_csv)\n",
    "      print('Number of winter and annual samples:', len(data_glamos))\n",
    "      print('Number of annual samples:',\n",
    "            len(data_glamos[data_glamos.PERIOD == 'annual']))\n",
    "      print('Number of winter samples:',\n",
    "            len(data_glamos[data_glamos.PERIOD == 'winter']))\n",
    "\n",
    "      # Add climate data:\n",
    "      # Specify the files of the climate data, that will be matched with the coordinates of the stake data\n",
    "      era5_climate_data = path_ERA5_raw + 'era5_monthly_averaged_data.nc'\n",
    "      geopotential_data = path_ERA5_raw + 'era5_geopotential_pressure.nc'\n",
    "\n",
    "      # Match the climate features, from the ERA5Land netCDF file, for each of the stake measurement dataset\n",
    "      dataset_gl.get_climate_features(climate_data=era5_climate_data,\n",
    "                                    geopotential_data=geopotential_data,\n",
    "                                    change_units=True)\n",
    "\n",
    "      # For each record, convert to a monthly time resolution\n",
    "      dataset_gl.convert_to_monthly(meta_data_columns=config.META_DATA,\n",
    "                                    vois_climate=vois_climate,\n",
    "                                    vois_topographical=vois_topographical)\n",
    "\n",
    "# Create a new DataLoader object with the monthly stake data measurements.\n",
    "dataloader_gl = mbm.DataLoader(data=dataset_gl.data,\n",
    "                               random_seed=config.SEED,\n",
    "                               meta_data_columns=config.META_DATA)\n",
    "\n",
    "print('Number of monthly rows:', len(dataloader_gl.data))\n",
    "print('Columns in the dataset:', dataloader_gl.data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate stakes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits, test_set, train_set = getCVSplits(dataloader_gl,\n",
    "                                          test_split_on='POINT_ID')\n",
    "\n",
    "print('Number of test stakes:', len(test_set['splits_vals']))\n",
    "print('Number of train stakes:', len(train_set['splits_vals']))\n",
    "\n",
    "test_glaciers = np.unique(\n",
    "    [re.split('_', i)[0] for i in test_set['splits_vals']])\n",
    "train_glaciers = np.unique(\n",
    "    [re.split('_', i)[0] for i in train_set['splits_vals']])\n",
    "\n",
    "print('Number of glaciers in test:', len(test_glaciers))\n",
    "print(test_glaciers)\n",
    "print('Number of glaciers in train:', len(train_glaciers))\n",
    "visualiseSplits(test_set['y'], train_set['y'], splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Grid search\n",
    "# For each of the XGBoost parameter, define the grid range\n",
    "parameters = {\n",
    "    'max_depth': [\n",
    "        3,\n",
    "        4,\n",
    "        5,\n",
    "        6,\n",
    "    ],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'gamma': [0, 1]\n",
    "}\n",
    "\n",
    "param_init = {}\n",
    "param_init['device'] = 'cuda:0'\n",
    "param_init['tree_method'] = 'hist'\n",
    "param_init[\"random_state\"] = config.SEED\n",
    "\n",
    "# Feature columns:\n",
    "feature_columns = ['ELEVATION_DIFFERENCE'] + list(vois_climate) + list(vois_topographical)+['pcsr']\n",
    "\n",
    "all_columns = feature_columns + config.META_DATA + config.NOT_METADATA_NOT_FEATURES\n",
    "df_X_train_subset = train_set['df_X'][all_columns]\n",
    "print('Shape of the dataset:', df_X_train_subset.shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "# Create a CustomXGBoostRegressor instance\n",
    "custom_xgboost = mbm.models.CustomXGBoostRegressor(**param_init)\n",
    "custom_xgboost.randomsearch(\n",
    "    parameters=parameters,\n",
    "    n_iter=20,\n",
    "    splits=splits,\n",
    "    features=df_X_train_subset,\n",
    "    targets=train_set['y'],\n",
    ")\n",
    "\n",
    "# save best model\n",
    "custom_xgboost.save_model(f'xgb_stake_split.pkl')\n",
    "\n",
    "# Get best parameters and estimator\n",
    "best_params = params = custom_xgboost.param_search.best_params_\n",
    "best_estimator = custom_xgboost.param_search.best_estimator_\n",
    "print(\"Best parameters:\\n\", best_params)\n",
    "print(\"Best score:\\n\", custom_xgboost.param_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to CPU for predictions:\n",
    "xgb = best_estimator.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on test\n",
    "features_test, metadata_test = xgb._create_features_metadata(\n",
    "    test_set['df_X'][all_columns], config.META_DATA)\n",
    "y_pred = xgb.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = xgb.aggrPredict(metadata_test, config.META_DATA, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = xgb.score(test_set['df_X'][all_columns], test_set['y'])  # negative\n",
    "mse, rmse, mae, pearson_corr = xgb.evalMetrics(metadata_test, y_pred,\n",
    "                                               test_set['y'])\n",
    "\n",
    "# Aggregate predictions to annual or winter:\n",
    "df_pred = test_set['df_X'][all_columns].copy()\n",
    "df_pred['target'] = test_set['y']\n",
    "grouped_ids = df_pred.groupby('ID').agg({\n",
    "    'target': 'mean',\n",
    "    'YEAR': 'first',\n",
    "    'POINT_ID': 'first'\n",
    "})\n",
    "grouped_ids['pred'] = y_pred_agg\n",
    "grouped_ids['PERIOD'] = test_set['df_X'][\n",
    "    feature_columns + config.META_DATA +\n",
    "    config.NOT_METADATA_NOT_FEATURES].groupby('ID')['PERIOD'].first()\n",
    "grouped_ids['GLACIER'] = grouped_ids['POINT_ID'].apply(\n",
    "    lambda x: x.split('_')[0])\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "predVSTruth(ax, grouped_ids, mae, rmse, pearson_corr)\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "grouped_ids.sort_values(by='YEAR', inplace=True)\n",
    "plotMeanPred(grouped_ids, ax)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title('Mean yearly target and prediction')\n",
    "plt.suptitle(f'XGBoost on split stakes', fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits, test_set, train_set = getCVSplits(dataloader_gl, test_split_on='YEAR')\n",
    "\n",
    "print('Number of test years:', len(test_set['splits_vals']))\n",
    "print('Number of train years:', len(train_set['splits_vals']))\n",
    "print('Train years:', train_set['splits_vals'])\n",
    "print('Test years:', test_set['splits_vals'])\n",
    "\n",
    "visualiseSplits(test_set['y'], train_set['y'], splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Grid search\n",
    "# For each of the XGBoost parameter, define the grid range\n",
    "parameters = {\n",
    "    'max_depth': [\n",
    "        3,\n",
    "        4,\n",
    "        5,\n",
    "        6,\n",
    "    ],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'gamma': [0, 1]\n",
    "}\n",
    "\n",
    "param_init = {}\n",
    "param_init['device'] = 'cuda:0'\n",
    "param_init['tree_method'] = 'hist'\n",
    "param_init[\"random_state\"] = config.SEED\n",
    "\n",
    "# Feature columns:\n",
    "feature_columns = ['ELEVATION_DIFFERENCE'] + list(vois_climate) + list(vois_topographical)+['pcsr']\n",
    "\n",
    "all_columns = feature_columns + config.META_DATA + config.NOT_METADATA_NOT_FEATURES\n",
    "df_X_train_subset = train_set['df_X'][all_columns]\n",
    "print('Shape of the dataset:', df_X_train_subset.shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "# Create a CustomXGBoostRegressor instance\n",
    "custom_xgboost = mbm.models.CustomXGBoostRegressor(**param_init)\n",
    "custom_xgboost.randomsearch(\n",
    "    parameters=parameters,\n",
    "    n_iter=20,\n",
    "    splits=splits,\n",
    "    features=df_X_train_subset,\n",
    "    targets=train_set['y'],\n",
    ")\n",
    "\n",
    "# save best model\n",
    "custom_xgboost.save_model(f'xgb_year_split.pkl')\n",
    "\n",
    "# Get best parameters and estimator\n",
    "best_params = params = custom_xgboost.param_search.best_params_\n",
    "best_estimator = custom_xgboost.param_search.best_estimator_\n",
    "print(\"Best parameters:\\n\", best_params)\n",
    "print(\"Best score:\\n\", custom_xgboost.param_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to CPU for predictions:\n",
    "xgb = best_estimator.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on test\n",
    "features_test, metadata_test = xgb._create_features_metadata(\n",
    "    test_set['df_X'][all_columns], config.META_DATA)\n",
    "y_pred = xgb.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = xgb.aggrPredict(metadata_test, config.META_DATA, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = xgb.score(test_set['df_X'][all_columns], test_set['y'])  # negative\n",
    "mse, rmse, mae, pearson_corr = xgb.evalMetrics(metadata_test, y_pred,\n",
    "                                               test_set['y'])\n",
    "\n",
    "# Aggregate predictions to annual or winter:\n",
    "df_pred = test_set['df_X'][all_columns].copy()\n",
    "df_pred['target'] = test_set['y']\n",
    "grouped_ids = df_pred.groupby('ID').agg({\n",
    "    'target': 'mean',\n",
    "    'YEAR': 'first',\n",
    "    'POINT_ID': 'first'\n",
    "})\n",
    "grouped_ids['pred'] = y_pred_agg\n",
    "grouped_ids['PERIOD'] = test_set['df_X'][\n",
    "    feature_columns + config.META_DATA +\n",
    "    config.NOT_METADATA_NOT_FEATURES].groupby('ID')['PERIOD'].first()\n",
    "grouped_ids['GLACIER'] = grouped_ids['POINT_ID'].apply(\n",
    "    lambda x: x.split('_')[0])\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "predVSTruth(ax, grouped_ids, mae, rmse, pearson_corr)\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "grouped_ids.sort_values(by='YEAR', inplace=True)\n",
    "plotMeanPred(grouped_ids, ax)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title('Mean yearly target and prediction')\n",
    "plt.suptitle(f'XGBoost on split stakes', fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split on glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_glaciers = ['pers']\n",
    "train_glaciers = [\n",
    "    i for i in data_glamos.GLACIER.unique() if i not in test_glaciers\n",
    "]\n",
    "\n",
    "splits, test_set, train_set = getCVSplits(dataloader_gl,\n",
    "                                          test_split_on='GLACIER',\n",
    "                                          test_splits=test_glaciers)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "\n",
    "visualiseSplits(test_set['y'], train_set['y'], splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 11, figsize=(16, 6), sharey='row', sharex='col')\n",
    "train_set['df_X']['POINT_BALANCE'].plot.hist(ax=ax[0, 0],\n",
    "                                             color=color_xgb,\n",
    "                                             alpha=0.6)\n",
    "ax[0, 0].set_title('PMB')\n",
    "ax[0, 0].set_ylabel('Frequency (train)')\n",
    "train_set['df_X']['POINT_ELEVATION'].plot.hist(ax=ax[0, 1],\n",
    "                                               color=color_xgb,\n",
    "                                               alpha=0.6)\n",
    "ax[0, 1].set_title('ELV')\n",
    "train_set['df_X']['YEAR'].plot.hist(ax=ax[0, 2], color=color_xgb, alpha=0.6)\n",
    "ax[0, 2].set_title('YEARS')\n",
    "train_set['df_X']['t2m'].plot.hist(ax=ax[0, 3], color=color_xgb, alpha=0.6)\n",
    "\n",
    "for i, voi_clim in enumerate(vois_climate+['pcsr']):\n",
    "    ax[0, 3 + i].set_title(voi_clim)\n",
    "    train_set['df_X'][voi_clim].plot.hist(ax=ax[0, 3 + i],\n",
    "                                          color=color_xgb,\n",
    "                                          alpha=0.6)\n",
    "\n",
    "test_set['df_X']['POINT_BALANCE'].plot.hist(ax=ax[1, 0],\n",
    "                                            color=color_tim,\n",
    "                                            alpha=0.6)\n",
    "ax[1, 0].set_ylabel('Frequency (test)')\n",
    "test_set['df_X']['POINT_ELEVATION'].plot.hist(bins=50,\n",
    "                                              ax=ax[1, 1],\n",
    "                                              color=color_tim,\n",
    "                                              alpha=0.6)\n",
    "test_set['df_X']['YEAR'].plot.hist(ax=ax[1, 2], color=color_tim, alpha=0.6)\n",
    "\n",
    "for i, voi_clim in enumerate(vois_climate+['pcsr']):\n",
    "    test_set['df_X'][voi_clim].plot.hist(ax=ax[1, 3 + i],\n",
    "                                         color=color_tim,\n",
    "                                         alpha=0.6)\n",
    "    # rotate xticks\n",
    "for ax in ax.flatten():\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.set_xlabel('')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only temp+ prec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Grid search\n",
    "# For each of the XGBoost parameter, define the grid range\n",
    "param_grid = {\n",
    "    'max_depth': [\n",
    "        3,\n",
    "        4,\n",
    "        5,\n",
    "        6,\n",
    "    ],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'gamma': [0, 1]\n",
    "}\n",
    "# param_grid = {\n",
    "#     'learning_rate': np.arange(0.01, 0.3, 0.01),\n",
    "#     'n_estimators': np.arange(50, 400, 15),\n",
    "#     'max_depth': np.arange(3, 10, 1),\n",
    "# }\n",
    "\n",
    "param_init = {}\n",
    "param_init['device'] = 'cuda:0'\n",
    "param_init['tree_method'] = 'hist'\n",
    "param_init[\"random_state\"] = config.SEED\n",
    "\n",
    "# Feature columns:\n",
    "# feature_columns = ['ELEVATION_DIFFERENCE'] + ['tp', 't2m'\n",
    "#                                               ] + list(vois_topographical)\n",
    "feature_columns = ['ELEVATION_DIFFERENCE'] + list(vois_climate) + list(vois_topographical)+['pcsr']\n",
    "all_columns = feature_columns + config.META_DATA + config.NOT_METADATA_NOT_FEATURES\n",
    "df_X_train_subset = train_set['df_X'][all_columns]\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of testing dataset:', test_set['df_X'][all_columns].shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "# Create a CustomXGBoostRegressor instance\n",
    "custom_xgboost = mbm.models.CustomXGBoostRegressor(**param_init)\n",
    "custom_xgboost.randomsearch(\n",
    "    parameters=param_grid,\n",
    "    n_iter=20,\n",
    "    splits=splits,\n",
    "    features=df_X_train_subset,\n",
    "    targets=train_set['y'],\n",
    ")\n",
    "# custom_xgboost.gridsearch(\n",
    "#     parameters=param_grid,\n",
    "#     splits=splits,\n",
    "#     features=df_X_train_subset,\n",
    "#     targets=train_set['y'],\n",
    "# )\n",
    "\n",
    "# save best model\n",
    "custom_xgboost.save_model(f'xgb_gl_split.pkl')\n",
    "\n",
    "# Get best parameters and estimator\n",
    "best_params = custom_xgboost.param_search.best_params_\n",
    "best_estimator = custom_xgboost.param_search.best_estimator_\n",
    "print(\"Best parameters:\\n\", best_params)\n",
    "print(\"Best score:\\n\", custom_xgboost.param_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotGridSearchScore(custom_xgboost)\n",
    "plotGridSearchParams(custom_xgboost, param_grid, custom_xgboost.param_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to CPU for predictions:\n",
    "xgb = best_estimator.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on test\n",
    "features_test, metadata_test = xgb._create_features_metadata(\n",
    "    test_set['df_X'][all_columns], config.META_DATA)\n",
    "y_pred = xgb.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = xgb.aggrPredict(metadata_test, config.META_DATA, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = xgb.score(test_set['df_X'][all_columns], test_set['y'])  # negative\n",
    "mse, rmse, mae, pearson_corr = xgb.evalMetrics(metadata_test, y_pred,\n",
    "                                               test_set['y'])\n",
    "\n",
    "# Aggregate predictions to annual or winter:\n",
    "df_pred = test_set['df_X'][all_columns].copy()\n",
    "df_pred['target'] = test_set['y']\n",
    "grouped_ids = df_pred.groupby('ID').agg({\n",
    "    'target': 'mean',\n",
    "    'YEAR': 'first',\n",
    "    'POINT_ID': 'first'\n",
    "})\n",
    "grouped_ids['pred'] = y_pred_agg\n",
    "grouped_ids['PERIOD'] = test_set['df_X'][\n",
    "    feature_columns + config.META_DATA +\n",
    "    config.NOT_METADATA_NOT_FEATURES].groupby('ID')['PERIOD'].first()\n",
    "grouped_ids['GLACIER'] = grouped_ids['POINT_ID'].apply(\n",
    "    lambda x: x.split('_')[0])\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "grouped_ids_annual = grouped_ids[grouped_ids.PERIOD == 'annual']\n",
    "predVSTruth(ax, grouped_ids_annual, mae, rmse, pearson_corr, hue='')\n",
    "ax.set_title('Annual MB', fontsize=24)\n",
    "\n",
    "grouped_ids_annual.sort_values(by='YEAR', inplace=True)\n",
    "ax = plt.subplot(2, 2, 2)\n",
    "plotMeanPred(grouped_ids_annual, ax)\n",
    "\n",
    "if 'winter' in grouped_ids.PERIOD.unique():\n",
    "    grouped_ids_winter = grouped_ids[grouped_ids.PERIOD == 'winter']\n",
    "    ax = plt.subplot(2, 2, 3)\n",
    "    predVSTruth(ax, grouped_ids_winter, mae, rmse, pearson_corr, hue='')\n",
    "    ax.set_title('Winter MB', fontsize=24)\n",
    "\n",
    "    ax = plt.subplot(2, 2, 4)\n",
    "    grouped_ids_winter.sort_values(by='YEAR', inplace=True)\n",
    "    plotMeanPred(grouped_ids_winter, ax)\n",
    "\n",
    "# ax.set_title('Mean yearly target and prediction')\n",
    "plt.suptitle(f'XGBoost tested on {test_glaciers}', fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIPlot(best_estimator, feature_columns, vois_climate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
