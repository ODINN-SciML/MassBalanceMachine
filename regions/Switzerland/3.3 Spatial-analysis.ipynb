{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import re\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import contextily as cx\n",
    "import matplotlib.colors as mcolors\n",
    "from shapely.geometry import box\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import massbalancemachine as mbm\n",
    "from cmcrameri import cm\n",
    "\n",
    "from scripts.geodata import *\n",
    "from scripts.geodata_plots import *\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.xgb_helpers import *\n",
    "#  Suppress warnings issued by Cartopy when downloading data files\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "cmap = cm.devon\n",
    "color_palette_glaciers = sns.color_palette(get_cmap_hex(cmap, 15))\n",
    "\n",
    "colors = get_cmap_hex(cm.batlow, 2)\n",
    "color_xgb = colors[0]\n",
    "color_tim = '#c51b7d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_name</th>\n",
       "      <th>sgi-id</th>\n",
       "      <th>rgi_id_v6_2016_shp</th>\n",
       "      <th>rgi_id.v6</th>\n",
       "      <th>rgi_id.v7</th>\n",
       "      <th>Issue</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adler</th>\n",
       "      <td>Adler</td>\n",
       "      <td>B56-14</td>\n",
       "      <td>RGI60-11.B56-14</td>\n",
       "      <td>RGI60-11.02764</td>\n",
       "      <td>RGI2000-v7.0-G-11-01075</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albigna</th>\n",
       "      <td>Albigna</td>\n",
       "      <td>C84-16</td>\n",
       "      <td>RGI60-11.C84-16</td>\n",
       "      <td>RGI60-11.02285</td>\n",
       "      <td>RGI2000-v7.0-G-11-02309</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           full_name  sgi-id rgi_id_v6_2016_shp       rgi_id.v6  \\\n",
       "short_name                                                        \n",
       "adler          Adler  B56-14    RGI60-11.B56-14  RGI60-11.02764   \n",
       "albigna      Albigna  C84-16    RGI60-11.C84-16  RGI60-11.02285   \n",
       "\n",
       "                          rgi_id.v7  Issue  \n",
       "short_name                                  \n",
       "adler       RGI2000-v7.0-G-11-01075  False  \n",
       "albigna     RGI2000-v7.0-G-11-02309   True  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "path_rgi = '../../../data/GLAMOS/CH_glacier_ids_long.csv'\n",
    "rgi_df = pd.read_csv(path_rgi, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "rgi_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blauschnee',\n",
       " 'diavolezza',\n",
       " 'schwarzbach',\n",
       " 'taelliboden',\n",
       " 'pizol',\n",
       " 'joeri',\n",
       " 'corvatsch_N',\n",
       " 'sanktanna',\n",
       " 'corvatsch',\n",
       " 'sexrouge',\n",
       " 'murtel',\n",
       " 'plattalva',\n",
       " 'chessjen',\n",
       " 'sardona',\n",
       " 'tortin',\n",
       " 'witenwasseren',\n",
       " 'cantun',\n",
       " 'vorab',\n",
       " 'basodino',\n",
       " 'limmern',\n",
       " 'adler',\n",
       " 'tiefen',\n",
       " 'hohlaub',\n",
       " 'albigna',\n",
       " 'tsanfleuron',\n",
       " 'silvretta',\n",
       " 'damma',\n",
       " 'oberaar',\n",
       " 'gries',\n",
       " 'claridenL',\n",
       " 'claridenU',\n",
       " 'clariden',\n",
       " 'gietro',\n",
       " 'schwarzberg',\n",
       " 'forno',\n",
       " 'plainemorte',\n",
       " 'allalin',\n",
       " 'otemma',\n",
       " 'findelen',\n",
       " 'rhone',\n",
       " 'morteratsch',\n",
       " 'corbassiere',\n",
       " 'unteraar',\n",
       " 'untgrindelwald',\n",
       " 'gorner',\n",
       " 'aletsch']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the shapefile\n",
    "shapefile_path = \"../../../data/GLAMOS/topo/SGI_2016_glaciers_copy.shp\"\n",
    "gdf_shapefiles = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Order glaciers by size:\n",
    "gl_area = {}\n",
    "for glacierName in rgi_df.index:\n",
    "    if glacierName == 'clariden':\n",
    "        sgi_id = rgi_df.loc['claridenL']['sgi-id'].strip()\n",
    "        rgi_shp = rgi_df.loc['claridenL']['rgi_id_v6_2016_shp']\n",
    "    else:\n",
    "        sgi_id = rgi_df.loc[glacierName]['sgi-id'].strip()\n",
    "        rgi_shp = rgi_df.loc[glacierName]['rgi_id_v6_2016_shp']\n",
    "\n",
    "    # 2016 shapefile of glacier\n",
    "    gdf_mask_gl = gdf_shapefiles[gdf_shapefiles.RGIId == rgi_shp]\n",
    "    gl_area[glacierName] = gdf_mask_gl.Area.values[0]\n",
    "\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "# sort by area (smallest first)\n",
    "glacier_list = sorted(gl_area, key=lambda x: gl_area[x], reverse=False)\n",
    "glacier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PREDICTIONS = 'results/nc/glamos_topo_simple/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glaciers covered by GLAMOS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read geodetic MB:\n",
    "geodeticMB = pd.read_csv(path_geodetic_MB_glamos + 'dV_DOI2024_allcomb.csv')\n",
    "\n",
    "# get rgi of those glaciers:\n",
    "rgi_gl = []\n",
    "for gl in os.listdir('results/nc/sgi_corr/'):\n",
    "    if gl == 'clariden':\n",
    "        rgi_gl.append(rgi_df.loc['claridenU']['rgi_id_v6_2016_shp'])\n",
    "    else:\n",
    "        rgi_gl.append(rgi_df.loc[gl]['rgi_id_v6_2016_shp'])\n",
    "\n",
    "sgi_gl = [\n",
    "    rgi_df[rgi_df['rgi_id_v6_2016_shp'] == rgi]['sgi-id'].values[0]\n",
    "    for rgi in rgi_gl\n",
    "]\n",
    "geodeticMB = geodeticMB[geodeticMB['SGI-ID'].isin(sgi_gl)]\n",
    "\n",
    "# Add glacierName to geodeticMB\n",
    "# based  on SGI-ID\n",
    "glacierNames = [\n",
    "    rgi_df[rgi_df['sgi-id'] == sgi_id].index[0]\n",
    "    for sgi_id in geodeticMB['SGI-ID'].values\n",
    "]\n",
    "# replace claridenL by clariden\n",
    "glacierNames = [\n",
    "    glacierName.replace('claridenL', 'clariden')\n",
    "    for glacierName in glacierNames\n",
    "]\n",
    "geodeticMB['glacierName'] = glacierNames\n",
    "\n",
    "periods_per_glacier = defaultdict(list)\n",
    "geoMB_per_glacier = defaultdict(list)\n",
    "# Iterate through the DataFrame rows\n",
    "for _, row in geodeticMB.iterrows():\n",
    "    glacierName = row['glacierName']\n",
    "    start_year = row['Astart']\n",
    "    end_year = row['A_end']\n",
    "    geoMB = row['Bgeod']\n",
    "\n",
    "    # Append the (start, end) tuple to the glacier's list\n",
    "    periods_per_glacier[glacierName].append((start_year, end_year))\n",
    "    geoMB_per_glacier[glacierName].append(geoMB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/nc/glamos_topo_simple/corvatsch/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/3.3 Spatial-analysis.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/3.3%20Spatial-analysis.ipynb#Y155sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m glaciers_in_distributed_glamos \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlistdir(path_distributed_MB_glamos)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/3.3%20Spatial-analysis.ipynb#Y155sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m glaciers_in_geodetic \u001b[39m=\u001b[39m periods_per_glacier\u001b[39m.\u001b[39mkeys()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/3.3%20Spatial-analysis.ipynb#Y155sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m glaciers_in_glamos_corr \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(PATH_PREDICTIONS\u001b[39m+\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mglacierName\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/3.3%20Spatial-analysis.ipynb#Y155sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Check if all glaciers in the first directory are in the second directory\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/3.3%20Spatial-analysis.ipynb#Y155sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m missing_glaciers \u001b[39m=\u001b[39m [\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/3.3%20Spatial-analysis.ipynb#Y155sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     glacier \u001b[39mfor\u001b[39;00m glacier \u001b[39min\u001b[39;00m glaciers_in_distributed_glamos\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/3.3%20Spatial-analysis.ipynb#Y155sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mif\u001b[39;00m glacier \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m glaciers_in_glamos_corr\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/3.3%20Spatial-analysis.ipynb#Y155sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m ]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/nc/glamos_topo_simple/corvatsch/'"
     ]
    }
   ],
   "source": [
    "# List the files in both directories\n",
    "glaciers_in_distributed_glamos = os.listdir(path_distributed_MB_glamos)\n",
    "glaciers_in_geodetic = periods_per_glacier.keys()\n",
    "glaciers_in_glamos_corr = os.listdir(PATH_PREDICTIONS+f\"{glacierName}/\")\n",
    "\n",
    "# Check if all glaciers in the first directory are in the second directory\n",
    "missing_glaciers = [\n",
    "    glacier for glacier in glaciers_in_distributed_glamos\n",
    "    if glacier not in glaciers_in_glamos_corr\n",
    "]\n",
    "missing_glaciers_geod = [\n",
    "    glacier for glacier in glaciers_in_geodetic\n",
    "    if glacier not in glaciers_in_glamos_corr\n",
    "]\n",
    "\n",
    "# Find the intersection of the two lists\n",
    "intersecting_glaciers = list(\n",
    "    set(missing_glaciers_geod) & set(missing_glaciers))\n",
    "\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=True)\n",
    "\n",
    "\n",
    "missing_glaciers_sorted_gridded = sort_by_area(missing_glaciers)\n",
    "missing_glaciers_geod_sorted = sort_by_area(missing_glaciers_geod)\n",
    "intersecting_glaciers_sorted = sort_by_area(intersecting_glaciers)\n",
    "\n",
    "print(\"The following glaciers are missing compared to gridded MB:\")\n",
    "print(missing_glaciers_sorted_gridded)\n",
    "\n",
    "print(\"The following glaciers are missing compared to the geodetic MB:\")\n",
    "print(missing_glaciers_geod_sorted)\n",
    "\n",
    "print(\"The following glaciers are missing in both lists:\")\n",
    "print(intersecting_glaciers_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersection with geodetic MB:\n",
    "glacier_list = sort_by_area([\n",
    "    glacier for glacier in glaciers_in_geodetic\n",
    "    if glacier in glaciers_in_glamos_corr\n",
    "])\n",
    "glacier_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stake coverage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vois_climate = [\n",
    "    't2m_corr', 'tp_corr', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]\n",
    "# Feature columns:\n",
    "feature_columns = [\n",
    "    'ELEVATION_DIFFERENCE'\n",
    "] + list(vois_climate) + list(vois_topographical) + ['pcsr']\n",
    "all_columns = feature_columns + config.META_DATA + config.NOT_METADATA_NOT_FEATURES\n",
    "\n",
    "year = 2018\n",
    "month = 'jul'\n",
    "\n",
    "emptyfolder('figures/stake_coverage/')\n",
    "\n",
    "for glacierName in glacier_list:\n",
    "    df_grid_monthly = pd.read_csv(path_glacier_grid_sgi +\n",
    "                                  f'{glacierName}_grid_{year}.csv')\n",
    "\n",
    "    # Correct climate grids:\n",
    "    df_grid_monthly = correct_vars_grid(df_grid_monthly)\n",
    "\n",
    "    # Rename aspect and slope to sgi\n",
    "    df_grid_monthly.rename(columns={\n",
    "        'aspect': 'aspect_sgi',\n",
    "        'slope': 'slope_sgi'\n",
    "    },\n",
    "                           inplace=True)\n",
    "    df_grid_monthly['POINT_ELEVATION'] = df_grid_monthly['topo']\n",
    "    df_grid_monthly.drop_duplicates(inplace=True)  # remove duplicates\n",
    "    df_grid_monthly = df_grid_monthly[all_columns]\n",
    "\n",
    "    df_grid_month = df_grid_monthly[df_grid_monthly.MONTHS == month]\n",
    "\n",
    "    # stakes\n",
    "    df_stakes = pd.read_csv(path_PMB_GLAMOS_csv + f'CH_wgms_dataset_all.csv')\n",
    "    df_stakes = df_stakes[(df_stakes['GLACIER'] == glacierName)]\n",
    "\n",
    "    # coordinates\n",
    "    geometry = [\n",
    "        Point(xy) for xy in zip(df_stakes.POINT_LON, df_stakes.POINT_LAT)\n",
    "    ]\n",
    "    gdf_stakes = gpd.GeoDataFrame(df_stakes,\n",
    "                                  crs='EPSG:4326',\n",
    "                                  geometry=geometry)\n",
    "\n",
    "    sgi_grid = xr.open_dataset(path_SGI_topo +\n",
    "                               f'xr_masked_grids/{glacierName}.nc')\n",
    "\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    ax = plt.subplot(111)\n",
    "    sgi_grid.glacier_mask.plot(cmap='binary', ax=ax, alpha=0.5)\n",
    "    sns.scatterplot(data=df_grid_month,\n",
    "                    x='POINT_LON',\n",
    "                    y='POINT_LAT',\n",
    "                    hue='ELEVATION_DIFFERENCE',\n",
    "                    ax=ax,\n",
    "                    s=5,\n",
    "                    palette='twilight_shifted')\n",
    "    # plot coordinates\n",
    "    gdf_stakes.plot(ax=ax, color='blue', markersize=3)\n",
    "    ax.legend().remove()\n",
    "    ax.set_title(f'{glacierName}')\n",
    "\n",
    "    # save figure\n",
    "    plt.savefig(f'figures/stake_coverage/{glacierName}.png', dpi=300)\n",
    "\n",
    "    # Clear the plot to free memory\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glacier wide MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "glacierName = 'gries'\n",
    "df_target = pd.read_csv(path_SMB_GLAMOS_csv + 'fix/' +\n",
    "                        f'{glacierName}_fix.csv')\n",
    "df_target = transformDates(df_target)\n",
    "\n",
    "# Remove obvious duplicates:\n",
    "df_target = df_target.drop_duplicates()\n",
    "df_target['YEAR'] = df_target['date1'].apply(lambda x: pd.to_datetime(x).year)\n",
    "df_target['GLAMOS Balance'] = df_target['Annual Balance'] / (1000)\n",
    "df_target = df_target[['YEAR', 'GLAMOS Balance']].set_index('YEAR')\n",
    "\n",
    "# Load the model data\n",
    "path_results = PATH_PREDICTIONS+f'{glacierName}/'\n",
    "years = [\n",
    "    int(re.split('_', f)[1])\n",
    "    for f in os.listdir(PATH_PREDICTIONS+f'{glacierName}/')\n",
    "]\n",
    "years.sort()\n",
    "pred_gl = []\n",
    "for year in years:\n",
    "    pred_gl.append(\n",
    "        xr.open_dataset(\n",
    "            path_results +\n",
    "            f'{glacierName}_{year}_annual.nc').pred_masked.mean().item())\n",
    "\n",
    "glwide_MB = pd.DataFrame(pred_gl, columns=['MBM Balance'])\n",
    "\n",
    "glwide_MB.index = years\n",
    "# add the target data\n",
    "glwide_MB = glwide_MB.join(df_target)\n",
    "\n",
    "# Plot the data\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "glwide_MB.plot(ax=ax, marker='o', color=[color_xgb, color_tim])\n",
    "plt.title(f'{glacierName.capitalize()} Glacier')\n",
    "plt.ylabel('Mass Balance [m w.e.]')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for all glaciers\n",
    "emptyfolder('figures/glacierwide/')\n",
    "for glacierName in glacier_list:\n",
    "    # if file exists:\n",
    "    if not os.path.exists(path_SMB_GLAMOS_csv + 'fix/' +\n",
    "                          f'{glacierName}_fix.csv'):\n",
    "        continue\n",
    "    df_target = pd.read_csv(path_SMB_GLAMOS_csv + 'fix/' +\n",
    "                            f'{glacierName}_fix.csv')\n",
    "    df_target = transformDates(df_target)\n",
    "\n",
    "    # Remove obvious duplicates:\n",
    "    df_target = df_target.drop_duplicates()\n",
    "    df_target['YEAR'] = df_target['date1'].apply(\n",
    "        lambda x: pd.to_datetime(x).year)\n",
    "    df_target['GLAMOS Balance'] = df_target['Annual Balance'] / (1000)\n",
    "    df_target = df_target[['YEAR', 'GLAMOS Balance']].set_index('YEAR')\n",
    "\n",
    "    # Load the model data\n",
    "    if not os.path.exists(PATH_PREDICTIONS+f'{glacierName}/'):\n",
    "        continue\n",
    "    path_results = PATH_PREDICTIONS+f'{glacierName}/'\n",
    "    pred_gl = []\n",
    "    years = [\n",
    "        int(re.split('_', f)[1])\n",
    "        for f in os.listdir(f'results/ncPATH_PREDICTIONS+f\"{glacierName}/\"{glacierName}/')\n",
    "    ]\n",
    "    years.sort()\n",
    "    #for year in range(2000, 2024):\n",
    "    for year in years:\n",
    "        pred_gl.append(\n",
    "            xr.open_dataset(\n",
    "                path_results +\n",
    "                f'{glacierName}_{year}_annual.nc').pred_masked.mean().item())\n",
    "\n",
    "    glwide_MB = pd.DataFrame(pred_gl, columns=['MBM Balance'])\n",
    "    #glwide_MB.index = range(2000, 2024)\n",
    "    glwide_MB.index = years\n",
    "    # add the target data\n",
    "    glwide_MB = glwide_MB.join(df_target)\n",
    "\n",
    "    # Plot the data\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    glwide_MB.plot(ax=ax, marker='o', color=[color_xgb, color_tim])\n",
    "    plt.title(f'{glacierName.capitalize()} Glacier')\n",
    "    plt.ylabel('Mass Balance [m w.e.]')\n",
    "    plt.grid()\n",
    "\n",
    "    # Save the figure\n",
    "    path_fig = 'figures/glacierwide/' + f'{glacierName}.png'\n",
    "    plt.savefig(path_fig, dpi=300)\n",
    "\n",
    "    # Clear the plot to free memory\n",
    "    #plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed GLAMOS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gaussian_filter(ds, variable_name='pred_masked', sigma: float = 1):\n",
    "    # Get the DataArray for the specified variable\n",
    "    data_array = ds[variable_name]\n",
    "\n",
    "    # Step 1: Create a mask of valid data (non-NaN values)\n",
    "    mask = ~np.isnan(data_array)\n",
    "\n",
    "    # Step 2: Replace NaNs with zero (or a suitable neutral value)\n",
    "    filled_data = data_array.fillna(0)\n",
    "\n",
    "    # Step 3: Apply Gaussian filter to the filled data\n",
    "    smoothed_data = gaussian_filter(filled_data, sigma=sigma)\n",
    "\n",
    "    # Step 4: Restore NaNs to their original locations\n",
    "    smoothed_data = xr.DataArray(smoothed_data,\n",
    "                                 dims=data_array.dims,\n",
    "                                 coords=data_array.coords,\n",
    "                                 attrs=data_array.attrs).where(\n",
    "                                     mask)  # Apply the mask to restore NaNs\n",
    "\n",
    "    ds[variable_name] = smoothed_data\n",
    "\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacierName = 'plainemorte'\n",
    "year = 2015\n",
    "\n",
    "fileName = f'{year}_ann_fix_lv95.grid'\n",
    "metadata, grid_data = load_grid_file(path_distributed_MB_glamos + glacierName +\n",
    "                                     '/' + fileName)\n",
    "# Convert to xarray\n",
    "ds = convert_to_xarray_geodata(grid_data, metadata)\n",
    "# Transform the coordinates to WGS84\n",
    "ds_wgs84 = transform_xarray_coords_lv95_to_wgs84(ds)\n",
    "\n",
    "ds_mbm = xr.open_dataset(\n",
    "    PATH_PREDICTIONS+f'{glacierName}/{glacierName}_{year}_annual.nc')\n",
    "ds_mbm = apply_gaussian_filter(ds_mbm)\n",
    "\n",
    "vmin = min(ds_wgs84.min().item(), ds_mbm.pred_masked.min().item())\n",
    "vmax = max(ds_wgs84.max().item(), ds_mbm.pred_masked.max().item())\n",
    "norm = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)\n",
    "\n",
    "print(vmin, vmax)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5), sharex=True, sharey=True)\n",
    "ds_wgs84.plot(ax=ax[0], cmap='coolwarm_r', norm=norm)\n",
    "ax[0].set_title('GLAMOS')\n",
    "\n",
    "# Plot MBM predictions:\n",
    "ds_mbm.pred_masked.plot(ax=ax[1], cmap='coolwarm_r', norm=norm)\n",
    "ax[1].set_title('MBM')\n",
    "plt.suptitle(f'{glacierName.capitalize()}: summer {year}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geodetic MB:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacierName = 'aletsch'\n",
    "\n",
    "GLAMOS_glwmb = pd.read_csv(path_SMB_GLAMOS_csv + 'fix/' +\n",
    "                           f'{glacierName}_fix.csv')\n",
    "GLAMOS_glwmb = transformDates(GLAMOS_glwmb)\n",
    "\n",
    "# Remove obvious duplicates:\n",
    "GLAMOS_glwmb = GLAMOS_glwmb.drop_duplicates()\n",
    "GLAMOS_glwmb['YEAR'] = GLAMOS_glwmb['date1'].apply(\n",
    "    lambda x: pd.to_datetime(x).year)\n",
    "GLAMOS_glwmb['GLAMOS Balance'] = GLAMOS_glwmb['Annual Balance'] / (1000)\n",
    "GLAMOS_glwmb = GLAMOS_glwmb[['YEAR', 'GLAMOS Balance']].set_index('YEAR')\n",
    "\n",
    "# get all periods\n",
    "periods = periods_per_glacier[glacierName]\n",
    "geoMBs = geoMB_per_glacier[glacierName]\n",
    "\n",
    "# Open glacier-wide MB nc files for all years of the period\n",
    "path = f'results/ncPATH_PREDICTIONS+f\"{glacierName}/\"{glacierName}/'\n",
    "\n",
    "mbm_mb_mean, glamos_mb_mean, geodetic_mb, target_period = [], [], [], []\n",
    "for period in periods:\n",
    "    mbm_mb, glamos_mb = [], []\n",
    "    for year in range(period[0], period[1] + 1):\n",
    "        # Get the glacier-wide MB\n",
    "        ds = xr.open_dataset(path + f'{glacierName}_{year}_annual.nc')\n",
    "        # Glacier wide MB is mean of all grid cells\n",
    "        mbm_mb.append(ds['pred_masked'].mean().values)\n",
    "\n",
    "        glamos_mb.append(GLAMOS_glwmb['GLAMOS Balance'].loc[year])\n",
    "\n",
    "    mbm_mb_mean.append(np.mean(mbm_mb))\n",
    "    glamos_mb_mean.append(np.mean(glamos_mb))\n",
    "    geodetic_mb.append(geoMBs[periods.index(period)])\n",
    "    target_period.append(period)\n",
    "\n",
    "# Plot the geodetic MB and the modelled MB\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.scatter(geodetic_mb, mbm_mb_mean, color=color_xgb, label='MBM MB')\n",
    "ax.scatter(geodetic_mb, glamos_mb_mean, color=color_tim, label='GLAMOS MB')\n",
    "ax.legend(loc='upper left')\n",
    "# add identity line\n",
    "# diagonal line\n",
    "pt = (0, 0)\n",
    "ax.axline(pt, slope=1, color=\"grey\", linestyle=\"-\", linewidth=0.2)\n",
    "ax.axvline(0, color=\"grey\", linestyle=\"--\", linewidth=1)\n",
    "ax.axhline(0, color=\"grey\", linestyle=\"--\", linewidth=1)\n",
    "ax.grid()\n",
    "# legend\n",
    "ax.set_xlabel('Geodetic MB [m w.e.]')\n",
    "ax.set_ylabel('Modelled MB [m w.e.]')\n",
    "# add title\n",
    "ax.set_title(f'{glacierName.capitalize()} Glacier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_years(folder_path, glacier_name, period):\n",
    "    start_year, end_year = period\n",
    "    expected_years = set(range(start_year, end_year + 1))\n",
    "\n",
    "    # Extract years from filenames\n",
    "    available_years = set()\n",
    "    pattern = re.compile(rf'{glacier_name}_(\\d{{4}})_annual\\.nc')\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            year = int(match.group(1))\n",
    "            available_years.add(year)\n",
    "\n",
    "    missing_years = expected_years - available_years\n",
    "\n",
    "    # if missing_years:\n",
    "    #     print(f\"The following years are missing in {folder_path} for glacier {glacier_name}:\")\n",
    "    #     print(sorted(missing_years))\n",
    "    # else:\n",
    "    #     print(f\"All years from {start_year} to {end_year} are present in {folder_path} for glacier {glacier_name}.\")\n",
    "    if missing_years:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for all glaciers\n",
    "path_figure = 'figures/geodetic/'\n",
    "emptyfolder(path_figure)\n",
    "\n",
    "for glacierName in glacier_list:\n",
    "    # if file exists:\n",
    "    if not os.path.exists(path_SMB_GLAMOS_csv + 'fix/' +\n",
    "                          f'{glacierName}_fix.csv'):\n",
    "        continue\n",
    "    GLAMOS_glwmb = pd.read_csv(path_SMB_GLAMOS_csv + 'fix/' +\n",
    "                               f'{glacierName}_fix.csv')\n",
    "    GLAMOS_glwmb = transformDates(GLAMOS_glwmb)\n",
    "\n",
    "    # Remove obvious duplicates:\n",
    "    GLAMOS_glwmb = GLAMOS_glwmb.drop_duplicates()\n",
    "    GLAMOS_glwmb['YEAR'] = GLAMOS_glwmb['date1'].apply(\n",
    "        lambda x: pd.to_datetime(x).year)\n",
    "    GLAMOS_glwmb['GLAMOS Balance'] = GLAMOS_glwmb['Annual Balance'] / (1000)\n",
    "    GLAMOS_glwmb = GLAMOS_glwmb[['YEAR', 'GLAMOS Balance']].set_index('YEAR')\n",
    "\n",
    "    # get all periods\n",
    "    periods = periods_per_glacier[glacierName]\n",
    "    geoMBs = geoMB_per_glacier[glacierName]\n",
    "\n",
    "    # Open glacier-wide MB nc files for all years of the period\n",
    "    folder_path = PATH_PREDICTIONS+f'{glacierName}/'\n",
    "\n",
    "    mbm_mb_mean, glamos_mb_mean, geodetic_mb, target_period = [], [], [], []\n",
    "    for period in periods:\n",
    "        if check_missing_years(folder_path, glacierName, period):\n",
    "            continue\n",
    "        mbm_mb, glamos_mb = [], []\n",
    "        for year in range(period[0], period[1] + 1):\n",
    "            # Get the glacier-wide MB\n",
    "            ds = xr.open_dataset(folder_path +\n",
    "                                 f'{glacierName}_{year}_annual.nc')\n",
    "            # Glacier wide MB is mean of all grid cells\n",
    "            mbm_mb.append(ds['pred_masked'].mean().values)\n",
    "            if year in GLAMOS_glwmb.index:\n",
    "                glamos_mb.append(GLAMOS_glwmb['GLAMOS Balance'].loc[year])\n",
    "\n",
    "        mbm_mb_mean.append(np.mean(mbm_mb))\n",
    "        glamos_mb_mean.append(np.mean(glamos_mb))\n",
    "        geodetic_mb.append(geoMBs[periods.index(period)])\n",
    "        target_period.append(period)\n",
    "\n",
    "    # only if more than one measurement\n",
    "    if len(geodetic_mb) > 1:\n",
    "        # Plot the geodetic MB and the modelled MB\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        ax.scatter(geodetic_mb, mbm_mb_mean, color=color_xgb, label='MBM MB')\n",
    "        ax.scatter(geodetic_mb,\n",
    "                   glamos_mb_mean,\n",
    "                   color=color_tim,\n",
    "                   label='GLAMOS MB')\n",
    "        ax.legend(loc='upper left')\n",
    "        # add identity line\n",
    "        # diagonal line\n",
    "        pt = (0, 0)\n",
    "        ax.axline(pt, slope=1, color=\"grey\", linestyle=\"-\", linewidth=0.2)\n",
    "        ax.axvline(0, color=\"grey\", linestyle=\"--\", linewidth=1)\n",
    "        ax.axhline(0, color=\"grey\", linestyle=\"--\", linewidth=1)\n",
    "        ax.grid()\n",
    "        # legend\n",
    "        ax.set_xlabel('Geodetic MB [m w.e.]')\n",
    "        ax.set_ylabel('Modelled MB [m w.e.]')\n",
    "        # add title\n",
    "        ax.set_title(f'{glacierName.capitalize()} Glacier')\n",
    "\n",
    "        # save the figure\n",
    "        fig.savefig(path_figure + f'{glacierName}.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_glaciers = [\n",
    "    'tortin', 'plattalva', 'sanktanna', 'schwarzberg', 'hohlaub', 'pizol',\n",
    "    'joeri', 'corvatsch', 'tsanfleuron'\n",
    "]\n",
    "\n",
    "mbm_mb_mean, glamos_mb_mean, geodetic_mb, gl, period_len, gl_type, area = [], [], [], [], [], [], []\n",
    "\n",
    "for glacierName in tqdm(os.listdir('results/ncPATH_PREDICTIONS+f\"{glacierName}/\"')):\n",
    "    # if file exists:\n",
    "    if not os.path.exists(path_SMB_GLAMOS_csv + 'fix/' +\n",
    "                          f'{glacierName}_fix.csv'):\n",
    "        continue\n",
    "    GLAMOS_glwmb = pd.read_csv(path_SMB_GLAMOS_csv + 'fix/' +\n",
    "                               f'{glacierName}_fix.csv')\n",
    "    GLAMOS_glwmb = transformDates(GLAMOS_glwmb)\n",
    "\n",
    "    # Remove obvious duplicates:\n",
    "    GLAMOS_glwmb = GLAMOS_glwmb.drop_duplicates()\n",
    "    GLAMOS_glwmb['YEAR'] = GLAMOS_glwmb['date1'].apply(\n",
    "        lambda x: pd.to_datetime(x).year)\n",
    "    GLAMOS_glwmb['GLAMOS Balance'] = GLAMOS_glwmb['Annual Balance'] / (1000)\n",
    "    GLAMOS_glwmb = GLAMOS_glwmb[['YEAR', 'GLAMOS Balance']].set_index('YEAR')\n",
    "\n",
    "    # get all periods\n",
    "    periods = periods_per_glacier[glacierName]\n",
    "    geoMBs = geoMB_per_glacier[glacierName]\n",
    "\n",
    "    # Open glacier-wide MB nc files for all years of the period\n",
    "    folder_path = PATH_PREDICTIONS+f'{glacierName}/'\n",
    "\n",
    "    for period in periods:\n",
    "        mbm_mb, glamos_mb = [], []\n",
    "\n",
    "        if check_missing_years(folder_path, glacierName, period):\n",
    "            continue\n",
    "\n",
    "        for year in range(period[0], period[1] + 1):\n",
    "            # Get the glacier-wide MB\n",
    "            ds = xr.open_dataset(folder_path +\n",
    "                                 f'{glacierName}_{year}_annual.nc')\n",
    "            # Glacier wide MB is mean of all grid cells\n",
    "            mbm_mb.append(ds['pred_masked'].mean().values)\n",
    "            if year in GLAMOS_glwmb.index:\n",
    "                glamos_mb.append(GLAMOS_glwmb['GLAMOS Balance'].loc[year])\n",
    "\n",
    "        mbm_mb_mean.append(np.mean(mbm_mb))\n",
    "        glamos_mb_mean.append(np.mean(glamos_mb))\n",
    "        geodetic_mb.append(geoMBs[periods.index(period)])\n",
    "        gl.append(glacierName)\n",
    "        gl_type.append(glacierName in test_glaciers)\n",
    "        period_len.append(period[1] - period[0])\n",
    "        area.append(gl_area[glacierName])\n",
    "\n",
    "df_all = pd.DataFrame({\n",
    "    'MBM MB': mbm_mb_mean,\n",
    "    'GLAMOS MB': glamos_mb_mean,\n",
    "    'Geodetic MB': geodetic_mb,\n",
    "    'GLACIER': gl,\n",
    "    'Period Length': period_len,\n",
    "    'Test glacier': gl_type,\n",
    "    'Area': area  # in km^2\n",
    "})\n",
    "df_all.sort_values(by='Area', inplace=True)\n",
    "df_all.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All glaciers\n",
    "scatter_geodeticMB(df_all)\n",
    "\n",
    "# Plot scatterplot but size is  proportional to glacier area\n",
    "scatter_geodeticMB(df_all, size=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only longest period per glacier\n",
    "df_longest = df_all.loc[df_all.groupby('GLACIER')['Period Length'].idxmax()]\n",
    "df_longest.sort_values(by='Area', inplace=True)\n",
    "scatter_geodeticMB(df_longest)\n",
    "plt.suptitle('Only longest period per glacier')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Only test glaciers\n",
    "df_test = df_all[df_all['Test glacier']]\n",
    "df_test.sort_values(by='Area', inplace=True)\n",
    "scatter_geodeticMB(df_test)\n",
    "plt.suptitle('Only test glaciers')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Only non-test glaciers\n",
    "df_non_test = df_all[~df_all['Test glacier']]\n",
    "df_non_test.sort_values(by='Area', inplace=True)\n",
    "scatter_geodeticMB(df_non_test)\n",
    "plt.suptitle('Only train glaciers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2 Sentinel data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get S2 rasters:\n",
    "Get the names of all satellite nc files and classifiy them per month and hydrological year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel-2\n",
    "path_S2 = '../../../data/Sentinel/'\n",
    "src_crs = 'EPSG:4326'  # Original CRS (lat/lon) wgs84\n",
    "\n",
    "# Organize the rasters by hydrological year\n",
    "satellite_years = [2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "\n",
    "rasters_S2 = organize_rasters_by_hydro_year(path_S2, satellite_years)\n",
    "\n",
    "# Print the organized rasters\n",
    "for hydro_year, months in rasters_S2.items():\n",
    "    print(f\"-----------------\\nHydrological Year: {hydro_year}\")\n",
    "    for month, files in months.items():\n",
    "        print(f\"  {month}: {files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create gdf for S2 rasters:\n",
    "Create panadas geodataframe for each satellite raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satellite_glaciers = [\n",
    "    'adler', 'aletsch', 'allalin', 'basodino', 'claridenL', 'claridenU',\n",
    "    'findelen', 'gries', 'hohlaub', 'limmern', 'oberaar', 'plattalva', 'rhone',\n",
    "    'sanktanna', 'schwarzbach', 'schwarzberg'\n",
    "]\n",
    "\n",
    "processed_gl = os.listdir('results/ncPATH_PREDICTIONS+f\"{glacierName}/\"')\n",
    "\n",
    "# list of gl in processed_gl that are also in satellite_glaciers\n",
    "satellite_glaciers = [gl for gl in processed_gl if gl in satellite_glaciers]\n",
    "satellite_glaciers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGI rasters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2 rasters over glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = False\n",
    "if RUN:\n",
    "    # emptyfolder(os.path.join(path_S2, 'perglacier/'))\n",
    "    # For each year and month where there is S2 data\n",
    "    for hydro_year, months in tqdm(rasters_S2.items(),\n",
    "                                   position=0,\n",
    "                                   desc='Hydrological Years'):\n",
    "        for month, files in tqdm(months.items(),\n",
    "                                 position=1,\n",
    "                                 leave=False,\n",
    "                                 desc='Months'):\n",
    "\n",
    "            # More than one if S2 A and B flew in the same month\n",
    "            for raster_name in files:\n",
    "                # Create raster for that year & month:\n",
    "                file_date = datetime.strptime(\n",
    "                    raster_name.split('_')[3][:8],\n",
    "                    \"%Y%m%d\")  # Extract the 8-digit date (YYYYMMDD)\n",
    "\n",
    "                # Path to the raster for that year & month:\n",
    "                raster_path = os.path.join(path_S2, file_date.strftime('%Y'),\n",
    "                                           raster_name)\n",
    "\n",
    "                # Creates a geodataframe from the .tif raster\n",
    "                # This is the longest part of the code\n",
    "                geo_data_S2 = mbm.GeoData(pd.DataFrame)\n",
    "                gdf_S2 = geo_data_S2.raster_to_gpd(raster_path)\n",
    "                geo_data_S2.set_gdf(gdf_S2)\n",
    "\n",
    "                # For each glacier, clip the S2 raster to the glacier extent\n",
    "                for glacierName in tqdm(satellite_glaciers,\n",
    "                                        position=2,\n",
    "                                        leave=False,\n",
    "                                        desc='Glaciers'):\n",
    "\n",
    "                    # Load MB predictions for that year and month\n",
    "                    path_nc_wgs84 = PATH_PREDICTIONS+f\"{glacierName}/\"\n",
    "                    filename_nc = f\"{glacierName}_{hydro_year}_{month_abbr_hydr_full[month]}.nc\"\n",
    "\n",
    "                    # check if file exists:\n",
    "                    if not os.path.exists(\n",
    "                            os.path.join(path_nc_wgs84, filename_nc)):\n",
    "                        continue\n",
    "\n",
    "                    # Open xarray dataset and set to class:\n",
    "                    geoData_gl = mbm.GeoData(pd.DataFrame)\n",
    "                    geoData_gl.set_ds_latlon(filename_nc, path_nc_wgs84)\n",
    "\n",
    "                    # Clip the S2 dataframe to the glacier extent\n",
    "                    # and resample it to the glacier resolution\n",
    "                    gdf_raster_res = mbm.GeoData.resample_satellite_to_glacier(\n",
    "                        geoData_gl.gdf, gdf_S2)\n",
    "\n",
    "                    # In case the glacier is outside of the bounds of the raster\n",
    "                    if gdf_raster_res is 0:\n",
    "                        continue\n",
    "                    # In case the raster is empty where the glacier is\n",
    "                    elif gdf_raster_res is 1:\n",
    "                        continue\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                    # Check the percentage of cloud cover\n",
    "                    cloud_cover_glacier = gdf_raster_res.classes[\n",
    "                        gdf_raster_res.classes ==\n",
    "                        5].count() / gdf_raster_res.classes.count()\n",
    "\n",
    "                    # If the cloud cover is too high, skip the glacier\n",
    "                    if cloud_cover_glacier > 0.5:\n",
    "                        continue\n",
    "\n",
    "                    # Save the glacier raster:\n",
    "                    S2_gl_name = '{}_{}.geojson'.format(\n",
    "                        glacierName, file_date.strftime('%Y_%m_%d'))\n",
    "                    S2_gl_path = os.path.join(path_S2, 'perglacier',\n",
    "                                              S2_gl_name)\n",
    "                    gdf_raster_res.to_file(S2_gl_path, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of one glacier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get glacier coordinates:\n",
    "glacierName = 'rhone'\n",
    "year = 2017\n",
    "month = '08'\n",
    "# monthNb = month_abbr_hydr_full[month]\n",
    "\n",
    "rasters_gl = [\n",
    "    f for f in os.listdir(os.path.join(path_S2, 'perglacier'))\n",
    "    if (glacierName in f) and (str(year) in f)\n",
    "]\n",
    "rasters_gl.sort()\n",
    "rasters_gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the glacier shapefile\n",
    "# filename_nc = f\"{glacierName}_{year}_{monthNb}.nc\"\n",
    "filename_nc = f\"{glacierName}_{year}_annual.nc\"\n",
    "\n",
    "# Open xarray dataset and set to class:\n",
    "path_nc_wgs84 = f\"results/ncPATH_PREDICTIONS+f\"{glacierName}/\"{glacierName}/\"\n",
    "geoData_gl = mbm.GeoData(pd.DataFrame)\n",
    "geoData_gl.set_ds_latlon(filename_nc, path_nc_wgs84)\n",
    "geoData_gl.classify_snow_cover(tol=0.1)\n",
    "\n",
    "# Read satellite raster\n",
    "filenameS2 = 'rhone_2017_08_20.geojson'\n",
    "gdf_S2_res = gpd.read_file(os.path.join(path_S2, 'perglacier', filenameS2))\n",
    "\n",
    "match = re.search(r'(\\d{4}_\\d{2}_\\d{2})', filenameS2)\n",
    "date_str = match.group(1)\n",
    "file_date = datetime.strptime(date_str, \"%Y_%m_%d\")\n",
    "gl_date = str(year) + '-' + month\n",
    "\n",
    "# Create the figure and subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "plotClasses(geoData_gl.gdf,\n",
    "            gdf_S2_res,\n",
    "            axs,\n",
    "            gl_date,\n",
    "            file_date,\n",
    "            band_size=50,\n",
    "            percentage_threshold=50)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vois_climate = [\n",
    "    't2m_corr', 'tp_corr', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]\n",
    "# Feature columns:\n",
    "feature_columns = [\n",
    "    'ELEVATION_DIFFERENCE'\n",
    "] + list(vois_climate) + list(vois_topographical) + ['pcsr']\n",
    "all_columns = feature_columns + config.META_DATA + config.NOT_METADATA_NOT_FEATURES\n",
    "\n",
    "df_grid_monthly = pd.read_csv(path_glacier_grid_sgi +\n",
    "                              f'{glacierName}_grid_{year}.csv')\n",
    "\n",
    "# Correct climate grids:\n",
    "# Take the biggest grid cell value for each month\n",
    "for voi in [\n",
    "        't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10',\n",
    "        'ALTITUDE_CLIMATE'\n",
    "]:\n",
    "    df_grid_monthly = correct_for_biggest_grid(\n",
    "        df_grid_monthly, group_columns=[\"YEAR\", \"MONTHS\"], value_column=voi)\n",
    "\n",
    "# New elevation difference:\n",
    "df_grid_monthly['ELEVATION_DIFFERENCE'] = df_grid_monthly[\n",
    "    \"POINT_ELEVATION\"] - df_grid_monthly[\"ALTITUDE_CLIMATE\"]\n",
    "\n",
    "# apply T & P correction\n",
    "# Apply temperature gradient correction\n",
    "temp_grad = -6.5 / 1000\n",
    "dpdz = 1.5 / 10000\n",
    "c_prec = 1.434,\n",
    "t_off = 0.617\n",
    "\n",
    "# Apply temperature correction factor\n",
    "df_grid_monthly['t2m_corr'] = df_grid_monthly['t2m'] + (\n",
    "    df_grid_monthly['ELEVATION_DIFFERENCE'] * temp_grad)\n",
    "df_grid_monthly['t2m_corr'] += t_off\n",
    "\n",
    "# Apply elevation correction factor\n",
    "df_grid_monthly['tp_corr'] = df_grid_monthly['tp'] * c_prec\n",
    "df_grid_monthly['tp_corr'] += df_grid_monthly['tp_corr'] * (\n",
    "    df_grid_monthly['ELEVATION_DIFFERENCE'] * dpdz)\n",
    "\n",
    "# Rename aspect and slope to sgi\n",
    "df_grid_monthly.rename(columns={\n",
    "    'aspect': 'aspect_sgi',\n",
    "    'slope': 'slope_sgi'\n",
    "},\n",
    "                       inplace=True)\n",
    "df_grid_monthly['POINT_ELEVATION'] = df_grid_monthly['topo']\n",
    "df_grid_monthly.drop_duplicates(inplace=True)  # remove duplicates\n",
    "df_grid_monthly = df_grid_monthly[all_columns]\n",
    "\n",
    "df_grid_month = df_grid_monthly[df_grid_monthly.MONTHS == month]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot input features\n",
    "# Plot climate features\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15, 15))\n",
    "for i, voi in enumerate(vois_climate):\n",
    "    sns.scatterplot(data=df_grid_month,\n",
    "                    x='POINT_LON',\n",
    "                    y='POINT_LAT',\n",
    "                    hue=voi,\n",
    "                    ax=axs[i // 3, i % 3],\n",
    "                    s=2)\n",
    "    axs[i // 3, i % 3].set_title(voi)\n",
    "    axs[i // 3, i % 3].set_ylabel('Lat')\n",
    "    axs[i // 3, i % 3].set_xlabel('Lon')\n",
    "    axs[i // 3, i % 3].legend().remove()\n",
    "    axs[i // 3, i % 3].grid(False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot input features\n",
    "# Plot topographical features\n",
    "fig, axs = plt.subplots(2, 4, figsize=(15, 10))\n",
    "for i, voi in enumerate(['pcsr'] + list(vois_topographical) +\n",
    "                        ['ELEVATION_DIFFERENCE']):\n",
    "    sns.scatterplot(data=df_grid_month,\n",
    "                    x='POINT_LON',\n",
    "                    y='POINT_LAT',\n",
    "                    hue=voi,\n",
    "                    ax=axs[i // 4, i % 4],\n",
    "                    s=2,\n",
    "                    palette='twilight_shifted')\n",
    "    axs[i // 4, i % 4].set_title(voi)\n",
    "    axs[i // 4, i % 4].set_ylabel('Lat')\n",
    "    axs[i // 4, i % 4].set_xlabel('Lon')\n",
    "    axs[i // 4, i % 4].legend().remove()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old RGI rasters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN = False\n",
    "# if RUN:\n",
    "#     emptyfolder(os.path.join(path_S2, 'perglacier/'))\n",
    "#     # For each year and month where there is S2 data\n",
    "#     for hydro_year, months in tqdm(rasters_S2.items(),\n",
    "#                                    position=0,\n",
    "#                                    desc='Hydrological Years'):\n",
    "#         for month, files in tqdm(months.items(),\n",
    "#                                  position=1,\n",
    "#                                  leave=False,\n",
    "#                                  desc='Months'):\n",
    "\n",
    "#             # More than one if S2 A and B flew in the same month\n",
    "#             for raster_name in files:\n",
    "\n",
    "#                 # Create raster for that year & month:\n",
    "#                 file_date = datetime.strptime(\n",
    "#                     raster_name.split('_')[3][:8],\n",
    "#                     \"%Y%m%d\")  # Extract the 8-digit date (YYYYMMDD)\n",
    "\n",
    "#                 # Path to the raster for that year & month:\n",
    "#                 raster_path = os.path.join(path_S2, file_date.strftime('%Y'),\n",
    "#                                            raster_name)\n",
    "\n",
    "#                 # Creates a geodataframe from the .tif raster\n",
    "#                 # This is the longest part of the code\n",
    "#                 geoData_raster = mbm.GeoData(pd.DataFrame)\n",
    "#                 gdf_S2 = geoData_raster.raster_to_gpd(raster_path)\n",
    "#                 geoData_raster.set_gdf(gdf_S2)\n",
    "\n",
    "#                 # For each glacier, clip the S2 raster to the glacier extent\n",
    "#                 for glacierName in tqdm(satellite_glaciers,\n",
    "#                                         position=2,\n",
    "#                                         leave=False,\n",
    "#                                         desc='Glaciers'):\n",
    "\n",
    "#                     # Load MB predictions for that year and month\n",
    "#                     path_nc_wgs84 = f\"results/nc/var_normal/{glacierName}/wgs84/\"\n",
    "#                     filename_nc = f\"{glacierName}_{hydro_year}_{month_abbr_hydr_full_hydr[month]}.nc\"\n",
    "\n",
    "#                     # Open xarray dataset and set to class:\n",
    "#                     geoData_gl = mbm.GeoData(pd.DataFrame)\n",
    "#                     geoData_gl.set_ds_latlon(filename_nc, path_nc_wgs84)\n",
    "\n",
    "#                     # Convert xr in lat lon to GeoPandas\n",
    "#                     geoData_gl.xr_to_gpd()\n",
    "\n",
    "#                     # Clip the S2 dataframe to the glacier extent\n",
    "#                     # and resample it to the glacier resolution\n",
    "#                     gdf_raster_res = mbm.GeoData.resample_satellite_to_glacier(\n",
    "#                         geoData_gl.gdf, gdf_S2)\n",
    "\n",
    "#                     # In case the glacier is outside of the bounds of the raster\n",
    "#                     if gdf_raster_res is 0:\n",
    "#                         continue\n",
    "#                     # In case the raster is empty where the glacier is\n",
    "#                     elif gdf_raster_res is 1:\n",
    "#                         continue\n",
    "#                     else:\n",
    "#                         pass\n",
    "\n",
    "#                     # Check the percentage of cloud cover\n",
    "#                     cloud_cover_glacier = gdf_raster_res.classes[\n",
    "#                         gdf_raster_res.classes ==\n",
    "#                         5].count() / gdf_raster_res.classes.count()\n",
    "\n",
    "#                     # If the cloud cover is too high, skip the glacier\n",
    "#                     if cloud_cover_glacier > 0.5:\n",
    "#                         continue\n",
    "\n",
    "#                     # Save the glacier raster:\n",
    "#                     S2_gl_name = '{}_{}.geojson'.format(\n",
    "#                         glacierName, file_date.strftime('%Y_%m_%d'))\n",
    "#                     S2_gl_path = os.path.join(path_S2, 'perglacier',\n",
    "#                                               S2_gl_name)\n",
    "#                     gdf_raster_res.to_file(S2_gl_path, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snow cover:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = True\n",
    "if RUN:\n",
    "\n",
    "    # first delete results file\n",
    "    if os.path.exists(f'results/snow_cover.csv'):\n",
    "        os.remove(f'results/snow_cover.csv')\n",
    "\n",
    "    # Add header\n",
    "    with open(f'results/snow_cover.csv', 'w') as f:\n",
    "        f.write(\n",
    "            \"year,month,monthNb,raster_date,glacier_name,snow_cover_S2,snow_cover_glacier\\n\"\n",
    "        )\n",
    "\n",
    "    # All rasters resampled over glaciers:\n",
    "    rasters_resampled = os.listdir(os.path.join(path_S2, 'perglacier'))\n",
    "\n",
    "    for raster_res in tqdm(rasters_resampled):\n",
    "        # Extract glacier name\n",
    "        glacierName = raster_res.split('_')[0]\n",
    "\n",
    "        # Extract date from satellite raster\n",
    "        match = re.search(r\"(\\d{4})_(\\d{2})_(\\d{2})\", raster_res)\n",
    "        year, month, day = match.groups()\n",
    "        date_str = match.group(1) + '-' + match.group(2) + '-' + match.group(3)\n",
    "        raster_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "        # Find closest hydrological year and month\n",
    "        closest_month, hydro_year = get_hydro_year_and_month(raster_date)\n",
    "        monthNb = month_abbr_hydr_full[closest_month]\n",
    "\n",
    "        if hydro_year > 2021:\n",
    "            continue\n",
    "\n",
    "        # Read satellite raster over glacier (previously resampled)\n",
    "        gdf_S2_res = gpd.read_file(\n",
    "            os.path.join(path_S2, 'perglacier', raster_res))\n",
    "\n",
    "        # Calculate percentage of snow cover (class 1)\n",
    "        snow_cover_S2 = IceSnowCover(gdf_S2_res, gdf_S2_res)\n",
    "\n",
    "        # Load MB predictions for that year and month\n",
    "        path_nc_wgs84 = PATH_PREDICTIONS+f\"{glacierName}/\"\n",
    "        filename_nc = f\"{glacierName}_{hydro_year}_{monthNb}.nc\"\n",
    "\n",
    "        geoData_gl = mbm.GeoData(pd.DataFrame)\n",
    "        geoData_gl.set_ds_latlon(filename_nc, path_nc_wgs84)\n",
    "        geoData_gl.classify_snow_cover(tol=0.1)\n",
    "\n",
    "        snow_cover_glacier = IceSnowCover(geoData_gl.gdf, gdf_S2_res)\n",
    "\n",
    "        # Save the results\n",
    "        with open(f'results/snow_cover.csv', 'a') as f:\n",
    "            f.write(\n",
    "                f\"{hydro_year},{closest_month},{monthNb},{date_str},{glacierName},{snow_cover_S2},{snow_cover_glacier}\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'results/snow_cover.csv').sort_values(by=['year', 'month'],\n",
    "                                                        ascending=True)\n",
    "# remove october and september\n",
    "df = df[~df.month.isin(['oct', 'sep'])]\n",
    "fig, axs = plot_snow_cover_scatter(df, add_corr=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gl = df[(df.glacier_name == 'aletsch') & ~df.month.isin(['oct', 'sep'])]\n",
    "fig, axs = plot_snow_cover_scatter_combined(df_gl)\n",
    "\n",
    "df_gl = df[(df.glacier_name == 'gries') & ~df.month.isin(['oct', 'sep'])]\n",
    "fig, axs = plot_snow_cover_scatter_combined(df_gl)\n",
    "\n",
    "df_gl = df[(df.glacier_name == 'rhone') & ~df.month.isin(['oct', 'sep'])]\n",
    "fig, axs = plot_snow_cover_scatter_combined(df_gl)\n",
    "\n",
    "df_gl = df[(df.glacier_name == 'adler') & ~df.month.isin(['oct', 'sep'])]\n",
    "fig, axs = plot_snow_cover_scatter_combined(df_gl)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the raster files and plot geoplots\n",
    "gl = 'aletsch'\n",
    "yr = 2021\n",
    "rasters_gl = [\n",
    "    f for f in os.listdir(os.path.join(path_S2, 'perglacier'))\n",
    "    if (gl in f) and (str(yr) in f)\n",
    "]\n",
    "rasters_gl.sort()\n",
    "for raster_res in rasters_gl:\n",
    "    plot_snow_cover_geoplots(raster_res, path_S2, month_abbr_hydr_full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity of tolerance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = False\n",
    "\n",
    "if RUN:\n",
    "    # first delete results file\n",
    "    if os.path.exists(f'results/snow_cover_tol.csv'):\n",
    "        os.remove(f'results/snow_cover_tol.csv')\n",
    "\n",
    "    # Add header\n",
    "    with open(f'results/snow_cover_tol.csv', 'w') as f:\n",
    "        f.write(\n",
    "            \"tol,year,month,monthNb,raster_date,glacier_name,snow_cover_S2,snow_cover_glacier,snow_cover_glacier_corr\\n\"\n",
    "        )\n",
    "\n",
    "    # All rasters resampled over glaciers:\n",
    "    rasters_resampled = [\n",
    "        f for f in os.listdir(os.path.join(path_S2, 'perglacier'))\n",
    "    ]\n",
    "\n",
    "    for tol in tqdm(np.arange(0, 1, 0.1), position=0, desc='Tolerance'):\n",
    "        for raster_res in tqdm(rasters_resampled):\n",
    "            # Extract glacier name\n",
    "            glacierName = raster_res.split('_')[0]\n",
    "\n",
    "            # Extract date from satellite raster\n",
    "            match = re.search(r\"(\\d{4})_(\\d{2})_(\\d{2})\", raster_res)\n",
    "            year, month, day = match.groups()\n",
    "            date_str = match.group(1) + '-' + match.group(\n",
    "                2) + '-' + match.group(3)\n",
    "            raster_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "            # Find closest hydrological year and month\n",
    "            closest_month, hydro_year = get_hydro_year_and_month(raster_date)\n",
    "            monthNb = month_abbr_hydr_full_hydr[closest_month]\n",
    "\n",
    "            if hydro_year > 2021:\n",
    "                continue\n",
    "\n",
    "            # Read satellite raster over glacier (previously resampled)\n",
    "            gdf_S2_res = gpd.read_file(\n",
    "                os.path.join(path_S2, 'perglacier', raster_res))\n",
    "\n",
    "            # Calculate percentage of snow cover (class 1)\n",
    "            snow_cover_S2 = IceSnowCover(gdf_S2_res, gdf_S2_res)\n",
    "\n",
    "            # Load MB predictions for that year and month\n",
    "            path_nc_wgs84 = f\"results/nc/var_normal/{glacierName}/wgs84/\"\n",
    "            path_nc_wgs84_corr = f\"results/nc/var_corr/{glacierName}/wgs84/\"\n",
    "            filename_nc = f\"{glacierName}_{hydro_year}_{monthNb}.nc\"\n",
    "\n",
    "            gdf_glacier = ClassSnowCover(path_nc_wgs84, filename_nc, tol)\n",
    "            snow_cover_glacier = IceSnowCover(gdf_glacier, gdf_S2_res)\n",
    "\n",
    "            # Corrected T and P\n",
    "            gdf_glacier_corr = ClassSnowCover(path_nc_wgs84_corr, filename_nc,\n",
    "                                              tol)\n",
    "            snow_cover_glacier_corr = IceSnowCover(gdf_glacier_corr,\n",
    "                                                   gdf_S2_res)\n",
    "\n",
    "            # Save the results\n",
    "            with open(f'results/snow_cover_tol.csv', 'a') as f:\n",
    "                f.write(\n",
    "                    f\"{tol},{hydro_year},{closest_month},{monthNb},{date_str},{glacierName},{snow_cover_S2},{snow_cover_glacier},{snow_cover_glacier_corr}\\n\"\n",
    "                )\n",
    "\n",
    "df = pd.read_csv(f'results/snow_cover_tol.csv').sort_values(\n",
    "    by=['year', 'month'], ascending=True)\n",
    "df_gl = df[~df.month.isin(['oct', 'sep'])]\n",
    "\n",
    "for tol in df_gl.tol.unique():\n",
    "    df_gl_tol = df_gl[df_gl.tol == tol]\n",
    "    # fig, axs = plot_snow_cover_scatter_combined(df_gl_tol)\n",
    "    fig, axs = plot_snow_cover_scatter(df_gl_tol, add_corr=False)\n",
    "    plt.suptitle(f'Tolerance: {np.round(tol, 4)}', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the results for each tolerance\n",
    "tol_results = {}\n",
    "month_labels = ['April', 'May', 'June', 'July', 'August']\n",
    "# Iterate over each tolerance value\n",
    "for tol in df_gl.tol.unique():\n",
    "    df_gl_tol = df_gl[df_gl.tol == tol]\n",
    "\n",
    "    # Get sorted unique months\n",
    "    months = np.sort(df_gl_tol['monthNb'].unique())\n",
    "\n",
    "    # Lists to store metrics for each month\n",
    "    r2 = []\n",
    "    rmse = []\n",
    "\n",
    "    # Loop over each month\n",
    "    for monthNb in months:\n",
    "        df_month = df_gl_tol[df_gl_tol['monthNb'] == monthNb]\n",
    "\n",
    "        # Calculate R^2\n",
    "        r2.append(\n",
    "            np.corrcoef(df_month['snow_cover_S2'],\n",
    "                        df_month['snow_cover_glacier'])[0, 1]**2)\n",
    "\n",
    "        # Calculate MSE\n",
    "        rmse.append(\n",
    "            mean_squared_error(df_month['snow_cover_glacier'],\n",
    "                               df_month['snow_cover_S2'],\n",
    "                               squared=False))\n",
    "\n",
    "    # Store the results in the dictionary\n",
    "    tol_results[np.round(tol, 4)] = {\n",
    "        'months': month_labels,\n",
    "        'r2': r2,\n",
    "        'rmse': rmse\n",
    "    }\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6), sharex=True)\n",
    "\n",
    "# Create a colormap\n",
    "cmap = plt.cm.Blues\n",
    "norm = plt.Normalize(vmin=min(df_gl.tol.unique()),\n",
    "                     vmax=max(df_gl.tol.unique()))\n",
    "\n",
    "# Plot R^2 values for each tolerance\n",
    "for tol, results in tol_results.items():\n",
    "    color = cmap(norm(tol))\n",
    "    axes[0].plot(results['months'],\n",
    "                 results['r2'],\n",
    "                 marker='o',\n",
    "                 label=f'Tol {tol} m',\n",
    "                 color=color)\n",
    "\n",
    "# Plot MSE values for each tolerance\n",
    "for tol, results in tol_results.items():\n",
    "    color = cmap(norm(tol))\n",
    "    axes[1].plot(results['months'],\n",
    "                 results['rmse'],\n",
    "                 marker='o',\n",
    "                 label=f'Tol {tol} m',\n",
    "                 color=color)\n",
    "\n",
    "# Customize the plots\n",
    "axes[0].set_title('R^2 by Month and Tolerance')\n",
    "axes[0].set_xlabel('Month')\n",
    "axes[0].set_ylabel('R^2')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_title('MSE by Month and Tolerance')\n",
    "axes[1].set_xlabel('Month')\n",
    "axes[1].set_ylabel('MSE')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity of prec & temp correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.read_csv('results/sensitivity_scores.csv')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_c_prec_t_off(filename):\n",
    "    \"\"\"\n",
    "    Extract c_prec and t_off values from a given filename.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): The string containing the values to extract.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple (c_prec, t_off) with the extracted values as floats.\n",
    "    \"\"\"\n",
    "    # Define the regular expression pattern\n",
    "    #pattern = r\"_(\\d+\\.\\d+)_(-?\\d+\\.\\d+)\\.nc$\"\n",
    "    pattern = r\"_(\\d+\\.\\d+)_(-?\\d+)\\.nc$\"\n",
    "\n",
    "    # Search for the pattern in the filename\n",
    "    match = re.search(pattern, filename)\n",
    "\n",
    "    if match:\n",
    "        # Extract the matched groups and convert to floats\n",
    "        c_prec = float(match.group(1))\n",
    "        t_off = float(match.group(2))\n",
    "        return c_prec, t_off\n",
    "    else:\n",
    "        raise ValueError(\"Filename does not match the expected pattern\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "filename = \"hohlaub_2015_1_1.6_0.3333.nc\"\n",
    "c_prec, t_off = extract_c_prec_t_off(filename)\n",
    "print(f\"c_prec: {c_prec}, t_off: {t_off}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_files_by_pattern(file_list,\n",
    "                            glacierName=None,\n",
    "                            hydro_year=None,\n",
    "                            monthNb=None):\n",
    "    \"\"\"\n",
    "    Filter files starting with the pattern \"{glacierName}_{hydro_year}_{monthNb}\".\n",
    "\n",
    "    Parameters:\n",
    "        file_list (list): List of filenames to filter.\n",
    "        glacierName (str, optional): The name of the glacier to match. Defaults to None (any glacier).\n",
    "        hydro_year (int, optional): The hydrological year to match. Defaults to None (any year).\n",
    "        monthNb (int, optional): The month number to match. Defaults to None (any month).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of filenames that match the pattern.\n",
    "    \"\"\"\n",
    "    # Build the regular expression pattern based on the provided attributes\n",
    "    glacier_pattern = glacierName if glacierName else r\"[a-zA-Z0-9]+\"\n",
    "    year_pattern = str(hydro_year) if hydro_year else r\"\\d{4}\"\n",
    "    month_pattern = str(monthNb) if monthNb else r\"\\d{1,2}\"\n",
    "\n",
    "    pattern = rf\"^{glacier_pattern}_{year_pattern}_{month_pattern}.*\"\n",
    "\n",
    "    # Filter the files that match the pattern\n",
    "    matching_files = [\n",
    "        filename for filename in file_list if re.match(pattern, filename)\n",
    "    ]\n",
    "\n",
    "    return matching_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('results/nc/sensitivity/adler/wgs84')\n",
    "matching_files = filter_files_by_pattern(files,\n",
    "                                         glacierName='adler',\n",
    "                                         hydro_year=2015,\n",
    "                                         monthNb=9)\n",
    "matching_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satellite_glaciers = os.listdir('results/nc/sensitivity/')\n",
    "RUN = True\n",
    "if RUN:\n",
    "    # first delete results file\n",
    "    if os.path.exists(f'results/snow_cover_corr.csv'):\n",
    "        os.remove(f'results/snow_cover_corr.csv')\n",
    "\n",
    "    # Add header\n",
    "    with open(f'results/snow_cover_corr.csv', 'w') as f:\n",
    "        f.write(\n",
    "            \"c_prec,t_off,year,month,monthNb,raster_date,glacier_name,snow_cover_S2,snow_cover_glacier\\n\"\n",
    "        )\n",
    "\n",
    "    # All rasters resampled over glaciers:\n",
    "    rasters_resampled = os.listdir(os.path.join(path_S2, 'perglacier'))\n",
    "\n",
    "    for raster_res in tqdm(rasters_resampled):\n",
    "        # Extract glacier name\n",
    "        glacierName = raster_res.split('_')[0]\n",
    "\n",
    "        # Extract date from satellite raster\n",
    "        match = re.search(r\"(\\d{4})_(\\d{2})_(\\d{2})\", raster_res)\n",
    "        year, month, day = match.groups()\n",
    "        date_str = match.group(1) + '-' + match.group(2) + '-' + match.group(3)\n",
    "        raster_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "        # Find closest hydrological year and month\n",
    "        closest_month, hydro_year = get_hydro_year_and_month(raster_date)\n",
    "        monthNb = month_abbr_hydr_full_hydr[closest_month]\n",
    "\n",
    "        if hydro_year > 2021:\n",
    "            continue\n",
    "\n",
    "        # Read satellite raster over glacier (previously resampled)\n",
    "        gdf_S2_res = gpd.read_file(\n",
    "            os.path.join(path_S2, 'perglacier', raster_res))\n",
    "\n",
    "        # Calculate percentage of snow cover (class 1)\n",
    "        snow_cover_S2 = IceSnowCover(gdf_S2_res, gdf_S2_res)\n",
    "\n",
    "        # Load MB predictions for that year and month\n",
    "        path_nc_wgs84 = f\"results/nc/sensitivity/{glacierName}/wgs84/\"\n",
    "\n",
    "        files_list = os.listdir(path_nc_wgs84)\n",
    "        matching_files_list = filter_files_by_pattern(files_list,\n",
    "                                                      glacierName=glacierName,\n",
    "                                                      hydro_year=hydro_year,\n",
    "                                                      monthNb=monthNb)\n",
    "        for filename_nc in matching_files_list:\n",
    "            c_prec, t_off = extract_c_prec_t_off(filename_nc)\n",
    "\n",
    "            gdf_glacier = ClassSnowCover(path_nc_wgs84, filename_nc)\n",
    "            snow_cover_glacier = IceSnowCover(gdf_glacier, gdf_S2_res)\n",
    "\n",
    "            # Save the results\n",
    "            with open(f'results/snow_cover_corr.csv', 'a') as f:\n",
    "                f.write(\n",
    "                    f\"{c_prec},{t_off},{hydro_year},{closest_month},{monthNb},{date_str},{glacierName},{snow_cover_S2},{snow_cover_glacier}\\n\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read results\n",
    "df = pd.read_csv(f'results/snow_cover_corr.csv').sort_values(\n",
    "    by=['year', 'month'], ascending=True)\n",
    "df_gl = df[~df.month.isin(['oct', 'sep'])]\n",
    "\n",
    "factors = df_gl[['c_prec', 't_off']].drop_duplicates().sort_values(by='c_prec')\n",
    "\n",
    "for i, row in factors.iterrows():\n",
    "    c_prec, t_off = row['c_prec'], row['t_off']\n",
    "    df_gl_c_prec = df_gl[(df_gl.c_prec == c_prec) & (df_gl.t_off == t_off)]\n",
    "    fig, axs = plot_snow_cover_scatter(df_gl_c_prec, add_corr=False)\n",
    "    plt.suptitle(f'c_prec: {c_prec}, t_off: {t_off}', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snowline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the raster files and plot geoplots\n",
    "gl = 'aletsch'\n",
    "yr = 2021\n",
    "rasters_gl = [\n",
    "    f for f in os.listdir(os.path.join(path_S2, 'perglacier'))\n",
    "    if (gl in f) and (str(yr) in f)\n",
    "]\n",
    "rasters_gl.sort()\n",
    "for raster_res in rasters_gl:\n",
    "    plot_snow_cover_geoplots(raster_res,\n",
    "                             path_S2,\n",
    "                             month_abbr_hydr_full_hydr,\n",
    "                             add_snowline=True,\n",
    "                             band_size=50,\n",
    "                             percentage_threshold=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the raster files and plot geoplots\n",
    "gl = 'rhone'\n",
    "yr = 2021\n",
    "rasters_gl = [\n",
    "    f for f in os.listdir(os.path.join(path_S2, 'perglacier'))\n",
    "    if (gl in f) and (str(yr) in f)\n",
    "]\n",
    "rasters_gl.sort()\n",
    "for raster_res in rasters_gl:\n",
    "    plot_snow_cover_geoplots(raster_res,\n",
    "                             path_S2,\n",
    "                             month_abbr_hydr_full_hydr,\n",
    "                             add_snowline=True,\n",
    "                             band_size=50,\n",
    "                             percentage_threshold=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missed glaciers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1: Glacier is in regions where raster is NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2015\n",
    "glacierName = 'taelliboden'\n",
    "filename_nc = f\"{glacierName}_{year}.nc\"\n",
    "path_nc_wgs84 = f\"results/nc/var_normal/{glacierName}/wgs84/\"\n",
    "path_nc_wgs84_corr = f\"results/nc/var_corr/{glacierName}/wgs84/\"\n",
    "\n",
    "gdf_raster = createRaster(raster_a_2015)\n",
    "\n",
    "# Calculate snow cover for glacier\n",
    "gdf_glacier, snow_cover_glacier, ice_cover_glacier = snowCover(\n",
    "    path_nc_wgs84, filename_nc)\n",
    "# Corrected for temperature & precipitation\n",
    "gdf_glacier, snow_cover_glacier_corr, ice_cover_glacier_corr = snowCover(\n",
    "    path_nc_wgs84_corr, filename_nc)\n",
    "\n",
    "# Clip the raster to the glacier extent and resample it to the glacier resolution\n",
    "# gdf_raster_res = resampleRaster(gdf_glacier, gdf_raster)\n",
    "\n",
    "bounding_box = gdf_glacier.total_bounds  # [minx, miny, maxx, maxy]\n",
    "raster_bounds = gdf_raster.total_bounds  # [minx, miny, maxx, maxy]\n",
    "\n",
    "# Check if glacier bounds are within raster bounds\n",
    "if not (bounding_box[0] >= raster_bounds[0]\n",
    "        and  # minx of glacier >= minx of raster\n",
    "        bounding_box[1] >= raster_bounds[1]\n",
    "        and  # miny of glacier >= miny of raster\n",
    "        bounding_box[2] <= raster_bounds[2]\n",
    "        and  # maxx of glacier <= maxx of raster\n",
    "        bounding_box[3]\n",
    "        <= raster_bounds[3]  # maxy of glacier <= maxy of raster\n",
    "        ):\n",
    "    print(f\"Glacier {glacierName} is out of bounds\")\n",
    "\n",
    "bbox_polygon = box(*bounding_box)\n",
    "\n",
    "# The raster might have no data (NaN values) in the region of the glacier:\n",
    "bounding_box = [7.8, 45.95854232, 8, 46.1]\n",
    "bbox_polygon = box(*bounding_box)\n",
    "gfd_res = gdf_raster[gdf_raster.intersects(bbox_polygon)]\n",
    "ax = gfd_res.plot(color='blue', alpha=0.5)\n",
    "gdf_glacier.plot(ax=ax, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform to tif rasters for QGIS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacierName = 'aletsch'\n",
    "year = 2021\n",
    "\n",
    "filename_nc = f\"{glacierName}_{year}.nc\"\n",
    "path_nc_lv95 = f\"results/nc/{glacierName}/lv95/\"\n",
    "path_nc_wgs84 = f\"results/nc/{glacierName}/wgs84/\"\n",
    "\n",
    "filename_tif = f\"{glacierName}_{year}.tif\"\n",
    "path_tif_wgs84 = f\"results/tif/{glacierName}/wgs84/\"\n",
    "path_tif_lv95 = f\"results/tif/{glacierName}/lv95/\"\n",
    "\n",
    "createPath(path_tif_lv95)\n",
    "createPath(path_tif_wgs84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf, gdf_class, raster_data, extent = TransformToRaster(\n",
    "    filename_nc, filename_tif, path_nc_wgs84, path_tif_wgs84, path_tif_lv95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"000378bd-b0f0-46e2-a46d-f2165b0c6c02\"\n",
    "provider = cx.providers.Stadia.StamenTerrain(api_key=API_KEY)\n",
    "provider[\"url\"] = provider[\"url\"] + f\"?api_key={API_KEY}\"\n",
    "\n",
    "vmin, vmax = gdf.data.min(), gdf.data.max()\n",
    "norm = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "gdf_clean = gdf.dropna(subset=[\"data\"])\n",
    "gdf_clean.plot(\n",
    "    column=\"data\",  # Column to visualize\n",
    "    cmap=\"RdBu\",  # Color map suitable for glacier data\n",
    "    norm=norm,\n",
    "    legend=True,  # Display a legend\n",
    "    ax=axs[0],\n",
    "    markersize=5,  # Adjust size if points are too small or large\n",
    "    missing_kwds={\"color\": \"lightgrey\"}  # Define color for NaN values\n",
    ")\n",
    "cx.add_basemap(axs[0], crs=gdf.crs, source=provider)\n",
    "\n",
    "gdf_clean = gdf_class.dropna(subset=[\"data\"])\n",
    "gdf_clean.plot(\n",
    "    column=\"data\",  # Column to visualize\n",
    "    legend=True,  # Display a legend\n",
    "    ax=axs[1],\n",
    "    markersize=5,  # Adjust size if points are too small or large\n",
    "    missing_kwds={\"color\": \"lightgrey\"}  # Define color for NaN values\n",
    ")\n",
    "cx.add_basemap(axs[1], crs=gdf.crs, source=provider)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin, vmax = gdf.data.min(), gdf.data.max()\n",
    "\n",
    "# Calculate the relative position of 0\n",
    "relative_position = (0 - vmin) / (vmax - vmin) * 100\n",
    "\n",
    "# Print the result\n",
    "print(f\"The relative position of 0 is {relative_position:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "for year in years:\n",
    "    print(year)\n",
    "    for month in month_abbr_hydr_full_hydr:\n",
    "        monthNb = month_abbr_hydr_full_hydr[month]\n",
    "\n",
    "        filename_nc = f\"{glacierName}_{year}_{monthNb}.nc\"\n",
    "        path_nc_lv95 = f\"results/nc/{glacierName}/lv95/\"\n",
    "        path_nc_wgs84 = f\"results/nc/{glacierName}/wgs84/\"\n",
    "\n",
    "        filename_tif = f\"{glacierName}_{year}_{monthNb}.tif\"\n",
    "        path_tif_wgs84 = f\"results/tif/{glacierName}/wgs84/\"\n",
    "        path_tif_lv95 = f\"results/tif/{glacierName}/lv95/\"  # normally EPSG Code: 2056\n",
    "\n",
    "        gdf, gdf_class, raster_data, extent = TransformToRaster(\n",
    "            filename_nc, filename_tif, path_nc_wgs84, path_tif_wgs84,\n",
    "            path_tif_lv95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"000378bd-b0f0-46e2-a46d-f2165b0c6c02\"\n",
    "provider = cx.providers.Stadia.StamenTerrain(api_key=API_KEY)\n",
    "provider[\"url\"] = provider[\"url\"] + f\"?api_key={API_KEY}\"\n",
    "\n",
    "vmin, vmax = gdf.data.min(), gdf.data.max()\n",
    "norm = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(5, 5))\n",
    "gdf_clean = gdf.dropna(subset=[\"data\"])\n",
    "gdf_clean.plot(\n",
    "    column=\"data\",  # Column to visualize\n",
    "    cmap=\"RdBu\",  # Color map suitable for glacier data\n",
    "    norm=norm,\n",
    "    legend=True,  # Display a legend\n",
    "    ax=axs[0],\n",
    "    markersize=5,  # Adjust size if points are too small or large\n",
    "    missing_kwds={\"color\": \"lightgrey\"}  # Define color for NaN values\n",
    ")\n",
    "cx.add_basemap(axs[0], crs=gdf.crs, source=provider)\n",
    "\n",
    "gdf_clean = gdf_class.dropna(subset=[\"data\"])\n",
    "gdf_clean.plot(\n",
    "    column=\"data\",  # Column to visualize\n",
    "    legend=True,  # Display a legend\n",
    "    ax=axs[1],\n",
    "    markersize=5,  # Adjust size if points are too small or large\n",
    "    missing_kwds={\"color\": \"lightgrey\"}  # Define color for NaN values\n",
    ")\n",
    "cx.add_basemap(axs[1], crs=gdf.crs, source=provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by step example of one file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Open the NetCDF file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_xy = xr.open_dataset(path_nc_lv95 + filename_nc)\n",
    "ds_latlon = xr.open_dataset(path_nc_wgs84 + filename_nc)\n",
    "\n",
    "# Smoothing\n",
    "ds_xy_g = GaussianFilter(ds_xy)\n",
    "ds_latlon_g = GaussianFilter(ds_latlon)\n",
    "\n",
    "# Show effet of Smoothing:\n",
    "vmin, vmax = np.min([\n",
    "    ds_xy.pred_masked.min().values,\n",
    "    ds_xy_g.pred_masked.min()\n",
    "]), np.max([ds_xy.pred_masked.max().values,\n",
    "            ds_xy_g.pred_masked.max()])\n",
    "max_abs_value = max(abs(vmin), abs(vmax))\n",
    "norm = mcolors.TwoSlopeNorm(vmin=-max_abs_value, vcenter=0, vmax=max_abs_value)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ds_xy.pred_masked.plot.imshow(cmap='RdBu', norm=norm, ax=axs[0])\n",
    "axs[0].set_title('Original')\n",
    "\n",
    "# Plot or analyze `smoothed_data` as needed\n",
    "ds_xy_g.pred_masked.plot.imshow(cmap='RdBu', norm=norm, ax=axs[1])\n",
    "axs[1].set_title('Gaussian Filter')\n",
    "\n",
    "# print min and max values\n",
    "print(ds_xy.pred_masked.min().values, ds_xy.pred_masked.max().values)\n",
    "print(ds_xy_g.pred_masked.min().values, ds_xy_g.pred_masked.max().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: transform to geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf, lon, lat = toGeoPandas(ds_latlon_g)\n",
    "\n",
    "# Reproject to LV95 (EPSG:2056) swiss coordinates\n",
    "# gdf_lv95 = gdf.to_crs(\"EPSG:2056\")\n",
    "\n",
    "API_KEY = \"000378bd-b0f0-46e2-a46d-f2165b0c6c02\"\n",
    "provider = cx.providers.Stadia.StamenTerrain(api_key=API_KEY)\n",
    "provider[\"url\"] = provider[\"url\"] + f\"?api_key={API_KEY}\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "gdf_clean = gdf.dropna(subset=[\"data\"])\n",
    "gdf_clean.plot(\n",
    "    column=\"data\",  # Column to visualize\n",
    "    cmap=\"Reds\",  # Color map suitable for glacier data\n",
    "    legend=True,  # Display a legend\n",
    "    ax=ax,\n",
    "    markersize=5,  # Adjust size if points are too small or large\n",
    "    missing_kwds={\"color\": \"lightgrey\"}  # Define color for NaN values\n",
    ")\n",
    "cx.add_basemap(ax, crs=gdf.crs, source=provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: transform to raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to raster from geopandas\n",
    "raster_data, extent = toRaster(gdf,\n",
    "                               lon,\n",
    "                               lat,\n",
    "                               file_name=path_tif_wgs84 + filename_tif)\n",
    "\n",
    "# reproject raster to Swiss coordinates (LV95)\n",
    "reproject_raster_to_lv95(path_tif_wgs84 + filename_tif,\n",
    "                         path_tif_lv95 + filename_tif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opt step 4: for clariden\n",
    "Need to merge two rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'clariden' in glacierName:\n",
    "    merge_rasters('results/tif/claridenL_2022_w_lv95.tif',\n",
    "                  'results/tif/claridenU_2022_w_lv95.tif',\n",
    "                  'results/tif/clariden_2022_w_lv95.tif')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
