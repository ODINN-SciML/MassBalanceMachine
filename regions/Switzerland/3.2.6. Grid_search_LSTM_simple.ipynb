{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(),\n",
    "                             '../../'))  # Add root of repo to import MBM\n",
    "import csv\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "from skorch.helper import SliceDataset\n",
    "from datetime import datetime\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "import itertools\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import WeightedRandomSampler, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'\n",
    "\n",
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "\n",
    "vois_climate = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:\n",
    "### Input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocking on glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(data_monthly.GLACIER.unique())\n",
    "missing_glaciers = [g for g in TEST_GLACIERS if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in TEST_GLACIERS]\n",
    "\n",
    "data_test = data_monthly[data_monthly.GLACIER.isin(TEST_GLACIERS)]\n",
    "print('Size of monthly test data:', len(data_test))\n",
    "\n",
    "data_train = data_monthly[data_monthly.GLACIER.isin(train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train))\n",
    "\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=TEST_GLACIERS,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))\n",
    "\n",
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "\n",
    "data_test = test_set['df_X']\n",
    "data_test['y'] = test_set['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'pcsr',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = [\n",
    "    'aspect_sgi', 'slope_sgi', 'hugonnet_dhdt', 'consensus_ice_thickness',\n",
    "    'millan_v'\n",
    "]\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data_train.copy()\n",
    "df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "df_test = data_test.copy()\n",
    "df_test['PERIOD'] = df_test['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "# --- build train dataset from dataframe ---\n",
    "ds_train = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_train,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "ds_test = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_test,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True  # you said test has real targets\n",
    ")\n",
    "\n",
    "train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train), val_ratio=0.2, seed=cfg.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "param_grid = {\n",
    "    \"lr\": [1e-3, 5e-4, 1e-4],\n",
    "    \"weight_decay\": [0.0, 1e-5, 1e-4],\n",
    "    \"hidden_size\": [64, 128],\n",
    "    \"num_layers\": [1, 2],\n",
    "    \"dropout\": [0.0, 0.2],\n",
    "    \"head_dropout\": [0.0, 0.1],\n",
    "}\n",
    "\n",
    "static = [\n",
    "    (0, 0, None),  # identity (use 0 here for robustness)\n",
    "    (2, [128, 64], 0.1),  # small two-layer MLP\n",
    "]\n",
    "\n",
    "def pack(static_triplet):\n",
    "    sl, sh, sd = static_triplet\n",
    "    return dict(\n",
    "        static_layers=sl,\n",
    "        static_hidden=sh,\n",
    "        static_dropout=sd,\n",
    "    )\n",
    "\n",
    "# ---- constants that should be the same for every sample ----\n",
    "const_params = {\n",
    "    \"Fm\": ds_train.Xm.shape[-1],  # monthly features\n",
    "    \"Fs\": ds_train.Xs.shape[-1],  # static features\n",
    "    \"bidirectional\": False,\n",
    "    \"loss_name\": \"neutral\",\n",
    "    \"loss_spec\": None,\n",
    "}\n",
    "\n",
    "def grid_iter_with_static_and_const(grid, static_list, const):\n",
    "    keys = list(grid.keys())\n",
    "    for values in product(*(grid[k] for k in keys), static_list):\n",
    "        params = dict(zip(keys, values[:-1]))  # non-static hyperparams\n",
    "        params.update(pack(values[-1]))  # add static config\n",
    "        params.update(const)  # add constants\n",
    "        yield params\n",
    "\n",
    "\n",
    "# ---- generate all sampled param sets ----\n",
    "sampled_params = list(\n",
    "    grid_iter_with_static_and_const(param_grid, static, const_params))\n",
    "print(len(sampled_params))\n",
    "print(sampled_params[0])  # preview one combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = True\n",
    "if RUN:\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "    log_filename = f'logs/lstm_simple_param_search_progress_{datetime.now().strftime(\"%Y-%m-%d\")}.csv'\n",
    "\n",
    "    # create log with header\n",
    "    with open(log_filename, mode='w', newline='') as log_file:\n",
    "        writer = csv.DictWriter(log_file,\n",
    "                                fieldnames=list(sampled_params[0].keys()) +\n",
    "                                ['valid_loss', 'test_rmse_a', 'test_rmse_w'])\n",
    "        writer.writeheader()\n",
    "\n",
    "    results = []\n",
    "    best_overall = {\"val\": float('inf'), \"row\": None, \"params\": None}\n",
    "\n",
    "    for i, params in enumerate(sampled_params):\n",
    "        seed_all(cfg.seed)\n",
    "        model_filename = 'models/best_lstm_mb_gs_simple.pt'\n",
    "\n",
    "        # delete existing model file:\n",
    "        if os.path.exists(model_filename):\n",
    "            os.remove(model_filename)\n",
    "            print(f\"Deleted existing model file: {model_filename}\")\n",
    "\n",
    "        # Build dataloaders:\n",
    "        ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "            ds_train)\n",
    "\n",
    "        ds_test_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "            ds_test)\n",
    "\n",
    "        # --- make train/val loaders and get split indices (scalers fit on TRAIN, applied to whole ds) ---\n",
    "        train_dl, val_dl = ds_train_copy.make_loaders(\n",
    "            train_idx=train_idx,\n",
    "            val_idx=val_idx,\n",
    "            batch_size_train=64,\n",
    "            batch_size_val=128,\n",
    "            seed=cfg.seed,\n",
    "            fit_and_transform=\n",
    "            True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "            shuffle_train=True,\n",
    "            use_weighted_sampler=True  # use weighted sampler for training\n",
    "        )\n",
    "\n",
    "        # --- build test dataset and loader (reuses train scalers) ---\n",
    "        test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "            ds_test_copy, ds_train_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "        print(f\"\\n--- Running config {i+1}/{len(sampled_params)} ---\")\n",
    "        print(params)\n",
    "\n",
    "        # Build model\n",
    "        model = mbm.models.LSTM_MB.build_model_from_params(cfg, params, device)\n",
    "\n",
    "        # Choose loss\n",
    "        loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(params)\n",
    "\n",
    "        # Train\n",
    "        history, best_val, best_state = model.train_loop(\n",
    "            device=device,\n",
    "            train_dl=train_dl,\n",
    "            val_dl=val_dl,\n",
    "            epochs=150,\n",
    "            lr=params['lr'],\n",
    "            weight_decay=params['weight_decay'],\n",
    "            clip_val=1,\n",
    "            # scheduler\n",
    "            sched_factor=0.5,\n",
    "            sched_patience=6,\n",
    "            sched_threshold=0.01,\n",
    "            sched_threshold_mode=\"rel\",\n",
    "            sched_cooldown=1,\n",
    "            sched_min_lr=1e-6,\n",
    "            # early stopping\n",
    "            es_patience=15,\n",
    "            es_min_delta=1e-4,\n",
    "            # logging\n",
    "            log_every=5,\n",
    "            verbose=False,\n",
    "            # checkpoint\n",
    "            save_best_path=model_filename,\n",
    "            loss_fn=loss_fn,\n",
    "        )\n",
    "\n",
    "        # Load the best weights\n",
    "        best_state = torch.load(model_filename, map_location=device)\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        # Evaluate on test\n",
    "        test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "            device, test_dl, ds_test_copy)\n",
    "        test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "            'RMSE_winter']\n",
    "\n",
    "        # Log row\n",
    "        row = {\n",
    "            **params, 'valid_loss': float(best_val),\n",
    "            'test_rmse_a': float(test_rmse_a),\n",
    "            'test_rmse_w': float(test_rmse_w)\n",
    "        }\n",
    "\n",
    "        print(test_rmse_a, test_rmse_w)\n",
    "\n",
    "        with open(log_filename, mode='a', newline='') as log_file:\n",
    "            writer = csv.DictWriter(log_file, fieldnames=list(row.keys()))\n",
    "            writer.writerow(row)\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "        # Track best by validation loss\n",
    "        if best_val < best_overall['val']:\n",
    "            best_overall = {\"val\": best_val, \"row\": row, \"params\": params}\n",
    "\n",
    "    print(\"\\n=== Best config by validation loss ===\")\n",
    "    print(best_overall['params'])\n",
    "    print(best_overall['row'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test best combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- pick the most recent grid-search log ---\n",
    "log_path = 'logs/lstm_simple_param_search_progress_2025-09-16.csv'\n",
    "\n",
    "df_log = pd.read_csv(log_path)\n",
    "best_row = df_log.loc[df_log[\"valid_loss\"].idxmin()].to_dict()\n",
    "\n",
    "\n",
    "# --- normalize params from CSV ---\n",
    "def _as_bool(x):\n",
    "    if isinstance(x, bool): return x\n",
    "    if isinstance(x, (int, float)): return bool(int(x))\n",
    "    return str(x).strip().lower() in {\"1\", \"true\", \"t\", \"yes\", \"y\"}\n",
    "\n",
    "\n",
    "def _as_opt_list(x):\n",
    "    if pd.isna(x): return None\n",
    "    s = str(x).strip()\n",
    "    if s.lower() in {\"\", \"none\"}: return None\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "best_params = {\n",
    "    \"Fm\":\n",
    "    int(best_row[\"Fm\"]),\n",
    "    \"Fs\":\n",
    "    int(best_row[\"Fs\"]),\n",
    "    \"hidden_size\":\n",
    "    int(best_row[\"hidden_size\"]),\n",
    "    \"num_layers\":\n",
    "    int(best_row[\"num_layers\"]),\n",
    "    \"bidirectional\":\n",
    "    _as_bool(best_row[\"bidirectional\"]),\n",
    "    \"dropout\":\n",
    "    float(best_row[\"dropout\"]),\n",
    "    \"static_layers\":\n",
    "    int(best_row[\"static_layers\"]),\n",
    "    \"static_hidden\":\n",
    "    _as_opt_list(best_row.get(\"static_hidden\")),\n",
    "    \"static_dropout\":\n",
    "    None if pd.isna(best_row.get(\"static_dropout\")) else float(\n",
    "        best_row[\"static_dropout\"]),\n",
    "    \"simple\":\n",
    "    True,  # grid is two-heads only\n",
    "    \"head_dropout\":\n",
    "    float(best_row[\"head_dropout\"]),\n",
    "    \"lr\":\n",
    "    float(best_row[\"lr\"]),\n",
    "    \"weight_decay\":\n",
    "    float(best_row[\"weight_decay\"]),\n",
    "    \"clip_val\":\n",
    "    float(best_row.get(\"clip_val\", 1.0)),\n",
    "    \"sched_factor\":\n",
    "    float(best_row.get(\"sched_factor\", 0.5)),\n",
    "    \"sched_patience\":\n",
    "    int(best_row.get(\"sched_patience\", 6)),\n",
    "    \"loss_name\":\n",
    "    str(best_row.get(\"loss_name\", \"neutral\")),\n",
    "}\n",
    "# Make \"weighted\" use your LSTM defaults\n",
    "best_params[\"loss_spec\"] = (\n",
    "    \"weighted\", {}) if best_params[\"loss_name\"] == \"weighted\" else None\n",
    "\n",
    "# --- deterministic seeding (process-wide) ---\n",
    "os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":4096:8\")\n",
    "import random\n",
    "\n",
    "random.seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "torch.cuda.manual_seed_all(cfg.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# Optional strict mode:\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# --- build datasets ONCE (fresh/pristine each run of this cell) ---\n",
    "ds_train = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_train,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "ds_test = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_test,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "# --- reuse fixed split if present, else compute with cfg.seed ---\n",
    "try:\n",
    "    train_idx, val_idx  # already defined earlier in your session?\n",
    "except NameError:\n",
    "    train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "        len(ds_train), val_ratio=0.2, seed=cfg.seed)\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train) ---\n",
    "train_dl, val_dl = ds_train.make_loaders(\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    seed=cfg.seed,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    fit_and_transform=True,\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True,\n",
    ")\n",
    "\n",
    "# --- test loader (copies TRAIN scalers into ds_test and transforms it) ---\n",
    "test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test, ds_train, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model_path = \"models/lstm_best_simple_retrain.pt\"\n",
    "if os.path.exists(model_path): os.remove(model_path)\n",
    "\n",
    "model = mbm.models.LSTM_MB.build_model_from_params(cfg, best_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(best_params)\n",
    "\n",
    "history, best_val, _ = model.train_loop(\n",
    "    device=device,\n",
    "    train_dl=train_dl,\n",
    "    val_dl=val_dl,\n",
    "    epochs=150,\n",
    "    lr=best_params[\"lr\"],\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    "    clip_val=best_params[\"clip_val\"],\n",
    "    # scheduler\n",
    "    sched_factor=best_params[\"sched_factor\"],\n",
    "    sched_patience=best_params[\"sched_patience\"],\n",
    "    sched_threshold=0.01,\n",
    "    sched_threshold_mode=\"rel\",\n",
    "    sched_cooldown=1,\n",
    "    sched_min_lr=1e-6,\n",
    "    # early stopping\n",
    "    es_patience=15,\n",
    "    es_min_delta=1e-4,\n",
    "    # logging\n",
    "    log_every=5,\n",
    "    verbose=True,\n",
    "    # checkpoint\n",
    "    save_best_path=model_path,\n",
    "    loss_fn=loss_fn,\n",
    ")\n",
    "\n",
    "state = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "\n",
    "# --- evaluate (IMPORTANT: pass ds_test, which now has scalers) ---\n",
    "test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "    device, test_dl, ds_test)\n",
    "print(\"\\nTest metrics:\", test_metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
