{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing of GLAMOS MB data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import massbalancemachine as mbm\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, LineString, Point\n",
    "from scipy.spatial.distance import cdist\n",
    "import pyproj\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Transform .dat files to .csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all files with pmb (for winter and annual mb):\n",
    "glamosfiles_mb_a, glamosfiles_mb_w = [], []\n",
    "for file in os.listdir(path_PMB_GLAMOS_a_raw):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(path_PMB_GLAMOS_a_raw, file)):\n",
    "        glamosfiles_mb_a.append(file)\n",
    "\n",
    "for file in os.listdir(path_PMB_GLAMOS_w_raw):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(path_PMB_GLAMOS_w_raw, file)):\n",
    "        glamosfiles_mb_w.append(file)\n",
    "\n",
    "print('Examples of index stake raw files:\\n', glamosfiles_mb_a[:5])\n",
    "\n",
    "# Transform all files to csv\n",
    "RUN = True\n",
    "if RUN:\n",
    "    emptyfolder(path_PMB_GLAMOS_csv_a)\n",
    "    emptyfolder(path_PMB_GLAMOS_csv_w)\n",
    "    for file in glamosfiles_mb_a:\n",
    "        fileName = re.split('.dat', file)[0]\n",
    "        processDatFile(fileName, path_PMB_GLAMOS_a_raw, path_PMB_GLAMOS_csv_a)\n",
    "\n",
    "    for file in glamosfiles_mb_w:\n",
    "        fileName = re.split('.dat', file)[0]\n",
    "        processDatFile(fileName, path_PMB_GLAMOS_w_raw, path_PMB_GLAMOS_csv_w)\n",
    "\n",
    "    # Separate clariden into clariden II and III\n",
    "    fileName = 'clariden_annual.csv'\n",
    "    clariden_csv_a = pd.read_csv(path_PMB_GLAMOS_csv_a + fileName,\n",
    "                                 sep=',',\n",
    "                                 header=0,\n",
    "                                 encoding='latin-1')\n",
    "    clariden_csv_a[clariden_csv_a['# name'] == 'L'].to_csv(\n",
    "        path_PMB_GLAMOS_csv_a + 'claridenL_annual.csv', index=False)\n",
    "    clariden_csv_a[clariden_csv_a['# name'] == 'U'].to_csv(\n",
    "        path_PMB_GLAMOS_csv_a + 'claridenU_annual.csv', index=False)\n",
    "\n",
    "    fileName = 'clariden_winter.csv'\n",
    "    clariden_csv_w = pd.read_csv(path_PMB_GLAMOS_csv_w + fileName,\n",
    "                                 sep=',',\n",
    "                                 header=0,\n",
    "                                 encoding='latin-1')\n",
    "    clariden_csv_w[clariden_csv_w['# name'] == 'L'].to_csv(\n",
    "        path_PMB_GLAMOS_csv_w + 'claridenL_winter.csv', index=False)\n",
    "    clariden_csv_w[clariden_csv_w['# name'] == 'U'].to_csv(\n",
    "        path_PMB_GLAMOS_csv_w + 'claridenU_winter.csv', index=False)\n",
    "\n",
    "    os.remove(path_PMB_GLAMOS_csv_a + 'clariden_annual.csv')\n",
    "    os.remove(path_PMB_GLAMOS_csv_w + 'clariden_winter.csv')\n",
    "\n",
    "# Example:\n",
    "fileName = 'aletsch_annual.csv'\n",
    "aletsch_csv = pd.read_csv(path_PMB_GLAMOS_csv_a + fileName,\n",
    "                          sep=',',\n",
    "                          header=0,\n",
    "                          encoding='latin-1')\n",
    "aletsch_csv.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Measurement periods:\n",
    "### 2.1. Annual measurements: \n",
    "Process annual measurements and put all stakes into one csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble all into one csv file:\n",
    "RUN = True\n",
    "if RUN:\n",
    "    # Annual:\n",
    "    df_annual_raw = pd.DataFrame()\n",
    "    for file in tqdm(os.listdir(path_PMB_GLAMOS_csv_a), desc='Annual stakes'):\n",
    "        fileName = re.split('.csv', file)[0]\n",
    "        glacierName = re.split('_', fileName)[0]\n",
    "        df = pd.read_csv(path_PMB_GLAMOS_csv_a + file,\n",
    "                         sep=',',\n",
    "                         header=0,\n",
    "                         encoding='latin-1')\n",
    "        df['glacier'] = glacierName\n",
    "        df['period'] = 'annual'\n",
    "\n",
    "        # Correct years and add hydrol. year:\n",
    "        df_processed = transformDates(df)\n",
    "\n",
    "        # Remove obvious duplicates:\n",
    "        df_processed = df_processed.drop_duplicates()\n",
    "\n",
    "        # Transform to lat/lon system\n",
    "        df_processed = LV03toWGS84(df_processed)\n",
    "\n",
    "        df_annual_raw = pd.concat([df_annual_raw, df_processed])\n",
    "\n",
    "    # Get the year (end of hydr. year):\n",
    "    df_annual_raw['YEAR'] = df_annual_raw['date1'].apply(\n",
    "        lambda x: pd.to_datetime(x).year)\n",
    "\n",
    "# Reshape to WGMS format:\n",
    "# re-order columns:\n",
    "df_annual_raw = df_annual_raw[[\n",
    "    'YEAR', '# name', 'glacier', 'date0', 'date1', 'lat', 'lon', 'height',\n",
    "    'mb_we', 'period', 'date_fix0', 'date_fix1', 'time0', 'time1',\n",
    "    'date_quality', 'position_quality', 'mb_raw', 'density', 'density_quality',\n",
    "    'measurement_quality', 'measurement_type', 'mb_error', 'reading_error',\n",
    "    'density_error', 'error_evaluation_method', 'source'\n",
    "]]\n",
    "df_annual_raw.rename(columns={\n",
    "    '# name': 'POINT_ID',\n",
    "    'lat': 'POINT_LAT',\n",
    "    'lon': 'POINT_LON',\n",
    "    'height': 'POINT_ELEVATION',\n",
    "    'date0': 'FROM_DATE',\n",
    "    'date1': 'TO_DATE',\n",
    "    'mb_we': 'POINT_BALANCE',\n",
    "    'glacier': 'GLACIER',\n",
    "    'period': 'PERIOD'\n",
    "},\n",
    "                     inplace=True)\n",
    "# remove duplicates:\n",
    "df_annual_raw = df_annual_raw.drop_duplicates()\n",
    "df_annual_raw.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that to_date is always from_date year + 1\n",
    "# drop rows where this is not the case:\n",
    "rows_to_drop = []\n",
    "for i, row in df_annual_raw.iterrows():\n",
    "    if pd.to_datetime(row['TO_DATE'], format='%Y%m%d').year - pd.to_datetime(\n",
    "            row['FROM_DATE'], format='%Y%m%d').year != 1:\n",
    "        # print(row['FROM_DATE'], row['TO_DATE'])\n",
    "        # drop row:\n",
    "        rows_to_drop.append(i)\n",
    "df_annual_raw.drop(rows_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Winter measurements:\n",
    "For each point in annual meas., take winter meas that was taken closest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winter_glaciers = [\n",
    "    re.split('_winter.csv', f)[0] for f in os.listdir(path_PMB_GLAMOS_csv_w)\n",
    "]\n",
    "RUN = False\n",
    "if RUN:\n",
    "    emptyfolder(path_PMB_GLAMOS_csv_w_clean)\n",
    "    for glacier in tqdm(df_annual_raw.GLACIER.unique(),\n",
    "                        desc='Glaciers',\n",
    "                        leave=False,\n",
    "                        position=0):\n",
    "        # If glacier does not have winter measurements:\n",
    "        if glacier not in winter_glaciers:\n",
    "            continue\n",
    "\n",
    "        # Get winter all PMB measurements (a lot more than annual)\n",
    "        df_winter = pd.read_csv(path_PMB_GLAMOS_csv_w + glacier +\n",
    "                                '_winter.csv',\n",
    "                                sep=',',\n",
    "                                header=0,\n",
    "                                encoding='latin-1')\n",
    "        df_winter['period'] = 'winter'\n",
    "        df_winter['glacier'] = glacier\n",
    "\n",
    "        # Correct years and add hydrol. year:\n",
    "        df_winter = transformDates(df_winter)\n",
    "\n",
    "        # Remove obvious duplicates:\n",
    "        df_winter = df_winter.drop_duplicates()\n",
    "\n",
    "        # Transform to lat/lon system\n",
    "        df_winter = LV03toWGS84(df_winter)\n",
    "\n",
    "        # Add the year:\n",
    "        df_winter['YEAR'] = df_winter['date1'].apply(\n",
    "            lambda x: pd.to_datetime(x).year)\n",
    "\n",
    "        # Reshape to WGMS format:\n",
    "        df_winter.rename(columns={\n",
    "            '# name': 'POINT_ID',\n",
    "            'lat': 'POINT_LAT',\n",
    "            'lon': 'POINT_LON',\n",
    "            'height': 'POINT_ELEVATION',\n",
    "            'date0': 'FROM_DATE',\n",
    "            'date1': 'TO_DATE',\n",
    "            'mb_we': 'POINT_BALANCE',\n",
    "            'glacier': 'GLACIER',\n",
    "            'period': 'PERIOD'\n",
    "        },\n",
    "                         inplace=True)\n",
    "        # re-order columns:\n",
    "        df_winter = df_winter[[\n",
    "            'YEAR', 'POINT_ID', 'GLACIER', 'FROM_DATE', 'TO_DATE', 'POINT_LAT',\n",
    "            'POINT_LON', 'POINT_ELEVATION', 'POINT_BALANCE', 'PERIOD',\n",
    "            'date_fix0', 'date_fix1', 'time0', 'time1', 'date_quality',\n",
    "            'position_quality', 'mb_raw', 'density', 'density_quality',\n",
    "            'measurement_quality', 'measurement_type', 'mb_error',\n",
    "            'reading_error', 'density_error', 'error_evaluation_method',\n",
    "            'source'\n",
    "        ]]\n",
    "\n",
    "        winter_indices = []\n",
    "        # Iterate over all rows in annual stakes:\n",
    "        for index in tqdm(range(\n",
    "                len(df_annual_raw[df_annual_raw.GLACIER == glacier])),\n",
    "                          desc='rows',\n",
    "                          leave=False,\n",
    "                          position=1):\n",
    "            pointA = df_annual_raw[df_annual_raw.GLACIER ==\n",
    "                                   glacier].iloc[index]\n",
    "            yearA = pointA.YEAR\n",
    "\n",
    "            # Filter winter to same year as point:\n",
    "            df_winter_year = df_winter[df_winter.YEAR == yearA]\n",
    "\n",
    "            # If no winter measurement was taken that year:\n",
    "            if len(df_winter_year) == 0:\n",
    "                continue\n",
    "\n",
    "            # Calculate closest winter point to annual stake:\n",
    "            df_winter_year['point'] = [(x, y) for x, y in zip(\n",
    "                df_winter_year['POINT_LAT'], df_winter_year['POINT_LON'])]\n",
    "            pointA['point'] = (pointA.POINT_LAT, pointA.POINT_LON)\n",
    "            closest = closest_point(pointA['point'],\n",
    "                                    list(df_winter_year['point']))\n",
    "\n",
    "            # Convert to LAEA Europe\n",
    "            x_laea, y_laea = latlon_to_laea(closest[0], closest[1])\n",
    "            x_laea_point, y_laea_point = latlon_to_laea(\n",
    "                pointA.POINT_LAT, pointA.POINT_LON)\n",
    "\n",
    "            # Calculate distance:\n",
    "            distance_m = np.sqrt((x_laea_point - x_laea)**2 +\n",
    "                                 (y_laea_point - y_laea)**2)\n",
    "\n",
    "            # Only keep winter stakes that are within 10m:\n",
    "            if distance_m > 10:\n",
    "                continue\n",
    "            else:\n",
    "                winter_indices.append(df_winter_year[df_winter_year['point'] ==\n",
    "                                                     closest].index[0])\n",
    "\n",
    "        # Save winter stake:\n",
    "        df_winter_stakes = df_winter.iloc[winter_indices]\n",
    "        df_winter_stakes.to_csv(path_PMB_GLAMOS_csv_w_clean + glacier +\n",
    "                                '_winter.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Assemble both periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_raw = df_annual_raw.copy()\n",
    "for file in os.listdir(path_PMB_GLAMOS_csv_w_clean):\n",
    "    fileName = re.split('.csv', file)[0]\n",
    "    glacierName = re.split('_', fileName)[0]\n",
    "    df_winter = pd.read_csv(path_PMB_GLAMOS_csv_w_clean + file,\n",
    "                            sep=',',\n",
    "                            header=0,\n",
    "                            encoding='latin-1').drop(columns='Unnamed: 0')\n",
    "\n",
    "    df_all_raw = pd.concat([df_all_raw, df_winter])\n",
    "\n",
    "print('Number of winter and annual samples:', len(df_all_raw))\n",
    "print('Number of winter samples:',\n",
    "      len(df_all_raw[df_all_raw.PERIOD == 'winter']))\n",
    "print('Number of annual samples:',\n",
    "      len(df_all_raw[df_all_raw.PERIOD == 'annual']))\n",
    "\n",
    "# download all stakes coordinates:\n",
    "# df_all_raw[['GLACIER', 'POINT_ID', 'POINT_LAT', 'POINT_LON',\n",
    "#             'PERIOD']].to_csv(path_PMB_GLAMOS_csv + 'coordinates_all.csv')\n",
    "\n",
    "df_all_raw.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for issues with dates:\n",
    "for i, row in df_all_raw.iterrows():\n",
    "    if pd.to_datetime(row['TO_DATE'], format='%Y%m%d').year - pd.to_datetime(\n",
    "            row['FROM_DATE'], format='%Y%m%d').year != 1:\n",
    "        print(row['GLACIER'], row['PERIOD'], row['FROM_DATE'],\n",
    "              pd.to_datetime(row['FROM_DATE'], format='%Y%m%d').year,\n",
    "              row['TO_DATE'],\n",
    "              pd.to_datetime(row['TO_DATE'], format='%Y%m%d').year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See above that the problem is that for some winter measurements the FROM_DATE is the same year as the TO_DATE (even same date)\n",
    "# Correct it by setting it to beginning of hydrological year:\n",
    "for index, row in df_all_raw.iterrows():\n",
    "    if row['PERIOD'] == 'winter':\n",
    "        df_all_raw.loc[index, 'FROM_DATE'] = str(\n",
    "            pd.to_datetime(row['TO_DATE'], format='%Y%m%d').year - 1) + '1001'\n",
    "\n",
    "# Check again:\n",
    "for i, row in df_all_raw.iterrows():\n",
    "    if pd.to_datetime(row['TO_DATE'], format='%Y%m%d').year - pd.to_datetime(\n",
    "            row['FROM_DATE'], format='%Y%m%d').year != 1:\n",
    "        print(row['GLACIER'], row['PERIOD'], row['FROM_DATE'],\n",
    "              pd.to_datetime(row['FROM_DATE'], format='%Y%m%d').year,\n",
    "              row['TO_DATE'],\n",
    "              pd.to_datetime(row['TO_DATE'], format='%Y%m%d').year)\n",
    "\n",
    "# Save all stakes:\n",
    "df_all_raw.to_csv(path_PMB_GLAMOS_csv + 'df_all_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add RGIs Ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "path_rgi = '../../../data/GLAMOS/CH_glacier_ids_long.csv'\n",
    "rgi_df = pd.read_csv(path_rgi, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "rgi_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep important features:\n",
    "df_all_raw = pd.read_csv(path_PMB_GLAMOS_csv + 'df_all_raw.csv',\n",
    "                         sep=',',\n",
    "                         header=0,\n",
    "                         encoding='latin-1').drop(columns='Unnamed: 0')\n",
    "df_pmb = df_all_raw[[\n",
    "    'YEAR',\n",
    "    'POINT_ID',\n",
    "    'GLACIER',\n",
    "    'FROM_DATE',\n",
    "    'TO_DATE',\n",
    "    'POINT_LAT',\n",
    "    'POINT_LON',\n",
    "    'POINT_ELEVATION',\n",
    "    'POINT_BALANCE',\n",
    "    'PERIOD',\n",
    "]]\n",
    "df_pmb.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add RGIs:\n",
    "# Specify the shape filename of the glaciers outline obtained from RGIv6\n",
    "glacier_outline_fname = '../../../data/GLAMOS/nsidc0770_11.rgi60.CentralEurope/11_rgi60_CentralEurope.shp'\n",
    "\n",
    "# Load the target data and the glacier outlines\n",
    "glacier_outline = gpd.read_file(glacier_outline_fname)\n",
    "\n",
    "# Add RGI IDs through intersection with shapefiles:\n",
    "df_pmb = mbm.data_processing.utils.get_rgi(data=df_pmb,\n",
    "                                           glacier_outlines=glacier_outline)\n",
    "\n",
    "# Add RGIs without intersections (by finding the closest polygon):\n",
    "# for points where polygon intersection is NaN (about a 1000)\n",
    "no_match_df = df_pmb[df_pmb.RGIId.isna()]\n",
    "geometry = [\n",
    "    Point(lon, lat)\n",
    "    for lon, lat in zip(no_match_df[\"POINT_LON\"], no_match_df[\"POINT_LAT\"])\n",
    "]\n",
    "points_gdf = gpd.GeoDataFrame(no_match_df,\n",
    "                              geometry=geometry,\n",
    "                              crs=glacier_outline.crs)\n",
    "for index in tqdm(no_match_df.index):\n",
    "    point = points_gdf.loc[index]['geometry']\n",
    "    polygon_index = glacier_outline.distance(point).sort_values().index[0]\n",
    "    closest_rgi = glacier_outline.loc[polygon_index].RGIId\n",
    "    df_pmb.at[index, 'RGIId'] = closest_rgi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at identified RGIs per glacier:\n",
    "rgiids6 = df_pmb[['GLACIER',\n",
    "                  'RGIId']].sort_values(by='GLACIER').drop_duplicates()\n",
    "rgis = {}\n",
    "for gl in rgiids6.GLACIER.unique():\n",
    "    rgis[gl] = list(rgiids6[rgiids6.GLACIER == gl].RGIId)\n",
    "rgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual pre-processing and removal of errors:\n",
    "# Silvretta: weird outlier coordinate\n",
    "df_pmb_clean = df_pmb.copy()\n",
    "index_outlier = df_pmb_clean[(df_pmb_clean.GLACIER == 'silvretta')\n",
    "                             & (df_pmb_clean.POINT_LAT > 46.9)].index\n",
    "df_pmb_clean.drop(index_outlier, inplace=True)\n",
    "\n",
    "# and remove the stake that is on the neighbouring glacier:\n",
    "index_outlier = df_pmb_clean[(df_pmb_clean.GLACIER == 'silvretta')\n",
    "                             & (df_pmb_clean.RGIId != 'RGI60-11.00804')].index\n",
    "df_pmb_clean.drop(index_outlier, inplace=True)\n",
    "\n",
    "# Albigna: different rgis, remove stakes that are for two neighbouring glaciers:\n",
    "index_outlier = df_pmb_clean[(df_pmb_clean.GLACIER == 'albigna')\n",
    "                             & (df_pmb_clean.RGIId != 'RGI60-11.02285')].index\n",
    "df_pmb_clean.drop(index_outlier, inplace=True)\n",
    "\n",
    "# Err glacier: remove stakes that are on neighbouring glacier:\n",
    "index_outlier = df_pmb_clean[(df_pmb_clean.GLACIER == 'err')\n",
    "                             & (df_pmb_clean.RGIId != 'RGI60-11.01516')].index\n",
    "df_pmb_clean.drop(index_outlier, inplace=True)\n",
    "\n",
    "# Gries: weird outlier coordinate\n",
    "index_outlier = df_pmb_clean[(df_pmb_clean.GLACIER == 'gries')\n",
    "                             & (df_pmb_clean.RGIId != 'RGI60-11.01876')].index\n",
    "df_pmb_clean.drop(index_outlier, inplace=True)\n",
    "\n",
    "# Limmern: three stakes on neighbouring glacier\n",
    "index_outlier = df_pmb_clean[(df_pmb_clean.GLACIER == 'limmern')\n",
    "                             & (df_pmb_clean.RGIId != 'RGI60-11.00918')].index\n",
    "df_pmb_clean.drop(index_outlier, inplace=True)\n",
    "\n",
    "# Offental: on no RGI v6 outline\n",
    "df_pmb_clean = df_pmb_clean[df_pmb_clean.GLACIER != 'ofental']\n",
    "\n",
    "# Orny: change to correct RGIId\n",
    "index_outlier = df_pmb_clean[(df_pmb_clean.GLACIER == 'orny')].index\n",
    "for i in index_outlier:\n",
    "    df_pmb_clean.at[i, 'RGIId'] = 'RGI60-11.02775'\n",
    "\n",
    "# Plattalva: change to correct RGIId\n",
    "index_outlier = df_pmb_clean[(df_pmb_clean.GLACIER == 'plattalva')].index\n",
    "for i in index_outlier:\n",
    "    df_pmb_clean.at[i, 'RGIId'] = 'RGI60-11.00892'\n",
    "\n",
    "# Rhone outliers outside of RGI:\n",
    "index_outlier = df_pmb_clean[(df_pmb_clean.GLACIER == 'rhone')\n",
    "                             & (df_pmb_clean.POINT_LAT < 46.58)].index\n",
    "df_pmb_clean.drop(index_outlier, inplace=True)\n",
    "\n",
    "# Blauschnee, not in any RGI v6 outline:\n",
    "df_pmb_clean = df_pmb_clean[df_pmb_clean.GLACIER != 'blauschnee']\n",
    "\n",
    "# Remove unteraar\n",
    "df_pmb_clean = df_pmb_clean[df_pmb_clean.GLACIER != 'unteraar']\n",
    "\n",
    "# Look at identified RGIs per glacier:\n",
    "rgiids6 = df_pmb_clean[['GLACIER',\n",
    "                        'RGIId']].sort_values(by='GLACIER').drop_duplicates()\n",
    "rgis = {}\n",
    "for gl in rgiids6.GLACIER.unique():\n",
    "    rgis[gl] = list(rgiids6[rgiids6.GLACIER == gl].RGIId)\n",
    "print('RGIs after pre-processing:')\n",
    "rgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv:\n",
    "df_pmb_clean.to_csv(path_PMB_GLAMOS_csv + 'df_pmb_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmb_clean.GLACIER.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cut from 1961:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to start of MS data (1961) or ERA5-Land data (1950):\n",
    "df_pmb_60s = df_pmb_clean[df_pmb_clean.YEAR > 1960].sort_values(\n",
    "    by=['GLACIER', 'YEAR'], ascending=[True, True])\n",
    "print('Number of winter and annual samples:', len(df_pmb_60s))\n",
    "print('Number of annual samples:',\n",
    "      len(df_pmb_60s[df_pmb_60s.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_pmb_60s[df_pmb_60s.PERIOD == 'winter']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of measurements per year:\n",
    "df_pmb_60s.groupby(['YEAR', 'PERIOD']).size().unstack().plot(kind='bar',\n",
    "                                                             stacked=True,\n",
    "                                                             figsize=(15, 5))\n",
    "plt.title('Number of total measurements per year since 1961')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gl = df_pmb_60s.groupby(['GLACIER']).size().sort_values()\n",
    "num_gl.plot(kind='bar', figsize=(15, 5))\n",
    "plt.title('Number of total measurements per glacier since 1961')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot number of measurements per year\n",
    "# Number of measurements per glacier per year:\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
    "num_gl_yr = df_pmb_60s[df_pmb_60s.GLACIER == 'claridenL'].groupby(\n",
    "    ['YEAR', 'PERIOD']).size().unstack().reset_index()\n",
    "num_gl_yr.plot(x='YEAR',\n",
    "               kind='bar',\n",
    "               stacked=True,\n",
    "               ax=ax,\n",
    "               title=f'{glacierName}')\n",
    "ax.set_ylabel('Number of measurements')\n",
    "ax.set_title(f'Number of measurements per year: {glacierName}', fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Merge stakes that are close: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean df_pmb_60s\n",
    "# Group similar stakes:\n",
    "df_pmb_60s_clean = pd.DataFrame()\n",
    "for gl in tqdm(df_pmb_60s.GLACIER.unique(), desc='glacier', position=0):\n",
    "    print('----------------\\n', gl, ':\\n----------------')\n",
    "    df_gl = df_pmb_60s[df_pmb_60s.GLACIER == gl]\n",
    "    df_gl_cleaned = remove_close_points(df_gl)\n",
    "    df_pmb_60s_clean = pd.concat([df_pmb_60s_clean, df_gl_cleaned])\n",
    "\n",
    "# save to csv:\n",
    "df_pmb_60s_clean.drop(['x', 'y', 'point'], axis=1).to_csv(\n",
    "    path_PMB_GLAMOS_csv + 'df_pmb_60s_clean.csv', index=False)\n",
    "\n",
    "# save coordinates:\n",
    "df_pmb_60s_clean[['GLACIER', 'POINT_ID', 'POINT_LAT', 'POINT_LON', 'PERIOD'\n",
    "                  ]].to_csv(path_PMB_GLAMOS_csv + 'coordinate_60s_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of measurements per glacier per year:\n",
    "num_gl_yr = df_pmb_60s_clean.groupby(['GLACIER', 'YEAR', 'PERIOD'\n",
    "                                      ]).size().unstack().reset_index()\n",
    "\n",
    "# Plot one glacier per column:\n",
    "big_gl = num_gl[num_gl > 250].index.sort_values()\n",
    "num_glaciers = len(big_gl)\n",
    "fig, ax = plt.subplots(num_glaciers, 1, figsize=(15, 5 * num_glaciers))\n",
    "for i, gl in enumerate(big_gl):\n",
    "    num_gl_yr[num_gl_yr.GLACIER == gl].plot(x='YEAR',\n",
    "                                            kind='bar',\n",
    "                                            stacked=True,\n",
    "                                            ax=ax[i],\n",
    "                                            title=gl)\n",
    "    ax[i].set_ylabel('Number of measurements')\n",
    "    ax[i].set_title(gl, fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add topographical information from OGGM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the column name for the column that has the RGI IDs for each of the stakes\n",
    "# dataset = mbm.Dataset(data=df_pmb_clean,\n",
    "#                       region_name='CH',\n",
    "#                       data_path='')\n",
    "# # Specify the topographical features of interest\n",
    "# # Please see the OGGM documentation what variables are available: https://oggm.org/tutorials/stable/notebooks/10minutes/machine_learning.html ('topo', 'slope_factor', 'dis_from_border')\n",
    "# voi_topographical = ['aspect', 'slope']\n",
    "\n",
    "# # Retrieve the topographical features for each stake measurement and add them to the dataset\n",
    "# dataset.get_topo_features(vois=voi_topographical)\n",
    "\n",
    "# Switch to oggm datapulling notebook:\n",
    "df_pmb_topo = pd.read_csv(path_PMB_GLAMOS_csv + 'CH_wgms_dataset.csv')\n",
    "\n",
    "# Change from mm w.e. to m w.e.:\n",
    "df_pmb_topo['POINT_BALANCE'] = df_pmb_topo['POINT_BALANCE'] / 1000\n",
    "\n",
    "print('Number of winter and annual samples:', len(df_pmb_topo))\n",
    "print('Number of annual samples:',\n",
    "      len(df_pmb_topo[df_pmb_topo.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_pmb_topo[df_pmb_topo.PERIOD == 'winter']))\n",
    "df_pmb_topo.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of mass balance\n",
    "df_pmb_topo['POINT_BALANCE'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give new stake IDs:\n",
    "Give new stake IDs with glacier name and then a number according to the elevation. This is because accross glaciers some stakes have the same ID which is not practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for glacierName in tqdm(df_pmb_topo.GLACIER.unique(), desc = 'glaciers'):\n",
    "    gl_data = df_pmb_topo[df_pmb_topo.GLACIER == glacierName]\n",
    "    stakeIDS = gl_data.groupby('POINT_ID')[[\n",
    "        'POINT_LAT', 'POINT_LON', 'POINT_ELEVATION'\n",
    "    ]].mean()\n",
    "    stakeIDS.reset_index(inplace=True)\n",
    "    # Change the ID according to elevation\n",
    "    new_ids = stakeIDS[['POINT_ID',\n",
    "                        'POINT_ELEVATION']].sort_values(by='POINT_ELEVATION')\n",
    "    new_ids['POINT_ID_new'] = [\n",
    "        f'{glacierName}_{i}' for i in range(len(new_ids))\n",
    "    ]\n",
    "    for i, row in new_ids.iterrows():\n",
    "        df_pmb_topo.loc[(df_pmb_topo.GLACIER == glacierName) &\n",
    "                        (df_pmb_topo.POINT_ID == row.POINT_ID),\n",
    "                        'POINT_ID'] = row.POINT_ID_new\n",
    "# Save to csv:\n",
    "df_pmb_topo.to_csv(path_PMB_GLAMOS_csv + 'CH_wgms_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of mass balance\n",
    "df_pmb_topo['POINT_BALANCE'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glacier wide MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all files with pmb (for winter and annual mb):\n",
    "glamosfiles_smb = []\n",
    "for file in os.listdir(path_SMB_GLAMOS_raw):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(path_SMB_GLAMOS_raw,\n",
    "                                   file)) and 'obs' in file:\n",
    "        glamosfiles_smb.append(file)\n",
    "print('Examples of index stake raw files:\\n', glamosfiles_smb[:5])\n",
    "# Transform all files to csv\n",
    "RUN = True\n",
    "if RUN:\n",
    "    emptyfolder(path_SMB_GLAMOS_csv + 'obs/')\n",
    "    for file in glamosfiles_smb:\n",
    "        fileName = re.split('.dat', file)[0]\n",
    "        processDatFileGLWMB(fileName, path_SMB_GLAMOS_raw,\n",
    "                            path_SMB_GLAMOS_csv + 'obs/')\n",
    "\n",
    "# Example:\n",
    "fileName = 'aletsch_obs.csv'\n",
    "aletsch_csv = pd.read_csv(path_SMB_GLAMOS_csv + 'obs/' + fileName,\n",
    "                          sep=',',\n",
    "                          header=0,\n",
    "                          encoding='latin-1')\n",
    "aletsch_csv.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all files with pmb (for winter and annual mb):\n",
    "glamosfiles_smb = []\n",
    "for file in os.listdir(path_SMB_GLAMOS_raw):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(path_SMB_GLAMOS_raw,\n",
    "                                   file)) and 'fix' in file:\n",
    "        glamosfiles_smb.append(file)\n",
    "print('Examples of index stake raw files:\\n', glamosfiles_smb[:5])\n",
    "# Transform all files to csv\n",
    "RUN = True\n",
    "if RUN:\n",
    "    emptyfolder(path_SMB_GLAMOS_csv + 'fix/')\n",
    "    for file in glamosfiles_smb:\n",
    "        fileName = re.split('.dat', file)[0]\n",
    "        processDatFileGLWMB(fileName, path_SMB_GLAMOS_raw,\n",
    "                            path_SMB_GLAMOS_csv + 'fix/')\n",
    "\n",
    "# Example:\n",
    "fileName = 'aletsch_fix.csv'\n",
    "aletsch_csv = pd.read_csv(path_SMB_GLAMOS_csv + 'fix/' + fileName,\n",
    "                          sep=',',\n",
    "                          header=0,\n",
    "                          encoding='latin-1')\n",
    "aletsch_csv.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
