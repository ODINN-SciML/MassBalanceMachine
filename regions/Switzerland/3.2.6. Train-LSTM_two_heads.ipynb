{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from skorch.helper import SliceDataset\n",
    "from datetime import datetime\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "import itertools\n",
    "import random\n",
    "from torch.utils.data import WeightedRandomSampler, SubsetRandomSampler\n",
    "\n",
    "import pickle \n",
    "from collections import Counter\n",
    "import ast\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.geodata_plots import *\n",
    "from scripts.lstm import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'\n",
    "\n",
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "\n",
    "vois_climate = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "# Capitalize glacier names:\n",
    "glacierCap = {}\n",
    "for gl in data_glamos['GLACIER'].unique():\n",
    "    if isinstance(gl, str):  # Ensure the glacier name is a string\n",
    "        if gl.lower() == 'claridenu':\n",
    "            glacierCap[gl] = 'Clariden_U'\n",
    "        elif gl.lower() == 'claridenl':\n",
    "            glacierCap[gl] = 'Clariden_L'\n",
    "        else:\n",
    "            glacierCap[gl] = gl.capitalize()\n",
    "    else:\n",
    "        print(f\"Warning: Non-string glacier name encountered: {gl}\")\n",
    "\n",
    "# drop taelliboden and plainemorte if in there\n",
    "if 'taelliboden' in data_glamos['GLACIER'].unique():\n",
    "    data_glamos = data_glamos[data_glamos['GLACIER'] != 'taelliboden']\n",
    "if 'plainemorte' in data_glamos['GLACIER'].unique():\n",
    "    data_glamos = data_glamos[data_glamos['GLACIER'] != 'plainemorte']\n",
    "\n",
    "print('-------------------')\n",
    "print('Number of glaciers:', len(data_glamos['GLACIER'].unique()))\n",
    "print('Number of winter and annual samples:', len(data_glamos))\n",
    "print('Number of annual samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'winter']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:\n",
    "### Input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM.csv')\n",
    "data_monthly = dataloader_gl.data\n",
    "\n",
    "data_monthly['GLWD_ID'] = data_monthly.apply(\n",
    "    lambda x: mbm.data_processing.utils.get_hash(f\"{x.GLACIER}_{x.YEAR}\"),\n",
    "    axis=1)\n",
    "data_monthly['GLWD_ID'] = data_monthly['GLWD_ID'].astype(str)\n",
    "\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)\n",
    "\n",
    "data_annual = dataloader_gl.data[dataloader_gl.data.PERIOD == 'annual']\n",
    "\n",
    "# print mean and std of N_MONTHS\n",
    "print('Mean number of months:', data_annual.N_MONTHS.mean())\n",
    "print('Std number of months:', data_annual.N_MONTHS.std())\n",
    "\n",
    "# same for winter\n",
    "data_winter = dataloader_gl.data[dataloader_gl.data.PERIOD == 'winter']\n",
    "print('Mean number of months (winter):', data_winter.N_MONTHS.mean())\n",
    "print('Std number of months (winter):', data_winter.N_MONTHS.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocking on glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in TEST_GLACIERS if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in TEST_GLACIERS]\n",
    "\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(TEST_GLACIERS)]\n",
    "print('Size of monthly test data:', len(data_test))\n",
    "\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train))\n",
    "\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "# Number of annual versus winter measurements:\n",
    "print('-------------\\nTrain:')\n",
    "print('Number of monthly winter and annual samples:', len(data_train))\n",
    "print('Number of monthly annual samples:',\n",
    "      len(data_train[data_train.PERIOD == 'annual']))\n",
    "print('Number of monthly winter samples:',\n",
    "      len(data_train[data_train.PERIOD == 'winter']))\n",
    "\n",
    "# Same for test\n",
    "data_test_annual = data_test[data_test.PERIOD == 'annual']\n",
    "data_test_winter = data_test[data_test.PERIOD == 'winter']\n",
    "\n",
    "print('Test:')\n",
    "print('Number of monthly winter and annual samples:', len(data_test))\n",
    "print('Number of monthly annual samples:', len(data_test_annual))\n",
    "print('Number of monthly winter samples:', len(data_test_winter))\n",
    "\n",
    "print('Total:')\n",
    "print('Number of monthly rows:', len(dataloader_gl.data))\n",
    "print('Number of annual rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'annual']))\n",
    "print('Number of winter rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'winter']))\n",
    "\n",
    "# same for original data:\n",
    "print('-------------\\nIn annual format:')\n",
    "print('Number of annual train rows:',\n",
    "      len(data_glamos[data_glamos.GLACIER.isin(train_glaciers)]))\n",
    "print('Number of annual test rows:',\n",
    "      len(data_glamos[data_glamos.GLACIER.isin(TEST_GLACIERS)]))\n",
    "\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=TEST_GLACIERS,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))\n",
    "\n",
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "\n",
    "data_test = test_set['df_X']\n",
    "data_test['y'] = test_set['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = ['t2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'pcsr', 'ELEVATION_DIFFERENCE', ]\n",
    "STATIC_COLS = [\n",
    "    'aspect_sgi', 'slope_sgi', 'hugonnet_dhdt',\n",
    "    'consensus_ice_thickness', 'millan_v'\n",
    "]\n",
    "\n",
    "HYDRO_MONTHS = [\n",
    "    'oct', 'nov', 'dec', 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul',\n",
    "    'aug', 'sep'\n",
    "]\n",
    "HYDRO_POS = {m: i for i, m in enumerate(HYDRO_MONTHS)}\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LSTM dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data_train.copy()\n",
    "df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "df_test = data_test.copy()\n",
    "df_test['PERIOD'] = df_test['PERIOD'].str.strip().str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- build train dataset from dataframe ---\n",
    "ds_train = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_train, MONTHLY_COLS, STATIC_COLS, HYDRO_POS, expect_target=True)\n",
    "\n",
    "# --- make train/val loaders and get split indices (scalers fit on TRAIN, applied to whole ds) ---\n",
    "train_dl, val_dl, train_idx, val_idx = ds_train.make_loaders(\n",
    "    val_ratio=0.2,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# sanity checks\n",
    "print(\"y mean/std (train):\", float(ds_train.y_mean), float(ds_train.y_std))\n",
    "for name, arr in [(\"Xm\", ds_train.Xm), (\"Xs\", ds_train.Xs), (\"y\", ds_train.y)]:\n",
    "    print(name, \"has NaN?\", bool(torch.isnan(arr).any().item()))\n",
    "print(\"Train counts:\", int(ds_train.iw[train_idx].sum()), \"winter |\",\n",
    "      int(ds_train.ia[train_idx].sum()), \"annual\")\n",
    "print(\"Val   counts:\", int(ds_train.iw[val_idx].sum()), \"winter |\",\n",
    "      int(ds_train.ia[val_idx].sum()), \"annual\")\n",
    "\n",
    "# --- build test dataset and loader (reuses train scalers) ---\n",
    "ds_test = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_test,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    HYDRO_POS,\n",
    "    expect_target=True  # you said test has real targets\n",
    ")\n",
    "test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test, ds_train, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_w, N_a = int(ds_train.iw.sum()), int(ds_train.ia.sum())\n",
    "w_annual = N_w / max(N_a, 1)\n",
    "w_annual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define & train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load grid search results:\n",
    "gs_results = pd.read_csv(\n",
    "    'logs/lstm_param_search_progress_2025-09-06_SEB_all_OGGM.csv').sort_values(\n",
    "        by='valid_loss', ascending=True)\n",
    "\n",
    "gs_results['avg_test_loss'] = (gs_results['test_rmse_a'] +\n",
    "                               gs_results['test_rmse_w']) / 2\n",
    "gs_results = gs_results.sort_values(by='avg_test_loss', ascending=True)\n",
    "best_params = gs_results.iloc[0].to_dict()\n",
    "\n",
    "print('Best parameters from grid search:')\n",
    "for key, value in best_params.items():\n",
    "    if key not in ['valid_loss', 'train_loss']:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "gs_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fm = ds_train.Xm.shape[-1]\n",
    "Fs = ds_train.Xs.shape[-1]\n",
    "\n",
    "params = {\n",
    "    'hidden_size': 128,\n",
    "    'num_layers': 2,\n",
    "    'bidirectional': True,\n",
    "    'dropout': 0.3,\n",
    "    'static_hidden': 64,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 0.0001,\n",
    "    'batch_size': 32,\n",
    "}\n",
    "\n",
    "# Build model\n",
    "model = mbm.models.LSTM_MB(Fm=ds_train.Xm.shape[-1],\n",
    "                           Fs=ds_train.Xs.shape[-1],\n",
    "                           hidden_size=params['hidden_size'],\n",
    "                           num_layers=params['num_layers'],\n",
    "                           bidirectional=params['bidirectional'],\n",
    "                           dropout=params['dropout'],\n",
    "                           static_hidden=params['static_hidden'],\n",
    "                           two_heads=True,\n",
    "                           head_dropout=0.1).to(device)\n",
    "\n",
    "TRAIN = True\n",
    "if TRAIN:\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    model_filename = f\"models/lstm_model_{current_date}_two_heads.pt\"\n",
    "\n",
    "    history, best_val, best_state = model.train_loop(\n",
    "        device=device,\n",
    "        train_dl=train_dl,\n",
    "        val_dl=val_dl,\n",
    "        epochs=150,\n",
    "        lr=params['lr'],\n",
    "        weight_decay=params['weight_decay'],\n",
    "        clip_val=1.0,\n",
    "        loss_fn=lambda outs, b: mbm.models.LSTM_MB.seasonal_mse_weighted(\n",
    "            outs, b, w_winter=1.0, w_annual=w_annual),\n",
    "        # scheduler\n",
    "        sched_factor=0.5,\n",
    "        sched_patience=6,\n",
    "        sched_threshold=0.01,\n",
    "        sched_threshold_mode=\"rel\",\n",
    "        sched_cooldown=1,\n",
    "        sched_min_lr=1e-6,\n",
    "        # early stopping\n",
    "        es_patience=15,\n",
    "        es_min_delta=1e-4,\n",
    "        # logging\n",
    "        log_every=5,\n",
    "        verbose=True,\n",
    "        # optional checkpoint\n",
    "        save_best_path=model_filename,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best weights\n",
    "best_state = torch.load(f\"models/lstm_model_2025-09-09_two_heads.pt\",\n",
    "                        map_location=device)\n",
    "model.load_state_dict(best_state)\n",
    "\n",
    "# Evaluate on test for this config\n",
    "test_metrics, test_df_preds = mbm.models.LSTM_MB.evaluate_with_preds(\n",
    "    model, device, test_dl, ds_test)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_annual, scores_winter = compute_seasonal_scores(test_df_preds,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "print(\"Annual scores:\", scores_annual)\n",
    "print(\"Winter scores:\", scores_winter)\n",
    "\n",
    "fig = plot_predictions_summary(grouped_ids=test_df_preds,\n",
    "                               scores_annual=scores_annual,\n",
    "                               scores_winter=scores_winter,\n",
    "                               predVSTruth=predVSTruth,\n",
    "                               plotMeanPred=plotMeanPred,\n",
    "                               ax_xlim=(-8, 6),\n",
    "                               ax_ylim=(-8, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_per_el = data_glamos[data_glamos.PERIOD == 'annual'].groupby(\n",
    "    ['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "\n",
    "test_gl_per_el = gl_per_el[TEST_GLACIERS].sort_values().index\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(20, 15), sharex=True)\n",
    "\n",
    "PlotIndividualGlacierPredVsTruth(test_df_preds,\n",
    "                                 axs=axs,\n",
    "                                 color_annual=color_dark_blue,\n",
    "                                 color_winter=color_pink,\n",
    "                                 custom_order=test_gl_per_el)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_space = {\n",
    "#     \"hidden_size\": [64, 128, 256],\n",
    "#     \"num_layers\": [1, 2],\n",
    "#     \"bidirectional\": [True, False],\n",
    "#     \"dropout\": [0.0, 0.1, 0.3],\n",
    "#     \"static_hidden\": [32, 64, 128],\n",
    "#     \"lr\": [1e-3, 5e-4, 1e-4],\n",
    "#     \"weight_decay\": [1e-4, 1e-5],\n",
    "#     \"batch_size\": [32, 64, 128],\n",
    "# }\n",
    "\n",
    "# def sample_param_combinations(space, n_samples):\n",
    "#     all_keys = list(space.keys())\n",
    "#     all_vals = list(space.values())\n",
    "#     all_combinations = list(itertools.product(*all_vals))\n",
    "#     random.shuffle(all_combinations)\n",
    "#     sampled = all_combinations[:n_samples]\n",
    "#     return [dict(zip(all_keys, vals)) for vals in sampled]\n",
    "\n",
    "# sampled_params = sample_param_combinations(param_space, n_samples=100)\n",
    "# sampled_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN = False\n",
    "# if RUN:\n",
    "#     # make sure logs dir exists\n",
    "#     os.makedirs(\"logs\", exist_ok=True)\n",
    "#     log_filename = f'logs/lstm_param_search_progress_{datetime.now().strftime(\"%Y-%m-%d\")}_SEB_all_OGGM.csv'\n",
    "\n",
    "#     # create log with header\n",
    "#     with open(log_filename, mode='w', newline='') as log_file:\n",
    "#         writer = csv.DictWriter(log_file,\n",
    "#                                 fieldnames=list(sampled_params[0].keys()) +\n",
    "#                                 ['valid_loss', 'test_rmse_a', 'test_rmse_w'])\n",
    "#         writer.writeheader()\n",
    "\n",
    "#     results = []\n",
    "#     best_overall = {\"val\": float('inf'), \"row\": None, \"params\": None}\n",
    "\n",
    "#     for i, params in enumerate(sampled_params):\n",
    "#         seed_all(cfg.seed)\n",
    "#         print(f\"\\n--- Running config {i+1}/{len(sampled_params)} ---\")\n",
    "#         print(params)\n",
    "\n",
    "#         # Build model\n",
    "#         model = mbm.models.LSTM_MB(\n",
    "#             Fm=ds_train.Xm.shape[-1],\n",
    "#             Fs=ds_train.Xs.shape[-1],\n",
    "#             hidden_size=params['hidden_size'],\n",
    "#             num_layers=params['num_layers'],\n",
    "#             bidirectional=params['bidirectional'],\n",
    "#             dropout=params['dropout'],\n",
    "#             static_hidden=params['static_hidden'],\n",
    "#             two_heads=True,\n",
    "#             head_dropout=params['head_dropout']).to(device)\n",
    "\n",
    "#         history, best_val, best_state = model.train_loop(\n",
    "#             device=device,\n",
    "#             train_dl=train_dl,\n",
    "#             val_dl=val_dl,\n",
    "#             epochs=150,\n",
    "#             lr=params['lr'],\n",
    "#             weight_decay=params['weight_decay'],\n",
    "#             clip_val=1.0,\n",
    "#             # scheduler\n",
    "#             sched_factor=0.5,\n",
    "#             sched_patience=6,\n",
    "#             sched_threshold=0.01,\n",
    "#             sched_threshold_mode=\"rel\",\n",
    "#             sched_cooldown=1,\n",
    "#             sched_min_lr=1e-6,\n",
    "#             # early stopping\n",
    "#             es_patience=15,\n",
    "#             es_min_delta=1e-4,\n",
    "#             # logging\n",
    "#             log_every=5,\n",
    "#             verbose=False,\n",
    "#             # optional checkpoint\n",
    "#             save_best_path=\"models/best_lstm_mb_gs.pt\",\n",
    "#         )\n",
    "\n",
    "#         # Load the best weights\n",
    "#         best_state = torch.load(\"models/best_lstm_mb_gs.pt\",\n",
    "#                                 map_location=device)\n",
    "#         model.load_state_dict(best_state)\n",
    "\n",
    "#         # Evaluate on test for this config\n",
    "#         test_metrics, test_df_preds = mbm.models.LSTM_MB.evaluate_with_preds(\n",
    "#             model, device, test_dl, ds_test)\n",
    "#         test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "#             'RMSE_winter']\n",
    "\n",
    "#         # Log a row for this config\n",
    "#         row = {\n",
    "#             **params, 'valid_loss': float(best_val),\n",
    "#             'test_rmse_a': float(test_rmse_a),\n",
    "#             'test_rmse_w': float(test_rmse_w)\n",
    "#         }\n",
    "#         with open(log_filename, mode='a', newline='') as log_file:\n",
    "#             writer = csv.DictWriter(log_file, fieldnames=list(row.keys()))\n",
    "#             writer.writerow(row)\n",
    "\n",
    "#         results.append(row)\n",
    "\n",
    "#         # Track overall best by validation loss (or swap to test RMSE if you prefer)\n",
    "#         if best_val < best_overall['val']:\n",
    "#             best_overall = {\"val\": best_val, \"row\": row, \"params\": params}\n",
    "\n",
    "#     print(\"\\n=== Best config by validation loss ===\")\n",
    "#     print(best_overall['params'])\n",
    "#     print(best_overall['row'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def make_capped_grid(Fm, Fs):\n",
    "    \"\"\"\n",
    "    Returns exactly 150 configs:\n",
    "      - 144 with two_heads=True (broad coverage)\n",
    "      - 6 with two_heads=False (light baseline)\n",
    "    Design:\n",
    "      Trunks (12): hs in {96,128,192} x nl in {1,2} x bi in {True, False}\n",
    "        - if nl==1 -> dropout=0.0\n",
    "        - if nl==2 -> dropout=0.1\n",
    "      Static options (2): identity; 2-layer [128,64] with dropout 0.1\n",
    "      Head dropout (2): {0.0, 0.2}\n",
    "      Train combos (3): (lr, wd, loss) fixed tuples (see below)\n",
    "      Single-head slice: 3 representative trunks, static identity only, head_dropout=0.0,\n",
    "                         2 train combos.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- helper to package a config ----\n",
    "    def pack(trunk, static, two_heads, head_dropout, train_combo, loss_name):\n",
    "        hs, nl, bi, do = trunk\n",
    "        sl, sh, sd = static  # layers, hidden, static_dropout\n",
    "        lr, wd = train_combo\n",
    "        loss_spec = None if loss_name == \"neutral\" else (\"weighted\",\n",
    "                                                         dict(w_winter=1.0,\n",
    "                                                              w_annual=3.33))\n",
    "        return dict(\n",
    "            Fm=Fm,\n",
    "            Fs=Fs,\n",
    "            hidden_size=hs,\n",
    "            num_layers=nl,\n",
    "            bidirectional=bi,\n",
    "            dropout=do,\n",
    "            static_layers=sl,\n",
    "            static_hidden=sh,\n",
    "            static_dropout=sd,\n",
    "            two_heads=two_heads,\n",
    "            head_dropout=head_dropout,\n",
    "            lr=lr,\n",
    "            weight_decay=wd,\n",
    "            clip_val=1.0,  # fixed to keep the cap\n",
    "            sched_factor=0.5,\n",
    "            sched_patience=6,  # fixed to keep the cap\n",
    "            loss_name=loss_name,\n",
    "            loss_spec=loss_spec)\n",
    "\n",
    "    # ---- trunk set (12 total) ----\n",
    "    # hs x nl x bi with pruned dropout rule\n",
    "    trunks = []\n",
    "    for hs in [96, 128, 192]:\n",
    "        for nl in [1, 2]:\n",
    "            for bi in [True, False]:\n",
    "                do = 0.0 if nl == 1 else 0.1\n",
    "                trunks.append((hs, nl, bi, do))\n",
    "    assert len(trunks) == 12\n",
    "\n",
    "    # ---- static options (2 total) ----\n",
    "    static_id = (0, None, None)  # identity\n",
    "    static_2L = (2, [128, 64], 0.1)  # two-layer MLP with modest dropout\n",
    "    statics = [static_id, static_2L]\n",
    "\n",
    "    # ---- head dropout (2) ----\n",
    "    head_dropouts = [0.0, 0.2]\n",
    "\n",
    "    # ---- training combos & loss variants ----\n",
    "    # Three combos for the two-head grid:\n",
    "    train_combos_th = [\n",
    "        (3e-4, 0.0),  # lr, wd\n",
    "        (1e-3, 1e-4),\n",
    "        (3e-4, 1e-4),\n",
    "    ]\n",
    "    loss_names_th = [\"weighted\", \"neutral\", \"weighted\"]  # align length=3\n",
    "\n",
    "    # Two combos for the single-head slice:\n",
    "    train_combos_sh = [\n",
    "        (3e-4, 0.0),\n",
    "        (1e-3, 1e-4),\n",
    "    ]\n",
    "    loss_names_sh = [\"weighted\", \"neutral\"]\n",
    "\n",
    "    grid = []\n",
    "\n",
    "    # ---- 144 two-head configs ----\n",
    "    for trunk in trunks:\n",
    "        for static in statics:\n",
    "            for hd in head_dropouts:\n",
    "                for (lr, wd), loss_name in zip(train_combos_th, loss_names_th):\n",
    "                    grid.append(\n",
    "                        pack(trunk, static, True, hd, (lr, wd), loss_name))\n",
    "\n",
    "    # ---- 6 single-head configs (light baseline) ----\n",
    "    # Choose 3 representative trunks (covering nl/bidirectional variety)\n",
    "    rep_trunks = [\n",
    "        (128, 1, True, 0.0),\n",
    "        (128, 2, True, 0.1),\n",
    "        (192, 2, False, 0.1),\n",
    "    ]\n",
    "    for trunk, (lr, wd), loss_name in zip(\n",
    "            itertools.chain.from_iterable([[t] * 2 for t in rep_trunks\n",
    "                                           ]),  # repeat each trunk 2 times\n",
    "            train_combos_sh * 3,\n",
    "            loss_names_sh * 3):\n",
    "        grid.append(pack(trunk, static_id, False, 0.0, (lr, wd), loss_name))\n",
    "\n",
    "    assert len(grid) == 150\n",
    "    return grid\n",
    "\n",
    "\n",
    "def build_model_from_params(params, mbm, device):\n",
    "    return mbm.models.LSTM_MB(\n",
    "        Fm=params['Fm'],\n",
    "        Fs=params['Fs'],\n",
    "        hidden_size=params['hidden_size'],\n",
    "        num_layers=params['num_layers'],\n",
    "        bidirectional=params['bidirectional'],\n",
    "        dropout=params['dropout'],\n",
    "        static_hidden=params['static_hidden'],\n",
    "        static_layers=params['static_layers'],\n",
    "        static_dropout=params['static_dropout'],\n",
    "        two_heads=params['two_heads'],\n",
    "        head_dropout=params['head_dropout'],\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "def resolve_loss_fn(params, mbm):\n",
    "    if params['loss_spec'] is None:\n",
    "        return mbm.models.LSTM_MB.custom_loss\n",
    "    kind, kw = params['loss_spec']\n",
    "    if kind == \"weighted\":\n",
    "        return partial(mbm.models.LSTM_MB.seasonal_mse_weighted, **kw)\n",
    "    return mbm.models.LSTM_MB.custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_params = make_capped_grid(ds_train.Xm.shape[-1], ds_train.Xs.shape[-1])\n",
    "print(\"Total configs:\", len(sampled_params))  # should print 150\n",
    "sampled_params[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = True\n",
    "if RUN:\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "    log_filename = f'logs/lstm_param_search_progress_{datetime.now().strftime(\"%Y-%m-%d\")}_SEB_all_OGGM.csv'\n",
    "\n",
    "    # create log with header\n",
    "    with open(log_filename, mode='w', newline='') as log_file:\n",
    "        writer = csv.DictWriter(log_file,\n",
    "                                fieldnames=list(sampled_params[0].keys()) +\n",
    "                                ['valid_loss', 'test_rmse_a', 'test_rmse_w'])\n",
    "        writer.writeheader()\n",
    "\n",
    "    results = []\n",
    "    best_overall = {\"val\": float('inf'), \"row\": None, \"params\": None}\n",
    "\n",
    "    for i, params in enumerate(sampled_params):\n",
    "        seed_all(cfg.seed)\n",
    "        print(f\"\\n--- Running config {i+1}/{len(sampled_params)} ---\")\n",
    "        print(params)\n",
    "\n",
    "        # Build model\n",
    "        model = build_model_from_params(params, mbm, device)\n",
    "\n",
    "        # Choose loss\n",
    "        loss_fn = resolve_loss_fn(params, mbm)\n",
    "\n",
    "        # Train\n",
    "        history, best_val, best_state = model.train_loop(\n",
    "            device=device,\n",
    "            train_dl=train_dl,\n",
    "            val_dl=val_dl,\n",
    "            epochs=150,\n",
    "            lr=params['lr'],\n",
    "            weight_decay=params['weight_decay'],\n",
    "            clip_val=params['clip_val'],\n",
    "            # scheduler\n",
    "            sched_factor=params['sched_factor'],\n",
    "            sched_patience=params['sched_patience'],\n",
    "            sched_threshold=0.01,\n",
    "            sched_threshold_mode=\"rel\",\n",
    "            sched_cooldown=1,\n",
    "            sched_min_lr=1e-6,\n",
    "            # early stopping\n",
    "            es_patience=15,\n",
    "            es_min_delta=1e-4,\n",
    "            # logging\n",
    "            log_every=5,\n",
    "            verbose=False,\n",
    "            # checkpoint\n",
    "            save_best_path=\"models/best_lstm_mb_gs.pt\",\n",
    "            loss_fn=loss_fn,\n",
    "        )\n",
    "\n",
    "        # Load the best weights\n",
    "        best_state = torch.load(\"models/best_lstm_mb_gs.pt\",\n",
    "                                map_location=device)\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        # Evaluate on test\n",
    "        test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "            device, test_dl, ds_test)\n",
    "        test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "            'RMSE_winter']\n",
    "\n",
    "        # Log row\n",
    "        row = {\n",
    "            **params, 'valid_loss': float(best_val),\n",
    "            'test_rmse_a': float(test_rmse_a),\n",
    "            'test_rmse_w': float(test_rmse_w)\n",
    "        }\n",
    "        with open(log_filename, mode='a', newline='') as log_file:\n",
    "            writer = csv.DictWriter(log_file, fieldnames=list(row.keys()))\n",
    "            writer.writerow(row)\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "        # Track best by validation loss\n",
    "        if best_val < best_overall['val']:\n",
    "            best_overall = {\"val\": best_val, \"row\": row, \"params\": params}\n",
    "\n",
    "    print(\"\\n=== Best config by validation loss ===\")\n",
    "    print(best_overall['params'])\n",
    "    print(best_overall['row'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrapolate in space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodetic_mb = get_geodetic_MB(cfg)\n",
    "\n",
    "# get years per glacier\n",
    "years_start_per_gl = geodetic_mb.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodetic_mb.groupby('glacier_name')['Aend'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "periods_per_glacier, geoMB_per_glacier = build_periods_per_glacier(geodetic_mb)\n",
    "\n",
    "glacier_list = list(data_glamos.GLACIER.unique())\n",
    "print('Number of glaciers with pcsr:', len(glacier_list))\n",
    "\n",
    "geodetic_glaciers = periods_per_glacier.keys()\n",
    "print('Number of glaciers with geodetic MB:', len(geodetic_glaciers))\n",
    "\n",
    "# Intersection of both\n",
    "common_glaciers = list(set(geodetic_glaciers) & set(glacier_list))\n",
    "print('Number of common glaciers:', len(common_glaciers))\n",
    "\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area(cfg)\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list, gl_area):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "\n",
    "glacier_list = sort_by_area(common_glaciers, gl_area)\n",
    "glacier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "# Required by the dataset builder regardless of your feature list\n",
    "REQUIRED = ['GLACIER', 'YEAR', 'ID', 'PERIOD', 'MONTHS']\n",
    "\n",
    "# Paths\n",
    "path_save_glw = os.path.join(cfg.dataPath, 'GLAMOS', 'distributed_MB_grids',\n",
    "                             'MBM/testing_LSTM/LSTM_two_heads')\n",
    "os.makedirs(path_save_glw, exist_ok=True)\n",
    "path_xr_grids = os.path.join(cfg.dataPath, 'GLAMOS', 'topo', 'GLAMOS_DEM',\n",
    "                             'xr_masked_grids')\n",
    "\n",
    "# Load model once\n",
    "\n",
    "ckpt_path = f\"models/lstm_model_2025-09-08_two_heads.pt\"\n",
    "best_state = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(best_state)\n",
    "model.eval()\n",
    "\n",
    "RUN = True\n",
    "if RUN:\n",
    "    emptyfolder(path_save_glw)\n",
    "\n",
    "    for glacier_name in glacier_list:\n",
    "        glacier_path = os.path.join(cfg.dataPath + path_glacier_grid_glamos,\n",
    "                                    glacier_name)\n",
    "        if not os.path.exists(glacier_path):\n",
    "            print(f\"Folder not found for {glacier_name}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        glacier_files = sorted(\n",
    "            [f for f in os.listdir(glacier_path) if glacier_name in f])\n",
    "\n",
    "        geodetic_range = range(np.min(periods_per_glacier[glacier_name]),\n",
    "                               np.max(periods_per_glacier[glacier_name]) + 1)\n",
    "\n",
    "        years = [int(f.split('_')[2].split('.')[0]) for f in glacier_files]\n",
    "        years = [y for y in years if y in geodetic_range]\n",
    "\n",
    "        print(\n",
    "            f\"Processing {glacier_name} ({len(years)} files): {geodetic_range}\"\n",
    "        )\n",
    "\n",
    "        for year in tqdm(years, desc=f\"Processing {glacier_name}\",\n",
    "                         leave=False):\n",
    "            file_name = f\"{glacier_name}_grid_{year}.parquet\"\n",
    "            df_grid_monthly = pd.read_parquet(\n",
    "                os.path.join(cfg.dataPath + path_glacier_grid_glamos,\n",
    "                             glacier_name, file_name)).copy()\n",
    "\n",
    "            df_grid_monthly.drop_duplicates(inplace=True)\n",
    "\n",
    "            # Keep required + feature columns; DON'T drop PERIOD/MONTHS/YEAR/ID/GLACIER\n",
    "            keep = [\n",
    "                c for c in (set(all_columns) | set(REQUIRED))\n",
    "                if c in df_grid_monthly.columns\n",
    "            ]\n",
    "            df_grid_monthly = df_grid_monthly[keep]\n",
    "\n",
    "            # Ensure PERIOD is set to 'annual' BEFORE sequence building\n",
    "            if 'PERIOD' not in df_grid_monthly.columns:\n",
    "                df_grid_monthly['PERIOD'] = 'annual'\n",
    "            else:\n",
    "                df_grid_monthly['PERIOD'] = df_grid_monthly['PERIOD'].fillna(\n",
    "                    'annual')\n",
    "            df_grid_monthly['PERIOD'] = df_grid_monthly['PERIOD'].str.strip(\n",
    "            ).str.lower()\n",
    "\n",
    "            # (Optional) minimal NaN clean-up: only drop if ID or MONTHS missing\n",
    "            df_grid_monthly = df_grid_monthly.dropna(subset=['ID', 'MONTHS'])\n",
    "\n",
    "            # (Optional) hydrological coverage check\n",
    "            have = set(df_grid_monthly['MONTHS'].str.lower().unique())\n",
    "            need = {\n",
    "                'oct', 'nov', 'dec', 'jan', 'feb', 'mar', 'apr', 'may', 'jun',\n",
    "                'jul', 'aug', 'sep'\n",
    "            }\n",
    "            if not need.issubset(have):\n",
    "                missing = sorted(list(need - have))\n",
    "                print(\n",
    "                    f\"WARNING [{glacier_name} {year}]: missing hydro months: {missing}\"\n",
    "                )\n",
    "\n",
    "            # --- Build ds_gl WITHOUT targets ---\n",
    "            ds_gl = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "                df_grid_monthly,\n",
    "                MONTHLY_COLS,\n",
    "                STATIC_COLS,\n",
    "                HYDRO_POS,\n",
    "                expect_target=False,\n",
    "                show_progress=False)\n",
    "            gl_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "                ds_gl, ds_train,\n",
    "                batch_size=128)  # copies train scalers & transforms ds_gl\n",
    "\n",
    "            # Predict (no metrics)\n",
    "            df_preds = model.predict_with_keys(device, gl_dl, ds_gl)\n",
    "\n",
    "            # Join preds back to unique cell IDs for saving\n",
    "            data = df_preds[['ID', 'pred']].set_index('ID')\n",
    "            grouped_ids = df_grid_monthly.groupby('ID')[[\n",
    "                'YEAR', 'POINT_LAT', 'POINT_LON', 'GLWD_ID'\n",
    "            ]].first()\n",
    "            grouped_ids = grouped_ids.merge(data,\n",
    "                                            left_index=True,\n",
    "                                            right_index=True,\n",
    "                                            how='left')\n",
    "\n",
    "            months_per_id = df_grid_monthly.groupby('ID')['MONTHS'].unique()\n",
    "            grouped_ids = grouped_ids.merge(months_per_id,\n",
    "                                            left_index=True,\n",
    "                                            right_index=True)\n",
    "\n",
    "            grouped_ids.reset_index(inplace=True)\n",
    "            grouped_ids.sort_values(by='ID', inplace=True)\n",
    "            grouped_ids['PERIOD'] = 'annual'\n",
    "\n",
    "            pred_y_annual = grouped_ids.drop(columns=['YEAR'], errors='ignore')\n",
    "\n",
    "            # Save\n",
    "            path_glacier_dem = os.path.join(cfg.dataPath, path_xr_grids,\n",
    "                                            f\"{glacier_name}_{year}.zarr\")\n",
    "            ds = xr.open_dataset(path_glacier_dem)\n",
    "            geoData = mbm.geodata.GeoData(df_grid_monthly)\n",
    "            geoData._save_prediction(ds, pred_y_annual, glacier_name, year,\n",
    "                                     path_save_glw, \"annual\")\n",
    "\n",
    "# quick viz\n",
    "glacier_name = 'aletsch'\n",
    "year = 2008\n",
    "xr.open_dataset(os.path.join(path_save_glw, f'{glacier_name}/{glacier_name}_{year}_annual.zarr'))\\\n",
    "  .pred_masked.plot(cmap='RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers_in_glamos = os.listdir(path_save_glw)\n",
    "\n",
    "geodetic_mb = get_geodetic_MB(cfg)\n",
    "\n",
    "# get years per glacier\n",
    "years_start_per_gl = geodetic_mb.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodetic_mb.groupby('glacier_name')['Aend'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "periods_per_glacier, geoMB_per_glacier = build_periods_per_glacier(geodetic_mb)\n",
    "\n",
    "# Glaciers with geodetic MB data:\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area(cfg)\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list, gl_area):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "\n",
    "glacier_list = [\n",
    "    f for f in list(periods_per_glacier.keys()) if f in glaciers_in_glamos\n",
    "]\n",
    "glacier_list = sort_by_area(glacier_list, gl_area)\n",
    "print('Number of glaciers:', len(glacier_list))\n",
    "print('Glaciers:', glacier_list)\n",
    "\n",
    "df_all_nn = process_geodetic_mass_balance_comparison(\n",
    "    glacier_list=glacier_list,\n",
    "    path_SMB_GLAMOS_csv=cfg.dataPath + path_SMB_GLAMOS_csv,\n",
    "    periods_per_glacier=periods_per_glacier,\n",
    "    geoMB_per_glacier=geoMB_per_glacier,\n",
    "    gl_area=gl_area,\n",
    "    test_glaciers=TEST_GLACIERS,\n",
    "    path_predictions=path_save_glw,  # or another path if needed\n",
    "    cfg=cfg)\n",
    "\n",
    "# Drop rows where any required columns are NaN\n",
    "df_all_nn = df_all_nn.dropna(subset=['Geodetic MB', 'MBM MB'])\n",
    "df_all_nn = df_all_nn.sort_values(by='Area')\n",
    "df_all_nn['GLACIER'] = df_all_nn['GLACIER'].apply(lambda x: x.capitalize())\n",
    "\n",
    "# Compute RMSE and Pearson correlation\n",
    "rmse_nn = mean_squared_error(df_all_nn[\"Geodetic MB\"],\n",
    "                             df_all_nn[\"MBM MB\"],\n",
    "                             squared=False)\n",
    "corr_nn = np.corrcoef(df_all_nn[\"Geodetic MB\"], df_all_nn[\"MBM MB\"])[0, 1]\n",
    "\n",
    "plot_mbm_vs_geodetic_by_area_bin(df_all_nn,\n",
    "                                 bins=[0, 1, 5, 10, 100, np.inf],\n",
    "                                 labels=['<1', '1-5', '5â€“10', '>10', '>100'],\n",
    "                                 max_bins=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
