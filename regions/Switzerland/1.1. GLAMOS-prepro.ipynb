{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing of GLAMOS MB data:\n",
    "\n",
    "Does the pre-processing of the point MB measurements from GLAMOS (winter and summer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Mass Balance data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import massbalancemachine as mbm\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import pyproj\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "from cmcrameri import cm\n",
    "from calendar import monthrange\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "cmap = cm.devon\n",
    "\n",
    "# For bars and lines:\n",
    "color_diff_xgb = '#4d4d4d'\n",
    "\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_1 = colors[0]\n",
    "color_2 = '#c51b7d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform .dat files to .csv:\n",
    "\n",
    "Transform the seasonal and winter PMB .dat files to .csv for simplicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all files with pmb (for winter and annual mb):\n",
    "glamosfiles_mb_a, glamosfiles_mb_w = [], []\n",
    "for file in os.listdir(path_PMB_GLAMOS_a_raw):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(path_PMB_GLAMOS_a_raw, file)):\n",
    "        glamosfiles_mb_a.append(file)\n",
    "\n",
    "for file in os.listdir(path_PMB_GLAMOS_w_raw):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(path_PMB_GLAMOS_w_raw, file)):\n",
    "        glamosfiles_mb_w.append(file)\n",
    "\n",
    "print('Examples of index stake raw files:\\n', glamosfiles_mb_a[:5])\n",
    "\n",
    "# Transform all files to csv\n",
    "emptyfolder(path_PMB_GLAMOS_csv_a)\n",
    "emptyfolder(path_PMB_GLAMOS_csv_w)\n",
    "for file in glamosfiles_mb_a:\n",
    "    fileName = re.split('.dat', file)[0]\n",
    "    processDatFile(fileName, path_PMB_GLAMOS_a_raw, path_PMB_GLAMOS_csv_a)\n",
    "\n",
    "for file in glamosfiles_mb_w:\n",
    "    fileName = re.split('.dat', file)[0]\n",
    "    processDatFile(fileName, path_PMB_GLAMOS_w_raw, path_PMB_GLAMOS_csv_w)\n",
    "\n",
    "# Separate clariden into clariden II and III\n",
    "fileName = 'clariden_annual.csv'\n",
    "clariden_csv_a = pd.read_csv(path_PMB_GLAMOS_csv_a + fileName,\n",
    "                                sep=',',\n",
    "                                header=0,\n",
    "                                encoding='latin-1')\n",
    "clariden_csv_a[clariden_csv_a['# name'] == 'L'].to_csv(\n",
    "    path_PMB_GLAMOS_csv_a + 'claridenL_annual.csv', index=False)\n",
    "clariden_csv_a[clariden_csv_a['# name'] == 'U'].to_csv(\n",
    "    path_PMB_GLAMOS_csv_a + 'claridenU_annual.csv', index=False)\n",
    "\n",
    "fileName = 'clariden_winter.csv'\n",
    "clariden_csv_w = pd.read_csv(path_PMB_GLAMOS_csv_w + fileName,\n",
    "                                sep=',',\n",
    "                                header=0,\n",
    "                                encoding='latin-1')\n",
    "clariden_csv_w[clariden_csv_w['# name'] == 'L'].to_csv(\n",
    "    path_PMB_GLAMOS_csv_w + 'claridenL_winter.csv', index=False)\n",
    "clariden_csv_w[clariden_csv_w['# name'] == 'U'].to_csv(\n",
    "    path_PMB_GLAMOS_csv_w + 'claridenU_winter.csv', index=False)\n",
    "\n",
    "os.remove(path_PMB_GLAMOS_csv_a + 'clariden_annual.csv')\n",
    "os.remove(path_PMB_GLAMOS_csv_w + 'clariden_winter.csv')\n",
    "\n",
    "# Example:\n",
    "fileName = 'aletsch_annual.csv'\n",
    "aletsch_csv = pd.read_csv(path_PMB_GLAMOS_csv_a + fileName,\n",
    "                          sep=',',\n",
    "                          header=0,\n",
    "                          encoding='latin-1')\n",
    "aletsch_csv.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Assemble measurement periods:\n",
    "### Annual measurements: \n",
    "Process annual measurements and put all stakes into one csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to collect processed DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Process files\n",
    "for file in tqdm(os.listdir(path_PMB_GLAMOS_csv_a), desc='Annual stakes'):\n",
    "    fileName = re.split('.csv', file)[0]\n",
    "    glacierName = re.split('_', fileName)[0]\n",
    "\n",
    "    # Read CSV\n",
    "    df = pd.read_csv(os.path.join(path_PMB_GLAMOS_csv_a, file),\n",
    "                     sep=',',\n",
    "                     header=0,\n",
    "                     encoding='latin-1')\n",
    "    df['glacier'] = glacierName\n",
    "    df['period'] = 'annual'\n",
    "\n",
    "    # Transform dates\n",
    "    df = transformDates(df)\n",
    "\n",
    "    # Drop duplicates early\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Transform coordinates (from LV03 to WGS84)\n",
    "    df = LV03toWGS84(df)\n",
    "\n",
    "    # Append processed DataFrame to the list\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all processed DataFrames\n",
    "df_annual_raw = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Process YEAR column and filter by year\n",
    "df_annual_raw['YEAR'] = pd.to_datetime(df_annual_raw['date1']).dt.year\n",
    "df_annual_raw = df_annual_raw[df_annual_raw['YEAR'] >= 1950]\n",
    "\n",
    "# Rename and reorder columns for WGMS format\n",
    "columns_mapping = {\n",
    "    '# name': 'POINT_ID',\n",
    "    'lat': 'POINT_LAT',\n",
    "    'lon': 'POINT_LON',\n",
    "    'height': 'POINT_ELEVATION',\n",
    "    'date0': 'FROM_DATE',\n",
    "    'date1': 'TO_DATE',\n",
    "    'mb_we': 'POINT_BALANCE',\n",
    "    'glacier': 'GLACIER',\n",
    "    'period': 'PERIOD'\n",
    "}\n",
    "df_annual_raw.rename(columns=columns_mapping, inplace=True)\n",
    "\n",
    "columns_order = [\n",
    "    'YEAR', 'POINT_ID', 'GLACIER', 'FROM_DATE', 'TO_DATE', 'POINT_LAT',\n",
    "    'POINT_LON', 'POINT_ELEVATION', 'POINT_BALANCE', 'PERIOD', 'date_fix0',\n",
    "    'date_fix1', 'time0', 'time1', 'date_quality', 'position_quality',\n",
    "    'mb_raw', 'density', 'density_quality', 'measurement_quality',\n",
    "    'measurement_type', 'mb_error', 'reading_error', 'density_error',\n",
    "    'error_evaluation_method', 'source'\n",
    "]\n",
    "df_annual_raw = df_annual_raw[columns_order]\n",
    "\n",
    "# Remove rows with invalid year difference\n",
    "valid_date_mask = (\n",
    "    pd.to_datetime(df_annual_raw['TO_DATE'], format='%Y%m%d').dt.year -\n",
    "    pd.to_datetime(df_annual_raw['FROM_DATE'], format='%Y%m%d').dt.year) == 1\n",
    "df_annual_raw = df_annual_raw[valid_date_mask]\n",
    "\n",
    "# Filter measurement type and quality\n",
    "df_annual_raw = df_annual_raw[(df_annual_raw['measurement_type'] <= 2)\n",
    "                              & (df_annual_raw['measurement_quality'] == 1)]\n",
    "\n",
    "# Remove duplicates as a final step\n",
    "df_annual_raw = df_annual_raw.drop_duplicates()\n",
    "\n",
    "# Display the first two rows\n",
    "df_annual_raw.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Winter measurements:\n",
    "For each point in annual meas., take winter meas that was taken closest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the output folder\n",
    "emptyfolder(path_PMB_GLAMOS_csv_w_clean)\n",
    "\n",
    "# Pre-filter glaciers with winter measurements\n",
    "winter_glaciers = {\n",
    "    re.split('_winter.csv', f)[0]\n",
    "    for f in os.listdir(path_PMB_GLAMOS_csv_w)\n",
    "}\n",
    "annual_glaciers = set(df_annual_raw.GLACIER.unique())\n",
    "glaciers_to_process = annual_glaciers.intersection(winter_glaciers)\n",
    "\n",
    "# Process each glacier\n",
    "for glacier in tqdm(glaciers_to_process,\n",
    "                    desc='Processing Winter Glaciers',\n",
    "                    leave=False):\n",
    "    # Read winter measurements\n",
    "    df_winter = pd.read_csv(os.path.join(path_PMB_GLAMOS_csv_w,\n",
    "                                         f\"{glacier}_winter.csv\"),\n",
    "                            sep=',',\n",
    "                            header=0,\n",
    "                            encoding='latin-1')\n",
    "    df_winter['period'] = 'winter'\n",
    "    df_winter['glacier'] = glacier\n",
    "\n",
    "    # Transform dates and remove duplicates\n",
    "    df_winter = transformDates(df_winter).drop_duplicates()\n",
    "\n",
    "    # Convert coordinates to lat/lon\n",
    "    df_winter = LV03toWGS84(df_winter)\n",
    "\n",
    "    # Add YEAR column and filter to >= 1950\n",
    "    df_winter['YEAR'] = pd.to_datetime(df_winter['date1']).dt.year\n",
    "    df_winter = df_winter[df_winter['YEAR'] >= 1950]\n",
    "\n",
    "    # Rename and reorder columns\n",
    "    columns_mapping = {\n",
    "        '# name': 'POINT_ID',\n",
    "        'lat': 'POINT_LAT',\n",
    "        'lon': 'POINT_LON',\n",
    "        'height': 'POINT_ELEVATION',\n",
    "        'date0': 'FROM_DATE',\n",
    "        'date1': 'TO_DATE',\n",
    "        'mb_we': 'POINT_BALANCE',\n",
    "        'glacier': 'GLACIER',\n",
    "        'period': 'PERIOD'\n",
    "    }\n",
    "    columns_order = [\n",
    "        'YEAR', 'POINT_ID', 'GLACIER', 'FROM_DATE', 'TO_DATE', 'POINT_LAT',\n",
    "        'POINT_LON', 'POINT_ELEVATION', 'POINT_BALANCE', 'PERIOD', 'date_fix0',\n",
    "        'date_fix1', 'time0', 'time1', 'date_quality', 'position_quality',\n",
    "        'mb_raw', 'density', 'density_quality', 'measurement_quality',\n",
    "        'measurement_type', 'mb_error', 'reading_error', 'density_error',\n",
    "        'error_evaluation_method', 'source'\n",
    "    ]\n",
    "    df_winter.rename(columns=columns_mapping, inplace=True)\n",
    "    df_winter = df_winter[columns_order]\n",
    "\n",
    "    # Filter measurements by type and quality\n",
    "    df_winter = df_winter[(df_winter['measurement_type'] <= 2)\n",
    "                          & (df_winter['measurement_quality'] == 1)]\n",
    "\n",
    "    # Save the processed DataFrame to a CSV file\n",
    "    output_file = os.path.join(path_PMB_GLAMOS_csv_w_clean,\n",
    "                               f\"{glacier}_winter_all.csv\")\n",
    "    df_winter.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble both periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy annual data\n",
    "df_all_raw = df_annual_raw.copy()\n",
    "\n",
    "# Collect all winter stake files\n",
    "files_stakes = [\n",
    "    f for f in os.listdir(path_PMB_GLAMOS_csv_w_clean) if '_winter_all' in f\n",
    "]\n",
    "\n",
    "# Combine winter stake data\n",
    "winter_dataframes = []\n",
    "for file in files_stakes:\n",
    "    glacier_name = re.split('_', re.split('.csv', file)[0])[0]\n",
    "    df_winter = pd.read_csv(os.path.join(path_PMB_GLAMOS_csv_w_clean, file),\n",
    "                            sep=',',\n",
    "                            header=0,\n",
    "                            encoding='latin-1').drop(columns='Unnamed: 0',\n",
    "                                                     errors='ignore')\n",
    "    winter_dataframes.append(df_winter)\n",
    "\n",
    "# Concatenate all winter dataframes with annual data\n",
    "if winter_dataframes:\n",
    "    df_all_raw = pd.concat([df_all_raw] + winter_dataframes, ignore_index=True)\n",
    "\n",
    "# Reset index\n",
    "df_all_raw.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Correct winter date issues\n",
    "df_all_raw = CleanWinterDates(df_all_raw)\n",
    "\n",
    "# Display data stats\n",
    "print('Number of winter and annual samples:', len(df_all_raw))\n",
    "print('Number of winter samples:',\n",
    "      len(df_all_raw[df_all_raw.PERIOD == 'winter']))\n",
    "print('Number of annual samples:',\n",
    "      len(df_all_raw[df_all_raw.PERIOD == 'annual']))\n",
    "\n",
    "# Remove Pers glacier as it's part of the Morteratsch ensemble\n",
    "df_all_raw = df_all_raw[df_all_raw.GLACIER != 'pers']\n",
    "\n",
    "# Save all data to CSV\n",
    "df_all_raw.to_csv(os.path.join(path_PMB_GLAMOS_csv, 'df_all_raw.csv'),\n",
    "                  index=False)\n",
    "\n",
    "# Save coordinates for all stakes\n",
    "df_all_raw[['GLACIER', 'POINT_ID', 'POINT_LAT', 'POINT_LON',\n",
    "            'PERIOD']].to_csv('coordinates_all.csv', index=False)\n",
    "\n",
    "# Plot: Number of measurements per year\n",
    "df_measurements_per_year = df_all_raw.groupby(['YEAR',\n",
    "                                               'PERIOD']).size().unstack()\n",
    "df_measurements_per_year.plot(kind='bar',\n",
    "                              stacked=True,\n",
    "                              figsize=(20, 5),\n",
    "                              color=[color_1, color_2])\n",
    "plt.title('Number of measurements per year for all glaciers')\n",
    "plt.ylabel('Number of Measurements')\n",
    "plt.xlabel('Year')\n",
    "plt.legend(title='Period')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add RGIs Ids:\n",
    "\n",
    "For each PMB measurement, we want to add the RGI ID (v6) of the shapefile it belongs to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take all winter and annual stakes (lots of snow probes)\n",
    "df_all_raw = pd.read_csv(path_PMB_GLAMOS_csv + 'df_all_raw.csv',\n",
    "                         sep=',',\n",
    "                         header=0,\n",
    "                         encoding='latin-1')\n",
    "\n",
    "# Keep relevant WGMS columns:\n",
    "df_pmb = df_all_raw[[\n",
    "    'YEAR',\n",
    "    'POINT_ID',\n",
    "    'GLACIER',\n",
    "    'FROM_DATE',\n",
    "    'TO_DATE',\n",
    "    'POINT_LAT',\n",
    "    'POINT_LON',\n",
    "    'POINT_ELEVATION',\n",
    "    'POINT_BALANCE',\n",
    "    'PERIOD',\n",
    "]]\n",
    "\n",
    "# Add RGIs:\n",
    "# Specify the shape filename of the glaciers outline obtained from RGIv6\n",
    "glacier_outline_fname = '../../../data/GLAMOS/RGI/nsidc0770_11.rgi60.CentralEurope/11_rgi60_CentralEurope.shp'\n",
    "\n",
    "# Load the target data and the glacier outlines\n",
    "glacier_outline = gpd.read_file(glacier_outline_fname)\n",
    "\n",
    "# Add RGI IDs through intersection\n",
    "df_pmb = mbm.data_processing.utils.get_rgi(data=df_pmb,\n",
    "                                           glacier_outlines=glacier_outline)\n",
    "\n",
    "# Handle unmatched points\n",
    "no_match_df = df_pmb[df_pmb['RGIId'].isna()]\n",
    "geometry = [\n",
    "    Point(lon, lat)\n",
    "    for lon, lat in zip(no_match_df[\"POINT_LON\"], no_match_df[\"POINT_LAT\"])\n",
    "]\n",
    "points_gdf = gpd.GeoDataFrame(no_match_df,\n",
    "                              geometry=geometry,\n",
    "                              crs=glacier_outline.crs)\n",
    "\n",
    "for index in tqdm(no_match_df.index):\n",
    "    point = points_gdf.loc[index]['geometry']\n",
    "    polygon_index = glacier_outline.distance(point).sort_values().index[0]\n",
    "    closest_rgi = glacier_outline.loc[polygon_index].RGIId\n",
    "    df_pmb.at[index, 'RGIId'] = closest_rgi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "rgiids6 = df_pmb[['GLACIER', 'RGIId']].drop_duplicates()\n",
    "print(\"RGIs before pre-processing:\")\n",
    "check_multiple_rgi_ids(rgiids6)\n",
    "\n",
    "# Clean the data\n",
    "df_pmb_clean = clean_rgi_ids(df_pmb.copy())\n",
    "df_pmb_clean.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Verify post-processing\n",
    "rgiids6_clean = df_pmb_clean[['GLACIER', 'RGIId']].drop_duplicates()\n",
    "print(\"RGIs after pre-processing:\")\n",
    "check_multiple_rgi_ids(rgiids6_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut from 1951:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to start of MS data (1951) or ERA5-Land data (1950):\n",
    "df_pmb_50s = df_pmb_clean[df_pmb_clean.YEAR > 1950].sort_values(\n",
    "    by=['GLACIER', 'YEAR'], ascending=[True, True])\n",
    "\n",
    "# Change from mm w.e. to m w.e.\n",
    "df_pmb_50s['POINT_BALANCE'] = df_pmb_50s['POINT_BALANCE'] / 1000\n",
    "\n",
    "# merge ClaridenL and ClaridenU into one glacier:\n",
    "df_pmb_50s.loc[df_pmb_50s.GLACIER == 'claridenU', 'GLACIER'] = 'clariden'\n",
    "df_pmb_50s.loc[df_pmb_50s.GLACIER == 'claridenL', 'GLACIER'] = 'clariden'\n",
    "\n",
    "print('Number of winter and annual samples:', len(df_pmb_50s))\n",
    "print('Number of annual samples:',\n",
    "      len(df_pmb_50s[df_pmb_50s.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_pmb_50s[df_pmb_50s.PERIOD == 'winter']))\n",
    "\n",
    "# Number of measurements per year:\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax = axs.flatten()[0]\n",
    "df_pmb_50s.groupby(['YEAR',\n",
    "                    'PERIOD']).size().unstack().plot(kind='bar',\n",
    "                                                     stacked=True,\n",
    "                                                     color=[color_1, color_2],\n",
    "                                                     ax=ax)\n",
    "ax.set_title('Number of measurements per year for all glaciers')\n",
    "\n",
    "ax = axs.flatten()[1]\n",
    "num_gl = df_pmb_50s.groupby(['GLACIER']).size().sort_values()\n",
    "num_gl.plot(kind='bar', ax=ax)\n",
    "ax.set_title('Number of total measurements per glacier since 1951')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge stakes that are close: \n",
    "Especially with winter probes, a lot of measurements were done at the same place in the raw data and this leads to noise. We merge the stakes that are very close and keep the mean of the measurement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean df_pmb_50s\n",
    "# Group similar stakes:\n",
    "df_pmb_50s_clean = pd.DataFrame()\n",
    "for gl in tqdm(df_pmb_50s.GLACIER.unique(), desc='glacier', position=0):\n",
    "    print('----------------\\n', gl, ':\\n----------------')\n",
    "    df_gl = df_pmb_50s[df_pmb_50s.GLACIER == gl]\n",
    "    df_gl_cleaned = remove_close_points(df_gl)\n",
    "    df_pmb_50s_clean = pd.concat([df_pmb_50s_clean, df_gl_cleaned])\n",
    "\n",
    "# save to csv:\n",
    "df_pmb_50s_clean.drop(['x', 'y'],\n",
    "                      axis=1).to_csv(path_PMB_GLAMOS_csv + f'df_pmb_all.csv',\n",
    "                                     index=False)\n",
    "\n",
    "# save coordinates:\n",
    "df_pmb_50s_clean[['GLACIER', 'POINT_ID', 'POINT_LAT', 'POINT_LON',\n",
    "                  'PERIOD']].to_csv(path_PMB_GLAMOS_csv +\n",
    "                                    f'coordinate_50s_clean_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of measurements per year:\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax = axs.flatten()[0]\n",
    "df_pmb_50s_clean.groupby(['YEAR', 'PERIOD'\n",
    "                          ]).size().unstack().plot(kind='bar',\n",
    "                                                   stacked=True,\n",
    "                                                   color=[color_1, color_2],\n",
    "                                                   ax=ax)\n",
    "ax.set_title('Number of measurements per year for all glaciers')\n",
    "\n",
    "ax = axs.flatten()[1]\n",
    "num_gl = df_pmb_50s_clean.groupby(['GLACIER']).size().sort_values()\n",
    "num_gl.plot(kind='bar', ax=ax)\n",
    "ax.set_title('Number of total measurements per glacier since 1951')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_list = list(df_pmb_50s_clean.GLACIER.unique())\n",
    "print('Number of glaciers:', len(glacier_list))\n",
    "glacier_list.sort()\n",
    "glacier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of measurements per glacier per year:\n",
    "num_gl_yr = df_pmb_50s_clean.groupby(['GLACIER', 'YEAR', 'PERIOD'\n",
    "                                      ]).size().unstack().reset_index()\n",
    "\n",
    "num_gl_annual = df_pmb_50s_clean[df_pmb_50s_clean.PERIOD == 'annual'].groupby(\n",
    "    ['GLACIER']).size().sort_values()\n",
    "\n",
    "# Plot one glacier per column:\n",
    "big_gl = num_gl_annual[num_gl_annual > 250].index.sort_values()\n",
    "num_glaciers = len(big_gl)\n",
    "fig, ax = plt.subplots(num_glaciers, 1, figsize=(15, 5 * num_glaciers))\n",
    "for i, gl in enumerate(big_gl):\n",
    "    num_gl_yr[num_gl_yr.GLACIER == gl].plot(x='YEAR',\n",
    "                                            kind='bar',\n",
    "                                            stacked=True,\n",
    "                                            ax=ax[i],\n",
    "                                            title=gl)\n",
    "    ax[i].set_ylabel('Number of measurements')\n",
    "    ax[i].set_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of winter and annual samples:', len(df_pmb_50s_clean))\n",
    "print('Number of annual samples:',\n",
    "      len(df_pmb_50s_clean[df_pmb_50s_clean.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_pmb_50s_clean[df_pmb_50s_clean.PERIOD == 'winter']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add topographical information from OGGM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = os.path.join(path_PMB_GLAMOS_csv, 'CH_wgms_dataset_all.csv')\n",
    "df_pmb_topo = pd.read_csv(file_path)\n",
    "\n",
    "# Remove 'pers' glacier\n",
    "df_pmb_topo = df_pmb_topo.loc[df_pmb_topo.GLACIER != 'pers']\n",
    "\n",
    "# Count and display the number of samples\n",
    "print(f\"Total number of winter and annual samples: {len(df_pmb_topo)}\")\n",
    "\n",
    "# Count occurrences of 'PERIOD' values\n",
    "period_counts = df_pmb_topo['PERIOD'].value_counts()\n",
    "print(f\"Number of annual samples: {period_counts.get('annual', 0)}\")\n",
    "print(f\"Number of winter samples: {period_counts.get('winter', 0)}\")\n",
    "\n",
    "# Unique years, sorted\n",
    "unique_years = np.sort(df_pmb_topo.YEAR.unique())\n",
    "print(f\"Unique years: {unique_years}\")\n",
    "\n",
    "# Unique glaciers, sorted\n",
    "glacier_list = sorted(df_pmb_topo.GLACIER.unique())\n",
    "print(f\"Number of glaciers: {len(glacier_list)}\")\n",
    "print(f\"Glaciers: {glacier_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give new stake IDs:\n",
    "Give new stake IDs with glacier name and then a number according to the elevation. This is because accross glaciers some stakes have the same ID which is not practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for glacierName in tqdm(df_pmb_topo.GLACIER.unique(), desc='glaciers'):\n",
    "    gl_data = df_pmb_topo[df_pmb_topo.GLACIER == glacierName]\n",
    "    stakeIDS = gl_data.groupby('POINT_ID')[[\n",
    "        'POINT_LAT', 'POINT_LON', 'POINT_ELEVATION'\n",
    "    ]].mean()\n",
    "    stakeIDS.reset_index(inplace=True)\n",
    "    # Change the ID according to elevation\n",
    "    new_ids = stakeIDS[['POINT_ID',\n",
    "                        'POINT_ELEVATION']].sort_values(by='POINT_ELEVATION')\n",
    "    new_ids['POINT_ID_new'] = [\n",
    "        f'{glacierName}_{i}' for i in range(len(new_ids))\n",
    "    ]\n",
    "    for i, row in new_ids.iterrows():\n",
    "        df_pmb_topo.loc[(df_pmb_topo.GLACIER == glacierName) &\n",
    "                        (df_pmb_topo.POINT_ID == row.POINT_ID),\n",
    "                        'POINT_ID'] = row.POINT_ID_new\n",
    "\n",
    "# Check the condition\n",
    "check_point_ids_contain_glacier(df_pmb_topo)\n",
    "print('Number of winter and annual samples:', len(df_pmb_topo))\n",
    "print('Number of annual samples:',\n",
    "      len(df_pmb_topo[df_pmb_topo.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_pmb_topo[df_pmb_topo.PERIOD == 'winter']))\n",
    "\n",
    "# Save to csv:\n",
    "df_pmb_topo.to_csv(path_PMB_GLAMOS_csv + f'CH_wgms_dataset_all.csv',\n",
    "                   index=False)\n",
    "\n",
    "# Histogram of mass balance\n",
    "df_pmb_topo['POINT_BALANCE'].hist(bins=20)\n",
    "plt.xlabel('Mass balance [m w.e.]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glacier wide MB:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obs: no fixed dates, but using observed periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all files with pmb (for winter and annual mb):\n",
    "glamosfiles_smb = []\n",
    "for file in os.listdir(path_SMB_GLAMOS_raw):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(path_SMB_GLAMOS_raw,\n",
    "                                   file)) and 'obs' in file:\n",
    "        glamosfiles_smb.append(file)\n",
    "print('Examples of index stake raw files:\\n', glamosfiles_smb[:5])\n",
    "\n",
    "# Transform all files to csv\n",
    "emptyfolder(path_SMB_GLAMOS_csv + 'obs/')\n",
    "for file in glamosfiles_smb:\n",
    "    fileName = re.split('.dat', file)[0]\n",
    "    processDatFileGLWMB(fileName, path_SMB_GLAMOS_raw,\n",
    "                        path_SMB_GLAMOS_csv + 'obs/')\n",
    "\n",
    "# Example:\n",
    "fileName = 'aletsch_obs.csv'\n",
    "aletsch_csv = pd.read_csv(path_SMB_GLAMOS_csv + 'obs/' + fileName,\n",
    "                          sep=',',\n",
    "                          header=0,\n",
    "                          encoding='latin-1')\n",
    "aletsch_csv.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix: with fixed periods (hydrological year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all files with pmb (for winter and annual mb):\n",
    "glamosfiles_smb = []\n",
    "for file in os.listdir(path_SMB_GLAMOS_raw):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(path_SMB_GLAMOS_raw,\n",
    "                                   file)) and 'fix' in file:\n",
    "        glamosfiles_smb.append(file)\n",
    "print('Examples of index stake raw files:\\n', glamosfiles_smb[:5])\n",
    "# Transform all files to csv\n",
    "emptyfolder(path_SMB_GLAMOS_csv + 'fix/')\n",
    "for file in glamosfiles_smb:\n",
    "    fileName = re.split('.dat', file)[0]\n",
    "    processDatFileGLWMB(fileName, path_SMB_GLAMOS_raw,\n",
    "                        path_SMB_GLAMOS_csv + 'fix/')\n",
    "\n",
    "# Example:\n",
    "fileName = 'aletsch_fix.csv'\n",
    "aletsch_csv = pd.read_csv(path_SMB_GLAMOS_csv + 'fix/' + fileName,\n",
    "                          sep=',',\n",
    "                          header=0,\n",
    "                          encoding='latin-1')\n",
    "aletsch_csv.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential incoming clear sky solar radiation:\n",
    "\n",
    "Pre-process glamos data of Potential incoming clear sky solar radiation (pcsr) used as a topographical variable. One per day grid per glacier for one year only, depends on the glacier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stakes data over all glaciers:\n",
    "data_glamos = pd.read_csv(path_PMB_GLAMOS_csv + f'CH_wgms_dataset_all.csv')\n",
    "\n",
    "glDirect = np.sort(os.listdir(path_pcsr + 'raw/')) # Glaciers with data\n",
    "\n",
    "print('Number of glacier with clear sky radiation data:', len(glDirect))\n",
    "print('Glaciers with clear sky radiation data:', glDirect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pcsr_save = path_pcsr + 'csv/'\n",
    "empty = False\n",
    "if empty:\n",
    "    emptyfolder(path_pcsr_save)\n",
    "else:\n",
    "    glProcessed = [\n",
    "        re.search(r\"xr_direct_(.*)\\.nc\", f).group(1)\n",
    "        for f in os.listdir(path_pcsr_save)\n",
    "    ]\n",
    "    glDirect = list(set(glDirect) - set(glProcessed))\n",
    "\n",
    "for glacierName in tqdm(glDirect, desc='glaciers', position=0):\n",
    "    print(glacierName)\n",
    "    grid = os.listdir(path_pcsr + 'raw/' + glacierName)\n",
    "    grid_year = int(re.findall(r'\\d+', grid[0])[0])\n",
    "    daily_grids = os.listdir(path_pcsr + 'raw/' + glacierName + '/' + grid[0])\n",
    "    # Sort by day number from 001 to 365\n",
    "    daily_grids.sort()\n",
    "    grids = []\n",
    "    for fileName in daily_grids:\n",
    "        if 'grid' not in fileName:\n",
    "            continue\n",
    "\n",
    "        # Load daily grid file\n",
    "        file_path = path_pcsr + 'raw/' + glacierName + '/' + grid[\n",
    "            0] + '/' + fileName\n",
    "        metadata, grid_data = load_grid_file(file_path)\n",
    "        grids.append(grid_data)\n",
    "\n",
    "    # Take monthly means:\n",
    "    monthly_grids = []\n",
    "    for i in range(12):\n",
    "        num_days_month = monthrange(grid_year, i + 1)[1]\n",
    "        monthly_grids.append(\n",
    "            np.mean(np.stack(grids[i * num_days_month:(i + 1) *\n",
    "                                   num_days_month],\n",
    "                             axis=0),\n",
    "                    axis=0))\n",
    "\n",
    "    monthly_grids = np.array(monthly_grids)\n",
    "    num_months = monthly_grids.shape[0]\n",
    "\n",
    "    # Convert to xarray (CH coordinates)\n",
    "    data_array = convert_to_xarray(monthly_grids, metadata, num_months)\n",
    "\n",
    "    # Convert to WGS84 (lat/lon) coordinates\n",
    "    data_array_transf = transform_xarray_coords_lv03_to_wgs84(data_array)\n",
    "\n",
    "    # Save xarray\n",
    "    if glacierName == 'findelen':\n",
    "        data_array_transf.to_netcdf(path_pcsr_save +\n",
    "                                    f'xr_direct_{glacierName}.nc')\n",
    "        data_array_transf.to_netcdf(path_pcsr_save + f'xr_direct_adler.nc')\n",
    "    elif glacierName == 'stanna':\n",
    "        data_array_transf.to_netcdf(path_pcsr_save + f'xr_direct_sanktanna.nc')\n",
    "    else:\n",
    "        data_array_transf.to_netcdf(path_pcsr_save +\n",
    "                                    f'xr_direct_{glacierName}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of processed glaciers:\n",
    "print('Number of processed glaciers:', len(os.listdir(path_pcsr_save)))\n",
    "\n",
    "# read an plot one file\n",
    "xr_file = xr.open_dataset(path_pcsr_save + 'xr_direct_clariden.nc')\n",
    "xr_file['grid_data'].plot(x='x', y='y', col='time', col_wrap=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
