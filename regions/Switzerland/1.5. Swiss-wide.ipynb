{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glacier grids from SGI or GLAMOS:\n",
    "\n",
    "Creates monthly grid files for the MBM to make PMB predictions over the whole glacier grid. The files come from the SGI grid and use OGGM topography. Computing takes a long time because of the conversion to monthly format.\n",
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import massbalancemachine as mbm\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import geopandas as gpd\n",
    "import csv\n",
    "\n",
    "# scripts\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.geodata import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.config_CH import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()  # in case no memory\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "# Climate columns\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "# Topographical columns\n",
    "voi_topographical = [\n",
    "    \"aspect\",\n",
    "    \"slope\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "    \"topo\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers_glamos_dem = os.listdir(os.path.join(path_GLAMOS_topo, 'lv95/'))\n",
    "\n",
    "# Glacier outlines:\n",
    "glacier_outline_sgi = gpd.read_file(\n",
    "    os.path.join(path_SGI_topo, 'inventory_sgi2016_r2020',\n",
    "                 'SGI_2016_glaciers_copy.shp'))  # Load the shapefile\n",
    "glacier_outline_rgi = gpd.read_file(path_rgi_outlines)\n",
    "\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area()\n",
    "gl_area['clariden'] = gl_area['claridenL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RGI data\n",
    "rgi_df = pd.read_csv(path_glacier_ids,\n",
    "                     sep=',').rename(columns=lambda x: x.strip())\n",
    "\n",
    "# Sort and set index for easier lookup\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "\n",
    "# Load geodetic mass balance data\n",
    "geodeticMB = pd.read_csv(f\"{path_geodetic_MB_glamos}dV_DOI2024_allcomb.csv\")\n",
    "\n",
    "rgi_df.reset_index(inplace=True)\n",
    "sgi_gl = rgi_df.loc[rgi_df.short_name.isin(\n",
    "    glaciers_glamos_dem)]['sgi-id'].unique()\n",
    "\n",
    "# add clariden\n",
    "clariden_L_sgi_id = rgi_df[rgi_df.short_name == 'claridenL']['sgi-id'].unique()\n",
    "\n",
    "# add to sgi_gl\n",
    "sgi_gl = np.concatenate((sgi_gl, clariden_L_sgi_id))\n",
    "\n",
    "# Filter geodeticMB for relevant SGI IDs\n",
    "geodeticMB = geodeticMB[geodeticMB['SGI-ID'].isin(sgi_gl)]\n",
    "\n",
    "# Create a mapping dictionary for glacier names\n",
    "sgi_to_glacier_name = rgi_df[[\n",
    "    'sgi-id', 'short_name'\n",
    "]].drop_duplicates().set_index('sgi-id')['short_name'].to_dict()\n",
    "\n",
    "# Add glacier names based on SGI-ID mapping\n",
    "geodeticMB['glacier_name'] = geodeticMB['SGI-ID'].map(sgi_to_glacier_name)\n",
    "\n",
    "# Standardize naming convention\n",
    "geodeticMB['glacier_name'].replace({'claridenU': 'clariden'}, inplace=True)\n",
    "\n",
    "# filter to glacier_list\n",
    "geodeticMB = geodeticMB[geodeticMB.glacier_name.isin(glaciers_glamos_dem)]\n",
    "\n",
    "# Extract unique start and end years per glacier\n",
    "years_start_per_gl = geodeticMB.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodeticMB.groupby('glacier_name')['A_end'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "glacier_list_geod = years_start_per_gl.keys()\n",
    "years_start_per_gl, years_end_per_gl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regional predictions (all CH glaciers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgi_list = [\n",
    "    re.split('_',\n",
    "             re.split('.grid', f)[0])[1]\n",
    "    for f in os.listdir(os.path.join(path_SGI_topo, 'aspect'))\n",
    "]\n",
    "\n",
    "# unique SGI IDs\n",
    "sgi_list = list(set(sgi_list))\n",
    "print('Number of unique SGI IDs:', len(sgi_list))\n",
    "\n",
    "glaciers_glamos_dems = os.listdir(os.path.join(path_GLAMOS_topo, 'lv95'))\n",
    "\n",
    "RUN = False\n",
    "if RUN:\n",
    "    # Create SGI topographical masks\n",
    "    # Note: This function will take a while to run\n",
    "    # It creates a mask for each glacier in the SGI list\n",
    "    # and saves them in the specified directory.\n",
    "    create_sgi_topo_masks(sgi_list,\n",
    "                          type='sgi_id',\n",
    "                          path_save=os.path.join(path_SGI_topo,\n",
    "                                                 'xr_masked_grids_sgi/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(path_SGI_topo, 'xr_masked_grids_sgi/')\n",
    "xr.open_dataset(path + 'A10g-02.zarr').masked_aspect.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = pd.read_csv(path_PMB_GLAMOS_csv + 'CH_wgms_dataset_all.csv')\n",
    "gl_area = get_gl_area()\n",
    "areas_train_set = [\n",
    "    gl_area[gl] for gl in data_glamos['GLACIER'].unique()\n",
    "    if gl in gl_area.keys()\n",
    "]\n",
    "\n",
    "# histogram\n",
    "plt.hist(areas_train_set, bins=50)\n",
    "plt.xlabel('Area (km2)')\n",
    "plt.title('Histogram of glacier areas with stakes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile\n",
    "shapefile_path = os.path.join(path_SGI_topo, 'inventory_sgi2016_r2020',\n",
    "                              'SGI_2016_glaciers.shp')\n",
    "gdf_shapefiles = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Histogram of area:\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "sns.histplot(gdf_shapefiles.area / (10**6),\n",
    "             color='blue',\n",
    "             kde=True,\n",
    "             bins=50,\n",
    "             ax=axs[0])\n",
    "\n",
    "# boxplot\n",
    "sns.boxplot(x=gdf_shapefiles.area / (10**6), color='blue', ax=axs[1])\n",
    "\n",
    "# set x label to km2\n",
    "axs[0].set_xlabel('Area (km2)')\n",
    "axs[1].set_xlabel('Area (km2)')\n",
    "\n",
    "plt.suptitle('Histogram and Boxplot of all glaciers in SGI 2016')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create grids:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2016 - 2022:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Set up logging ===\n",
    "log_filename = f\"process_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "logging.basicConfig(filename=log_filename,\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# === Set up CSV progress log ===\n",
    "csv_log_path = f\"swiss_wide_progress_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "with open(csv_log_path, mode='w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"sgi_id\", \"year\", \"status\", \"message\"])\n",
    "\n",
    "years = range(2016, 2023)\n",
    "RUN = False\n",
    "\n",
    "if RUN:\n",
    "    for year in years:\n",
    "        path_save_monthly = f'../../../data/GLAMOS/topo/gridded_topo_inputs/SGI_regional_preds/{year}/'\n",
    "        if not os.path.exists(path_save_monthly):\n",
    "            os.makedirs(path_save_monthly)\n",
    "            logging.info(f\"Created directory {path_save_monthly}\")\n",
    "        else:\n",
    "            emptyfolder(path_save_monthly)\n",
    "            logging.info(f\"Emptied directory {path_save_monthly}\")\n",
    "\n",
    "        for sgi_id in tqdm(sgi_list, desc='Processing glaciers'):\n",
    "            try:\n",
    "                path_save = os.path.join(path_SGI_topo, 'xr_masked_grids_sgi/')\n",
    "                path = os.path.join(path_save, f\"{sgi_id}.zarr\")\n",
    "                ds_coarsened = xr.open_dataset(path)\n",
    "            except Exception as e:\n",
    "                msg = f\"Error loading dataset for {sgi_id}: {e}\"\n",
    "                logging.error(msg)\n",
    "                print(msg)\n",
    "                with open(csv_log_path, 'a', newline='') as f:\n",
    "                    csv.writer(f).writerow([sgi_id, year, \"error\", str(e)])\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                rgi_id = None\n",
    "                df_grid = create_glacier_grid_SGI(sgi_id, year, rgi_id,\n",
    "                                                  ds_coarsened)\n",
    "                df_grid.reset_index(drop=True, inplace=True)\n",
    "                dataset_grid = mbm.Dataset(cfg=cfg,\n",
    "                                           data=df_grid,\n",
    "                                           region_name='CH',\n",
    "                                           data_path=path_PMB_GLAMOS_csv)\n",
    "            except Exception as e:\n",
    "                msg = f\"Error creating glacier grid for {sgi_id} in {year}: {e}\"\n",
    "                logging.error(msg)\n",
    "                print(msg)\n",
    "                with open(csv_log_path, 'a', newline='') as f:\n",
    "                    csv.writer(f).writerow([sgi_id, year, \"error\", str(e)])\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                era5_climate_data = os.path.join(\n",
    "                    path_ERA5_raw, 'era5_monthly_averaged_data.nc')\n",
    "                geopotential_data = os.path.join(\n",
    "                    path_ERA5_raw, 'era5_geopotential_pressure.nc')\n",
    "                dataset_grid.get_climate_features(\n",
    "                    climate_data=era5_climate_data,\n",
    "                    geopotential_data=geopotential_data,\n",
    "                    change_units=True,\n",
    "                    smoothing_vois={\n",
    "                        'vois_climate': vois_climate,\n",
    "                        'vois_other': ['ALTITUDE_CLIMATE']\n",
    "                    })\n",
    "\n",
    "                if dataset_grid.data.empty:\n",
    "                    raise ValueError(\n",
    "                        f\"No climate data for glacier {sgi_id} in {year}\")\n",
    "            except Exception as e:\n",
    "                msg = f\"Error adding climate data for {sgi_id} in {year}: {e}\"\n",
    "                logging.error(msg)\n",
    "                print(msg)\n",
    "                with open(csv_log_path, 'a', newline='') as f:\n",
    "                    csv.writer(f).writerow([sgi_id, year, \"error\", str(e)])\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                df_y_gl = dataset_grid.data\n",
    "                df_y_gl.rename(columns={'RGIId': 'RGIId_old'}, inplace=True)\n",
    "                df_y_gl = mbm.data_processing.utils.get_rgi(\n",
    "                    data=df_y_gl, glacier_outlines=glacier_outline_rgi)\n",
    "                df_y_gl = df_y_gl.dropna(subset=['RGIId'])\n",
    "\n",
    "                if df_y_gl.empty:\n",
    "                    raise ValueError(\"No valid RGI intersection\")\n",
    "            except Exception as e:\n",
    "                msg = f\"No RGI intersection for {sgi_id} in {year}. Skipping...\"\n",
    "                logging.warning(msg)\n",
    "                print(msg)\n",
    "                with open(csv_log_path, 'a', newline='') as f:\n",
    "                    csv.writer(f).writerow(\n",
    "                        [sgi_id, year, \"skipped\", \"No RGI intersection\"])\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                voi = [\"hugonnet_dhdt\", \"consensus_ice_thickness\", \"millan_v\"]\n",
    "                df_y_gl = add_OGGM_features(df_y_gl, voi, path_OGGM)\n",
    "                df_y_gl['GLWD_ID'] = df_y_gl.apply(\n",
    "                    lambda x: get_hash(f\"{x.GLACIER}_{x.YEAR}\"),\n",
    "                    axis=1).astype(str)\n",
    "\n",
    "                dataset_grid = mbm.Dataset(cfg=cfg,\n",
    "                                           data=df_y_gl,\n",
    "                                           region_name='CH',\n",
    "                                           data_path=path_PMB_GLAMOS_csv)\n",
    "            except Exception as e:\n",
    "                msg = f\"Error adding OGGM data for {sgi_id} in {year}: {e}\"\n",
    "                logging.error(msg)\n",
    "                print(msg)\n",
    "                with open(csv_log_path, 'a', newline='') as f:\n",
    "                    csv.writer(f).writerow([sgi_id, year, \"error\", str(e)])\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                dataset_grid.convert_to_monthly(\n",
    "                    meta_data_columns=cfg.metaData,\n",
    "                    vois_climate=vois_climate,\n",
    "                    vois_topographical=voi_topographical)\n",
    "            except Exception as e:\n",
    "                msg = f\"Error converting to monthly for {sgi_id} in {year}: {e}\"\n",
    "                logging.error(msg)\n",
    "                print(msg)\n",
    "                with open(csv_log_path, 'a', newline='') as f:\n",
    "                    csv.writer(f).writerow([sgi_id, year, \"error\", str(e)])\n",
    "                continue\n",
    "\n",
    "            # Final save\n",
    "            df_oggm = dataset_grid.data\n",
    "            df_oggm.rename(columns={\n",
    "                'aspect': 'aspect_sgi',\n",
    "                'slope': 'slope_sgi'\n",
    "            },\n",
    "                           inplace=True)\n",
    "            df_oggm['POINT_ELEVATION'] = df_oggm['topo']\n",
    "\n",
    "            save_path = os.path.join(path_save_monthly,\n",
    "                                     f\"{sgi_id}_grid_{year}.parquet\")\n",
    "            try:\n",
    "                dataset_grid.data.to_parquet(save_path,\n",
    "                                             engine=\"pyarrow\",\n",
    "                                             compression=\"snappy\")\n",
    "                logging.info(f\"Successfully saved {sgi_id} for {year}\")\n",
    "                with open(csv_log_path, 'a', newline='') as f:\n",
    "                    csv.writer(f).writerow([sgi_id, year, \"success\", \"\"])\n",
    "            except Exception as e:\n",
    "                msg = f\"Error saving dataset for {sgi_id} in {year}: {e}\"\n",
    "                logging.error(msg)\n",
    "                print(msg)\n",
    "                with open(csv_log_path, 'a', newline='') as f:\n",
    "                    csv.writer(f).writerow([sgi_id, year, \"error\", str(e)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2021\n",
    "path_save_monthly = f'../../../data/GLAMOS/topo/gridded_topo_inputs/SGI_regional_preds/{year}/'\n",
    "\n",
    "sgi_id = 'B36-26'  # Aletsch\n",
    "# Plot all OGGM variables\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_save_monthly, f\"{sgi_id}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == 'sep']\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "voi = [\n",
    "    't2m', 'tp', 'ALTITUDE_CLIMATE', 'ELEVATION_DIFFERENCE', 'hugonnet_dhdt',\n",
    "    'consensus_ice_thickness'\n",
    "]\n",
    "axs = axs.flatten()\n",
    "for i, var in enumerate(voi):\n",
    "    sns.scatterplot(df,\n",
    "                    x='POINT_LON',\n",
    "                    y='POINT_LAT',\n",
    "                    hue=var,\n",
    "                    s=5,\n",
    "                    alpha=0.5,\n",
    "                    palette='twilight_shifted',\n",
    "                    ax=axs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ML model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]\n",
    "\n",
    "data_glamos = pd.read_csv(path_PMB_GLAMOS_csv + 'CH_wgms_dataset_all.csv')\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data': path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data': path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    add_pcsr=False,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_swisswide.csv',\n",
    ")\n",
    "data_monthly = dataloader_gl.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_glaciers = [\n",
    "#     'tortin', 'plattalva', 'sanktanna', 'schwarzberg', 'hohlaub', 'pizol',\n",
    "#     'corvatsch', 'tsanfleuron', 'forno'\n",
    "# ]\n",
    "\n",
    "test_glaciers = []\n",
    "\n",
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in test_glaciers]\n",
    "\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(test_glaciers)]\n",
    "print('Size of test data:', len(data_test))\n",
    "\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of train data:', len(data_train))\n",
    "\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "# Number of annual versus winter measurements:\n",
    "print('Train:')\n",
    "print('Number of winter and annual samples:', len(data_train))\n",
    "print('Number of annual samples:',\n",
    "      len(data_train[data_train.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_train[data_train.PERIOD == 'winter']))\n",
    "\n",
    "# Same for test\n",
    "data_test_annual = data_test[data_test.PERIOD == 'annual']\n",
    "data_test_winter = data_test[data_test.PERIOD == 'winter']\n",
    "\n",
    "print('Test:')\n",
    "print('Number of winter and annual samples:', len(data_test))\n",
    "print('Number of annual samples:', len(data_test_annual))\n",
    "print('Number of winter samples:', len(data_test_winter))\n",
    "\n",
    "print('Total:')\n",
    "print('Number of monthly rows:', len(dataloader_gl.data))\n",
    "print('Number of annual rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'annual']))\n",
    "print('Number of winter rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'winter']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))\n",
    "\n",
    "visualiseSplits(test_set['y'], train_set['y'], splits)\n",
    "visualiseInputs(train_set, test_set, vois_climate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_init = {}\n",
    "param_init['device'] = 'cuda:0'\n",
    "param_init['tree_method'] = 'hist'\n",
    "param_init[\"random_state\"] = cfg.seed\n",
    "param_init[\"n_jobs\"] = cfg.numJobs\n",
    "\n",
    "custom_params = {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 800}\n",
    "\n",
    "# Feature columns:\n",
    "feature_columns = ['ELEVATION_DIFFERENCE'\n",
    "                   ] + list(vois_climate) + list(vois_topographical)\n",
    "# feature_columns = ['ELEVATION_DIFFERENCE'\n",
    "#                    ] + list(vois_climate) + list(vois_topographical)\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "df_X_train_subset = train_set['df_X'][all_columns]\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of testing dataset:', test_set['df_X'][all_columns].shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "params = {**param_init, **custom_params}\n",
    "print(params)\n",
    "custom_model = mbm.models.CustomXGBoostRegressor(cfg, **params)\n",
    "\n",
    "# Fit on train data:\n",
    "custom_model.fit(train_set['df_X'][all_columns], train_set['y'])\n",
    "\n",
    "# Make predictions on test\n",
    "custom_model = custom_model.set_params(device='cpu')\n",
    "features_test, metadata_test = custom_model._create_features_metadata(\n",
    "    test_set['df_X'][all_columns])\n",
    "y_pred = custom_model.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = custom_model.aggrPredict(metadata_test, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = custom_model.score(test_set['df_X'][all_columns],\n",
    "                           test_set['y'])  # negative\n",
    "print('Overall score:', np.abs(score))\n",
    "\n",
    "grouped_ids = getDfAggregatePred(test_set, y_pred_agg, all_columns)\n",
    "# PlotPredictions(grouped_ids, y_pred, metadata_test, test_set, custom_model)\n",
    "# plt.suptitle(f'MBM tested on {test_glaciers}', fontsize=20)\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = False\n",
    "\n",
    "# Define paths\n",
    "path_xr_grids = '../../../data/GLAMOS/topo/SGI2020/xr_masked_grids_sgi/'  # SGI DEMs\n",
    "\n",
    "if RUN:\n",
    "    years = range(2016, 2023)\n",
    "    # years = [2016]\n",
    "    for year in years:\n",
    "        path_save_glw = f'../../../data/GLAMOS/distributed_MB_grids/MBM/swisswide/{year}/'\n",
    "        path_monthly_grids = f'../../../data/GLAMOS/topo/gridded_topo_inputs/SGI_regional_preds/{year}/'\n",
    "        sgi_id_list = [\n",
    "            re.split('_', f)[0] for f in os.listdir(path_monthly_grids)\n",
    "        ]\n",
    "\n",
    "        # check if path exists\n",
    "        if not os.path.exists(path_save_glw):\n",
    "            os.makedirs(path_save_glw)\n",
    "        else:\n",
    "            emptyfolder(path_save_glw)\n",
    "\n",
    "        # Feature columns\n",
    "        vois_climate = [\n",
    "            't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "        ]\n",
    "        feature_columns = ['ELEVATION_DIFFERENCE'\n",
    "                           ] + list(vois_climate) + list(vois_topographical)\n",
    "        all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "        print('Running for feature columns:', all_columns)\n",
    "\n",
    "        for sgi_id in tqdm(sgi_id_list, desc='SGI Ids'):\n",
    "            print(sgi_id)\n",
    "            # Load parquet input glacier grid file in monthly format (pre-processed)\n",
    "            df_grid_monthly = pd.read_parquet(\n",
    "                os.path.join(path_monthly_grids,\n",
    "                             f\"{sgi_id}_grid_{year}.parquet\"))\n",
    "\n",
    "            df_grid_monthly.drop_duplicates(inplace=True)\n",
    "\n",
    "            # Keep only necessary columns, avoiding missing columns issues\n",
    "            df_grid_monthly = df_grid_monthly[[\n",
    "                col for col in all_columns if col in df_grid_monthly.columns\n",
    "            ]]\n",
    "\n",
    "            # Create geodata object\n",
    "            geoData = mbm.GeoData(df_grid_monthly)\n",
    "\n",
    "            # Computes and saves gridded MB for a year and glacier\n",
    "            path_glacier_dem = os.path.join(path_xr_grids, f\"{sgi_id}.zarr\")\n",
    "            geoData.gridded_MB_pred(custom_model,\n",
    "                                    sgi_id,\n",
    "                                    year,\n",
    "                                    all_columns,\n",
    "                                    path_glacier_dem,\n",
    "                                    path_save_glw,\n",
    "                                    cfg,\n",
    "                                    save_monthly_pred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open an example\n",
    "sgi_id = 'B36-26'  # Aletsch\n",
    "year = 2016\n",
    "path_save_glw = f'../../../data/GLAMOS/distributed_MB_grids/MBM/swisswide/{year}/'\n",
    "path = os.path.join(path_save_glw, f\"{sgi_id}/{sgi_id}_{year}_annual.zarr\")\n",
    "\n",
    "xr.open_dataset(path).pred_masked.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parquet input glacier grid file in monthly format (pre-processed)\n",
    "sgi_id = 'B36-26'  # Aletsch\n",
    "year = 2016\n",
    "path_monthly_grids = f'../../../data/GLAMOS/topo/gridded_topo_inputs/SGI_regional_preds/{year}/'\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_monthly_grids, f\"{sgi_id}_grid_{year}.parquet\"))\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "df = df[df.MONTHS == 'sep']\n",
    "voi = ['t2m', 'tp', 'hugonnet_dhdt', 'consensus_ice_thickness']\n",
    "for i, var in enumerate(voi):\n",
    "    sns.scatterplot(df,\n",
    "                    x='POINT_LON',\n",
    "                    y='POINT_LAT',\n",
    "                    hue=var,\n",
    "                    s=5,\n",
    "                    alpha=0.5,\n",
    "                    palette='twilight_shifted',\n",
    "                    ax=axs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at 2016:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean predicted MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2016\n",
    "path_save_glw = f'../../../data/GLAMOS/distributed_MB_grids/MBM/swisswide/{year}/'\n",
    "sgi_id_list = os.listdir(path_save_glw)\n",
    "\n",
    "def get_mean_mb_year(year):\n",
    "    path_save_glw = f'../../../data/GLAMOS/distributed_MB_grids/MBM/swisswide/{year}/'\n",
    "\n",
    "    # Calculate mean predicted mb for each glacier\n",
    "    rows = []\n",
    "    for sgi_id in tqdm(sgi_id_list):\n",
    "        gridd_mb = xr.open_dataset(\n",
    "            os.path.join(path_save_glw, f\"{sgi_id}/{sgi_id}_{year}_annual.zarr\"))\n",
    "        mean_value = gridd_mb.pred_masked.mean().values.item()\n",
    "        rows.append({'sgi_id': sgi_id, 'mean_mb': mean_value})\n",
    "\n",
    "    mean_mb = pd.DataFrame(rows)\n",
    "    return mean_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mb_2016 = get_mean_mb_year(2016)\n",
    "mean_mb_2022 = get_mean_mb_year(2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get volumes and areas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_id(id_str):\n",
    "    return id_str.replace('/', '-')\n",
    "\n",
    "path_volumes = '../../../data/GLAMOS/volumes/'\n",
    "path_areas = '../../../data/GLAMOS/topo/SGI2020/inventory_sgi2016_r2020'\n",
    "\n",
    "# Load the shapefile of volumes\n",
    "volgdf = gpd.read_file(os.path.join(path_volumes, 'Summary.shp'))\n",
    "volgdf['sgi-id'] = volgdf['pk_sgi'].apply(convert_id)  # directly creating 'sgi-id'\n",
    "volgdf['V_2016'] = volgdf['V_2016'] * 10**9  # convert to m3\n",
    "\n",
    "# Load the shapefile of areas\n",
    "areagdf = gpd.read_file(os.path.join(path_areas, 'SGI_2016_glaciers.shp'))\n",
    "areagdf['area_2016'] = areagdf['area_km2'] * 10**6  # convert to m2\n",
    "\n",
    "# Load the mean mass balance data (assuming this is loaded before)\n",
    "mean_mb_2016['sgi-id'] = mean_mb_2016['sgi_id'].apply(convert_id)\n",
    "mean_mb_2022['sgi-id'] = mean_mb_2022['sgi_id'].apply(convert_id)\n",
    "\n",
    "# Merge the dataframes on 'sgi-id'\n",
    "glacier_info = volgdf[['sgi-id', 'V_2016']].merge(\n",
    "    areagdf[['sgi-id', 'area_2016']], on='sgi-id', how='inner'\n",
    ").merge(\n",
    "    mean_mb_2016[['sgi-id', 'mean_mb']].rename(columns={'mean_mb': 'mean_mb_2016'}), on='sgi-id', how='inner'\n",
    ").merge(\n",
    "    mean_mb_2022[['sgi-id', 'mean_mb']].rename(columns={'mean_mb': 'mean_mb_2022'}), on='sgi-id', how='inner'\n",
    ")\n",
    "\n",
    "# Calculate volume changes\n",
    "glacier_info['vol_change_2016'] = glacier_info['area_2016'] * glacier_info['mean_mb_2016']\n",
    "glacier_info['vol_change_2022'] = glacier_info['area_2016'] * glacier_info['mean_mb_2022']\n",
    "\n",
    "glacier_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_info.area_2016.sum()/10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_CH_2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total vol change 2016:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_change_2016 = glacier_info['vol_change_2016'].sum() / 10**9  # convert to km3\n",
    "volume_2016 = glacier_info['V_2016'].sum() / 10**9  # convert to km3\n",
    "area_2016 = glacier_info['area_2016'].sum() / 10**6  # convert to km2\n",
    "volume_2016, area_2016, vol_change_2016\n",
    "\n",
    "# open reference GLAMOS\n",
    "df_reference = pd.read_csv(\n",
    "    '../../../data/GLAMOS/massbalance_swisswide_2024_r2024_clean.csv').iloc[1:]\n",
    "ref_CH_2016 = df_reference[(df_reference.catchment == 'Switzerland')\n",
    "                      & (df_reference.year == '2016')]\n",
    "\n",
    "print('Volume change from GLAMOS:', ref_CH_2016['volume change'].values[0])\n",
    "print('Volume change from MBM:', vol_change_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_change_2022 = glacier_info['vol_change_2022'].sum() / 10**9  # convert to km3\n",
    "volume_2022 = glacier_info['V_2016'].sum() / 10**9  # convert to km3\n",
    "area_2022 = glacier_info['area_2016'].sum() / 10**6  # convert to km2\n",
    "volume_2022, area_2022, vol_change_2022\n",
    "\n",
    "# open reference GLAMOS\n",
    "ref_CH_2022 = df_reference[(df_reference.catchment == 'Switzerland')\n",
    "                      & (df_reference.year == '2022')]\n",
    "\n",
    "print('Volume change from GLAMOS:', ref_CH_2022['volume change'].values[0])\n",
    "print('Volume change from MBM:', vol_change_2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume area scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate c for every glacier\n",
    "beta = 1.36\n",
    "glacier_info['c'] = volgdf['V_2016'] / (areagdf['area_2016']**beta)\n",
    "\n",
    "# Inialisation:\n",
    "density = 916.7  # kg/m3\n",
    "\n",
    "t1, t2 = 2015, 2017\n",
    "years = np.arange(t1, t2 + 1, 1)\n",
    "\n",
    "# Initialize arrays to store volume and area\n",
    "volume_ev = pd.DataFrame(columns=years, index=glacier_info.index)\n",
    "area_ev = pd.DataFrame(columns=years, index=glacier_info.index)\n",
    "mb_ev = pd.DataFrame(columns=years, index=glacier_info.index)\n",
    "\n",
    "# Set initial conditions\n",
    "volume_ev[2016] = glacier_info['V_2016'].values\n",
    "area_ev[2016] = glacier_info['area_2016'].values\n",
    "mb_ev[2016] = glacier_info['mean_mb_2016'].values\n",
    "\n",
    "idx_16 = 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward simulation\n",
    "for i, y in enumerate(np.arange(2016, t2, 1)):\n",
    "    # Calculate volume change\n",
    "    volume_change = mb_ev[idx_16 + i] * area_ev[idx_16 + i] / density\n",
    "\n",
    "    # Update volume\n",
    "    volume_ev[idx_16 + i + 1] = np.maximum(\n",
    "        volume_ev[idx_16 + i] + volume_change, 0)\n",
    "\n",
    "    # Update area using V-A scaling\n",
    "    area_ev[idx_16 + i + 1] = np.maximum(\n",
    "        (volume_ev[idx_16 + i + 1] / glacier_info['c'])**(1 / beta), 0)\n",
    "\n",
    "#Backward simulation\n",
    "for i, y in enumerate(np.arange(2016, t1, -1)):\n",
    "    # Calculate volume change\n",
    "    volume_change = mb_ev[idx_16 - i] * area_ev[idx_16 - i] / density\n",
    "\n",
    "    # Update volume\n",
    "    volume_ev[idx_16 - i - 1] = np.maximum(\n",
    "        volume_ev[idx_16 - i] - volume_change, 0)\n",
    "\n",
    "    # Update area using V-A scaling\n",
    "    area_ev[idx_16 - i - 1] = np.maximum(\n",
    "        (volume_ev[idx_16 - i - 1] / glacier_info['c'])**(1 / beta), 0)\n",
    "\n",
    "area_change = (area_ev.sub(area_ev[2016], axis=0)).div(area_ev[2016], axis=0)\n",
    "volume_change = (volume_ev.sub(volume_ev[2016], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Parameters\n",
    "----------\n",
    "gdir: `py_class:crampon.GlacierDirectory`\n",
    "    The GlacierDirectory to process the ice thickness for.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "None\n",
    "\"\"\"\n",
    "#input_cal = 'swisstopo_exact_date'\n",
    "input_cal = 'marty'\n",
    "\n",
    "if input_cal == 'marty':\n",
    "    print('calibration marty gmbs')\n",
    "    marty_df = pd.read_csv(\n",
    "        '/scratch-fourth/acremona/crampon/data/geodetic_mb/geod_mb_ch_linear_1std_bin_50_t100_500_tband_01.csv',\n",
    "        encoding=\"iso-8859-1\")\n",
    "else:\n",
    "    marty_df = pd.read_csv(\n",
    "        '/scratch-fourth/acremona/crampon/data/geodetic_mb/geodetic_mb_swisstopo_exact_dates.csv',\n",
    "        encoding=\"iso-8859-1\")\n",
    "\n",
    "volgdf = gpd.read_file(\n",
    "    '/scratch-fourth/acremona/crampon/data/volumes/Summary.shp')\n",
    "\n",
    "if mb_model:\n",
    "    if type(mb_model) == list:\n",
    "        mb_models = mb_model\n",
    "    else:\n",
    "        mb_models = [mb_model]\n",
    "else:\n",
    "    mb_models = [eval(m) for m in cfg.MASSBALANCE_MODELS]\n",
    "\n",
    "density = cfg.RHO / cfg.RHO_W\n",
    "\n",
    "heights, widths = gdir.get_inversion_flowline_hw()\n",
    "\n",
    "#iterate over marty df for gdir\n",
    "marty_df_gdir = marty_df[marty_df.RGI_Id == gdir.rgi_id]\n",
    "marty_df_gdir = marty_df_gdir.drop(\n",
    "    marty_df_gdir[marty_df_gdir.geod_mb_mwey_1.isna()].index)\n",
    "\n",
    "if marty_df_gdir.empty:\n",
    "    print('No geodetic mass balance available for calibrations')\n",
    "    return\n",
    "\n",
    "df_indexes = marty_df_gdir.index.values\n",
    "\n",
    "for inde in df_indexes:\n",
    "    skipcali = False\n",
    "    if os.path.exists(\n",
    "            gdir.get_filepath('calibration', filesuffix='_marty_multiple')):\n",
    "        os.remove(\n",
    "            gdir.get_filepath('calibration', filesuffix='_marty_multiple'))\n",
    "\n",
    "    cal_data = marty_df_gdir.loc[inde]\n",
    "\n",
    "    t1_date = pd.to_datetime(cal_data.date_1, dayfirst=True)\n",
    "    t2_date = pd.to_datetime(cal_data.date_2, dayfirst=True)\n",
    "\n",
    "    years = np.arange(t1_year_run, t2_year_run + 1, 1)\n",
    "\n",
    "    #inputs v-a scaling\n",
    "    volume_inventory_16 = volgdf[volgdf.pk_sgi == '/'.join(\n",
    "        gdir.rgi_id.split('.')[1].split('-'))].V_2016.values[0] * 10**9\n",
    "    area_inventory_16 = gdir.area_km2 * 10**6  # in km2!!\n",
    "\n",
    "    # calculate c for every glacier\n",
    "    c = volume_inventory_16 / (area_inventory_16**beta)\n",
    "\n",
    "    idx_16 = 2016 - t1_year_run\n",
    "\n",
    "    try:\n",
    "        area_df = pd.read_csv(gdir.get_filepath('area_change'))\n",
    "\n",
    "        if f'area_change_rate_perc_{t1_date.day}_{t1_date.month}_{t1_date.year}_{t2_date.day}_{t2_date.month}_{t2_date.year}_{input_cal}' in area_df.drop(\n",
    "            ['year'], axis=1).columns:\n",
    "            continue\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        data = {'year': years}\n",
    "        area_df = pd.DataFrame(data=data)\n",
    "\n",
    "    try:\n",
    "        param_df = pd.read_csv(\n",
    "            gdir.get_filepath('initial_calibrated_parameters'))\n",
    "\n",
    "        if f'params_{t1_date.day}_{t1_date.month}_{t1_date.year}_{t2_date.day}_{t2_date.month}_{t2_date.year}_{input_cal}' in param_df.drop(\n",
    "            ['params'], axis=1).columns:\n",
    "            skipcali = True\n",
    "            print((param_df[param_df.params == 'HockModel_mu_hock']))\n",
    "            mu_hock = (\n",
    "                param_df[param_df.params == 'HockModel_mu_hock']\n",
    "            )[f'params_{t1_date.day}_{t1_date.month}_{t1_date.year}_{t2_date.day}_{t2_date.month}_{t2_date.year}_{input_cal}'].values[\n",
    "                0]\n",
    "            a_ice = (\n",
    "                param_df[param_df.params == 'HockModel_a_ice']\n",
    "            )[f'params_{t1_date.day}_{t1_date.month}_{t1_date.year}_{t2_date.day}_{t2_date.month}_{t2_date.year}_{input_cal}'].values[\n",
    "                0]\n",
    "            p_fh = (\n",
    "                param_df[param_df.params == 'HockModel_prcp_fac']\n",
    "            )[f'params_{t1_date.day}_{t1_date.month}_{t1_date.year}_{t2_date.day}_{t2_date.month}_{t2_date.year}_{input_cal}'].values[\n",
    "                0]\n",
    "\n",
    "            tf = (\n",
    "                param_df[param_df.params == 'PellicciottiModel_tf']\n",
    "            )[f'params_{t1_date.day}_{t1_date.month}_{t1_date.year}_{t2_date.day}_{t2_date.month}_{t2_date.year}_{input_cal}'].values[\n",
    "                0]\n",
    "            srf = (\n",
    "                param_df[param_df.params == 'PellicciottiModel_srf']\n",
    "            )[f'params_{t1_date.day}_{t1_date.month}_{t1_date.year}_{t2_date.day}_{t2_date.month}_{t2_date.year}_{input_cal}'].values[\n",
    "                0]\n",
    "            p_fp = (\n",
    "                param_df[param_df.params == 'PellicciottiModel_prcp_fac']\n",
    "            )[f'params_{t1_date.day}_{t1_date.month}_{t1_date.year}_{t2_date.day}_{t2_date.month}_{t2_date.year}_{input_cal}'].values[\n",
    "                0]\n",
    "\n",
    "            if np.isnan([mu_hock, a_ice, p_fh, tf, srf, p_fp]).any():\n",
    "                area_df.loc[:,\n",
    "                            f'area_change_rate_perc_{t1_date.day}_{t1_date.month}_{t1_date.year}_{t2_date.day}_{t2_date.month}_{t2_date.year}_{input_cal}'] = np.zeros_like(\n",
    "                                years) * np.nan\n",
    "                area_df.to_csv(gdir.get_filepath('area_change'), index=False)\n",
    "                continue\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # uncomment when the initial parameter file should be writen!!!\n",
    "\n",
    "        to_calibrate_csv = []\n",
    "        for mo in mb_models:\n",
    "            for i in mo.cali_params_guess.keys():\n",
    "                to_calibrate_csv.append(mo.prefix + i)\n",
    "        print(to_calibrate_csv)\n",
    "        data = {'params': to_calibrate_csv}\n",
    "        param_df = pd.DataFrame(data=data)\n",
    "        print(param_df)\n",
    "\n",
    "    out_params = []\n",
    "    braker = False\n",
    "    for count in range(2):\n",
    "        mb_ens = []\n",
    "        #if braker:\n",
    "        #    break\n",
    "        for m in mb_models:\n",
    "            print(m)\n",
    "            if skipcali:\n",
    "                if count == 0:\n",
    "                    cfg.PARAMS['geometry_evolution'] = False\n",
    "\n",
    "                    if m.prefix == 'HockModel_':\n",
    "                        day_model = m(gdir,\n",
    "                                      mu_hock=mu_hock,\n",
    "                                      a_ice=a_ice,\n",
    "                                      prcp_fac=p_fh,\n",
    "                                      bias=0.,\n",
    "                                      snow_redist=False)\n",
    "                    elif m.prefix == 'PellicciottiModel_':\n",
    "                        day_model = m(gdir,\n",
    "                                      tf=tf,\n",
    "                                      srf=srf,\n",
    "                                      prcp_fac=p_fp,\n",
    "                                      bias=0.,\n",
    "                                      snow_redist=False)\n",
    "                    else:\n",
    "                        raise NotImplementedError\n",
    "                        print('use Hock and Pellicciotti Models')\n",
    "\n",
    "                else:\n",
    "                    cfg.PARAMS['geometry_evolution'] = True\n",
    "\n",
    "                    if m.prefix == 'HockModel_':\n",
    "                        day_model = m(gdir,\n",
    "                                      mu_hock=mu_hock,\n",
    "                                      a_ice=a_ice,\n",
    "                                      prcp_fac=p_fh,\n",
    "                                      bias=0.,\n",
    "                                      snow_redist=False,\n",
    "                                      area_change_dict=area_change_dict)\n",
    "                    elif m.prefix == 'PellicciottiModel_':\n",
    "                        day_model = m(gdir,\n",
    "                                      tf=tf,\n",
    "                                      srf=srf,\n",
    "                                      prcp_fac=p_fp,\n",
    "                                      bias=0.,\n",
    "                                      snow_redist=False,\n",
    "                                      area_change_dict=area_change_dict)\n",
    "                    else:\n",
    "                        raise NotImplementedError\n",
    "                        print('use Hock and Pellicciotti Models')\n",
    "\n",
    "            else:\n",
    "                if count == 0:\n",
    "                    cfg.PARAMS['geometry_evolution'] = False\n",
    "\n",
    "                    if p_fac:\n",
    "                        param_dict = m.cali_params_guess.copy()\n",
    "                        p = {}\n",
    "                        for k, v in param_dict.items():\n",
    "                            if k not in ['prcp_fac']:\n",
    "                                p.update({k: v})\n",
    "                            else:\n",
    "                                p.update({k: p_fac})\n",
    "                        pdict = p\n",
    "\n",
    "                        calibrate_mb_model_on_geod_mb_one_paramset_snowlines(\n",
    "                            gdir,\n",
    "                            mb_model=m,\n",
    "                            conv_thresh=0.005,\n",
    "                            initial_param_guess=pdict,\n",
    "                            gmb_delta=0.,\n",
    "                            it_thresh=5,\n",
    "                            cali_suffix='_marty_multiple',\n",
    "                            cal_data=cal_data)\n",
    "\n",
    "                    else:\n",
    "                        calibrate_mb_model_on_geod_mb_one_paramset_snowlines(\n",
    "                            gdir,\n",
    "                            mb_model=m,\n",
    "                            conv_thresh=0.005,\n",
    "                            gmb_delta=0.,\n",
    "                            it_thresh=5,\n",
    "                            cali_suffix='_marty_multiple',\n",
    "                            cal_data=cal_data)\n",
    "\n",
    "                    day_model = m(gdir,\n",
    "                                  bias=0.,\n",
    "                                  cali_suffix='_marty_multiple',\n",
    "                                  snow_redist=False)\n",
    "\n",
    "                    for p in day_model.cali_params_list:\n",
    "                        if len(\n",
    "                                getattr(day_model, p).dropna().values\n",
    "                        ) == 0:  #np.isnan(np.unique(getattr(day_model, p))):\n",
    "                            braker = True\n",
    "\n",
    "                            area_df.loc[:,\n",
    "                                        f'area_change_rate_perc_{t1_date.day}_{t1_date.month}_{t1_date.year}_{t2_date.day}_{t2_date.month}_{t2_date.year}_{input_cal}'] = np.zeros_like(\n",
    "                                            years) * np.nan\n",
    "                            area_df.to_csv(gdir.get_filepath('area_change'),\n",
    "                                           index=False)\n",
    "\n",
    "                            param_df.loc[:,\n",
    "                                         f'params_{t1_date.day}_{t1_date.month}_{t1_date.year}_{t2_date.day}_{t2_date.month}_{t2_date.year}_{input_cal}'] = [\n",
    "                                             np.nan, np.nan, np.nan, np.nan,\n",
    "                                             np.nan, np.nan\n",
    "                                         ]\n",
    "                            param_df.to_csv(gdir.get_filepath(\n",
    "                                'initial_calibrated_parameters'),\n",
    "                                            index=False)\n",
    "                            #continue\n",
    "\n",
    "                            break\n",
    "                        else:\n",
    "                            out_params.append(\n",
    "                                np.unique(\n",
    "                                    getattr(day_model, p).dropna().values)[0])\n",
    "                    if braker:\n",
    "                        break\n",
    "                else:\n",
    "                    cfg.PARAMS['geometry_evolution'] = True\n",
    "                    day_model = m(gdir,\n",
    "                                  bias=0.,\n",
    "                                  cali_suffix='_marty_multiple',\n",
    "                                  snow_redist=False,\n",
    "                                  area_change_dict=area_change_dict)\n",
    "\n",
    "            mb_years = []\n",
    "            for y in years:\n",
    "                tmp = day_model.get_specific_mb(heights, widths, year=y)\n",
    "                mb_years.append(tmp)\n",
    "            mb_ens.append(mb_years)\n",
    "\n",
    "        if braker:\n",
    "            break\n",
    "\n",
    "        mb_med = np.median(mb_ens, axis=0)\n",
    "        print('mb_med', mb_med)\n",
    "        # Initialize arrays to store volume and area\n",
    "        volume = np.zeros_like(years)\n",
    "        area = np.zeros_like(years)\n",
    "\n",
    "        # Set initial conditions\n",
    "        volume[idx_16] = volume_inventory_16\n",
    "        area[idx_16] = area_inventory_16\n",
    "\n",
    "        # Forward simulation\n",
    "        for i, y in enumerate(np.arange(2016, t2_year_run, 1)):\n",
    "            # Calculate volume change\n",
    "            volume_change = mb_med[idx_16 + i] * area[idx_16 + i] / density\n",
    "\n",
    "            # Update volume\n",
    "            volume[idx_16 + i + 1] = volume[idx_16 + i] + volume_change\n",
    "            volume[idx_16 + i + 1] = max(\n",
    "                volume[idx_16 + i + 1], 0)  # Ensure volume doesn't go negative\n",
    "\n",
    "            # Calculate area using V-A scaling\n",
    "            area[idx_16 + i + 1] = (volume[idx_16 + i + 1] / c)**(1 / beta)\n",
    "            area[idx_16 + i + 1] = max(area[idx_16 + i + 1],\n",
    "                                       0)  # Ensure area doesn't go negative\n",
    "        #Backward simulation\n",
    "        for i, y in enumerate(np.arange(2016, t1_year_run, -1)):\n",
    "            # Calculate volume change\n",
    "            volume_change = mb_med[idx_16 - i] * area[idx_16 - i] / density\n",
    "            # Update volume\n",
    "            volume[idx_16 - i - 1] = volume[idx_16 - i] - volume_change\n",
    "            volume[idx_16 - i - 1] = max(\n",
    "                volume[idx_16 - i - 1], 0)  # Ensure volume doesn't go negative\n",
    "            # Calculate area using V-A scaling\n",
    "            area[idx_16 - i - 1] = (volume[idx_16 - i - 1] / c)**(1 / beta)\n",
    "            area[idx_16 - i - 1] = max(area[idx_16 - i - 1],\n",
    "                                       0)  # Ensure area doesn't go negative\n",
    "\n",
    "        area_change = (area - area_inventory_16) / area_inventory_16\n",
    "        volume_change = volume - volume_inventory_16\n",
    "\n",
    "        area_change_dict = dict(zip(years, area_change))\n",
    "\n",
    "    if braker:\n",
    "        continue\n",
    "\n",
    "    area_df.loc[:,\n",
    "                f'area_change_rate_perc_{t1_date.day}_{t1_date.month}_{t1_date.year}_{t2_date.day}_{t2_date.month}_{t2_date.year}_{input_cal}'] = area_change\n",
    "    area_df.to_csv(gdir.get_filepath('area_change'), index=False)\n",
    "\n",
    "    if not skipcali:\n",
    "        param_df.loc[:,\n",
    "                     f'params_{t1_date.day}_{t1_date.month}_{t1_date.year}_{t2_date.day}_{t2_date.month}_{t2_date.year}_{input_cal}'] = out_params\n",
    "        param_df.to_csv(gdir.get_filepath('initial_calibrated_parameters'),\n",
    "                        index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
