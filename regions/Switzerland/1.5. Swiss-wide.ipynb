{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glacier grids from SGI or GLAMOS:\n",
    "\n",
    "Creates monthly grid files for the MBM to make PMB predictions over the whole glacier grid. The files come from the SGI grid and use OGGM topography. Computing takes a long time because of the conversion to monthly format.\n",
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import massbalancemachine as mbm\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import geopandas as gpd\n",
    "import csv\n",
    "\n",
    "# scripts\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.geodata import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.config_CH import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()  # in case no memory\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "# Climate columns\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "# Topographical columns\n",
    "voi_topographical = [\n",
    "    \"aspect\",\n",
    "    \"slope\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "    \"topo\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers_glamos_dem = os.listdir(os.path.join(path_GLAMOS_topo, 'lv95/'))\n",
    "\n",
    "# Glacier outlines:\n",
    "glacier_outline_sgi = gpd.read_file(\n",
    "    os.path.join(path_SGI_topo, 'inventory_sgi2016_r2020',\n",
    "                 'SGI_2016_glaciers_copy.shp'))  # Load the shapefile\n",
    "glacier_outline_rgi = gpd.read_file(path_rgi_outlines)\n",
    "\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area()\n",
    "gl_area['clariden'] = gl_area['claridenL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RGI data\n",
    "rgi_df = pd.read_csv(path_glacier_ids,\n",
    "                     sep=',').rename(columns=lambda x: x.strip())\n",
    "\n",
    "# Sort and set index for easier lookup\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "\n",
    "# Load geodetic mass balance data\n",
    "geodeticMB = pd.read_csv(f\"{path_geodetic_MB_glamos}dV_DOI2024_allcomb.csv\")\n",
    "\n",
    "rgi_df.reset_index(inplace=True)\n",
    "sgi_gl = rgi_df.loc[rgi_df.short_name.isin(\n",
    "    glaciers_glamos_dem)]['sgi-id'].unique()\n",
    "\n",
    "# add clariden\n",
    "clariden_L_sgi_id = rgi_df[rgi_df.short_name == 'claridenL']['sgi-id'].unique()\n",
    "\n",
    "# add to sgi_gl\n",
    "sgi_gl = np.concatenate((sgi_gl, clariden_L_sgi_id))\n",
    "\n",
    "# Filter geodeticMB for relevant SGI IDs\n",
    "geodeticMB = geodeticMB[geodeticMB['SGI-ID'].isin(sgi_gl)]\n",
    "\n",
    "# Create a mapping dictionary for glacier names\n",
    "sgi_to_glacier_name = rgi_df[[\n",
    "    'sgi-id', 'short_name'\n",
    "]].drop_duplicates().set_index('sgi-id')['short_name'].to_dict()\n",
    "\n",
    "# Add glacier names based on SGI-ID mapping\n",
    "geodeticMB['glacier_name'] = geodeticMB['SGI-ID'].map(sgi_to_glacier_name)\n",
    "\n",
    "# Standardize naming convention\n",
    "geodeticMB['glacier_name'].replace({'claridenU': 'clariden'}, inplace=True)\n",
    "\n",
    "# filter to glacier_list\n",
    "geodeticMB = geodeticMB[geodeticMB.glacier_name.isin(glaciers_glamos_dem)]\n",
    "\n",
    "# Extract unique start and end years per glacier\n",
    "years_start_per_gl = geodeticMB.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodeticMB.groupby('glacier_name')['A_end'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "glacier_list_geod = years_start_per_gl.keys()\n",
    "years_start_per_gl, years_end_per_gl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regional predictions (all CH glaciers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgi_list = [\n",
    "    re.split('_',\n",
    "             re.split('.grid', f)[0])[1]\n",
    "    for f in os.listdir(os.path.join(path_SGI_topo, 'aspect'))\n",
    "]\n",
    "\n",
    "# unique SGI IDs\n",
    "sgi_list = list(set(sgi_list))\n",
    "print('Number of unique SGI IDs:', len(sgi_list))\n",
    "\n",
    "glaciers_glamos_dems = os.listdir(os.path.join(path_GLAMOS_topo, 'lv95'))\n",
    "\n",
    "RUN = False\n",
    "if RUN:\n",
    "    # Create SGI topographical masks\n",
    "    # Note: This function will take a while to run\n",
    "    # It creates a mask for each glacier in the SGI list\n",
    "    # and saves them in the specified directory.\n",
    "    create_sgi_topo_masks(sgi_list,\n",
    "                          type='sgi_id',\n",
    "                          path_save=os.path.join(path_SGI_topo,\n",
    "                                                 'xr_masked_grids_sgi/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(path_SGI_topo, 'xr_masked_grids_sgi/')\n",
    "xr.open_dataset(path + 'A10g-02.zarr').masked_aspect.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = pd.read_csv(path_PMB_GLAMOS_csv + 'CH_wgms_dataset_all.csv')\n",
    "gl_area = get_gl_area()\n",
    "areas_train_set = [\n",
    "    gl_area[gl] for gl in data_glamos['GLACIER'].unique()\n",
    "    if gl in gl_area.keys()\n",
    "]\n",
    "\n",
    "# histogram\n",
    "plt.hist(areas_train_set, bins=50)\n",
    "plt.xlabel('Area (km2)')\n",
    "plt.title('Histogram of glacier areas with stakes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile\n",
    "shapefile_path = os.path.join(path_SGI_topo, 'inventory_sgi2016_r2020',\n",
    "                              'SGI_2016_glaciers.shp')\n",
    "gdf_shapefiles = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Histogram of area:\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "sns.histplot(gdf_shapefiles.area / (10**6),\n",
    "             color='blue',\n",
    "             kde=True,\n",
    "             bins=50,\n",
    "             ax=axs[0])\n",
    "\n",
    "# boxplot\n",
    "sns.boxplot(x=gdf_shapefiles.area / (10**6), color='blue', ax=axs[1])\n",
    "\n",
    "# set x label to km2\n",
    "axs[0].set_xlabel('Area (km2)')\n",
    "axs[1].set_xlabel('Area (km2)')\n",
    "\n",
    "plt.suptitle('Histogram and Boxplot of all glaciers in SGI 2016')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create grids:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2016 - 2022:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Set up logging ===\n",
    "log_filename = f\"process_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "logging.basicConfig(filename=log_filename,\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# === Set up CSV progress log ===\n",
    "csv_log_path = f\"swiss_wide_progress_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "with open(csv_log_path, mode='w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"sgi_id\", \"year\", \"status\", \"message\"])\n",
    "\n",
    "years = range(2016, 2023)\n",
    "RUN = False\n",
    "\n",
    "if RUN:\n",
    "    for year in years:\n",
    "        path_save_monthly = f'../../../data/GLAMOS/topo/gridded_topo_inputs/SGI_regional_preds/{year}/'\n",
    "        if not os.path.exists(path_save_monthly):\n",
    "            os.makedirs(path_save_monthly)\n",
    "            logging.info(f\"Created directory {path_save_monthly}\")\n",
    "        else:\n",
    "            emptyfolder(path_save_monthly)\n",
    "            logging.info(f\"Emptied directory {path_save_monthly}\")\n",
    "\n",
    "        for sgi_id in tqdm(sgi_list, desc='Processing glaciers'):\n",
    "            try:\n",
    "                path_save = os.path.join(path_SGI_topo, 'xr_masked_grids_sgi/')\n",
    "                path = os.path.join(path_save, f\"{sgi_id}.zarr\")\n",
    "                ds_coarsened = xr.open_dataset(path)\n",
    "            except Exception as e:\n",
    "                msg = f\"Error loading dataset for {sgi_id}: {e}\"\n",
    "                logging.error(msg)\n",
    "                print(msg)\n",
    "                with open(csv_log_path, 'a', newline='') as f:\n",
    "                    csv.writer(f).writerow([sgi_id, year, \"error\", str(e)])\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                rgi_id = None\n",
    "                df_grid = create_glacier_grid_SGI(sgi_id, year, rgi_id,\n",
    "                                                  ds_coarsened)\n",
    "                df_grid.reset_index(drop=True, inplace=True)\n",
    "                dataset_grid = mbm.Dataset(cfg=cfg,\n",
    "                                           data=df_grid,\n",
    "                                           region_name='CH',\n",
    "                                           data_path=path_PMB_GLAMOS_csv)\n",
    "            except Exception as e:\n",
    "                msg = f\"Error creating glacier grid for {sgi_id} in {year}: {e}\"\n",
    "                logging.error(msg)\n",
    "                print(msg)\n",
    "                with open(csv_log_path, 'a', newline='') as f:\n",
    "                    csv.writer(f).writerow([sgi_id, year, \"error\", str(e)])\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                era5_climate_data = os.path.join(\n",
    "                    path_ERA5_raw, 'era5_monthly_averaged_data.nc')\n",
    "                geopotential_data = os.path.join(\n",
    "                    path_ERA5_raw, 'era5_geopotential_pressure.nc')\n",
    "                dataset_grid.get_climate_features(\n",
    "                    climate_data=era5_climate_data,\n",
    "                    geopotential_data=geopotential_data,\n",
    "                    change_units=True,\n",
    "                    smoothing_vois={\n",
    "                        'vois_climate': vois_climate,\n",
    "                        'vois_other': ['ALTITUDE_CLIMATE']\n",
    "                    })\n",
    "\n",
    "                if dataset_grid.data.empty:\n",
    "                    raise ValueError(\n",
    "                        f\"No climate data for glacier {sgi_id} in {year}\")\n",
    "            except Exception as e:\n",
    "                msg = f\"Error adding climate data for {sgi_id} in {year}: {e}\"\n",
    "                logging.error(msg)\n",
    "                print(msg)\n",
    "                with open(csv_log_path, 'a', newline='') as f:\n",
    "                    csv.writer(f).writerow([sgi_id, year, \"error\", str(e)])\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                df_y_gl = dataset_grid.data\n",
    "                df_y_gl.rename(columns={'RGIId': 'RGIId_old'}, inplace=True)\n",
    "                df_y_gl = mbm.data_processing.utils.get_rgi(\n",
    "                    data=df_y_gl, glacier_outlines=glacier_outline_rgi)\n",
    "                df_y_gl = df_y_gl.dropna(subset=['RGIId'])\n",
    "\n",
    "                if df_y_gl.empty:\n",
    "                    raise ValueError(\"No valid RGI intersection\")\n",
    "            except Exception as e:\n",
    "                msg = f\"No RGI intersection for {sgi_id} in {year}. Skipping...\"\n",
    "                logging.warning(msg)\n",
    "                print(msg)\n",
    "                with open(csv_log_path, 'a', newline='') as f:\n",
    "                    csv.writer(f).writerow(\n",
    "                        [sgi_id, year, \"skipped\", \"No RGI intersection\"])\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                voi = [\"hugonnet_dhdt\", \"consensus_ice_thickness\", \"millan_v\"]\n",
    "                df_y_gl = add_OGGM_features(df_y_gl, voi, path_OGGM)\n",
    "                df_y_gl['GLWD_ID'] = df_y_gl.apply(\n",
    "                    lambda x: get_hash(f\"{x.GLACIER}_{x.YEAR}\"),\n",
    "                    axis=1).astype(str)\n",
    "\n",
    "                dataset_grid = mbm.Dataset(cfg=cfg,\n",
    "                                           data=df_y_gl,\n",
    "                                           region_name='CH',\n",
    "                                           data_path=path_PMB_GLAMOS_csv)\n",
    "            except Exception as e:\n",
    "                msg = f\"Error adding OGGM data for {sgi_id} in {year}: {e}\"\n",
    "                logging.error(msg)\n",
    "                print(msg)\n",
    "                with open(csv_log_path, 'a', newline='') as f:\n",
    "                    csv.writer(f).writerow([sgi_id, year, \"error\", str(e)])\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                dataset_grid.convert_to_monthly(\n",
    "                    meta_data_columns=cfg.metaData,\n",
    "                    vois_climate=vois_climate,\n",
    "                    vois_topographical=voi_topographical)\n",
    "            except Exception as e:\n",
    "                msg = f\"Error converting to monthly for {sgi_id} in {year}: {e}\"\n",
    "                logging.error(msg)\n",
    "                print(msg)\n",
    "                with open(csv_log_path, 'a', newline='') as f:\n",
    "                    csv.writer(f).writerow([sgi_id, year, \"error\", str(e)])\n",
    "                continue\n",
    "\n",
    "            # Final save\n",
    "            df_oggm = dataset_grid.data\n",
    "            df_oggm.rename(columns={\n",
    "                'aspect': 'aspect_sgi',\n",
    "                'slope': 'slope_sgi'\n",
    "            },\n",
    "                           inplace=True)\n",
    "            df_oggm['POINT_ELEVATION'] = df_oggm['topo']\n",
    "\n",
    "            save_path = os.path.join(path_save_monthly,\n",
    "                                     f\"{sgi_id}_grid_{year}.parquet\")\n",
    "            try:\n",
    "                dataset_grid.data.to_parquet(save_path,\n",
    "                                             engine=\"pyarrow\",\n",
    "                                             compression=\"snappy\")\n",
    "                logging.info(f\"Successfully saved {sgi_id} for {year}\")\n",
    "                with open(csv_log_path, 'a', newline='') as f:\n",
    "                    csv.writer(f).writerow([sgi_id, year, \"success\", \"\"])\n",
    "            except Exception as e:\n",
    "                msg = f\"Error saving dataset for {sgi_id} in {year}: {e}\"\n",
    "                logging.error(msg)\n",
    "                print(msg)\n",
    "                with open(csv_log_path, 'a', newline='') as f:\n",
    "                    csv.writer(f).writerow([sgi_id, year, \"error\", str(e)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2021\n",
    "path_save_monthly = f'../../../data/GLAMOS/topo/gridded_topo_inputs/SGI_regional_preds/{year}/'\n",
    "\n",
    "sgi_id = 'B36-26'  # Aletsch\n",
    "# Plot all OGGM variables\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_save_monthly, f\"{sgi_id}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == 'sep']\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "voi = [\n",
    "    't2m', 'tp', 'ALTITUDE_CLIMATE', 'ELEVATION_DIFFERENCE', 'hugonnet_dhdt',\n",
    "    'consensus_ice_thickness'\n",
    "]\n",
    "axs = axs.flatten()\n",
    "for i, var in enumerate(voi):\n",
    "    sns.scatterplot(df,\n",
    "                    x='POINT_LON',\n",
    "                    y='POINT_LAT',\n",
    "                    hue=var,\n",
    "                    s=5,\n",
    "                    alpha=0.5,\n",
    "                    palette='twilight_shifted',\n",
    "                    ax=axs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ML model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]\n",
    "\n",
    "data_glamos = pd.read_csv(path_PMB_GLAMOS_csv + 'CH_wgms_dataset_all.csv')\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data': path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data': path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    add_pcsr=False,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_swisswide.csv',\n",
    ")\n",
    "data_monthly = dataloader_gl.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_glaciers = [\n",
    "#     'tortin', 'plattalva', 'sanktanna', 'schwarzberg', 'hohlaub', 'pizol',\n",
    "#     'corvatsch', 'tsanfleuron', 'forno'\n",
    "# ]\n",
    "\n",
    "test_glaciers = []\n",
    "\n",
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in test_glaciers]\n",
    "\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(test_glaciers)]\n",
    "print('Size of test data:', len(data_test))\n",
    "\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of train data:', len(data_train))\n",
    "\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "# Number of annual versus winter measurements:\n",
    "print('Train:')\n",
    "print('Number of winter and annual samples:', len(data_train))\n",
    "print('Number of annual samples:',\n",
    "      len(data_train[data_train.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_train[data_train.PERIOD == 'winter']))\n",
    "\n",
    "# Same for test\n",
    "data_test_annual = data_test[data_test.PERIOD == 'annual']\n",
    "data_test_winter = data_test[data_test.PERIOD == 'winter']\n",
    "\n",
    "print('Test:')\n",
    "print('Number of winter and annual samples:', len(data_test))\n",
    "print('Number of annual samples:', len(data_test_annual))\n",
    "print('Number of winter samples:', len(data_test_winter))\n",
    "\n",
    "print('Total:')\n",
    "print('Number of monthly rows:', len(dataloader_gl.data))\n",
    "print('Number of annual rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'annual']))\n",
    "print('Number of winter rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'winter']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))\n",
    "\n",
    "visualiseSplits(test_set['y'], train_set['y'], splits)\n",
    "visualiseInputs(train_set, test_set, vois_climate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_init = {}\n",
    "param_init['device'] = 'cuda:0'\n",
    "param_init['tree_method'] = 'hist'\n",
    "param_init[\"random_state\"] = cfg.seed\n",
    "param_init[\"n_jobs\"] = cfg.numJobs\n",
    "\n",
    "custom_params = {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 800}\n",
    "\n",
    "# Feature columns:\n",
    "feature_columns = ['ELEVATION_DIFFERENCE'\n",
    "                   ] + list(vois_climate) + list(vois_topographical)\n",
    "# feature_columns = ['ELEVATION_DIFFERENCE'\n",
    "#                    ] + list(vois_climate) + list(vois_topographical)\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "df_X_train_subset = train_set['df_X'][all_columns]\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of testing dataset:', test_set['df_X'][all_columns].shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "params = {**param_init, **custom_params}\n",
    "print(params)\n",
    "custom_model = mbm.models.CustomXGBoostRegressor(cfg, **params)\n",
    "\n",
    "# Fit on train data:\n",
    "custom_model.fit(train_set['df_X'][all_columns], train_set['y'])\n",
    "\n",
    "# Make predictions on test\n",
    "custom_model = custom_model.set_params(device='cpu')\n",
    "features_test, metadata_test = custom_model._create_features_metadata(\n",
    "    test_set['df_X'][all_columns])\n",
    "y_pred = custom_model.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = custom_model.aggrPredict(metadata_test, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = custom_model.score(test_set['df_X'][all_columns],\n",
    "                           test_set['y'])  # negative\n",
    "print('Overall score:', np.abs(score))\n",
    "\n",
    "grouped_ids = getDfAggregatePred(test_set, y_pred_agg, all_columns)\n",
    "# PlotPredictions(grouped_ids, y_pred, metadata_test, test_set, custom_model)\n",
    "# plt.suptitle(f'MBM tested on {test_glaciers}', fontsize=20)\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = False\n",
    "\n",
    "# Define paths\n",
    "path_xr_grids = '../../../data/GLAMOS/topo/SGI2020/xr_masked_grids_sgi/'  # SGI DEMs\n",
    "\n",
    "if RUN:\n",
    "    years = range(2016, 2023)\n",
    "    # years = [2016]\n",
    "    for year in years:\n",
    "        path_save_glw = f'../../../data/GLAMOS/distributed_MB_grids/MBM/swisswide/{year}/'\n",
    "        path_monthly_grids = f'../../../data/GLAMOS/topo/gridded_topo_inputs/SGI_regional_preds/{year}/'\n",
    "        sgi_id_list = [\n",
    "            re.split('_', f)[0] for f in os.listdir(path_monthly_grids)\n",
    "        ]\n",
    "\n",
    "        # check if path exists\n",
    "        if not os.path.exists(path_save_glw):\n",
    "            os.makedirs(path_save_glw)\n",
    "        else:\n",
    "            emptyfolder(path_save_glw)\n",
    "\n",
    "        # Feature columns\n",
    "        vois_climate = [\n",
    "            't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "        ]\n",
    "        feature_columns = ['ELEVATION_DIFFERENCE'\n",
    "                           ] + list(vois_climate) + list(vois_topographical)\n",
    "        all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "        print('Running for feature columns:', all_columns)\n",
    "\n",
    "        for sgi_id in tqdm(sgi_id_list, desc='SGI Ids'):\n",
    "            print(sgi_id)\n",
    "            # Load parquet input glacier grid file in monthly format (pre-processed)\n",
    "            df_grid_monthly = pd.read_parquet(\n",
    "                os.path.join(path_monthly_grids,\n",
    "                             f\"{sgi_id}_grid_{year}.parquet\"))\n",
    "\n",
    "            df_grid_monthly.drop_duplicates(inplace=True)\n",
    "\n",
    "            # Keep only necessary columns, avoiding missing columns issues\n",
    "            df_grid_monthly = df_grid_monthly[[\n",
    "                col for col in all_columns if col in df_grid_monthly.columns\n",
    "            ]]\n",
    "\n",
    "            # Create geodata object\n",
    "            geoData = mbm.GeoData(df_grid_monthly)\n",
    "\n",
    "            # Computes and saves gridded MB for a year and glacier\n",
    "            path_glacier_dem = os.path.join(path_xr_grids, f\"{sgi_id}.zarr\")\n",
    "            geoData.gridded_MB_pred(custom_model,\n",
    "                                    sgi_id,\n",
    "                                    year,\n",
    "                                    all_columns,\n",
    "                                    path_glacier_dem,\n",
    "                                    path_save_glw,\n",
    "                                    cfg,\n",
    "                                    save_monthly_pred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open an example\n",
    "sgi_id = 'B36-26'  # Aletsch\n",
    "year = 2016\n",
    "path_save_glw = f'../../../data/GLAMOS/distributed_MB_grids/MBM/swisswide/{year}/'\n",
    "path = os.path.join(path_save_glw, f\"{sgi_id}/{sgi_id}_{year}_annual.zarr\")\n",
    "\n",
    "xr.open_dataset(path).pred_masked.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parquet input glacier grid file in monthly format (pre-processed)\n",
    "sgi_id = 'B36-26'  # Aletsch\n",
    "year = 2016\n",
    "path_monthly_grids = f'../../../data/GLAMOS/topo/gridded_topo_inputs/SGI_regional_preds/{year}/'\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_monthly_grids, f\"{sgi_id}_grid_{year}.parquet\"))\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "df = df[df.MONTHS == 'sep']\n",
    "voi = ['t2m', 'tp', 'hugonnet_dhdt', 'consensus_ice_thickness']\n",
    "for i, var in enumerate(voi):\n",
    "    sns.scatterplot(df,\n",
    "                    x='POINT_LON',\n",
    "                    y='POINT_LAT',\n",
    "                    hue=var,\n",
    "                    s=5,\n",
    "                    alpha=0.5,\n",
    "                    palette='twilight_shifted',\n",
    "                    ax=axs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at 2016:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean predicted MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2016\n",
    "path_save_glw = f'../../../data/GLAMOS/distributed_MB_grids/MBM/swisswide/{year}/'\n",
    "sgi_id_list = os.listdir(path_save_glw)\n",
    "\n",
    "def get_mean_mb_year(year):\n",
    "    path_save_glw = f'../../../data/GLAMOS/distributed_MB_grids/MBM/swisswide/{year}/'\n",
    "\n",
    "    # Calculate mean predicted mb for each glacier\n",
    "    rows = []\n",
    "    for sgi_id in tqdm(sgi_id_list):\n",
    "        gridd_mb = xr.open_dataset(\n",
    "            os.path.join(path_save_glw,\n",
    "                         f\"{sgi_id}/{sgi_id}_{year}_annual.zarr\"))\n",
    "        mean_value = gridd_mb.pred_masked.mean().values.item()\n",
    "        rows.append({'sgi_id': sgi_id, 'mean_mb': mean_value})\n",
    "\n",
    "    mean_mb = pd.DataFrame(rows)\n",
    "    return mean_mb\n",
    "\n",
    "\n",
    "mean_mb_2016 = get_mean_mb_year(2016)\n",
    "mean_mb_2017 = get_mean_mb_year(2017)\n",
    "mean_mb_2018 = get_mean_mb_year(2018)\n",
    "mean_mb_2019 = get_mean_mb_year(2019)\n",
    "mean_mb_2020 = get_mean_mb_year(2020)\n",
    "mean_mb_2021 = get_mean_mb_year(2021)\n",
    "mean_mb_2022 = get_mean_mb_year(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean mb from MBM and GLAMOS:\n",
    "# open reference GLAMOS\n",
    "df_reference = pd.read_csv(\n",
    "    '../../../data/GLAMOS/massbalance_swisswide_2024_r2024_clean.csv').iloc[1:]\n",
    "\n",
    "ref_MB_glamos = []\n",
    "\n",
    "for year in range(2016, 2023):\n",
    "    ref_CH_y = df_reference[(df_reference.catchment == 'Switzerland')\n",
    "                            & (df_reference.year == str(year))]\n",
    "    ref_MB_glamos.append(ref_CH_y['massbalance evolution'].values[0])\n",
    "\n",
    "# Prepare the data\n",
    "years = list(range(2016, 2023))\n",
    "mbm_mb = [\n",
    "    mean_mb_2016.mean_mb.mean(),\n",
    "    mean_mb_2017.mean_mb.mean(),\n",
    "    mean_mb_2018.mean_mb.mean(),\n",
    "    mean_mb_2019.mean_mb.mean(),\n",
    "    mean_mb_2020.mean_mb.mean(),\n",
    "    mean_mb_2021.mean_mb.mean(),\n",
    "    mean_mb_2022.mean_mb.mean()\n",
    "]\n",
    "\n",
    "# Build DataFrame correctly\n",
    "df = pd.DataFrame({\n",
    "    'MBM MB': mbm_mb,\n",
    "    'GLAMOS MB': ref_MB_glamos\n",
    "}, index=years)\n",
    "\n",
    "# give same type to columns\n",
    "df['MBM MB'] = df['MBM MB'].astype(float)\n",
    "df['GLAMOS MB'] = df['GLAMOS MB'].astype(float)\n",
    "\n",
    "# Now plotting works\n",
    "df.plot(kind='bar', figsize=(8, 5))\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Mean Mass Balance (m w.e.)')\n",
    "plt.title('Comparison of Mean Mass Balance: MBM vs GLAMOS')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get volumes and areas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_id(id_str):\n",
    "    return id_str.replace('/', '-')\n",
    "\n",
    "# Paths\n",
    "path_volumes = '../../../data/GLAMOS/volumes/'\n",
    "path_areas = '../../../data/GLAMOS/topo/SGI2020/inventory_sgi2016_r2020'\n",
    "\n",
    "# Load the shapefile of volumes\n",
    "volgdf = gpd.read_file(os.path.join(path_volumes, 'Summary.shp'))\n",
    "volgdf['sgi-id'] = volgdf['pk_sgi'].apply(convert_id)\n",
    "volgdf['V_2016'] = volgdf['V_2016'] * 10**9  # convert to m³\n",
    "\n",
    "# Load the shapefile of areas from SGI 2016\n",
    "areagdf = gpd.read_file(os.path.join(path_areas, 'SGI_2016_glaciers.shp'))\n",
    "areagdf['area_2016'] = areagdf['area_km2'] * 10**6  # convert to m²\n",
    "\n",
    "# Initialize glacier_info with volumes and areas\n",
    "glacier_info = volgdf[['sgi-id',\n",
    "                       'V_2016']].merge(areagdf[['sgi-id', 'area_2016']],\n",
    "                                        on='sgi-id',\n",
    "                                        how='inner')\n",
    "\n",
    "# List of years you want to process\n",
    "years = range(2016, 2023)  # includes 2022\n",
    "\n",
    "# Now loop over the years and merge mean mass balance year by year\n",
    "for year in years:\n",
    "    mean_mb_df = globals().get(f\"mean_mb_{year}\", None)\n",
    "    if mean_mb_df is not None:\n",
    "        mean_mb_df = mean_mb_df.copy()\n",
    "        mean_mb_df['sgi-id'] = mean_mb_df['sgi_id'].apply(convert_id)\n",
    "        glacier_info = glacier_info.merge(\n",
    "            mean_mb_df[['sgi-id', 'mean_mb'\n",
    "                        ]].rename(columns={'mean_mb': f'mean_mb_{year}'}),\n",
    "            on='sgi-id',\n",
    "            how=\n",
    "            'left'  # use 'left' to avoid dropping glaciers if some years are missing\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Warning: mean_mb_{year} not found in globals.\")\n",
    "\n",
    "glacier_info.dropna(inplace=True)  # Drop rows with NaN values\n",
    "glacier_info.set_index('sgi-id', inplace=True)\n",
    "glacier_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total vol change 2016:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_ice = 916.7  # or 917 kg/m³\n",
    "density_water = 1000  # kg/m³\n",
    "\n",
    "# Calculate volume changes\n",
    "glacier_info['vol_change_2016'] = (glacier_info['area_2016'] *\n",
    "                                   glacier_info['mean_mb_2016']) * (\n",
    "                                       density_water / density_ice)\n",
    "\n",
    "vol_change_2016 = glacier_info['vol_change_2016'].sum(\n",
    ") / 10**9  # convert to km3\n",
    "volume_2016 = glacier_info['V_2016'].sum() / 10**9  # convert to km3\n",
    "area_2016 = glacier_info['area_2016'].sum() / 10**6  # convert to km2\n",
    "volume_change_2016_perc = vol_change_2016 / volume_2016 * 100\n",
    "mb_2016 = glacier_info['mean_mb_2016'].mean()\n",
    "\n",
    "ref_CH_2016 = df_reference[(df_reference.catchment == 'Switzerland')\n",
    "                           & (df_reference.year == '2016')]\n",
    "\n",
    "print('Volume change from GLAMOS:', ref_CH_2016['volume change'].values[0],\n",
    "      '%')  # in %\n",
    "print('Volume change from MBM:', np.round(volume_change_2016_perc, 2),\n",
    "      '%')  # in %\n",
    "\n",
    "print('Mean mass balance from GLAMOS:',\n",
    "      ref_CH_2016['massbalance evolution'].values[0], 'm w.e.')\n",
    "print('Mean mass balance from MBM:', np.round(mb_2016, 2), 'm w.e.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume area scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def volume_area_scaling(\n",
    "#         glacier_info,\n",
    "#         t1,\n",
    "#         beta=1.36,\n",
    "#         density_ice=916.7,  # or 917 kg/m³\n",
    "#         density_water=1000  # kg/m³\n",
    "# ):\n",
    "#     # calculate c for every glacier\n",
    "#     glacier_info['c'] = glacier_info[f'V_{t1}'] / (glacier_info[f'area_{t1}']**\n",
    "#                                                    beta)\n",
    "#     years = [t1, t1 + 1]\n",
    "\n",
    "#     # Initialize arrays to store volume and area\n",
    "#     volume_ev = pd.DataFrame(columns=years, index=glacier_info.index)\n",
    "#     area_ev = pd.DataFrame(columns=years, index=glacier_info.index)\n",
    "#     mb_ev = pd.DataFrame(columns=years, index=glacier_info.index)\n",
    "\n",
    "#     # Set initial conditions\n",
    "#     volume_ev[t1] = glacier_info[f'V_{t1}'].values\n",
    "#     area_ev[t1] = glacier_info[f'area_{t1}'].values\n",
    "#     mb_ev[t1] = glacier_info[f'mean_mb_{t1}'].values\n",
    "\n",
    "#     # Add mb evolution\n",
    "#     mb_ev[t1 + 1] = glacier_info[f'mean_mb_{t1+1}'].values\n",
    "\n",
    "#     # Forward simulation\n",
    "#     # Calculate volume change\n",
    "#     volume_change = mb_ev[t1] * area_ev[t1] * (density_water / density_ice)\n",
    "\n",
    "#     # Update volume\n",
    "#     volume_ev[t1 + 1] = np.maximum(volume_ev[t1] + volume_change, 0)\n",
    "\n",
    "#     # Update area using V-A scaling\n",
    "#     area_ev[t1 + 1] = np.maximum(\n",
    "#         (volume_ev[t1 + 1] / glacier_info['c'])**(1 / beta), 0)\n",
    "\n",
    "#     glacier_info[f'area_{year+1}'] = area_ev[year + 1]\n",
    "#     glacier_info[f'V_{year+1}'] = volume_ev[year + 1]\n",
    "\n",
    "\n",
    "# end_year = 2022\n",
    "# for year in range(2016, end_year):\n",
    "#     volume_area_scaling(\n",
    "#         glacier_info,\n",
    "#         t1=year,\n",
    "#         beta=1.36,\n",
    "#     )\n",
    "# glacier_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def volume_area_scaling(\n",
    "    glacier_info,\n",
    "    t1,\n",
    "    beta=1.36,\n",
    "    density_ice=916.7,  # kg/m³\n",
    "    density_water=1000  # kg/m³\n",
    "):\n",
    "    \"\"\"\n",
    "    Update glacier_info by applying volume-area scaling from year t1 to t1+1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate c if not already done\n",
    "    if 'c' not in glacier_info.columns:\n",
    "        glacier_info['c'] = glacier_info[f'V_{t1}'] / (glacier_info[f'area_{t1}'] ** beta)\n",
    "\n",
    "    # Get starting volume and area\n",
    "    V_t1 = glacier_info[f'V_{t1}']\n",
    "    A_t1 = glacier_info[f'area_{t1}']\n",
    "\n",
    "    # Get mass balance for the following year (mean_mb at t1+1)\n",
    "    mb = glacier_info[f'mean_mb_{t1}']\n",
    "\n",
    "    # Calculate volume change [m³ of ice]\n",
    "    vol_change = mb * A_t1 * (density_water / density_ice)\n",
    "\n",
    "    # Update volume, ensuring non-negative\n",
    "    V_t2 = (V_t1 + vol_change).clip(lower=0)\n",
    "\n",
    "    # Update area using volume-area scaling, ensuring non-negative\n",
    "    A_t2 = (V_t2 / glacier_info['c']) ** (1 / beta)\n",
    "    A_t2 = A_t2.clip(lower=0)\n",
    "\n",
    "    # Save results back to glacier_info\n",
    "    glacier_info[f'V_{t1+1}'] = V_t2\n",
    "    glacier_info[f'area_{t1+1}'] = A_t2\n",
    "    \n",
    "end_year = 2022\n",
    "for year in range(2016, end_year):\n",
    "    volume_area_scaling(\n",
    "        glacier_info,\n",
    "        t1=year,\n",
    "        beta=1.36\n",
    "    )\n",
    "\n",
    "glacier_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_info['vol_change_2022'] = (glacier_info['area_2022'] *\n",
    "                                   glacier_info['mean_mb_2022']) * (\n",
    "                                       density_water / density_ice)\n",
    "\n",
    "vol_change_2022 = glacier_info['vol_change_2022'].sum(\n",
    ") / 10**9  # convert to km3\n",
    "\n",
    "volume_2022 = glacier_info['V_2022'].sum() / 10**9  # convert to km3\n",
    "mb_2022 = glacier_info['mean_mb_2022'].mean()\n",
    "\n",
    "volume_change_2022_perc = vol_change_2022 / volume_2022 * 100\n",
    "\n",
    "ref_CH_2022 = df_reference[(df_reference.catchment == 'Switzerland')\n",
    "                           & (df_reference.year == '2022')]\n",
    "\n",
    "print('Volume change from GLAMOS:', ref_CH_2022['volume change'].values[0],\n",
    "      '%')  # in %\n",
    "print('Volume change from MBM:', np.round(volume_change_2022_perc, 2),\n",
    "      '%')  # in %\n",
    "\n",
    "print('Mean mass balance from GLAMOS:',\n",
    "      ref_CH_2022['massbalance evolution'].values[0], 'm w.e.')\n",
    "print('Mean mass balance from MBM:', np.round(mb_2022, 2), 'm w.e.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_change_y_perc, ref_V_glamos = [], []\n",
    "for year in range(2016, 2023):\n",
    "    glacier_info[f'vol_change_{year}'] = (glacier_info[f'area_{year}'] *\n",
    "                                    glacier_info[f'mean_mb_{year}']) * (\n",
    "                                        density_water / density_ice)\n",
    "\n",
    "    vol_change_y = glacier_info[f'vol_change_{year}'].sum(\n",
    "    ) / 10**9  # convert to km3\n",
    "\n",
    "    volume_y = glacier_info[f'V_{year}'].sum() / 10**9  # convert to km3\n",
    "    mb_y = glacier_info[f'mean_mb_{year}'].mean()\n",
    "\n",
    "    volume_change_y_perc.append(vol_change_y / volume_y * 100)\n",
    "    \n",
    "    ref_CH_y = df_reference[(df_reference.catchment == 'Switzerland')\n",
    "                           & (df_reference.year == str(year))]\n",
    "    \n",
    "    ref_V_glamos.append(ref_CH_y['volume change'].values[0])\n",
    "    \n",
    "\n",
    "# Build DataFrame correctly\n",
    "df = pd.DataFrame({\n",
    "    'MBM V': volume_change_y_perc,\n",
    "    'GLAMOS V': ref_V_glamos\n",
    "}, index=years)\n",
    "\n",
    "# give same type to columns\n",
    "df['MBM V'] = df['MBM V'].astype(float)\n",
    "df['GLAMOS V'] = df['GLAMOS V'].astype(float)\n",
    "\n",
    "# Now plotting works\n",
    "df.plot(kind='bar', figsize=(8, 5))\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Mean Mass Balance (m w.e.)')\n",
    "plt.title('Comparison of volume change: MBM vs GLAMOS')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = glacier_info.sort_values(by='area_2016', ascending=False).head(20)\n",
    "df_sub['area_2016'] = df_sub['area_2016'] / 10**6  # convert to km2\n",
    "df_sub['area_2022'] = df_sub['area_2022'] / 10**6  # convert to km2\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "index = np.arange(len(df_sub))\n",
    "bar_width = 0.35\n",
    "\n",
    "# Bars for 2016 and 2022 areas\n",
    "ax.bar(index, df_sub['area_2016'], bar_width, label='Area 2016')\n",
    "ax.bar(index + bar_width, df_sub['area_2022'], bar_width, label='Area 2022')\n",
    "\n",
    "# Axis labels and title\n",
    "ax.set_xlabel('Glacier Index')\n",
    "ax.set_ylabel('Area (km²)')\n",
    "ax.set_title('Glacier Area Comparison: 2016 vs 2022')\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels(df_sub.index)\n",
    "\n",
    "# rotate x labels\n",
    "plt.xticks(rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "# Layout optimization\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(12, 6))\n",
    "ax = axs.flatten()\n",
    "for i, year in enumerate(range(2017, 2023)):\n",
    "    area_perc_loss_y = (\n",
    "        glacier_info['area_2016'] -\n",
    "        glacier_info[f'area_{year}']) / glacier_info['area_2016']\n",
    "\n",
    "    sns.boxplot(area_perc_loss_y, ax=ax[i], color='blue')\n",
    "    ax[i].set_title(f'Loss from 2016 to {year}')\n",
    "    ax[i].set_ylabel('Area loss (%)')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_info[glacier_info['area_2018'] == 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
