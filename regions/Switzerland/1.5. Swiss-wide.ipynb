{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glacier grids from SGI or GLAMOS:\n",
    "\n",
    "Creates monthly grid files for the MBM to make PMB predictions over the whole glacier grid. The files come from the SGI grid and use OGGM topography. Computing takes a long time because of the conversion to monthly format.\n",
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import massbalancemachine as mbm\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import geopandas as gpd\n",
    "\n",
    "# scripts\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.geodata import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.config_CH import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()  # in case no memory\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "# Climate columns\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "# Topographical columns\n",
    "voi_topographical = [\n",
    "    \"aspect\",\n",
    "    \"slope\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "    \"topo\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers_glamos_dem = os.listdir(os.path.join(path_GLAMOS_topo, 'lv95/'))\n",
    "\n",
    "# Glacier outlines:\n",
    "glacier_outline_sgi = gpd.read_file(\n",
    "    os.path.join(path_SGI_topo, 'inventory_sgi2016_r2020',\n",
    "                 'SGI_2016_glaciers_copy.shp'))  # Load the shapefile\n",
    "glacier_outline_rgi = gpd.read_file(path_rgi_outlines)\n",
    "\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area()\n",
    "gl_area['clariden'] = gl_area['claridenL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RGI data\n",
    "rgi_df = pd.read_csv(path_glacier_ids,\n",
    "                     sep=',').rename(columns=lambda x: x.strip())\n",
    "\n",
    "# Sort and set index for easier lookup\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "\n",
    "# Load geodetic mass balance data\n",
    "geodeticMB = pd.read_csv(f\"{path_geodetic_MB_glamos}dV_DOI2024_allcomb.csv\")\n",
    "\n",
    "rgi_df.reset_index(inplace=True)\n",
    "sgi_gl = rgi_df.loc[rgi_df.short_name.isin(\n",
    "    glaciers_glamos_dem)]['sgi-id'].unique()\n",
    "\n",
    "# add clariden\n",
    "clariden_L_sgi_id = rgi_df[rgi_df.short_name == 'claridenL']['sgi-id'].unique()\n",
    "\n",
    "# add to sgi_gl\n",
    "sgi_gl = np.concatenate((sgi_gl, clariden_L_sgi_id))\n",
    "\n",
    "# Filter geodeticMB for relevant SGI IDs\n",
    "geodeticMB = geodeticMB[geodeticMB['SGI-ID'].isin(sgi_gl)]\n",
    "\n",
    "# Create a mapping dictionary for glacier names\n",
    "sgi_to_glacier_name = rgi_df[[\n",
    "    'sgi-id', 'short_name'\n",
    "]].drop_duplicates().set_index('sgi-id')['short_name'].to_dict()\n",
    "\n",
    "# Add glacier names based on SGI-ID mapping\n",
    "geodeticMB['glacier_name'] = geodeticMB['SGI-ID'].map(sgi_to_glacier_name)\n",
    "\n",
    "# Standardize naming convention\n",
    "geodeticMB['glacier_name'].replace({'claridenU': 'clariden'}, inplace=True)\n",
    "\n",
    "# filter to glacier_list\n",
    "geodeticMB = geodeticMB[geodeticMB.glacier_name.isin(glaciers_glamos_dem)]\n",
    "\n",
    "# Extract unique start and end years per glacier\n",
    "years_start_per_gl = geodeticMB.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodeticMB.groupby('glacier_name')['A_end'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "glacier_list_geod = years_start_per_gl.keys()\n",
    "years_start_per_gl, years_end_per_gl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regional predictions (all CH glaciers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgi_list = [\n",
    "    re.split('_',\n",
    "             re.split('.grid', f)[0])[1]\n",
    "    for f in os.listdir(os.path.join(path_SGI_topo, 'aspect'))\n",
    "]\n",
    "\n",
    "# unique SGI IDs\n",
    "sgi_list = list(set(sgi_list))\n",
    "print('Number of unique SGI IDs:', len(sgi_list))\n",
    "\n",
    "glaciers_glamos_dems = os.listdir(os.path.join(path_GLAMOS_topo, 'lv95'))\n",
    "\n",
    "RUN = False\n",
    "if RUN:\n",
    "    # Create SGI topographical masks\n",
    "    # Note: This function will take a while to run\n",
    "    # It creates a mask for each glacier in the SGI list\n",
    "    # and saves them in the specified directory.\n",
    "    create_sgi_topo_masks(sgi_list,\n",
    "                          type='sgi_id',\n",
    "                          path_save=os.path.join(path_SGI_topo,\n",
    "                                                 'xr_masked_grids_sgi/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(path_SGI_topo, 'xr_masked_grids_sgi/')\n",
    "xr.open_dataset(path + 'A10g-02.zarr').masked_aspect.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = pd.read_csv(path_PMB_GLAMOS_csv + 'CH_wgms_dataset_all.csv')\n",
    "gl_area = get_gl_area()\n",
    "areas_train_set = [\n",
    "    gl_area[gl] for gl in data_glamos['GLACIER'].unique()\n",
    "    if gl in gl_area.keys()\n",
    "]\n",
    "\n",
    "# histogram\n",
    "plt.hist(areas_train_set, bins=50)\n",
    "plt.xlabel('Area (km2)')\n",
    "plt.title('Histogram of glacier areas with stakes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile\n",
    "shapefile_path = os.path.join(path_SGI_topo, 'inventory_sgi2016_r2020',\n",
    "                              'SGI_2016_glaciers.shp')\n",
    "gdf_shapefiles = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Histogram of area:\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "sns.histplot(gdf_shapefiles.area / (10**6),\n",
    "             color='blue',\n",
    "             kde=True,\n",
    "             bins=50,\n",
    "             ax=axs[0])\n",
    "\n",
    "# boxplot\n",
    "sns.boxplot(x=gdf_shapefiles.area / (10**6), color='blue', ax=axs[1])\n",
    "\n",
    "# set x label to km2\n",
    "axs[0].set_xlabel('Area (km2)')\n",
    "axs[1].set_xlabel('Area (km2)')\n",
    "\n",
    "plt.suptitle('Histogram and Boxplot of all glaciers in SGI 2016')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create grids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2016\n",
    "path_save_monthly = '../../../data/GLAMOS/topo/gridded_topo_inputs/SGI_regional_preds/2016/'\n",
    "\n",
    "RUN = False\n",
    "if RUN:\n",
    "    emptyfolder(path_save_monthly)\n",
    "    for sgi_id in tqdm(sgi_list, desc='Processing glaciers'):\n",
    "        print(f\"\\n-----------------------------------\\nProcessing {sgi_id}\")\n",
    "\n",
    "        # Load SGI masked grid (previously resampled)\n",
    "        try:\n",
    "            path_save = os.path.join(path_SGI_topo, 'xr_masked_grids_sgi/')\n",
    "            path = os.path.join(path_save, f\"{sgi_id}.zarr\")\n",
    "            ds_coarsened = xr.open_dataset(path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset for {sgi_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Create glacier grid\n",
    "        try:\n",
    "            rgi_id = None\n",
    "            df_grid = create_glacier_grid_SGI(sgi_id, year, rgi_id,\n",
    "                                              ds_coarsened)\n",
    "            df_grid.reset_index(drop=True, inplace=True)\n",
    "            dataset_grid = mbm.Dataset(cfg=cfg,\n",
    "                                       data=df_grid,\n",
    "                                       region_name='CH',\n",
    "                                       data_path=path_PMB_GLAMOS_csv)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating glacier grid for {sgi_id} in {year}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Add climate data\n",
    "        try:\n",
    "            era5_climate_data = os.path.join(path_ERA5_raw,\n",
    "                                             'era5_monthly_averaged_data.nc')\n",
    "            geopotential_data = os.path.join(path_ERA5_raw,\n",
    "                                             'era5_geopotential_pressure.nc')\n",
    "            dataset_grid.get_climate_features(\n",
    "                climate_data=era5_climate_data,\n",
    "                geopotential_data=geopotential_data,\n",
    "                change_units=True)\n",
    "\n",
    "            # if dataset_grid.data is empty throw error\n",
    "            if dataset_grid.data.empty:\n",
    "                raise ValueError(\n",
    "                    f\"No climate data for glacier {sgi_id} in {year}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding climate data for {sgi_id} in {year}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Add OGGM topographic data\n",
    "        try:\n",
    "            df_y_gl = dataset_grid.data\n",
    "            df_y_gl.rename(columns={'RGIId': 'RGIId_old'}, inplace=True)\n",
    "\n",
    "            # Add RGI IDs for OGGM data through intersection with shapefiles\n",
    "            df_y_gl = mbm.data_processing.utils.get_rgi(\n",
    "                data=df_y_gl, glacier_outlines=glacier_outline_rgi)\n",
    "\n",
    "            # Drop points without RGI ID (outside of RGI outlines)\n",
    "            df_y_gl = df_y_gl.dropna(subset=['RGIId'])\n",
    "\n",
    "            # if df_y_gl is empty throw error\n",
    "            if df_y_gl.empty:\n",
    "                raise ValueError()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error: no intersection for glacier {sgi_id} with RGI outlines (too small). Skipping...\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Variables of interest\n",
    "            voi = [\"hugonnet_dhdt\", \"consensus_ice_thickness\", \"millan_v\"]\n",
    "\n",
    "            df_y_gl = add_OGGM_features(df_y_gl, voi, path_OGGM)\n",
    "\n",
    "            # Add GLWD_ID\n",
    "            # print('  - Adding GLWD ID...')\n",
    "            df_y_gl['GLWD_ID'] = df_y_gl.apply(\n",
    "                lambda x: get_hash(f\"{x.GLACIER}_{x.YEAR}\"), axis=1)\n",
    "            df_y_gl['GLWD_ID'] = df_y_gl['GLWD_ID'].astype(str)\n",
    "\n",
    "            dataset_grid = mbm.Dataset(cfg=cfg,\n",
    "                                       data=df_y_gl,\n",
    "                                       region_name='CH',\n",
    "                                       data_path=path_PMB_GLAMOS_csv)\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding OGGM data for {sgi_id} in {year}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Convert to monthly time resolution\n",
    "        try:\n",
    "            dataset_grid.convert_to_monthly(\n",
    "                meta_data_columns=cfg.metaData,\n",
    "                vois_climate=vois_climate,\n",
    "                vois_topographical=voi_topographical)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error converting to monthly resolution for {sgi_id} in {year}: {e}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Rename columns\n",
    "        df_oggm = dataset_grid.data\n",
    "        df_oggm.rename(columns={\n",
    "            'aspect': 'aspect_sgi',\n",
    "            'slope': 'slope_sgi'\n",
    "        },\n",
    "                       inplace=True)\n",
    "\n",
    "        df_oggm['POINT_ELEVATION'] = df_oggm['topo']\n",
    "\n",
    "        # Save gridded dataset\n",
    "        save_path = os.path.join(path_save_monthly,\n",
    "                                 f\"{sgi_id}_grid_{year}.parquet\")\n",
    "        try:\n",
    "            dataset_grid.data.to_parquet(save_path,\n",
    "                                         engine=\"pyarrow\",\n",
    "                                         compression=\"snappy\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving dataset for {sgi_id} in {year}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save_monthly = '../../../data/GLAMOS/topo/gridded_topo_inputs/SGI_regional_preds/2016/'\n",
    "year = 2016\n",
    "sgi_id = 'A50i-16'\n",
    "# Plot all OGGM variables\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_save_monthly, f\"{sgi_id}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == 'sep']\n",
    "voi = ['hugonnet_dhdt', 'consensus_ice_thickness', 'millan_v']\n",
    "for i, var in enumerate(voi):\n",
    "    sns.scatterplot(df,\n",
    "                    x='POINT_LON',\n",
    "                    y='POINT_LAT',\n",
    "                    hue=var,\n",
    "                    s=5,\n",
    "                    alpha=0.5,\n",
    "                    palette='twilight_shifted',\n",
    "                    ax=axs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir(path_save_monthly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ML model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = pd.read_csv(path_PMB_GLAMOS_csv + 'CH_wgms_dataset_all.csv')\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data': path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data': path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data(run_flag=RUN,\n",
    "                                     data_glamos=data_glamos,\n",
    "                                     paths=paths,\n",
    "                                     cfg=cfg,\n",
    "                                     vois_climate=vois_climate,\n",
    "                                     vois_topographical=vois_topographical)\n",
    "data_monthly = dataloader_gl.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_glaciers = [\n",
    "    'tortin', 'plattalva', 'sanktanna', 'schwarzberg', 'hohlaub', 'pizol',\n",
    "    'corvatsch', 'tsanfleuron', 'forno'\n",
    "]\n",
    "\n",
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in test_glaciers]\n",
    "\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(test_glaciers)]\n",
    "print('Size of test data:', len(data_test))\n",
    "\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of train data:', len(data_train))\n",
    "\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "# Number of annual versus winter measurements:\n",
    "print('Train:')\n",
    "print('Number of winter and annual samples:', len(data_train))\n",
    "print('Number of annual samples:',\n",
    "      len(data_train[data_train.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_train[data_train.PERIOD == 'winter']))\n",
    "\n",
    "# Same for test\n",
    "data_test_annual = data_test[data_test.PERIOD == 'annual']\n",
    "data_test_winter = data_test[data_test.PERIOD == 'winter']\n",
    "\n",
    "print('Test:')\n",
    "print('Number of winter and annual samples:', len(data_test))\n",
    "print('Number of annual samples:', len(data_test_annual))\n",
    "print('Number of winter samples:', len(data_test_winter))\n",
    "\n",
    "print('Total:')\n",
    "print('Number of monthly rows:', len(dataloader_gl.data))\n",
    "print('Number of annual rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'annual']))\n",
    "print('Number of winter rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'winter']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))\n",
    "\n",
    "visualiseSplits(test_set['y'], train_set['y'], splits)\n",
    "visualiseInputs(train_set, test_set, vois_climate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_init = {}\n",
    "param_init['device'] = 'cuda:0'\n",
    "param_init['tree_method'] = 'hist'\n",
    "param_init[\"random_state\"] = cfg.seed\n",
    "param_init[\"n_jobs\"] = cfg.numJobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_params = {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 800}\n",
    "\n",
    "# Feature columns:\n",
    "feature_columns = ['ELEVATION_DIFFERENCE'\n",
    "                   ] + list(vois_climate) + list(vois_topographical)\n",
    "# feature_columns = ['ELEVATION_DIFFERENCE'\n",
    "#                    ] + list(vois_climate) + list(vois_topographical)\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "df_X_train_subset = train_set['df_X'][all_columns]\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of testing dataset:', test_set['df_X'][all_columns].shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "params = {**param_init, **custom_params}\n",
    "print(params)\n",
    "custom_model = mbm.models.CustomXGBoostRegressor(cfg, **params)\n",
    "\n",
    "# Fit on train data:\n",
    "custom_model.fit(train_set['df_X'][all_columns], train_set['y'])\n",
    "\n",
    "# Make predictions on test\n",
    "custom_model = custom_model.set_params(device='cpu')\n",
    "features_test, metadata_test = custom_model._create_features_metadata(\n",
    "    test_set['df_X'][all_columns])\n",
    "y_pred = custom_model.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = custom_model.aggrPredict(metadata_test, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = custom_model.score(test_set['df_X'][all_columns],\n",
    "                           test_set['y'])  # negative\n",
    "print('Overall score:', np.abs(score))\n",
    "\n",
    "grouped_ids = getDfAggregatePred(test_set, y_pred_agg, all_columns)\n",
    "PlotPredictions(grouped_ids, y_pred, metadata_test, test_set, custom_model)\n",
    "plt.suptitle(f'MBM tested on {test_glaciers}', fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = True\n",
    "\n",
    "# Define paths\n",
    "path_save_glw = '../../../data/GLAMOS/distributed_MB_grids/MBM/swisswide/2016/'\n",
    "path_xr_grids = '../../../data/GLAMOS/topo/SGI2020/xr_masked_grids_sgi/'  # GLAMOS DEMs\n",
    "\n",
    "path_monthly_grids = '../../../data/GLAMOS/topo/gridded_topo_inputs/SGI_regional_preds/2016/'\n",
    "\n",
    "sgi_id_list = [re.split('_', f)[0] for f in os.listdir(path_monthly_grids)]\n",
    "\n",
    "year = 2016\n",
    "\n",
    "if RUN:\n",
    "    # check if path exists\n",
    "    if not os.path.exists(path_save_glw):\n",
    "        os.makedirs(path_save_glw)\n",
    "    else:\n",
    "        emptyfolder(path_save_glw)\n",
    "\n",
    "    # Feature columns\n",
    "    vois_climate = [\n",
    "        't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "    ]\n",
    "    feature_columns = ['ELEVATION_DIFFERENCE'\n",
    "                       ] + list(vois_climate) + list(vois_topographical)\n",
    "    all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "    print('Running for feature columns:', all_columns)\n",
    "\n",
    "    for sgi_id in tqdm(sgi_id_list, desc='SGI Ids'):\n",
    "        print(sgi_id)\n",
    "        # Load parquet input glacier grid file in monthly format (pre-processed)\n",
    "        df_grid_monthly = pd.read_parquet(\n",
    "            os.path.join(path_monthly_grids, f\"{sgi_id}_grid_{year}.parquet\"))\n",
    "\n",
    "        df_grid_monthly.drop_duplicates(inplace=True)\n",
    "\n",
    "        # Keep only necessary columns, avoiding missing columns issues\n",
    "        df_grid_monthly = df_grid_monthly[[\n",
    "            col for col in all_columns if col in df_grid_monthly.columns\n",
    "        ]]\n",
    "\n",
    "        # Create geodata object\n",
    "        geoData = mbm.GeoData(df_grid_monthly)\n",
    "\n",
    "        # Computes and saves gridded MB for a year and glacier\n",
    "        path_glacier_dem = os.path.join(path_xr_grids, f\"{sgi_id}.zarr\")\n",
    "        geoData.gridded_MB_pred(custom_model,\n",
    "                                sgi_id,\n",
    "                                year,\n",
    "                                all_columns,\n",
    "                                path_glacier_dem,\n",
    "                                path_save_glw,\n",
    "                                cfg,\n",
    "                                save_monthly_pred=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
