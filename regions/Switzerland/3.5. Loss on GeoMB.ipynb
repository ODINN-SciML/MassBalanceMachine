{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from calendar import month_abbr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "import hashlib\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig(\n",
    "    metaData=[\"RGIId\", \"POINT_ID\", \"ID\", \"GLWD_ID\", \"N_MONTHS\", \"MONTHS\", \"PERIOD\", \"GLACIER\", \"YEAR\", \"POINT_LAT\", \"POINT_LON\"],\n",
    "    notMetaDataNotFeatures=[\"POINT_BALANCE\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'\n",
    "\n",
    "# Read glacier ids:\n",
    "glacier_ids = get_glacier_ids(cfg)\n",
    "\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    # \"aspect\", # OGGM\n",
    "    # \"slope\", # OGGM\n",
    "    \"aspect_sgi\",  # SGI\n",
    "    \"slope_sgi\",  # SGI\n",
    "    \"hugonnet_dhdt\",  # OGGM\n",
    "    \"consensus_ice_thickness\",  # OGGM\n",
    "    \"millan_v\",  # OGGM\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = pd.read_csv(cfg.dataPath + path_PMB_GLAMOS_csv + 'CH_wgms_dataset_all.csv')\n",
    "\n",
    "print('Number of glaciers:', len(data_glamos['GLACIER'].unique()))\n",
    "print('Number of winter and annual samples:', len(data_glamos))\n",
    "print('Number of annual samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'winter']))\n",
    "\n",
    "# Capitalize glacier names:\n",
    "glacierCap = {}\n",
    "for gl in data_glamos['GLACIER'].unique():\n",
    "    if isinstance(gl, str):  # Ensure the glacier name is a string\n",
    "        if gl.lower() == 'claridenu':\n",
    "            glacierCap[gl] = 'Clariden_U'\n",
    "        elif gl.lower() == 'claridenl':\n",
    "            glacierCap[gl] = 'Clariden_L'\n",
    "        else:\n",
    "            glacierCap[gl] = gl.capitalize()\n",
    "    else:\n",
    "        print(f\"Warning: Non-string glacier name encountered: {gl}\")\n",
    "\n",
    "data_glamos.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glaciers with pot. radiadation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glaciers with data of potential clear sky radiation\n",
    "# Format to same names as stakes:\n",
    "glDirect = np.sort([\n",
    "    re.search(r'xr_direct_(.*?)\\.zarr', f).group(1)\n",
    "    for f in os.listdir(cfg.dataPath + path_pcsr + 'zarr/')\n",
    "])\n",
    "\n",
    "restgl = np.sort(Diff(list(glDirect), list(data_glamos.GLACIER.unique())))\n",
    "\n",
    "print('Glaciers with potential clear sky radiation data:\\n', glDirect)\n",
    "print('Number of glaciers:', len(glDirect))\n",
    "print('Glaciers without potential clear sky radiation data:\\n', restgl)\n",
    "\n",
    "# Filter out glaciers without data:\n",
    "data_glamos = data_glamos[data_glamos.GLACIER.isin(glDirect)]\n",
    "\n",
    "# Look at the data of the ERA5 dataset:\n",
    "xr.open_dataset(cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geodetic MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_geodetic_grids_present(folder_path, glacier_name,\n",
    "                                 periods_per_glacier):\n",
    "    \"\"\"\n",
    "    Checks if all years between min_start and max_end are present in the folder.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path: Path to the folder containing the files.\n",
    "    - glacier_name: Name of the glacier to filter relevant files.\n",
    "\n",
    "    Returns:\n",
    "    - A set of missing years (if any) and a boolean indicating if all years are present.\n",
    "    \"\"\"\n",
    "    min_start = [min([p[0] for p in periods_per_glacier[glacier_name]])][0]\n",
    "    max_end = [max([p[1] for p in periods_per_glacier[glacier_name]])][0]\n",
    "\n",
    "    # Extract list of files related to the given glacier\n",
    "    files = [\n",
    "        f for f in os.listdir(folder_path)\n",
    "        if f.startswith(f\"{glacier_name}_grid_\") and f.endswith(\".parquet\")\n",
    "    ]\n",
    "\n",
    "    # Extract available years from filenames\n",
    "    year_pattern = re.compile(rf\"{glacier_name}_grid_(\\d{{4}})\\.parquet\")\n",
    "    available_years = {\n",
    "        int(year_pattern.search(f).group(1))\n",
    "        for f in files if year_pattern.search(f)\n",
    "    }\n",
    "\n",
    "    # Expected years\n",
    "    expected_years = set(range(min_start, max_end + 1))\n",
    "\n",
    "    # Identify missing years\n",
    "    missing_years = expected_years - available_years\n",
    "    missing_years = sorted(list(missing_years))\n",
    "\n",
    "    all_years_present = len(missing_years) == 0\n",
    "    return missing_years, all_years_present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process geodetic MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodetic_mb = get_geodetic_MB(cfg)\n",
    "\n",
    "# filter to glaciers with potential clear sky radiation data\n",
    "geodetic_mb = geodetic_mb[geodetic_mb.glacier_name.isin(glDirect)]\n",
    "\n",
    "# get years per glacier\n",
    "years_start_per_gl = geodetic_mb.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodetic_mb.groupby('glacier_name')['Aend'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "periods_per_glacier, _ = build_periods_per_glacier(geodetic_mb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get glacier list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_list = [f for f in list(periods_per_glacier.keys())]\n",
    "\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area(cfg)\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list, gl_area):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "glacier_list = sort_by_area(glacier_list, gl_area)\n",
    "# print len and list\n",
    "print('Number of glaciers:', len(glacier_list))\n",
    "print('Glaciers:', glacier_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_xr_masked_grids = cfg.dataPath+path_GLAMOS_topo+'/xr_masked_grids/'\n",
    "existing_files = set(\n",
    "    os.listdir(path_xr_masked_grids))  # Load file list once for efficiency\n",
    "\n",
    "for glacier_name in glacier_list:\n",
    "    print(f'{glacier_name.capitalize()}:')\n",
    "\n",
    "    min_start = min(p[0] for p in periods_per_glacier[glacier_name])\n",
    "    max_end = max(p[1] for p in periods_per_glacier[glacier_name])\n",
    "\n",
    "    print(f'Longest geodetic period: {min_start} - {max_end}')\n",
    "    print(f'Geodetic periods: {periods_per_glacier[glacier_name]}')\n",
    "\n",
    "    # Geodetic MB:\n",
    "    missing_years, all_years_present = check_geodetic_grids_present(\n",
    "        os.path.join(cfg.dataPath, path_glacier_grid_glamos, glacier_name), glacier_name,\n",
    "        periods_per_glacier)\n",
    "    if not all_years_present:\n",
    "        print(f'Missing DEMS geodetic MB: {missing_years}')\n",
    "\n",
    "    # Gridded MB:\n",
    "    print('...')\n",
    "    GLAMOS_glwmb = get_GLAMOS_glwmb(glacier_name, cfg)\n",
    "    if GLAMOS_glwmb is None:\n",
    "        print('-------------------------------')\n",
    "        continue\n",
    "\n",
    "    start = max(GLAMOS_glwmb.index.min(), 1951)\n",
    "    end = GLAMOS_glwmb.index.max()\n",
    "\n",
    "    print(f'Gridded MB period: {start} - {end}')\n",
    "\n",
    "    # Check that each year in the range has an xr_masked_grids\n",
    "    missing_years = [\n",
    "        year for year in range(start, end + 1)\n",
    "        if year >= 1951 and f'{glacier_name}_{year}.zarr' not in existing_files\n",
    "    ]\n",
    "\n",
    "    if missing_years:\n",
    "        print(f'Missing DEMS gridded MB: {missing_years}')\n",
    "\n",
    "    print('-------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodetic_mb[geodetic_mb.glacier_name == 'corvatsch']['SGI-ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One glacier example: Gries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stake data:\n",
    "### Input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_name = 'gries'\n",
    "\n",
    "data_gl = data_glamos[data_glamos.GLACIER == glacier_name]\n",
    "\n",
    "min_start = min(p[0] for p in periods_per_glacier[glacier_name])\n",
    "max_end = max(p[1] for p in periods_per_glacier[glacier_name])\n",
    "\n",
    "print(f'Longest geodetic period: {min_start} - {max_end}')\n",
    "print(f'Geodetic periods: {periods_per_glacier[glacier_name]}')\n",
    "\n",
    "# Geodetic MB:\n",
    "missing_years, all_years_present = check_geodetic_grids_present(\n",
    "    os.path.join(cfg.dataPath, path_glacier_grid_glamos, glacier_name), glacier_name,\n",
    "    periods_per_glacier)\n",
    "if not all_years_present:\n",
    "    print(f'Missing DEMS geodetic MB: {missing_years}')\n",
    "\n",
    "# Gridded MB:\n",
    "print('...')\n",
    "GLAMOS_glwmb = get_GLAMOS_glwmb(glacier_name, cfg)\n",
    "\n",
    "start = max(GLAMOS_glwmb.index.min(), 1951)\n",
    "end = GLAMOS_glwmb.index.max()\n",
    "\n",
    "print(f'Gridded MB period: {start} - {end}')\n",
    "\n",
    "# Check that each year in the range has an xr_masked_grids\n",
    "missing_years = [\n",
    "    year for year in range(start, end + 1)\n",
    "    if year >= 1951 and f'{glacier_name}_{year}.zarr' not in existing_files\n",
    "]\n",
    "\n",
    "if missing_years:\n",
    "    print(f'Missing DEMS gridded MB: {missing_years}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data': cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data': cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data(run_flag=RUN,\n",
    "                                     data_glamos=data_gl,\n",
    "                                     paths=paths,\n",
    "                                     cfg=cfg,\n",
    "                                     vois_climate=vois_climate,\n",
    "                                     vois_topographical=vois_topographical,\n",
    "                                     output_file='CH_wgms_dataset_gries.csv')\n",
    "\n",
    "data_monthly = dataloader_gl.data\n",
    "\n",
    "data_monthly['GLWD_ID'] = data_monthly.apply(\n",
    "    lambda x: mbm.data_processing.utils.get_hash(f\"{x.GLACIER}_{x.YEAR}\"), axis=1)\n",
    "data_monthly['GLWD_ID'] = data_monthly['GLWD_ID'].astype(str)\n",
    "\n",
    "data_seas = transform_df_to_seasonal(data_monthly)\n",
    "print('Number of seasonal rows', len(data_seas))\n",
    "\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                               data=data_seas,\n",
    "                               random_seed=cfg.seed,\n",
    "                               meta_data_columns=cfg.metaData)\n",
    "\n",
    "data_seas.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocking on stakes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split on measurements (IDs)\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='ID',\n",
    "                                            random_state=cfg.seed,\n",
    "                                            test_size=0.1)\n",
    "\n",
    "# Check that no ID from train set is in test set\n",
    "assert len(set(train_set['df_X'].ID).intersection(set(\n",
    "    test_set['df_X'].ID))) == 0\n",
    "\n",
    "data_train = train_set['df_X']\n",
    "data_test = test_set['df_X']\n",
    "\n",
    "# Number of annual versus winter measurements:\n",
    "print('Train:')\n",
    "print('Number of winter and annual samples:', len(data_train))\n",
    "print('Number of annual samples:',\n",
    "      len(data_train[data_train.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_train[data_train.PERIOD == 'winter']))\n",
    "\n",
    "# Same for test\n",
    "data_test_annual = data_test[data_test.PERIOD == 'annual']\n",
    "data_test_winter = data_test[data_test.PERIOD == 'winter']\n",
    "\n",
    "print('Test:')\n",
    "print('Number of winter and annual samples:', len(data_test))\n",
    "print('Number of annual samples:', len(data_test_annual))\n",
    "print('Number of winter samples:', len(data_test_winter))\n",
    "\n",
    "print('Total:')\n",
    "print('Number of rows:', len(dataloader_gl.data))\n",
    "print('Number of annual rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'annual']))\n",
    "print('Number of winter rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'winter']))\n",
    "\n",
    "visualiseSplits(test_set['y'], train_set['y'], splits)\n",
    "visualiseInputs(train_set, test_set, vois_climate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of measurements per year:\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 10))\n",
    "data_test.groupby(['YEAR', 'PERIOD']).size().unstack().plot(\n",
    "    kind='bar', stacked=True, color=[color_dark_blue, color_pink], ax=ax[0])\n",
    "ax[0].set_title('Number of measurements per year for test set')\n",
    "\n",
    "# Number of measurements per year:\n",
    "data_train.groupby(['YEAR', 'PERIOD']).size().unstack().plot(\n",
    "    kind='bar', stacked=True, color=[color_dark_blue, color_pink], ax=ax[1])\n",
    "ax[1].set_title('Number of measurements per year for train set')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "# For each of the XGBoost parameter, define the grid range\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, 6, 7, 8],\n",
    "    'n_estimators':\n",
    "    [50, 100, 200, 300, 400, 500, 600,\n",
    "     700],  # number of trees (too many = overfitting, too few = underfitting)\n",
    "    'learning_rate': [0.01, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "}\n",
    "\n",
    "param_init = {}\n",
    "param_init['device'] = 'cuda:0'\n",
    "param_init['tree_method'] = 'hist'\n",
    "param_init[\"random_state\"] = cfg.seed\n",
    "param_init[\"n_jobs\"] = cfg.numJobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions of custom parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_params = {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 800}\n",
    "\n",
    "# Feature columns:\n",
    "feature_columns = [\n",
    "    'ELEVATION_DIFFERENCE'\n",
    "] + list(vois_climate) + list(vois_topographical) + ['pcsr']\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "df_X_train_subset = train_set['df_X'][all_columns]\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of testing dataset:', test_set['df_X'][all_columns].shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "params = {**param_init, **custom_params}\n",
    "print(params)\n",
    "custom_model = mbm.models.CustomXGBoostRegressor(cfg, **params)\n",
    "\n",
    "# Fit on train data:\n",
    "custom_model.fit(train_set['df_X'][all_columns], train_set['y'])\n",
    "\n",
    "# Make predictions on test\n",
    "custom_model = custom_model.set_params(device='cpu')\n",
    "features_test, metadata_test = custom_model._create_features_metadata(\n",
    "    test_set['df_X'][all_columns])\n",
    "y_pred = custom_model.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = custom_model.aggrPredict(metadata_test, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = custom_model.score(test_set['df_X'][all_columns],\n",
    "                           test_set['y'])  # negative\n",
    "print('Overall score:', np.abs(score))\n",
    "\n",
    "grouped_ids = getDfAggregatePred(test_set, y_pred_agg, all_columns)\n",
    "PlotPredictions(grouped_ids, y_pred, metadata_test, test_set, custom_model)\n",
    "plt.suptitle(f'MBM tested on test stakes', fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIPlot(custom_model, feature_columns, vois_climate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_gl.data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions on geod MB:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create input array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_name = 'gries'\n",
    "\n",
    "geodetic_period = periods_per_glacier[glacier_name][0]\n",
    "geodetic_range = range(geodetic_period[0], geodetic_period[1] + 1)\n",
    "folder_path = os.path.join(cfg.dataPath, path_glacier_grid_glamos, glacier_name)\n",
    "\n",
    "# check that parquet files for each year\n",
    "files = [\n",
    "    f for f in os.listdir(folder_path)\n",
    "    if f.startswith(f\"{glacier_name}_grid_\") and f.endswith(\".parquet\")\n",
    "]\n",
    "\n",
    "# Extract available years from filenames\n",
    "year_pattern = re.compile(rf\"{glacier_name}_grid_(\\d{{4}})\\.parquet\")\n",
    "available_years = {\n",
    "    int(year_pattern.search(f).group(1))\n",
    "    for f in files if year_pattern.search(f)\n",
    "}\n",
    "\n",
    "# check that period overlaps with available years\n",
    "assert (len(set(available_years).intersection(\n",
    "    set(geodetic_range))) == len(geodetic_range))\n",
    "\n",
    "# Create geodetic input array for MBM for one glacier:\n",
    "df_X_geod = create_geodetic_input(cfg,\n",
    "                                  glacier_name,\n",
    "                                  periods_per_glacier,\n",
    "                                  to_seasonal=True)\n",
    "\n",
    "# Check that each ID has two seasons only\n",
    "assert (df_X_geod.groupby('ID').count().SEASON.unique() == 2)\n",
    "\n",
    "print('Shape of the geodetic input array:', df_X_geod.shape)\n",
    "\n",
    "df_X_geod.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate annual predictions\n",
    "pred_annual = custom_model.glacier_wide_pred(df_X_geod[all_columns])\n",
    "\n",
    "# Calculate mean SMB per year and store in a DataFrame\n",
    "mean_SMB = pred_annual.groupby('GLWD_ID').agg({\n",
    "    'pred':\n",
    "    'mean',\n",
    "    'YEAR':\n",
    "    'first',\n",
    "})\n",
    "mean_SMB = mean_SMB.sort_values(by='YEAR').reset_index().set_index('YEAR')\n",
    "mean_SMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the geodetic mb for each range (manually)\n",
    "geodetic_MB_pred, geodetic_MB_target = [], []\n",
    "for geodetic_period in periods_per_glacier[glacier_name]:\n",
    "    geodetic_range = range(geodetic_period[0], geodetic_period[1] + 1)\n",
    "    geodetic_MB_pred.append(mean_SMB.loc[geodetic_range].pred.mean())\n",
    "\n",
    "y_target = prepareGeoTargets(geodetic_mb, periods_per_glacier, glacier_name)\n",
    "score = -(\n",
    "    (np.array(geodetic_MB_pred) - np.array(y_target))**2).mean()\n",
    "\n",
    "# Calculate the geodetic mb for each range (with implemented function)\n",
    "score_2 = custom_model.score_geod(df_X_geod[all_columns],\n",
    "                                  y_target,\n",
    "                                  periods=periods_per_glacier[glacier_name])\n",
    "\n",
    "# Test: should be the same\n",
    "print('Score:', score)\n",
    "print('Score 2:', score_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
