{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import massbalancemachine as mbm\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, LineString, Point\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import GroupKFold, KFold, train_test_split, GroupShuffleSplit\n",
    "from calendar import month_abbr\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "from oggm import cfg, utils, workflow, tasks\n",
    "import logging\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "\n",
    "import config\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.xgb_helpers import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(config.SEED)\n",
    "\n",
    "# Specify the short names of the climate variables available in the dataset\n",
    "vois_climate = ['t2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str']\n",
    "# vois_topographical = ['aspect', 'slope', 'dis_from_border', 'hugonnet_dhdt']\n",
    "vois_topographical = ['aspect', 'slope', 'dis_from_border']\n",
    "# in case no memory\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "cmap = cm.devon\n",
    "color_palette_glaciers = sns.color_palette(get_cmap_hex(cmap, 15))\n",
    "\n",
    "# For bars and lines:\n",
    "# color_diff_xgb = '#878787'\n",
    "color_diff_xgb = '#4d4d4d'\n",
    "\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_xgb = colors[0]\n",
    "color_xgb_winter = colors[1]\n",
    "\n",
    "color_tim = '#c51b7d'\n",
    "\n",
    "# Violin and boxplots:\n",
    "colors_temp_freq = sns.color_palette(get_cmap_hex(cm.devon, 8))\n",
    "boxplot_style = {\n",
    "    \"width\": .6,\n",
    "    \"showcaps\": False,\n",
    "    \"palette\": colors_temp_freq,\n",
    "    \"flierprops\": {\n",
    "        \"marker\": \"x\"\n",
    "    },\n",
    "    \"showmeans\": True,\n",
    "    \"meanprops\": {\n",
    "        \"markerfacecolor\": \"white\"\n",
    "    }\n",
    "}\n",
    "\n",
    "marker_tim = 's'\n",
    "marker_xgb = 'o'\n",
    "marker_std = '_'\n",
    "\n",
    "custom_working_dir = '../../../data/OGGM/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "path_rgi = '../../../data/GLAMOS/CH_glacier_ids_long.csv'\n",
    "rgi_df = pd.read_csv(path_rgi, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stakes data over all glaciers:\n",
    "data_glamos = pd.read_csv(path_PMB_GLAMOS_csv + 'CH_wgms_dataset.csv')\n",
    "\n",
    "print('Number of glaciers:', len(data_glamos['GLACIER'].unique()))\n",
    "data_glamos.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of mean mass balance per glacier:\n",
    "# Get the mean mass balance per glacier\n",
    "mean_mb_per_glacier = data_glamos.groupby(['GLACIER',\n",
    "                                           'YEAR'])['POINT_BALANCE'].mean()\n",
    "matrix = pd.DataFrame(mean_mb_per_glacier).reset_index().pivot(\n",
    "    index='GLACIER', columns='YEAR',\n",
    "    values='POINT_BALANCE').sort_values(by='GLACIER')\n",
    "\n",
    "# get elevation of glaciers:\n",
    "gl_per_el = data_glamos.groupby(['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "\n",
    "matrix = matrix.loc[gl_per_el.sort_values(ascending=True).index]\n",
    "\n",
    "# make index categorical\n",
    "matrix.index = pd.Categorical(matrix.index,\n",
    "                              categories=matrix.index,\n",
    "                              ordered=True)\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "sns.heatmap(data=matrix,\n",
    "            center=0,\n",
    "            cmap=cm.vik_r,\n",
    "            cbar_kws={'label': '[m w.e. $a^{-1}$]'},\n",
    "            ax=ax)\n",
    "\n",
    "# Plot elevation:\n",
    "fig = plt.figure(figsize=(10, 2))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "sns.lineplot(gl_per_el.sort_values(ascending=True),\n",
    "             ax=ax,\n",
    "             color='gray',\n",
    "             marker='v')\n",
    "ax.set_xticklabels('', rotation=90)\n",
    "ax.set_ylabel('')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gl = data_glamos.groupby(['GLACIER']).size().sort_values()\n",
    "num_gl.plot(kind='bar', figsize=(15, 5), cmap=cmap)\n",
    "plt.title('Number of total measurements per glacier since 1961')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glaciers with potential radiadation data from GLAMOS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glaciers with data of potential clear sky radiation\n",
    "# Format to same names as stakes:\n",
    "glDirect = [\n",
    "    re.search(r'xr_direct_(.*?)\\.nc', f).group(1)\n",
    "    for f in os.listdir(path_direct_save)\n",
    "]\n",
    "glDirect.sort()\n",
    "\n",
    "restgl = Diff(list(glDirect), list(data_glamos.GLACIER.unique()))\n",
    "restgl.sort()\n",
    "print('Glaciers with potential clear sky radiation data:\\n', glDirect)\n",
    "print('Glaciers without potential clear sky radiation data:\\n', restgl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data:\n",
    "### Input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = True\n",
    "if RUN:\n",
    "    # Filter data_glamos\n",
    "    data_glamos = data_glamos[data_glamos.GLACIER.isin(glDirect)]\n",
    "    print('Running on {} glaciers:\\n {}'.format(\n",
    "        len(data_glamos.GLACIER.unique()), data_glamos.GLACIER.unique()))\n",
    "    # Create dataloader:\n",
    "    dataset_gl = mbm.Dataset(data=data_glamos,\n",
    "                             region_name='CH',\n",
    "                             data_path=path_PMB_GLAMOS_csv)\n",
    "    print('Number of winter and annual samples:', len(data_glamos))\n",
    "    print('Number of annual samples:',\n",
    "          len(data_glamos[data_glamos.PERIOD == 'annual']))\n",
    "    print('Number of winter samples:',\n",
    "          len(data_glamos[data_glamos.PERIOD == 'winter']))\n",
    "\n",
    "    # Add climate data:\n",
    "    # Specify the files of the climate data, that will be matched with the coordinates of the stake data\n",
    "    era5_climate_data = path_ERA5_raw + 'era5_monthly_averaged_data.nc'\n",
    "    geopotential_data = path_ERA5_raw + 'era5_geopotential_pressure.nc'\n",
    "\n",
    "    # Match the climate features, from the ERA5Land netCDF file, for each of the stake measurement dataset\n",
    "    dataset_gl.get_climate_features(climate_data=era5_climate_data,\n",
    "                                    geopotential_data=geopotential_data,\n",
    "                                    change_units=True)\n",
    "\n",
    "    # Add potential clear sky radiation:\n",
    "    print('----------------------\\nAdding potential clear sky radiation:')\n",
    "    print('Shape before pot rad:', dataset_gl.data.shape)\n",
    "    dataset_gl.get_potential_rad(path_direct_save)\n",
    "    print('Shape after pot rad:', dataset_gl.data.shape)\n",
    "\n",
    "    print('----------------------\\nConverting to monthly resolution:')\n",
    "    # For each record, convert to a monthly time resolution\n",
    "    dataset_gl.convert_to_monthly(meta_data_columns=config.META_DATA,\n",
    "                                  vois_climate=vois_climate + ['pcsr'],\n",
    "                                  vois_topographical=vois_topographical)\n",
    "\n",
    "    # Create a new DataLoader object with the monthly stake data measurements.\n",
    "    dataloader_gl = mbm.DataLoader(data=dataset_gl.data,\n",
    "                                   random_seed=config.SEED,\n",
    "                                   meta_data_columns=config.META_DATA)\n",
    "\n",
    "    print('Number of monthly rows:', len(dataloader_gl.data))\n",
    "    print('Columns in the dataset:', dataloader_gl.data.columns)\n",
    "\n",
    "    # save the data\n",
    "    dataloader_gl.data.to_csv(path_PMB_GLAMOS_csv +\n",
    "                              'CH_wgms_dataset_monthly_pcsr.csv',\n",
    "                              index=False)\n",
    "else:\n",
    "    # read data\n",
    "    data_monthly = pd.read_csv(path_PMB_GLAMOS_csv +\n",
    "                               'CH_wgms_dataset_monthly_pcsr.csv')\n",
    "    dataloader_gl = mbm.DataLoader(data=data_monthly,\n",
    "                                   random_seed=config.SEED,\n",
    "                                   meta_data_columns=config.META_DATA)\n",
    "    print('Number of monthly rows:', len(dataloader_gl.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check of variables:\n",
    "df = dataloader_gl.data\n",
    "var_to_plot = ['POINT_BALANCE'] + vois_climate + ['pcsr']\n",
    "df = df[(df.GLACIER == 'aletsch') & (df.YEAR == 1961)].groupby(\n",
    "    ['MONTHS'])[var_to_plot].mean().reset_index()\n",
    "df['month_nb'] = df.MONTHS.apply(\n",
    "    lambda x: list(month_abbr).index(x.capitalize()))\n",
    "df.sort_values(by='month_nb', inplace=True)\n",
    "fig, ax = plt.subplots(3, 3, figsize=(10, 8))\n",
    "\n",
    "for i, ax in enumerate(ax.flatten()):\n",
    "    df.plot(x='MONTHS', y=var_to_plot[i], marker='o', ax=ax)\n",
    "    ax.set_title(var_to_plot[i])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocking on glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_glaciers = [\n",
    "#     'tortin', 'plattalva', 'sanktanna', 'schwarzberg', 'hohlaub', 'rhone'\n",
    "# ]\n",
    "test_glaciers = [\n",
    "    'tortin', 'plattalva', 'sanktanna', 'schwarzberg', 'hohlaub', 'gorner',\n",
    "    'joeri', 'corvatsch', 'tsanfleuron'\n",
    "]\n",
    "train_glaciers = [\n",
    "    i for i in data_glamos.GLACIER.unique() if i not in test_glaciers\n",
    "]\n",
    "\n",
    "data_test = data_glamos[data_glamos.GLACIER.isin(test_glaciers)]\n",
    "print('Size of test data:', len(data_test))\n",
    "data_train = data_glamos[data_glamos.GLACIER.isin(train_glaciers)]\n",
    "print('Size of train data:', len(data_train))\n",
    "\n",
    "test_perc = (len(data_test) / len(data_train)) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of mean mass balance per glacier:\n",
    "# Get the mean mass balance per glacier\n",
    "data_with_pot = data_glamos[data_glamos.GLACIER.isin(glDirect)]\n",
    "mean_mb_per_glacier = data_with_pot.groupby(['GLACIER',\n",
    "                                             'YEAR'])['POINT_BALANCE'].mean()\n",
    "matrix = pd.DataFrame(mean_mb_per_glacier).reset_index().pivot(\n",
    "    index='GLACIER', columns='YEAR',\n",
    "    values='POINT_BALANCE').sort_values(by='GLACIER')\n",
    "gl_per_el = data_with_pot.groupby(['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "matrix = matrix.loc[gl_per_el.sort_values(ascending=True).index]\n",
    "\n",
    "# make index categorical\n",
    "matrix.index = pd.Categorical(matrix.index,\n",
    "                              categories=matrix.index,\n",
    "                              ordered=True)\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "sns.heatmap(data=matrix,\n",
    "            center=0,\n",
    "            cmap=cm.vik_r,\n",
    "            cbar_kws={'label': '[m w.e. $a^{-1}$]'},\n",
    "            ax=ax)\n",
    "\n",
    "# add patches for test glaciers\n",
    "for test_gl in test_glaciers:\n",
    "    height = matrix.index.get_loc(test_gl)\n",
    "    row = np.where(matrix.loc[test_gl].notna())[0]\n",
    "    split_indices = np.where(np.diff(row) != 1)[0] + 1\n",
    "    continuous_sequences = np.split(row, split_indices)\n",
    "    for patch in continuous_sequences:\n",
    "        ax.add_patch(\n",
    "            Rectangle((patch.min(), height),\n",
    "                      patch.max() - patch.min() + 1,\n",
    "                      1,\n",
    "                      fill=False,\n",
    "                      edgecolor='blue',\n",
    "                      lw=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits, test_set, train_set = getCVSplits(dataloader_gl,\n",
    "                                          test_split_on='GLACIER',\n",
    "                                          test_splits=test_glaciers)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))\n",
    "visualiseSplits(test_set['y'], train_set['y'], splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualiseInputs(train_set, test_set, vois_climate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distributions of test glaciers:\n",
    "f, ax = plt.subplots(len(test_glaciers),\n",
    "                     10,\n",
    "                     figsize=(16, 10),\n",
    "                     sharey='row',\n",
    "                     sharex='col')\n",
    "\n",
    "for i, test_gl in enumerate(test_glaciers):\n",
    "    test_df_gl = test_set['df_X'][test_set['df_X'].GLACIER == test_gl]\n",
    "    test_df_gl['POINT_BALANCE'].plot.hist(ax=ax[i, 0],\n",
    "                                          color=color_xgb,\n",
    "                                          alpha=0.6,\n",
    "                                          density=False)\n",
    "    ax[i, 0].set_title('PMB')\n",
    "    ax[i, 0].set_ylabel(test_gl)\n",
    "    ax[i, 0].set_xlabel('[m w.e.]')\n",
    "    test_df_gl['ELEVATION_DIFFERENCE'].plot.hist(ax=ax[i, 1],\n",
    "                                                 color=color_xgb,\n",
    "                                                 alpha=0.6,\n",
    "                                                 density=False)\n",
    "    ax[i, 1].set_title('ELV_DIFF]')\n",
    "    ax[i, 1].set_xlabel('[m]')\n",
    "\n",
    "    for j, voi_clim in enumerate(vois_climate + ['pcsr']):\n",
    "        ax[i, 2 + j].set_title(voi_clim)\n",
    "        test_df_gl[voi_clim].plot.hist(ax=ax[i, 2 + j],\n",
    "                                       color=color_xgb,\n",
    "                                       alpha=0.6,\n",
    "                                       density=False)\n",
    "        ax[i, 2 + j].set_xlabel(vois_units[voi_clim])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,\n",
    "                     len(test_glaciers),\n",
    "                     figsize=(20, 5),\n",
    "                     sharey='row',\n",
    "                     sharex='row')\n",
    "\n",
    "for i, test_gl in enumerate(test_glaciers):\n",
    "    test_df_gl = test_set['df_X'][test_set['df_X'].GLACIER == test_gl]\n",
    "    test_df_gl.POINT_ELEVATION.plot.hist(color=color_xgb,\n",
    "                                         alpha=0.5,\n",
    "                                         density=False,\n",
    "                                         ax=ax[i])\n",
    "    # add vertical line for altitude climate\n",
    "    alt_climate = test_df_gl.ALTITUDE_CLIMATE.mean()\n",
    "    ax[i].axvline(x=alt_climate,\n",
    "                  color='red',\n",
    "                  linestyle='--',\n",
    "                  label='Altitude climate')\n",
    "    ax[i].set_xlabel('Elevation [m]')\n",
    "    ax[i].legend()\n",
    "    ax[i].set_title(test_gl)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of measurements per year:\n",
    "data_test.groupby(['YEAR', 'PERIOD'\n",
    "                   ]).size().unstack().plot(kind='bar',\n",
    "                                            stacked=True,\n",
    "                                            figsize=(15, 5),\n",
    "                                            color=[color_xgb, color_tim])\n",
    "plt.title('Number of measurements per year for test glaciers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of measurements per year:\n",
    "data_train.groupby(['YEAR', 'PERIOD'\n",
    "                    ]).size().unstack().plot(kind='bar',\n",
    "                                             stacked=True,\n",
    "                                             figsize=(15, 5),\n",
    "                                             color=[color_xgb, color_tim])\n",
    "plt.title('Number of measurements per year for train glaciers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "# For each of the XGBoost parameter, define the grid range\n",
    "# param_grid = {\n",
    "#     'max_depth': [\n",
    "#         3,\n",
    "#         4,\n",
    "#         5,\n",
    "#         6,\n",
    "#     ],\n",
    "#     'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "#     'n_estimators': [100, 200, 300],\n",
    "#     'gamma': [0, 1]\n",
    "# }\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, 6, 7, 8],\n",
    "    'n_estimators':\n",
    "    [50, 100, 200, 300, 400, 500, 600,\n",
    "     700],  # number of trees (too many = overfitting, too few = underfitting)\n",
    "    'learning_rate': [0.01, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "}\n",
    "\n",
    "param_init = {}\n",
    "param_init['device'] = 'cuda:0'\n",
    "param_init['tree_method'] = 'hist'\n",
    "param_init[\"random_state\"] = config.SEED\n",
    "param_init[\"n_jobs\"] = config.NUM_JOBS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elevation difference:\n",
    "#### All variables grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Feature columns:\n",
    "feature_columns = [\n",
    "    'ELEVATION_DIFFERENCE'\n",
    "] + list(vois_climate) + list(vois_topographical) + ['pcsr']\n",
    "all_columns = feature_columns + config.META_DATA + config.NOT_METADATA_NOT_FEATURES\n",
    "df_X_train_subset = train_set['df_X'][all_columns]\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of testing dataset:', test_set['df_X'][all_columns].shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "RUN = True\n",
    "if RUN:\n",
    "    # Create a CustomXGBoostRegressor instance\n",
    "    custom_xgboost = mbm.models.CustomXGBoostRegressor(**param_init)\n",
    "    custom_xgboost.randomsearch(\n",
    "        parameters=param_grid,\n",
    "        n_iter=45,\n",
    "        splits=splits,\n",
    "        features=df_X_train_subset,\n",
    "        targets=train_set['y'],\n",
    "    )\n",
    "\n",
    "    custom_xgboost.gridsearch(\n",
    "        parameters=param_grid,\n",
    "        splits=splits,\n",
    "        features=df_X_train_subset,\n",
    "        targets=train_set['y'],\n",
    "    )\n",
    "\n",
    "    # save best model\n",
    "    custom_xgboost.save_model(f'xgb_gl_split_pcsr_gs.pkl')\n",
    "else:\n",
    "    # read model\n",
    "    custom_xgboost = mbm.models.CustomXGBoostRegressor()\n",
    "    custom_xgboost.load_model(f'xgb_gl_split_pcsr_gs.pkl')\n",
    "\n",
    "# Get best parameters and estimator\n",
    "best_params = custom_xgboost.param_search.best_params_\n",
    "best_estimator = custom_xgboost.param_search.best_estimator_\n",
    "print(\"Best parameters:\\n\", best_params)\n",
    "print(\"Best score:\\n\", custom_xgboost.param_search.best_score_)\n",
    "\n",
    "# Make predictions on test:\n",
    "# Set to CPU for predictions:\n",
    "best_estimator_cpu = best_estimator.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on test\n",
    "features_test, metadata_test = best_estimator_cpu._create_features_metadata(\n",
    "    test_set['df_X'][all_columns], config.META_DATA)\n",
    "y_pred = best_estimator_cpu.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = best_estimator_cpu.aggrPredict(metadata_test, config.META_DATA,\n",
    "                                            features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = best_estimator_cpu.score(test_set['df_X'][all_columns],\n",
    "                                 test_set['y'])  # negative\n",
    "print('Overall score:', np.abs(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualiseValPreds(best_estimator, splits, train_set, feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotGridSearchScore(cv_results_=custom_xgboost.param_search.cv_results_)\n",
    "plotGridSearchParams(custom_xgboost.param_search.cv_results_, param_grid)\n",
    "plotGridSearchParams(custom_xgboost.param_search.cv_results_, param_grid, N=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictions of best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test:\n",
    "# Set to CPU for predictions:\n",
    "best_estimator_cpu = best_estimator.set_params(device='cpu')\n",
    "\n",
    "features_test, metadata_test = best_estimator_cpu._create_features_metadata(\n",
    "    test_set['df_X'][all_columns], config.META_DATA)\n",
    "y_pred = best_estimator_cpu.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "y_pred_agg = best_estimator_cpu.aggrPredict(metadata_test, config.META_DATA,\n",
    "                                            features_test)\n",
    "grouped_ids = getDfAggregatePred(test_set, y_pred_agg, all_columns)\n",
    "PlotPredictions(grouped_ids, y_pred, metadata_test, test_set,\n",
    "                best_estimator_cpu)\n",
    "plt.suptitle(f'XGBoost tested on {test_glaciers}', fontsize=20)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictions of custom parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_params =  {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 800}\n",
    "\n",
    "params = {**param_init, **custom_params}\n",
    "print(params)\n",
    "custom_model = mbm.models.CustomXGBoostRegressor(**params)\n",
    "\n",
    "# Fit on train data:\n",
    "custom_model.fit(train_set['df_X'][all_columns], train_set['y'])\n",
    "custom_model = custom_model.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on test\n",
    "features_test, metadata_test = custom_model._create_features_metadata(\n",
    "    test_set['df_X'][all_columns], config.META_DATA)\n",
    "y_pred = custom_model.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = custom_model.aggrPredict(metadata_test, config.META_DATA,\n",
    "                                      features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = custom_model.score(test_set['df_X'][all_columns],\n",
    "                           test_set['y'])  # negative\n",
    "print('Overall score:', np.abs(score))\n",
    "\n",
    "grouped_ids = getDfAggregatePred(test_set, y_pred_agg, all_columns)\n",
    "PlotPredictions(grouped_ids, y_pred, metadata_test, test_set, custom_model)\n",
    "plt.suptitle(f'XGBoost tested on {test_glaciers}', fontsize=20)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate predictions to annual or winter:\n",
    "PlotIndividualGlacierPred(grouped_ids, figsize=(15, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIPlot(custom_model, feature_columns, vois_climate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only annual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_monthly = pd.read_csv(path_PMB_GLAMOS_csv +\n",
    "                           'CH_wgms_dataset_monthly_pcsr.csv')\n",
    "data_monthly = data_monthly[data_monthly.PERIOD == 'annual'].reset_index(\n",
    "    drop=True)\n",
    "dataloader_gl_annual = mbm.DataLoader(data=data_monthly,\n",
    "                                      random_seed=config.SEED,\n",
    "                                      meta_data_columns=config.META_DATA)\n",
    "print('Number of monthly rows:', len(dataloader_gl_annual.data))\n",
    "\n",
    "splits, test_set, train_set = getCVSplits(dataloader_gl_annual,\n",
    "                                          test_split_on='GLACIER',\n",
    "                                          test_splits=test_glaciers)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))\n",
    "visualiseSplits(test_set['y'], train_set['y'], splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Feature columns:\n",
    "feature_columns = [\n",
    "    'ELEVATION_DIFFERENCE'\n",
    "] + list(vois_climate) + list(vois_topographical) + ['pcsr']\n",
    "all_columns = feature_columns + config.META_DATA + config.NOT_METADATA_NOT_FEATURES\n",
    "df_X_train_subset = train_set['df_X'][all_columns]\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of testing dataset:', test_set['df_X'][all_columns].shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "RUN = False\n",
    "if RUN:\n",
    "    # Create a CustomXGBoostRegressor instance\n",
    "    custom_xgboost = mbm.models.CustomXGBoostRegressor(**param_init)\n",
    "    custom_xgboost.randomsearch(\n",
    "        parameters=param_grid,\n",
    "        n_iter=45,\n",
    "        splits=splits,\n",
    "        features=df_X_train_subset,\n",
    "        targets=train_set['y'],\n",
    "    )\n",
    "\n",
    "    # save best model\n",
    "    custom_xgboost.save_model(f'xgb_gl_split_pcsr_annual.pkl')\n",
    "else:\n",
    "    # read model\n",
    "    custom_xgboost = mbm.models.CustomXGBoostRegressor()\n",
    "    custom_xgboost.load_model(f'xgb_gl_split_pcsr_annual.pkl')\n",
    "\n",
    "# Get best parameters and estimator\n",
    "best_params = custom_xgboost.param_search.best_params_\n",
    "best_estimator = custom_xgboost.param_search.best_estimator_\n",
    "print(\"Best parameters:\\n\", best_params)\n",
    "print(\"Best score:\\n\", custom_xgboost.param_search.best_score_)\n",
    "\n",
    "# Make predictions on test:\n",
    "# Set to CPU for predictions:\n",
    "xgb = best_estimator.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on test\n",
    "features_test, metadata_test = xgb._create_features_metadata(\n",
    "    test_set['df_X'][all_columns], config.META_DATA)\n",
    "y_pred = xgb.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = xgb.aggrPredict(metadata_test, config.META_DATA, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = xgb.score(test_set['df_X'][all_columns], test_set['y'])  # negative\n",
    "print('Overall score:', np.abs(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = custom_xgboost.param_search.best_estimator_\n",
    "\n",
    "# Make predictions on test:\n",
    "# Set to CPU for predictions:\n",
    "xgb = best_estimator.set_params(device='cpu')\n",
    "\n",
    "PlotPredictions(xgb, test_glaciers, metadata_test, test_set, y_pred,\n",
    "                y_pred_agg, feature_columns, all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIPlot(best_estimator, feature_columns, vois_climate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point Elevation & Altitude climate:\n",
    "#### All variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Feature columns:\n",
    "feature_columns = [\n",
    "    'POINT_ELEVATION', 'ALTITUDE_CLIMATE'\n",
    "] + list(vois_climate) + list(vois_topographical) + ['pcsr']\n",
    "all_columns = feature_columns + config.META_DATA + config.NOT_METADATA_NOT_FEATURES\n",
    "df_X_train_subset = train_set['df_X'][all_columns]\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of testing dataset:', test_set['df_X'][all_columns].shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "RUN = False\n",
    "if RUN:\n",
    "    # Create a CustomXGBoostRegressor instance\n",
    "    custom_xgboost = mbm.models.CustomXGBoostRegressor(**param_init)\n",
    "    custom_xgboost.randomsearch(\n",
    "        parameters=param_grid,\n",
    "        n_iter=45,\n",
    "        splits=splits,\n",
    "        features=df_X_train_subset,\n",
    "        targets=train_set['y'],\n",
    "    )\n",
    "\n",
    "    # save best model\n",
    "    custom_xgboost.save_model(f'xgb_gl_split_pcsr_pe.pkl')\n",
    "else:\n",
    "    # read model\n",
    "    custom_xgboost = mbm.models.CustomXGBoostRegressor()\n",
    "    custom_xgboost.load_model(f'xgb_gl_split_pcsr_pe.pkl')\n",
    "\n",
    "# Get best parameters and estimator\n",
    "best_params = custom_xgboost.param_search.best_params_\n",
    "best_estimator = custom_xgboost.param_search.best_estimator_\n",
    "print(\"Best parameters:\\n\", best_params)\n",
    "print(\"Best score:\\n\", custom_xgboost.param_search.best_score_)\n",
    "\n",
    "# Make predictions on test:\n",
    "# Set to CPU for predictions:\n",
    "xgb = best_estimator.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on test\n",
    "features_test, metadata_test = xgb._create_features_metadata(\n",
    "    test_set['df_X'][all_columns], config.META_DATA)\n",
    "y_pred = xgb.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = xgb.aggrPredict(metadata_test, config.META_DATA, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = xgb.score(test_set['df_X'][all_columns], test_set['y'])  # negative\n",
    "print('Overall score:', np.abs(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = custom_xgboost.param_search.best_estimator_\n",
    "\n",
    "# Make predictions on test:\n",
    "# Set to CPU for predictions:\n",
    "xgb = best_estimator.set_params(device='cpu')\n",
    "\n",
    "PlotPredictions(xgb, test_glaciers, metadata_test, test_set, y_pred,\n",
    "                y_pred_agg, feature_columns, all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIPlot(best_estimator, feature_columns, vois_climate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
