{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glacier grids from SGI or GLAMOS:\n",
    "\n",
    "Creates monthly grid files for the MBM to make PMB predictions over the whole glacier grid. The files come from the SGI grid and use OGGM topography. Computing takes a long time because of the conversion to monthly format.\n",
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import massbalancemachine as mbm\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import geopandas as gpd\n",
    "\n",
    "# scripts\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.geodata import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()  # in case no memory\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "custom_working_dir = '../../../data/OGGM/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Climate columns\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "# Topographical columns\n",
    "voi_topographical = [\n",
    "    \"aspect\",\n",
    "    \"slope\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "    \"topo\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read PMB data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the SGI shapefiles use a slightly different RGI number than OGGM and this is why we use `rgi_id_v6_2016_shp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of glaciers with pcsr: 31\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_gl_area' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/2.2. Glacier grids-SGI.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/2.2.%20Glacier%20grids-SGI.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m satellite_glaciers \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/2.2.%20Glacier%20grids-SGI.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39madler\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maletsch\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mallalin\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbasodino\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mclariden\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfindelen\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgries\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/2.2.%20Glacier%20grids-SGI.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mhohlaub\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlimmern\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39moberaar\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mplattalva\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrhone\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msanktanna\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/2.2.%20Glacier%20grids-SGI.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mschwarzbach\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mschwarzberg\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/2.2.%20Glacier%20grids-SGI.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m ]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/2.2.%20Glacier%20grids-SGI.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Sort glaciers by area\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/2.2.%20Glacier%20grids-SGI.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m gl_area \u001b[39m=\u001b[39m get_gl_area()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/2.2.%20Glacier%20grids-SGI.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m gl_area[\u001b[39m'\u001b[39m\u001b[39mclariden\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m gl_area[\u001b[39m'\u001b[39m\u001b[39mclaridenL\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/2.2.%20Glacier%20grids-SGI.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Sort the lists by area if available in gl_area\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_gl_area' is not defined"
     ]
    }
   ],
   "source": [
    "data_glamos = pd.read_csv(path_PMB_GLAMOS_csv + 'CH_wgms_dataset_all.csv')\n",
    "\n",
    "glDirect = [\n",
    "    re.search(r'xr_direct_(.*?)\\.nc', f).group(1)\n",
    "    for f in os.listdir(path_pcsr + 'csv/')\n",
    "]\n",
    "data_glamos = data_glamos[data_glamos.GLACIER.isin(glDirect)]\n",
    "glacier_list = list(data_glamos.GLACIER.unique())\n",
    "print('Number of glaciers with pcsr:', len(glacier_list))\n",
    "\n",
    "satellite_glaciers = [\n",
    "    'adler', 'aletsch', 'allalin', 'basodino', 'clariden', 'findelen', 'gries',\n",
    "    'hohlaub', 'limmern', 'oberaar', 'plattalva', 'rhone', 'sanktanna',\n",
    "    'schwarzbach', 'schwarzberg'\n",
    "]\n",
    "\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area()\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list, gl_area):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "glacier_list = sort_by_area(glacier_list, gl_area)\n",
    "glacier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cut the data to the glaciers that have pcsr data:\n",
    "glDirect = [\n",
    "    re.search(r'xr_direct_(.*?)\\.nc', f).group(1)\n",
    "    for f in os.listdir(path_pcsr + 'csv/')\n",
    "]\n",
    "\n",
    "data_glamos = data_glamos[data_glamos.GLACIER.isin(glDirect)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "path_rgi = '../../../data/GLAMOS/CH_glacier_ids_long.csv'\n",
    "rgi_df = pd.read_csv(path_rgi, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "rgi_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order glaciers per area:\n",
    "shapefile_path = \"../../../data/GLAMOS/topo/SGI2020/SGI_2016_glaciers_copy.shp\"\n",
    "gdf_shapefiles = gpd.read_file(shapefile_path)  # Load the shapefile\n",
    "\n",
    "# Grid files\n",
    "path_aspect = path_SGI_topo + 'aspect/'\n",
    "path_slope = path_SGI_topo + 'slope/'\n",
    "path_DEM = path_SGI_topo + 'dem_HR/'\n",
    "\n",
    "gl_area = {}\n",
    "for glacierName in rgi_df.index:\n",
    "    if glacierName == 'clariden':\n",
    "        sgi_id = rgi_df.loc['claridenL']['sgi-id'].strip()\n",
    "        rgi_shp = rgi_df.loc['claridenL']['rgi_id_v6_2016_shp']\n",
    "    else:\n",
    "        sgi_id = rgi_df.loc[glacierName]['sgi-id'].strip()\n",
    "        rgi_shp = rgi_df.loc[glacierName]['rgi_id_v6_2016_shp']\n",
    "\n",
    "    # 2016 shapefile of glacier\n",
    "    gdf_mask_gl = gdf_shapefiles[gdf_shapefiles.RGIId == rgi_shp]\n",
    "    gl_area[glacierName] = gdf_mask_gl.Area.values[0]\n",
    "\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list, gl_area):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "glacier_list = sort_by_area(data_glamos.GLACIER.unique(), gl_area)\n",
    "glacier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_shp_rgi6 = '../../../data/GLAMOS/RGI/nsidc0770_11.rgi60.CentralEurope/11_rgi60_CentralEurope.shp'\n",
    "glacier_outline_rgi = gpd.read_file(glacier_shp_rgi6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGI (2015) grids:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine SGI glacier masks over SGI aspect:\n",
    "This is to check that there is a good overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear output folder\n",
    "emptyfolder('figures/SGI_mask/')\n",
    "\n",
    "for glacierName in tqdm(glacier_list, desc=\"Processing glaciers\"):\n",
    "    \n",
    "    # Handle 'clariden' separately due to special ID format\n",
    "    if glacierName == 'clariden':\n",
    "        sgi_id = rgi_df.at['claridenU', 'sgi-id'].strip() if 'claridenU' in rgi_df.index else ''\n",
    "        rgi_shp = rgi_df.at['claridenU', 'rgi_id_v6_2016_shp'] if 'claridenU' in rgi_df.index else ''\n",
    "    else:\n",
    "        sgi_id = rgi_df.at[glacierName, 'sgi-id'].strip() if glacierName in rgi_df.index else ''\n",
    "        rgi_shp = rgi_df.at[glacierName, 'rgi_id_v6_2016_shp'] if glacierName in rgi_df.index else ''\n",
    "\n",
    "    # Skip if no SGI ID\n",
    "    if not sgi_id:\n",
    "        print(f'No SGI ID found for {glacierName}')\n",
    "        continue\n",
    "\n",
    "    # Get glacier mask\n",
    "    gdf_mask_gl = gdf_shapefiles[gdf_shapefiles.RGIId == rgi_shp]\n",
    "\n",
    "    # Skip if no glacier mask found\n",
    "    if gdf_mask_gl.empty:\n",
    "        print(f'No glacier mask found for {glacierName}')\n",
    "        continue\n",
    "\n",
    "    # Locate aspect grid file\n",
    "    path_aspect = os.path.join(path_SGI_topo, 'aspect')\n",
    "    aspect_gl = next((f for f in os.listdir(path_aspect) if sgi_id in f), None)\n",
    "\n",
    "    # Skip if no aspect file found\n",
    "    if not aspect_gl:\n",
    "        print(f'No aspect file found for {glacierName}')\n",
    "        continue\n",
    "\n",
    "    # Load grid file\n",
    "    metadata_aspect, grid_data_aspect = load_grid_file(os.path.join(path_aspect, aspect_gl))\n",
    "\n",
    "    # Convert to xarray\n",
    "    aspect = convert_to_xarray_geodata(grid_data_aspect, metadata_aspect)\n",
    "\n",
    "    # Transform to WGS84\n",
    "    aspect_wgs84 = transform_xarray_coords_lv95_to_wgs84(aspect)\n",
    "\n",
    "    # Plot the data\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    aspect_wgs84.plot(ax=ax)\n",
    "    gdf_mask_gl.plot(ax=ax, alpha=0.5)\n",
    "\n",
    "    # Save the figure\n",
    "    output_path = os.path.join('figures', 'SGI_mask', f\"{glacierName}.png\")\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare one example grid of SGI to OGGM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glacier name\n",
    "glacierName = 'rhone'\n",
    "\n",
    "# Get SGI ID and RGI shapefile ID safely\n",
    "try:\n",
    "    sgi_id = rgi_df.at[glacierName, 'sgi-id'].strip() if glacierName in rgi_df.index else ''\n",
    "    rgi_id = rgi_df.at[glacierName, 'rgi_id.v6'] if glacierName in rgi_df.index else ''\n",
    "    rgi_shp = rgi_df.at[glacierName, 'rgi_id_v6_2016_shp'] if glacierName in rgi_df.index else ''\n",
    "except KeyError:\n",
    "    print(f\"Error: {glacierName} not found in rgi_df\")\n",
    "    sgi_id, rgi_id, rgi_shp = '', '', ''\n",
    "\n",
    "if not sgi_id or not rgi_id or not rgi_shp:\n",
    "    print(f\"Warning: Missing data for {glacierName}. Skipping...\")\n",
    "else:\n",
    "    # Load SGI masked dataset\n",
    "    ds = xr_SGI_masked_topo(rgi_shp, gdf_shapefiles, path_aspect, path_slope, path_DEM, sgi_id)\n",
    "\n",
    "    if ds is None:\n",
    "        print(f\"Warning: Failed to load SGI dataset for {glacierName}. Skipping...\")\n",
    "    else:\n",
    "        # Load OGGM dataset\n",
    "        oggm_path = os.path.join(path_OGGM, 'xr_grids', f'{rgi_id}.nc')\n",
    "\n",
    "        try:\n",
    "            ds_oggm = xr.open_dataset(oggm_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: OGGM dataset not found for {glacierName}. Skipping...\")\n",
    "            ds_oggm = None\n",
    "\n",
    "        # Calculate SGI resolution\n",
    "        dx_sgi = abs(ds.x[1] - ds.x[0])\n",
    "        dy_sgi = abs(ds.y[1] - ds.y[0])\n",
    "        print(f\"Cell size of SGI: {dx_sgi:.2f} x {dy_sgi:.2f} meters\")\n",
    "\n",
    "        if ds_oggm is not None:\n",
    "            # Calculate OGGM resolution\n",
    "            dx_oggm = abs(ds_oggm.x[1] - ds_oggm.x[0])\n",
    "            dy_oggm = abs(ds_oggm.y[1] - ds_oggm.y[0])\n",
    "            print(f\"Cell size of OGGM: {dx_oggm:.2f} x {dy_oggm:.2f} meters\")\n",
    "\n",
    "            # Plot the data\n",
    "            fig, axs = plt.subplots(2, 4, figsize=(15, 8))\n",
    "\n",
    "            # SGI Data\n",
    "            ds.masked_aspect.plot(ax=axs[0, 0], cmap='twilight_shifted', add_colorbar=False)\n",
    "            ds.masked_slope.plot(ax=axs[0, 1], cmap='cividis', add_colorbar=False)\n",
    "            ds.masked_elev.plot(ax=axs[0, 2], cmap='terrain', add_colorbar=False)\n",
    "            ds.glacier_mask.plot(ax=axs[0, 3], cmap='binary', add_colorbar=False)\n",
    "\n",
    "            axs[0, 0].set_title(\"Aspect SGI\")\n",
    "            axs[0, 1].set_title(\"Slope SGI\")\n",
    "            axs[0, 2].set_title(\"DEM SGI\")\n",
    "            axs[0, 3].set_title(\"Glacier mask SGI\")\n",
    "\n",
    "            # OGGM Data\n",
    "            if all(var in ds_oggm for var in ['aspect', 'slope', 'topo', 'glacier_mask']):\n",
    "                ds_oggm.aspect.plot(ax=axs[1, 0], cmap='twilight_shifted', add_colorbar=False)\n",
    "                ds_oggm.slope.plot(ax=axs[1, 1], cmap='cividis', add_colorbar=False)\n",
    "                ds_oggm.topo.plot(ax=axs[1, 2], cmap='terrain', add_colorbar=False)\n",
    "                ds_oggm.glacier_mask.plot(ax=axs[1, 3], cmap='binary', add_colorbar=False)\n",
    "\n",
    "                axs[1, 0].set_title(\"Aspect OGGM\")\n",
    "                axs[1, 1].set_title(\"Slope OGGM\")\n",
    "                axs[1, 2].set_title(\"DEM OGGM\")\n",
    "                axs[1, 3].set_title(\"Glacier mask OGGM\")\n",
    "            else:\n",
    "                print(f\"Warning: Some OGGM variables are missing in {oggm_path}\")\n",
    "\n",
    "            # Set axis labels\n",
    "            for ax in axs.flatten():\n",
    "                ax.set_xlabel(\"x\")\n",
    "                ax.set_ylabel(\"y\")\n",
    "                ax.legend().remove()\n",
    "\n",
    "            # Optimize layout\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample SGI grid:\n",
    "# Coarson to 30 m resolution\n",
    "ds_resampled = coarsenDS(ds)\n",
    "\n",
    "# Calculate resolution\n",
    "dx = abs(ds_resampled.x[1] - ds_resampled.x[0])\n",
    "dy = abs(ds_resampled.y[1] - ds_resampled.y[0])\n",
    "print(f\"Cell size: {dx} x {dy} meters\")\n",
    "\n",
    "# Plot resampled grid\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 6))\n",
    "ds_resampled.masked_aspect.plot(ax=axs[0], cmap='twilight_shifted')\n",
    "ds_resampled.masked_slope.plot(ax=axs[1], cmap='cividis', add_colorbar=False)\n",
    "ds_resampled.masked_elev.plot(ax=axs[2], cmap='terrain', add_colorbar=False)\n",
    "ds_resampled.glacier_mask.plot(ax=axs[3], cmap='binary', add_colorbar=False)\n",
    "\n",
    "axs[0].set_title(\"Aspect\")\n",
    "axs[1].set_title(\"Slope\")\n",
    "axs[2].set_title(\"DEM\")\n",
    "axs[3].set_title(\"Glacier mask\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yearly masked grids - xarrays:\n",
    "Save an .nc file per glacier per year (not in monthly format) needed in the MBM later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define save path and ensure it exists\n",
    "path_save = os.path.join(path_SGI_topo, 'xr_masked_grids/')\n",
    "emptyfolder(path_save)\n",
    "\n",
    "for glacierName in tqdm(glacier_list, desc=\"Processing glaciers\"):\n",
    "    print(f\"\\nProcessing {glacierName}...\")\n",
    "\n",
    "    # Handle 'clariden' separately due to special ID format\n",
    "    if glacierName == 'clariden':\n",
    "        sgi_id = rgi_df.at['claridenU', 'sgi-id'].strip() if 'claridenU' in rgi_df.index else ''\n",
    "        rgi_shp = rgi_df.at['claridenU', 'rgi_id_v6_2016_shp'] if 'claridenU' in rgi_df.index else ''\n",
    "    else:\n",
    "        sgi_id = rgi_df.at[glacierName, 'sgi-id'].strip() if glacierName in rgi_df.index else ''\n",
    "        rgi_shp = rgi_df.at[glacierName, 'rgi_id_v6_2016_shp'] if glacierName in rgi_df.index else ''\n",
    "\n",
    "    # Skip glacier if required data is missing\n",
    "    if not sgi_id or not rgi_shp:\n",
    "        print(f\"Warning: Missing SGI ID or shapefile for {glacierName}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Load dataset\n",
    "    try:\n",
    "        ds = xr_SGI_masked_topo(rgi_shp, gdf_shapefiles, path_aspect, path_slope, path_DEM, sgi_id)\n",
    "        if ds is None:\n",
    "            print(f\"Warning: Failed to load dataset for {glacierName}. Skipping...\")\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset for {glacierName}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Coarsen dataset to 30m resolution\n",
    "    try:\n",
    "        ds_resampled = coarsenDS(ds)\n",
    "        if ds_resampled is None:\n",
    "            print(f\"Warning: Resampling failed for {glacierName}. Skipping...\")\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error during resampling for {glacierName}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Save dataset\n",
    "    save_path = os.path.join(path_save, f\"{glacierName}.nc\")\n",
    "    try:\n",
    "        ds_resampled.to_netcdf(save_path)\n",
    "        print(f\"Saved {glacierName} dataset to {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving dataset for {glacierName}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly masked grids - dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_processed_and_incomplete_glaciers(folder_path, all_glaciers):\n",
    "    \"\"\"\n",
    "    Identifies glaciers that are fully processed and those with missing years.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path (str): Path to the folder containing the files.\n",
    "        all_glaciers (list): List of all glaciers that need to be processed.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A list of fully processed glaciers and a dictionary of incomplete glaciers with missing years.\n",
    "    \"\"\"\n",
    "    processed_years = defaultdict(set)\n",
    "\n",
    "    # Regular expression to match the filename format\n",
    "    pattern = re.compile(\n",
    "        r'^(?P<glacier>[a-zA-Z_]+)_grid_(?P<year>\\d{4})\\.csv$')\n",
    "\n",
    "    # Iterate over all files in the folder\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    for filename in os.listdir(folder_path):\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            glacier = match.group('glacier')\n",
    "            year = int(match.group('year'))\n",
    "            if 2000 <= year <= 2023:\n",
    "                processed_years[glacier].add(year)\n",
    "\n",
    "    fully_processed = []\n",
    "    incomplete_glaciers = {}\n",
    "\n",
    "    # Check for completeness for each glacier\n",
    "    for glacier in all_glaciers:\n",
    "        all_years = set(range(2000, 2024))\n",
    "        if glacier in processed_years:\n",
    "            missing_years = all_years - processed_years[glacier]\n",
    "            if not missing_years:\n",
    "                fully_processed.append(glacier)\n",
    "            else:\n",
    "                incomplete_glaciers[glacier] = sorted(missing_years)\n",
    "        else:\n",
    "            incomplete_glaciers[glacier] = sorted(all_years)\n",
    "\n",
    "    return fully_processed, incomplete_glaciers\n",
    "\n",
    "\n",
    "fully_processed, incomplete_glaciers = find_processed_and_incomplete_glaciers(\n",
    "    path_glacier_grid_sgi, glacier_list)\n",
    "fully_processed, incomplete_glaciers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = True\n",
    "if RUN:\n",
    "    # Uncomment if you want to clear previous files\n",
    "    # emptyfolder(path_glacier_grid_sgi)\n",
    "\n",
    "    for glacierName in tqdm(incomplete_glaciers.keys(), desc='Processing glaciers'):\n",
    "        print(f\"\\n-----------------------------------\\nProcessing {glacierName}\")\n",
    "\n",
    "        for year in tqdm(incomplete_glaciers[glacierName], desc='Years', leave=False):\n",
    "            print(f\"  - Processing year: {year}\")\n",
    "\n",
    "            # Handle 'clariden' separately due to its unique ID format\n",
    "            if glacierName == 'clariden':\n",
    "                sgi_id = rgi_df.at['claridenU', 'sgi-id'].strip() if 'claridenU' in rgi_df.index else ''\n",
    "                rgi_id = rgi_df.at['claridenU', 'rgi_id.v6'] if 'claridenU' in rgi_df.index else ''\n",
    "                rgi_shp = rgi_df.at['claridenU', 'rgi_id_v6_2016_shp'] if 'claridenU' in rgi_df.index else ''\n",
    "            else:\n",
    "                sgi_id = rgi_df.at[glacierName, 'sgi-id'].strip() if glacierName in rgi_df.index else ''\n",
    "                rgi_id = rgi_df.at[glacierName, 'rgi_id.v6'] if glacierName in rgi_df.index else ''\n",
    "                rgi_shp = rgi_df.at[glacierName, 'rgi_id_v6_2016_shp'] if glacierName in rgi_df.index else ''\n",
    "\n",
    "            # Skip glacier if required data is missing\n",
    "            if not sgi_id or not rgi_id or not rgi_shp:\n",
    "                print(f\"Warning: Missing SGI ID or RGI shapefile for {glacierName}. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Load SGI masked grid\n",
    "            try:\n",
    "                ds = xr_SGI_masked_topo(rgi_shp, gdf_shapefiles, path_aspect, path_slope, path_DEM, sgi_id)\n",
    "                if ds is None:\n",
    "                    print(f\"Warning: Failed to load SGI dataset for {glacierName}. Skipping...\")\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading dataset for {glacierName}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Coarsen to 30 m resolution\n",
    "            try:\n",
    "                ds_coarsened = coarsenDS(ds)\n",
    "                if ds_coarsened is None:\n",
    "                    print(f\"Warning: Resampling failed for {glacierName}. Skipping...\")\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error during resampling for {glacierName}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Create glacier grid\n",
    "            try:\n",
    "                df_grid = create_glacier_grid_SGI(glacierName, year, rgi_id, ds_coarsened)\n",
    "                df_grid.reset_index(drop=True, inplace=True)\n",
    "                dataset_grid = mbm.Dataset(data=df_grid, region_name='CH', data_path=path_PMB_GLAMOS_csv)\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating glacier grid for {glacierName} in {year}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Add climate data\n",
    "            if year == 2000:\n",
    "                print('  - Adding climate data...')\n",
    "            try:\n",
    "                era5_climate_data = os.path.join(path_ERA5_raw, 'era5_monthly_averaged_data.nc')\n",
    "                geopotential_data = os.path.join(path_ERA5_raw, 'era5_geopotential_pressure.nc')\n",
    "                dataset_grid.get_climate_features(climate_data=era5_climate_data, geopotential_data=geopotential_data, change_units=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Error adding climate data for {glacierName} in {year}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Add potential clear sky radiation\n",
    "            if year == 2000:\n",
    "                print('  - Adding potential clear sky radiation...')\n",
    "            try:\n",
    "                dataset_grid.get_potential_rad(os.path.join(path_pcsr, 'csv/'))\n",
    "            except Exception as e:\n",
    "                print(f\"Error adding clear sky radiation for {glacierName} in {year}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Add OGGM data\n",
    "            if year == 2000:\n",
    "                print('  - Adding OGGM data...')\n",
    "            try:\n",
    "                df_y_gl = dataset_grid.data\n",
    "                df_y_gl.rename(columns={'RGIId': 'RGIId_old'}, inplace=True)\n",
    "\n",
    "                # Add RGI IDs through intersection with shapefiles\n",
    "                df_y_gl = mbm.data_processing.utils.get_rgi(data=df_y_gl, glacier_outlines=glacier_outline_rgi)\n",
    "\n",
    "                # Drop points without RGI ID\n",
    "                df_y_gl = df_y_gl.dropna(subset=['RGIId'])\n",
    "\n",
    "                # Variables of interest\n",
    "                voi = [\"hugonnet_dhdt\", \"consensus_ice_thickness\", \"millan_v\"]\n",
    "\n",
    "                df_y_gl = add_OGGM_features(df_y_gl, voi, path_OGGM)\n",
    "                dataset_grid = mbm.Dataset(cfg = cfg, data=df_y_gl, region_name='CH', data_path=path_PMB_GLAMOS_csv)\n",
    "            except Exception as e:\n",
    "                print(f\"Error adding OGGM data for {glacierName} in {year}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Convert to monthly time resolution\n",
    "            if year == 2000:\n",
    "                print('  - Converting to monthly time resolution...')\n",
    "            try:\n",
    "                dataset_grid.convert_to_monthly(meta_data_columns=config.META_DATA, vois_climate=vois_climate + ['pcsr'], vois_topographical=voi_topographical)\n",
    "                assert 'pcsr' in dataset_grid.data.columns, \"Missing 'pcsr' column after conversion\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting to monthly resolution for {glacierName} in {year}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if year == 2000:\n",
    "                print(f\"  - DF grid shape after conversion: {dataset_grid.data.shape}\")\n",
    "\n",
    "            # Save gridded dataset\n",
    "            save_path = os.path.join(path_glacier_grid_sgi, f\"{glacierName}_grid_{year}.csv\")\n",
    "            if year == 2000:\n",
    "                print(f\"  - Saving dataset to {save_path}\")\n",
    "            try:\n",
    "                dataset_grid.data.to_csv(save_path, index=False)\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving dataset for {glacierName} in {year}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all OGGM variables\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "df = pd.read_csv(path_glacier_grid_sgi + 'adler_grid_2000.csv')\n",
    "df = df[df.MONTHS == 'sep']\n",
    "voi = ['hugonnet_dhdt', 'consensus_ice_thickness', 'millan_v']\n",
    "for i, var in enumerate(voi):\n",
    "    sns.scatterplot(df,\n",
    "                    x='POINT_LON',\n",
    "                    y='POINT_LAT',\n",
    "                    hue=var,\n",
    "                    s=5,\n",
    "                    alpha=0.5,\n",
    "                    palette='twilight_shifted',\n",
    "                    ax=axs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLAMOS grids:\n",
    "\n",
    "For the geodetic MB and gridded MB products computed by GLAMOS, they did not use the SGI grids (from 2015) but their own yearly DEMs. They're not available for all years, but we still compute monthly grids for these available glaciers and years, in order to make the comparison with geodetic MB fairer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of one glacier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacierName = 'plattalva'\n",
    "sgi_id = rgi_df.loc[glacierName]['sgi-id'].strip()\n",
    "rgi_shp = rgi_df.loc[glacierName]['rgi_id_v6_2016_shp']\n",
    "\n",
    "folder_path = path_GLAMOS_topo + glacierName\n",
    "pattern = re.compile(r'gl_(\\d{4})_lv95\\.grid')\n",
    "\n",
    "# Extract years from filenames\n",
    "years = set()\n",
    "for filename in os.listdir(folder_path):\n",
    "    match = pattern.match(filename)\n",
    "    if match:\n",
    "        years.add(int(match.group(1)))\n",
    "\n",
    "# Sort and display the years\n",
    "sorted_years = sorted(years)\n",
    "print(sorted_years)\n",
    "\n",
    "fileName = 'gl_1951_lv95.grid'\n",
    "metadata, grid_data = load_grid_file(folder_path + '/' + fileName)\n",
    "\n",
    "# convert to xarray\n",
    "dem_y = convert_to_xarray_geodata(grid_data, metadata)\n",
    "# Transform the coordinates to WGS84\n",
    "dem_wgs84_y = transform_xarray_coords_lv95_to_wgs84(dem_y)\n",
    "\n",
    "# Create a mask where 'elevation' is not NaN (1 if not NaN, 0 if NaN)\n",
    "ds_gl = xr.Dataset({'dem': dem_wgs84_y})\n",
    "ds_gl[\"glacier_mask\"] = ds_gl[\"dem\"].notnull().astype(np.uint8)\n",
    "\n",
    "# Calculate resolution\n",
    "dx = abs(ds_gl.x[1] - ds_gl.x[0])\n",
    "dy = abs(ds_gl.y[1] - ds_gl.y[0])\n",
    "print(f\"Cell size: {dx} x {dy} meters\")\n",
    "\n",
    "ds = xr_GLAMOS_masked_topo(path_aspect, path_slope, sgi_id, ds_gl)\n",
    "\n",
    "# Coarson to 30 m resolution\n",
    "ds = coarsenDS(ds)\n",
    "\n",
    "# Plot the masked data\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 6))\n",
    "ds.masked_aspect.plot(ax=axs[0], cmap='twilight_shifted', add_colorbar=False)\n",
    "ds.masked_slope.plot(ax=axs[1], cmap='cividis', add_colorbar=False)\n",
    "ds.masked_elev.plot(ax=axs[2], cmap='terrain', add_colorbar=False)\n",
    "ds.glacier_mask.plot(ax=axs[3], cmap='binary', add_colorbar=False)\n",
    "\n",
    "# Calculate resolution\n",
    "dx = abs(ds.x[1] - ds.x[0])\n",
    "dy = abs(ds.y[1] - ds.y[0])\n",
    "print(f\"Cell size: {dx} x {dy} meters\")\n",
    "\n",
    "axs[0].set_title(\"Aspect\")\n",
    "axs[1].set_title(\"Slope\")\n",
    "axs[2].set_title(\"DEM\")\n",
    "axs[3].set_title(\"Glacier mask\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yearly masked grids - xarrays:\n",
    "Save an .nc file per glacier per year (not in monthly format) needed in the MBM later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define save path and ensure it exists\n",
    "path_xr_grids = os.path.join(\"../../../data/GLAMOS/topo/GLAMOS_DEM/xr_masked_grids/\")\n",
    "RUN = False\n",
    "\n",
    "if RUN:\n",
    "    emptyfolder(path_xr_grids)\n",
    "\n",
    "    for glacierName in tqdm(glacier_list, desc=\"Processing glaciers\"):\n",
    "        print(f\"\\nProcessing {glacierName}...\")\n",
    "\n",
    "        # Handle 'clariden' separately due to special ID format\n",
    "        if glacierName == 'clariden':\n",
    "            sgi_id = rgi_df.at['claridenU', 'sgi-id'].strip() if 'claridenU' in rgi_df.index else ''\n",
    "            rgi_shp = rgi_df.at['claridenU', 'rgi_id_v6_2016_shp'] if 'claridenU' in rgi_df.index else ''\n",
    "        else:\n",
    "            sgi_id = rgi_df.at[glacierName, 'sgi-id'].strip() if glacierName in rgi_df.index else ''\n",
    "            rgi_shp = rgi_df.at[glacierName, 'rgi_id_v6_2016_shp'] if glacierName in rgi_df.index else ''\n",
    "\n",
    "        # Skip glacier if required data is missing\n",
    "        if not sgi_id or not rgi_shp:\n",
    "            print(f\"Warning: Missing SGI ID or shapefile for {glacierName}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Define glacier folder path\n",
    "        folder_path = os.path.join(path_GLAMOS_topo, 'stanna' if glacierName == 'sanktanna' else glacierName)\n",
    "\n",
    "        # Check if folder exists\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Warning: Folder does not exist: {folder_path}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Regular expression to extract years from filenames\n",
    "        pattern = re.compile(r'gl_(\\d{4})_lv95\\.grid')\n",
    "\n",
    "        # Extract available years from filenames\n",
    "        years = sorted({\n",
    "            int(match.group(1)) for filename in os.listdir(folder_path)\n",
    "            if (match := pattern.match(filename))\n",
    "        })\n",
    "\n",
    "        if not years:\n",
    "            print(f\"Warning: No valid year files found in {folder_path}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        for year in tqdm(years, desc='Years', leave=False):\n",
    "            if year < 1951: # no ERA5 data available before 1950\n",
    "                continue\n",
    "            \n",
    "            file_name = f'gl_{year}_lv95.grid'\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            try:\n",
    "                # Load grid file\n",
    "                metadata, grid_data = load_grid_file(file_path)\n",
    "\n",
    "                # Convert to xarray\n",
    "                dem_y = convert_to_xarray_geodata(grid_data, metadata)\n",
    "\n",
    "                # Transform the coordinates to WGS84\n",
    "                dem_wgs84_y = transform_xarray_coords_lv95_to_wgs84(dem_y)\n",
    "\n",
    "                # Create a mask where 'elevation' is not NaN (1 if not NaN, 0 if NaN)\n",
    "                ds_gl = xr.Dataset({'dem': dem_wgs84_y})\n",
    "                ds_gl[\"glacier_mask\"] = ds_gl[\"dem\"].notnull().astype(np.uint8)\n",
    "\n",
    "                # Apply GLAMOS masked topo function\n",
    "                ds = xr_GLAMOS_masked_topo(path_aspect, path_slope, sgi_id, ds_gl)\n",
    "\n",
    "                # Coarsen to 30 m resolution\n",
    "                ds = coarsenDS(ds)\n",
    "\n",
    "                # Save xarray dataset\n",
    "                save_path = os.path.join(path_xr_grids, f\"{glacierName}_{year}.nc\")\n",
    "                ds.to_netcdf(save_path)\n",
    "                print(f\"Saved: {save_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {glacierName} in {year}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open an example grid file\n",
    "ds = xr.open_dataset(path_xr_grids + 'plattalva_1951.nc')\n",
    "ds.glacier_mask.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly masked grids - dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read geodetic MB:\n",
    "geodeticMB = pd.read_csv(path_geodetic_MB_glamos + 'dV_DOI2024_allcomb.csv')\n",
    "\n",
    "# get rgi of those glaciers:\n",
    "rgi_gl = data_glamos[data_glamos.GLACIER.isin(glacier_list)].RGIId.unique()\n",
    "sgi_gl = [\n",
    "    rgi_df[rgi_df['rgi_id.v6'] == rgi]['sgi-id'].values[0] for rgi in rgi_gl\n",
    "]\n",
    "geodeticMB = geodeticMB[geodeticMB['SGI-ID'].isin(sgi_gl)]\n",
    "\n",
    "# Add glacierName to geodeticMB\n",
    "# based  on SGI-ID\n",
    "glacierNames = [\n",
    "    rgi_df[rgi_df['sgi-id'] == sgi_id].index[0]\n",
    "    for sgi_id in geodeticMB['SGI-ID'].values\n",
    "]\n",
    "geodeticMB['glacierName'] = glacierNames\n",
    "\n",
    "# replace claridenL by clariden\n",
    "geodeticMB['glacierName'] = geodeticMB['glacierName'].apply(\n",
    "    lambda x: 'clariden' if x == 'claridenL' else x)\n",
    "\n",
    "# get years per glacier\n",
    "years_start_per_gl = geodeticMB.groupby(\n",
    "    'glacierName')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodeticMB.groupby('glacierName')['A_end'].unique().apply(\n",
    "    list).to_dict()\n",
    "years_end_per_gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = True\n",
    "\n",
    "if RUN:\n",
    "    for glacierName in tqdm(glacier_list, desc=\"Processing glaciers\"):\n",
    "        folder_path = os.path.join(path_glacier_grid_glamos, glacierName)\n",
    "        os.makedirs(folder_path, exist_ok=True)  # Ensure folder exists\n",
    "\n",
    "        # Get existing processed years\n",
    "        existing_files = [f for f in os.listdir(folder_path) if re.search(r'_grid_(\\d{4})\\.csv$', f)]\n",
    "        existing_years = {int(re.search(r'_grid_(\\d{4})\\.csv$', f).group(1)) for f in existing_files}\n",
    "        \n",
    "        # Get the longest period dynamically for the current glacier\n",
    "        if glacierName in years_start_per_gl and glacierName in years_end_per_gl:\n",
    "            longest_period = (years_start_per_gl[glacierName][0], years_end_per_gl[glacierName][-1])\n",
    "        else:\n",
    "            print(f\"Skipping {glacierName}: missing start/end years\")\n",
    "            continue\n",
    "\n",
    "        # Process only geodetic MB years\n",
    "        # existing_years = range(longest_period[0], longest_period[1] + 1)\n",
    "\n",
    "        # Get available NetCDF files for this glacier\n",
    "        nc_files = [f for f in os.listdir(path_xr_grids) if glacierName in f]\n",
    "        nc_files.sort()\n",
    "        print(f\"\\nProcessing {glacierName}: {len(nc_files)} files found\")\n",
    "\n",
    "        if not nc_files:\n",
    "            print(f\"Warning: No NetCDF files found for {glacierName}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Identify missing years\n",
    "        missing_years = []\n",
    "        for fileName in nc_files:\n",
    "            match = re.search(r'_(\\d{4})\\.nc$', fileName)\n",
    "            if match:\n",
    "                year = int(match.group(1))\n",
    "                if year >= 1951 and year not in existing_years:\n",
    "                    missing_years.append((year, fileName))\n",
    "\n",
    "        if not missing_years:\n",
    "            print(f\"All years processed for {glacierName}. Skipping...\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Missing years for {glacierName}: {[y[0] for y in missing_years]}\")\n",
    "\n",
    "        for year, fileName in tqdm(missing_years, desc=\"Processing missing years\", leave=False):\n",
    "            try:\n",
    "                #fileName = f'{glacierName}_{year}.nc'\n",
    "                # Load GLAMOS masked grid\n",
    "                file_path = os.path.join(path_xr_grids, fileName)\n",
    "                ds = xr.open_dataset(file_path)\n",
    "\n",
    "                # Handle 'clariden' separately due to its unique ID format\n",
    "                if glacierName == 'clariden':\n",
    "                    sgi_id = rgi_df.at['claridenU', 'sgi-id'].strip() if 'claridenU' in rgi_df.index else ''\n",
    "                    rgi_id = rgi_df.at['claridenU', 'rgi_id.v6'] if 'claridenU' in rgi_df.index else ''\n",
    "                    rgi_shp = rgi_df.at['claridenU', 'rgi_id_v6_2016_shp'] if 'claridenU' in rgi_df.index else ''\n",
    "                else:\n",
    "                    sgi_id = rgi_df.at[glacierName, 'sgi-id'].strip() if glacierName in rgi_df.index else ''\n",
    "                    rgi_id = rgi_df.at[glacierName, 'rgi_id.v6'] if glacierName in rgi_df.index else ''\n",
    "                    rgi_shp = rgi_df.at[glacierName, 'rgi_id_v6_2016_shp'] if glacierName in rgi_df.index else ''\n",
    "\n",
    "                # Skip glacier if required data is missing\n",
    "                if not sgi_id or not rgi_id or not rgi_shp:\n",
    "                    print(f\"Warning: Missing SGI ID or RGI shapefile for {glacierName}. Skipping...\")\n",
    "                    continue\n",
    "\n",
    "                # Create glacier grid\n",
    "                df_grid = create_glacier_grid_SGI(glacierName, year, rgi_id, ds)\n",
    "                df_grid.reset_index(drop=True, inplace=True)\n",
    "                dataset_grid = mbm.Dataset(cfg = cfg, data=df_grid, region_name='CH', data_path=path_PMB_GLAMOS_csv)\n",
    "\n",
    "                # Add climate data\n",
    "                era5_climate_data = os.path.join(path_ERA5_raw, 'era5_monthly_averaged_data.nc')\n",
    "                geopotential_data = os.path.join(path_ERA5_raw, 'era5_geopotential_pressure.nc')\n",
    "\n",
    "                dataset_grid.get_climate_features(\n",
    "                    climate_data=era5_climate_data,\n",
    "                    geopotential_data=geopotential_data,\n",
    "                    change_units=True\n",
    "                )\n",
    "\n",
    "                # Add potential clear sky radiation\n",
    "                dataset_grid.get_potential_rad(os.path.join(path_pcsr, 'csv/'))\n",
    "\n",
    "                # Process OGGM data\n",
    "                # df_y_gl = dataset_grid.data\n",
    "                # df_y_gl.rename(columns={'RGIId': 'RGIId_old'}, inplace=True)\n",
    "\n",
    "                # # Add RGI IDs through intersection with shapefiles\n",
    "                # df_y_gl = mbm.data_processing.utils.get_rgi(data=df_y_gl, glacier_outlines=glacier_outline_rgi)\n",
    "\n",
    "                # # Drop points without RGI ID\n",
    "                # df_y_gl = df_y_gl.dropna(subset=['RGIId'])\n",
    "\n",
    "                # Add OGGM features\n",
    "                # voi = [\"hugonnet_dhdt\", \"consensus_ice_thickness\", \"millan_v\"]\n",
    "                # df_y_gl = add_OGGM_features(df_y_gl, voi, path_OGGM)\n",
    "\n",
    "                # dataset_grid_oggm = mbm.Dataset(cfg = cfg, data=df_y_gl, region_name='CH', data_path=path_PMB_GLAMOS_csv)\n",
    "                \n",
    "                # Save normal and oggm grids:\n",
    "                \n",
    "                # Convert to monthly time resolution\n",
    "                dataset_grid.convert_to_monthly(\n",
    "                    meta_data_columns=config.META_DATA,\n",
    "                    vois_climate=vois_climate + ['pcsr'],\n",
    "                    vois_topographical=voi_topographical\n",
    "                )\n",
    "\n",
    "                assert 'pcsr' in dataset_grid.data.columns, \"Missing 'pcsr' column after conversion\"\n",
    "\n",
    "                # Save gridded dataset\n",
    "                save_path = os.path.join(folder_path, f\"{glacierName}_grid_{year}.csv\")\n",
    "                dataset_grid.data.to_csv(save_path, index=False)\n",
    "                print(f\"Saved: {save_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {glacierName} ({year}): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GLAMOS masked grid\n",
    "fileName = 'plattalva_1951.nc'\n",
    "glacierName = 'plattalva'\n",
    "year = 1951\n",
    "\n",
    "file_path = os.path.join(path_xr_grids, fileName)\n",
    "ds = xr.open_dataset(file_path)\n",
    "\n",
    "# Handle 'clariden' separately due to its unique ID format\n",
    "if glacierName == 'clariden':\n",
    "    sgi_id = rgi_df.at['claridenU', 'sgi-id'].strip() if 'claridenU' in rgi_df.index else ''\n",
    "    rgi_id = rgi_df.at['claridenU', 'rgi_id.v6'] if 'claridenU' in rgi_df.index else ''\n",
    "    rgi_shp = rgi_df.at['claridenU', 'rgi_id_v6_2016_shp'] if 'claridenU' in rgi_df.index else ''\n",
    "else:\n",
    "    sgi_id = rgi_df.at[glacierName, 'sgi-id'].strip() if glacierName in rgi_df.index else ''\n",
    "    rgi_id = rgi_df.at[glacierName, 'rgi_id.v6'] if glacierName in rgi_df.index else ''\n",
    "    rgi_shp = rgi_df.at[glacierName, 'rgi_id_v6_2016_shp'] if glacierName in rgi_df.index else ''\n",
    "\n",
    "# Skip glacier if required data is missing\n",
    "if not sgi_id or not rgi_id or not rgi_shp:\n",
    "    print(f\"Warning: Missing SGI ID or RGI shapefile for {glacierName}. Skipping...\")\n",
    "\n",
    "# Create glacier grid\n",
    "df_grid = create_glacier_grid_SGI(glacierName, year, rgi_id, ds)\n",
    "df_grid.reset_index(drop=True, inplace=True)\n",
    "dataset_grid = mbm.Dataset(cfg = cfg, data=df_grid, region_name='CH', data_path=path_PMB_GLAMOS_csv)\n",
    "\n",
    "# Add climate data\n",
    "era5_climate_data = os.path.join(path_ERA5_raw, 'era5_monthly_averaged_data.nc')\n",
    "geopotential_data = os.path.join(path_ERA5_raw, 'era5_geopotential_pressure.nc')\n",
    "\n",
    "dataset_grid.get_climate_features(\n",
    "    climate_data=era5_climate_data,\n",
    "    geopotential_data=geopotential_data,\n",
    "    change_units=True\n",
    ")\n",
    "\n",
    "# Add potential clear sky radiation\n",
    "dataset_grid.get_potential_rad(os.path.join(path_pcsr, 'csv/'))\n",
    "\n",
    "# Process OGGM data\n",
    "df_y_gl = dataset_grid.data\n",
    "df_y_gl.rename(columns={'RGIId': 'RGIId_old'}, inplace=True)\n",
    "\n",
    "# Add RGI IDs through intersection with shapefiles\n",
    "df_y_gl = mbm.data_processing.utils.get_rgi(data=df_y_gl, glacier_outlines=glacier_outline_rgi)\n",
    "\n",
    "# Drop points without RGI ID\n",
    "df_y_gl = df_y_gl.dropna(subset=['RGIId'])\n",
    "\n",
    "# Add OGGM features\n",
    "voi = [\"hugonnet_dhdt\", \"consensus_ice_thickness\", \"millan_v\"]\n",
    "df_y_gl = add_OGGM_features(df_y_gl, voi, path_OGGM)\n",
    "\n",
    "dataset_grid = mbm.Dataset(cfg = cfg, data=df_y_gl, region_name='CH', data_path=path_PMB_GLAMOS_csv)\n",
    "\n",
    "# Variables of interest\n",
    "voi = [\n",
    "    \"aspect\",\n",
    "    \"slope\",\n",
    "]\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n",
    "for i, var in enumerate(voi):\n",
    "    sns.scatterplot(dataset_grid.data,\n",
    "                    x='POINT_LON',\n",
    "                    y='POINT_LAT',\n",
    "                    hue=var,\n",
    "                    s=20,\n",
    "                    alpha=0.5,\n",
    "                    palette='twilight_shifted',\n",
    "                    ax=axs[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
