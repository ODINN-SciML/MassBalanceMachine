{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import joypy\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from skorch.helper import SliceDataset\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.geodata_plots import *\n",
    "from scripts.NN_networks import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'\n",
    "\n",
    "# Glacier outlines:\n",
    "glacier_outline_sgi = gpd.read_file(\n",
    "    os.path.join(cfg.dataPath, path_SGI_topo, 'inventory_sgi2016_r2020',\n",
    "                 'SGI_2016_glaciers_copy.shp'))  # Load the shapefile\n",
    "glacier_outline_rgi = gpd.read_file(cfg.dataPath + path_rgi_outlines)\n",
    "\n",
    "vois_climate = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "print('-------------------')\n",
    "print('Number of glaciers:', len(data_glamos['GLACIER'].unique()))\n",
    "print('Number of winter and annual samples:', len(data_glamos))\n",
    "print('Number of annual samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'winter']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(data_glamos.GLACIER.unique())\n",
    "missing_glaciers = [g for g in TEST_GLACIERS if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in TEST_GLACIERS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of measurements per glacier:\n",
    "glacier_info = data_glamos.groupby('GLACIER').size().sort_values(\n",
    "    ascending=False).reset_index()\n",
    "glacier_info.rename(columns={0: 'Nb. measurements'}, inplace=True)\n",
    "glacier_info.set_index('GLACIER', inplace=True)\n",
    "\n",
    "glacier_loc = data_glamos.groupby('GLACIER')[['POINT_LAT', 'POINT_LON']].mean()\n",
    "\n",
    "glacier_info = glacier_loc.merge(glacier_info, on='GLACIER')\n",
    "\n",
    "glacier_period = data_glamos.groupby(['GLACIER', 'PERIOD'\n",
    "                                      ]).size().unstack().fillna(0).astype(int)\n",
    "\n",
    "glacier_info = glacier_info.merge(glacier_period, on='GLACIER')\n",
    "\n",
    "glacier_info['Train/Test glacier'] = glacier_info.apply(\n",
    "    lambda x: 'Test' if x.name in TEST_GLACIERS else 'Train', axis=1)\n",
    "glacier_info.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign glaciers to river basin names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load RGI glacier IDs ===\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids)\n",
    "rgi_df.columns = rgi_df.columns.str.strip()\n",
    "rgi_df = rgi_df.sort_values(by='short_name').set_index('short_name')\n",
    "\n",
    "# === Load SGI region geometries ===\n",
    "SGI_regions = gpd.read_file(\n",
    "    os.path.join(cfg.dataPath, path_SGI_topo, 'inventory_sgi2016_r2020',\n",
    "                 'sgi_regions.geojson'))\n",
    "\n",
    "# Clean object columns\n",
    "SGI_regions[SGI_regions.select_dtypes(include='object').columns] = \\\n",
    "    SGI_regions.select_dtypes(include='object').apply(lambda col: col.str.strip())\n",
    "\n",
    "SGI_regions = SGI_regions.drop_duplicates().dropna()\n",
    "SGI_regions = SGI_regions.set_index('pk_sgi_region')\n",
    "\n",
    "# === Map to Level 0 river basins ===\n",
    "catchment_lv0 = {\n",
    "    'A': 'Rhine',\n",
    "    'B': 'Rhone',\n",
    "    'C': 'Po',\n",
    "    'D': 'Adige',\n",
    "    'E': 'Danube'\n",
    "}\n",
    "rgi_df['rvr_lv0'] = rgi_df['sgi-id'].str[0].map(catchment_lv0)\n",
    "\n",
    "\n",
    "# === Map to Level 1 river basins using SGI regions ===\n",
    "def get_river_basin(sgi_id):\n",
    "    key = sgi_id.split('-')[0]\n",
    "    if key not in SGI_regions.index:\n",
    "        return None\n",
    "    basin = SGI_regions.loc[key, 'river_basin_name']\n",
    "    if isinstance(basin, pd.Series):\n",
    "        return basin.dropna().unique()[0] if not basin.dropna().empty else None\n",
    "    return basin if pd.notna(basin) else None\n",
    "\n",
    "\n",
    "rgi_df['rvr_lv1'] = rgi_df['sgi-id'].apply(get_river_basin)\n",
    "\n",
    "# Final formatting\n",
    "rgi_df = rgi_df.reset_index().rename(columns={\n",
    "    'short_name': 'GLACIER'\n",
    "}).set_index('GLACIER')\n",
    "\n",
    "glacier_info = glacier_info.merge(rgi_df[['rvr_lv0', 'rvr_lv1']],\n",
    "                                  on='GLACIER',\n",
    "                                  how='left')\n",
    "glacier_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_NN.csv')\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils.build_head_tail_pads_from_monthly_df(\n",
    "    data_monthly)\n",
    "\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=TEST_GLACIERS,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "dataloader = mbm.dataloader.DataLoader(cfg, data=data_train)\n",
    "\n",
    "train_itr, val_itr = dataloader.set_train_test_split(test_size=0.2)\n",
    "\n",
    "# Get all indices of the training and valing dataset at once from the iterators. Once called, the iterators are empty.\n",
    "train_indices, val_indices = list(train_itr), list(val_itr)\n",
    "\n",
    "df_X_train = data_train.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Get val set\n",
    "df_X_val = data_train.iloc[val_indices]\n",
    "y_val = df_X_val['POINT_BALANCE'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_topo = [\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "    'pcsr',\n",
    "] + list(vois_topographical)\n",
    "\n",
    "feature_columns = features_topo + list(vois_climate)\n",
    "\n",
    "cfg.setFeatures(feature_columns)\n",
    "\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "# Because CH has some extra columns, we need to cut those\n",
    "df_X_train_subset = df_X_train[all_columns]\n",
    "df_X_val_subset = df_X_val[all_columns]\n",
    "df_X_test_subset = test_set['df_X'][all_columns]\n",
    "\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of validation dataset:', df_X_val_subset.shape)\n",
    "print('Shape of testing dataset:', df_X_test_subset.shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "assert all(train_set['df_X'].POINT_BALANCE == train_set['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models:\n",
    "### XGBoost model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_init_xgb = {\n",
    "    # 'device': 'cuda:0',\n",
    "    'device': 'cpu',\n",
    "    'tree_method': 'hist',\n",
    "    \"random_state\": cfg.seed,\n",
    "    \"n_jobs\": cfg.numJobs\n",
    "}\n",
    "\n",
    "custom_params = {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 800}\n",
    "\n",
    "params = {**param_init_xgb, **custom_params}\n",
    "print(params)\n",
    "custom_xgb_model = mbm.models.CustomXGBoostRegressor(cfg, **params)\n",
    "\n",
    "# Fit on train data:\n",
    "custom_xgb_model.fit(df_X_train_subset, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test\n",
    "custom_xgb_model = custom_xgb_model.set_params(device='cpu')\n",
    "features_test, metadata_test = mbm.data_processing.utils.create_features_metadata(\n",
    "    cfg, test_set['df_X'][all_columns])\n",
    "y_pred = custom_xgb_model.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = custom_xgb_model.aggrPredict(metadata_test, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = custom_xgb_model.score(test_set['df_X'][all_columns],\n",
    "                               test_set['y'])  # negative\n",
    "print('Overall score:', np.abs(score))\n",
    "grouped_ids_xgb = getDfAggregatePred(test_set, y_pred_agg, all_columns)\n",
    "\n",
    "scores_annual_xgb, scores_winter_xgb = compute_seasonal_scores(\n",
    "    grouped_ids_xgb, target_col='target', pred_col='pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_predictions_summary(grouped_ids=grouped_ids_xgb,\n",
    "                               scores_annual=scores_annual_xgb,\n",
    "                               scores_winter=scores_winter_xgb,\n",
    "                               predVSTruth=predVSTruth,\n",
    "                               plotMeanPred=plotMeanPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_xgb = pd.DataFrame(metadata_test, columns=cfg.metaData)\n",
    "df_pred_xgb['pred'] = y_pred\n",
    "\n",
    "pred_per_id = pd.DataFrame(df_pred_xgb.groupby('ID').pred.unique())\n",
    "pred_per_id['MONTHS'] = df_pred_xgb.groupby('ID').MONTHS.unique()\n",
    "pred_per_id.reset_index(inplace=True)\n",
    "\n",
    "# df_pred_months_annual = df_pred_months[df_pred_months['PERIOD'] == 'annual']\n",
    "months_extended = [\n",
    "    'sep', 'oct', 'nov', 'dec', 'jan', 'feb', 'mar', 'apr', 'may', 'jun',\n",
    "    'jul', 'aug', 'sep_', 'oct_'\n",
    "]\n",
    "\n",
    "df_months_xgb = pd.DataFrame(columns=months_extended)\n",
    "\n",
    "for i, row in pred_per_id.iterrows():\n",
    "    dic = {}\n",
    "    for i, month in enumerate(row.MONTHS):\n",
    "        if month in dic.keys():\n",
    "            month = month + '_'\n",
    "        dic[month] = row.pred[i]\n",
    "\n",
    "    # add missing months from months extended\n",
    "    for month in months_extended:\n",
    "        if month not in dic.keys():\n",
    "            dic[month] = np.nan\n",
    "\n",
    "    df_months_xgb = pd.concat(\n",
    "        [df_months_xgb, pd.DataFrame([dic])], ignore_index=True)\n",
    "df_months_xgb = df_months_xgb.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, metadata_train = mbm.data_processing.utils.create_features_metadata(\n",
    "    cfg, data_train[all_columns])\n",
    "y_pred = custom_xgb_model.predict(features_train)\n",
    "y_pred_agg = custom_xgb_model.aggrPredict(metadata_train, features_train)\n",
    "\n",
    "grouped_ids_xgb_train = getDfAggregatePred(train_set, y_pred_agg, all_columns)\n",
    "\n",
    "scores_annual_xgb, scores_winter_xgb = compute_seasonal_scores(\n",
    "    grouped_ids_xgb_train, target_col='target', pred_col='pred')\n",
    "fig = plot_predictions_summary(grouped_ids=grouped_ids_xgb_train,\n",
    "                               scores_annual=scores_annual_xgb,\n",
    "                               scores_winter=scores_winter_xgb,\n",
    "                               predVSTruth=predVSTruth,\n",
    "                               plotMeanPred=plotMeanPred,\n",
    "                               ax_xlim=(-14, 8),\n",
    "                               ax_ylim=(-14, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_glaciers = [i for i in existing_glaciers if i not in TEST_GLACIERS]\n",
    "gl_per_el = data_glamos[data_glamos.PERIOD == 'annual'].groupby(\n",
    "    ['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "train_gl_per_el = gl_per_el[train_glaciers].sort_values().index\n",
    "\n",
    "fig, axs = plt.subplots(8, 3, figsize=(20, 30), sharex=False)\n",
    "\n",
    "PlotIndividualGlacierPredVsTruth(grouped_ids_xgb_train,\n",
    "                                 axs=axs,\n",
    "                                 color_annual=color_dark_blue,\n",
    "                                 color_winter=color_pink,\n",
    "                                 custom_order=train_gl_per_el,\n",
    "                                 ax_xlim=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_val = None  # Initialized hereafter\n",
    "\n",
    "\n",
    "def my_train_split(ds, y=None, **fit_params):\n",
    "    return dataset, dataset_val\n",
    "\n",
    "\n",
    "# --- features ---\n",
    "features_topo = ['ELEVATION_DIFFERENCE', 'pcsr'] + list(vois_topographical)\n",
    "feature_columns = features_topo + list(vois_climate)\n",
    "cfg.setFeatures(feature_columns)\n",
    "\n",
    "# keep only features + any required non-feature fields your code needs later\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "# --- test subset (no blanket dropna here; keep what your model expects) ---\n",
    "df_X_test_subset_MLP = test_set['df_X'][all_columns].copy()\n",
    "y_test = test_set['y']\n",
    "\n",
    "# --- load saved model (no callbacks/scheduler/early stop needed) ---\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "model_filename = \"nn_model_2025-09-22.pt\"  # adjust if needed\n",
    "params_filename = \"nn_params_2025-09-22.pkl\"  # adjust if needed\n",
    "\n",
    "with open(f\"models/{params_filename}\", \"rb\") as f:\n",
    "    params = pickle.load(f)\n",
    "\n",
    "nInp = len(feature_columns)\n",
    "\n",
    "args = {\n",
    "    'module': FlexibleNetwork,\n",
    "    'nbFeatures': nInp,\n",
    "    'module__input_dim': nInp,\n",
    "    'module__dropout': params['module__dropout'],\n",
    "    'module__hidden_layers': params['module__hidden_layers'],\n",
    "    'module__use_batchnorm': params['module__use_batchnorm'],\n",
    "    # the rest is irrelevant for inference but harmless if present:\n",
    "    'batch_size': params.get('batch_size', 128),\n",
    "    'verbose': 0,\n",
    "}\n",
    "\n",
    "loaded_MLP = mbm.models.CustomNeuralNetRegressor.load_model(\n",
    "    cfg,\n",
    "    model_filename,\n",
    "    **args,\n",
    "    device='cpu',\n",
    ").to('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plots:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- evaluate on test ---\n",
    "grouped_ids_NN, scores_NN, ids_NN, y_pred_NN = evaluate_model_and_group_predictions(\n",
    "    loaded_MLP, df_X_test_subset_MLP, y_test, cfg, months_head_pad,\n",
    "    months_tail_pad)\n",
    "\n",
    "print(\"Test scores:\", scores_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_annual_NN, scores_winter_NN = compute_seasonal_scores(\n",
    "    grouped_ids_NN, target_col='target', pred_col='pred')\n",
    "fig = plot_predictions_summary(grouped_ids=grouped_ids_NN,\n",
    "                               scores_annual=scores_annual_NN,\n",
    "                               scores_winter=scores_winter_NN,\n",
    "                               predVSTruth=predVSTruth,\n",
    "                               plotMeanPred=plotMeanPred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_ids_NN_train, scores_NN_train, ids_NN_train, y_pred_NN_train = evaluate_model_and_group_predictions(\n",
    "    loaded_MLP, data_train[all_columns], data_train['POINT_BALANCE'].values,\n",
    "    cfg, months_head_pad, months_tail_pad)\n",
    "scores_annual_NN, scores_winter_NN = compute_seasonal_scores(\n",
    "    grouped_ids_NN_train, target_col='target', pred_col='pred')\n",
    "fig = plot_predictions_summary(grouped_ids=grouped_ids_NN_train,\n",
    "                               scores_annual=scores_annual_NN,\n",
    "                               scores_winter=scores_winter_NN,\n",
    "                               predVSTruth=predVSTruth,\n",
    "                               plotMeanPred=plotMeanPred,\n",
    "                               ax_xlim=(-14, 8),\n",
    "                               ax_ylim=(-14, 8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_per_el = data_glamos[data_glamos.PERIOD == 'annual'].groupby(\n",
    "    ['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "\n",
    "test_gl_per_el = gl_per_el[TEST_GLACIERS].sort_values().index\n",
    "elvs = gl_per_el[TEST_GLACIERS].sort_values()\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(20, 15))\n",
    "\n",
    "PlotIndividualGlacierPredVsTruth(grouped_ids_NN,\n",
    "                                 color_annual=color_dark_blue,\n",
    "                                 color_winter=color_pink,\n",
    "                                 axs=axs,\n",
    "                                 custom_order=test_gl_per_el)\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    test_gl = test_gl_per_el[i]\n",
    "    el = elvs[test_gl]\n",
    "    ax.set_title(f'{test_gl.capitalize()}: {round(el, 2)} m', fontsize=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look for diff in test gl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(4, 4, figsize=(15, 15))\n",
    "\n",
    "vois = ['POINT_BALANCE', 'ELEVATION_DIFFERENCE', 'YEAR'\n",
    "        ] + vois_climate + vois_topographical + ['pcsr']\n",
    "\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Draw the plots and remove legends initially\n",
    "for i, voi in enumerate(vois):\n",
    "    axs[i].set_title(voi, fontsize=12)\n",
    "    g = sns.histplot(df_X_test_subset,\n",
    "                     x=voi,\n",
    "                     ax=axs[i],\n",
    "                     alpha=0.6,\n",
    "                     hue='GLACIER',\n",
    "                     stat='density',\n",
    "                     kde=True,\n",
    "                     fill=False)\n",
    "    axs[i].set_xlabel('')\n",
    "\n",
    "for ax in axs[0:17]:\n",
    "    ax.legend_.remove()  # Remove individual legends\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train_subset_gl = df_X_train_subset[df_X_train_subset.GLACIER ==\n",
    "                                         'aletsch']\n",
    "\n",
    "f, axs = plt.subplots(5, 4, figsize=(15, 15))\n",
    "\n",
    "vois = ['POINT_BALANCE', 'ELEVATION_DIFFERENCE', 'YEAR'\n",
    "        ] + vois_climate + vois_topographical + ['pcsr']\n",
    "\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Draw the plots and remove legends initially\n",
    "for i, voi in enumerate(vois):\n",
    "    axs[i].set_title(voi, fontsize=12)\n",
    "    g = sns.histplot(\n",
    "        df_X_train_subset_gl,\n",
    "        x=voi,\n",
    "        ax=axs[i],\n",
    "        alpha=0.6,\n",
    "        #  hue='GLACIER',\n",
    "        stat='density',\n",
    "        kde=True,\n",
    "        fill=False)\n",
    "    axs[i].set_xlabel('')\n",
    "\n",
    "# for ax in axs[0:17]:\n",
    "#     ax.legend_.remove()  # Remove individual legends\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter on test glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_annual_NN, scores_winter_NN = compute_seasonal_scores(\n",
    "    grouped_ids_NN, target_col='target', pred_col='pred')\n",
    "scores_annual_xgb, scores_winter_xgb = compute_seasonal_scores(\n",
    "    grouped_ids_xgb, target_col='target', pred_col='pred')\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "ax1 = plt.subplot(1, 2, 2)\n",
    "ax1.set_title('XGB predictions', fontsize=20)\n",
    "predVSTruth(ax1,\n",
    "            grouped_ids_xgb,\n",
    "            scores_annual_xgb,\n",
    "            hue='PERIOD',\n",
    "            palette=[color_dark_blue, color_pink])\n",
    "\n",
    "legend_xgb = \"\\n\".join((\n",
    "    (r\"$\\mathrm{RMSE_a}=%.3f$, $\\mathrm{RMSE_w}=%.3f$,\" %\n",
    "     (scores_annual_xgb[\"rmse\"], scores_winter_xgb[\"rmse\"])),\n",
    "    (r\"$\\mathrm{R^2_a}=%.3f$, $\\mathrm{R^2_w}=%.3f$\" %\n",
    "     (scores_annual_xgb[\"R2\"], scores_winter_xgb[\"R2\"])),\n",
    "    r\"$\\mathrm{B_a}=%.3f$, $\\mathrm{B_w}=%.3f$\" %\n",
    "    (scores_annual_xgb[\"Bias\"], scores_winter_xgb[\"Bias\"]),\n",
    "))\n",
    "ax1.text(0.03,\n",
    "         0.96,\n",
    "         legend_xgb,\n",
    "         transform=ax1.transAxes,\n",
    "         verticalalignment=\"top\",\n",
    "         fontsize=18,\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0))\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 1)\n",
    "ax2.set_title('NN predictions', fontsize=20)\n",
    "predVSTruth(ax2,\n",
    "            grouped_ids_NN,\n",
    "            scores_annual_NN,\n",
    "            hue='PERIOD',\n",
    "            palette=[color_dark_blue, color_pink])\n",
    "\n",
    "legend_NN = \"\\n\".join((\n",
    "    (r\"$\\mathrm{RMSE_a}=%.3f$, $\\mathrm{RMSE_w}=%.3f$,\" %\n",
    "     (scores_annual_NN[\"rmse\"], scores_winter_NN[\"rmse\"])),\n",
    "    (r\"$\\mathrm{R^2_a}=%.3f$, $\\mathrm{R^2_w}=%.3f$\" %\n",
    "     (scores_annual_NN[\"R2\"], scores_winter_NN[\"R2\"])),\n",
    "    r\"$\\mathrm{B_a}=%.3f$, $\\mathrm{B_w}=%.3f$\" %\n",
    "    (scores_annual_NN[\"Bias\"], scores_winter_NN[\"Bias\"]),\n",
    "))\n",
    "ax2.text(0.03,\n",
    "         0.96,\n",
    "         legend_NN,\n",
    "         transform=ax2.transAxes,\n",
    "         verticalalignment=\"top\",\n",
    "         fontsize=18,\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial reconstruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save_glw = os.path.join(\n",
    "    cfg.dataPath, 'GLAMOS', 'distributed_MB_grids',\n",
    "    'MBM/testing_combis/glamos_dems_NN_SEB_full_OGGM')\n",
    "\n",
    "glaciers = os.listdir(path_save_glw)\n",
    "hydro_months = [\n",
    "    'sep', 'oct', 'nov', 'dec', 'jan', 'feb', 'mar', 'apr', 'may', 'jun',\n",
    "    'jul', 'aug'\n",
    "]\n",
    "# Initialize final storage for all glacier data\n",
    "all_glacier_data = []\n",
    "\n",
    "# Loop over glaciers\n",
    "for glacier_name in tqdm(glaciers):\n",
    "    glacier_path = os.path.join(path_save_glw, glacier_name)\n",
    "    if not os.path.isdir(glacier_path):\n",
    "        continue  # skip non-directories\n",
    "\n",
    "    # Regex pattern adapted for current glacier name\n",
    "    pattern = re.compile(rf'{glacier_name}_(\\d{{4}})_[a-z]{{3}}\\.zarr')\n",
    "\n",
    "    # Extract available years\n",
    "    years = set()\n",
    "    for fname in os.listdir(glacier_path):\n",
    "        match = pattern.match(fname)\n",
    "        if match:\n",
    "            years.add(int(match.group(1)))\n",
    "    years = sorted(years)\n",
    "\n",
    "    # Collect all year-month data\n",
    "    all_years_data = []\n",
    "    for year in years:\n",
    "        monthly_data = {}\n",
    "        for month in hydro_months:\n",
    "            zarr_path = os.path.join(glacier_path,\n",
    "                                     f'{glacier_name}_{year}_{month}.zarr')\n",
    "            if not os.path.exists(zarr_path):\n",
    "                continue\n",
    "\n",
    "            ds = xr.open_dataset(zarr_path)\n",
    "            df = ds.pred_masked.to_dataframe().drop(['x', 'y'],\n",
    "                                                    axis=1).reset_index()\n",
    "            df_pred_months = df[df.pred_masked.notna()]\n",
    "\n",
    "            df_el = ds.masked_elev.to_dataframe().drop(['x', 'y'],\n",
    "                                                       axis=1).reset_index()\n",
    "            df_elv_months = df_el[df.pred_masked.notna()]\n",
    "\n",
    "            df_pred_months['elevation'] = df_elv_months.masked_elev.values\n",
    "\n",
    "            monthly_data[month] = df_pred_months.pred_masked.values\n",
    "\n",
    "        if monthly_data:\n",
    "            df_months = pd.DataFrame(monthly_data)\n",
    "            df_months['year'] = year\n",
    "            df_months['glacier'] = glacier_name  # add glacier name\n",
    "            df_months['elevation'] = df_pred_months.elevation.values\n",
    "            all_years_data.append(df_months)\n",
    "\n",
    "    # Concatenate this glacier's data\n",
    "    if all_years_data:\n",
    "        df_glacier = pd.concat(all_years_data, axis=0, ignore_index=True)\n",
    "        all_glacier_data.append(df_glacier)\n",
    "\n",
    "# Final full DataFrame for all glaciers\n",
    "df_months_NN = pd.concat(all_glacier_data, axis=0, ignore_index=True)\n",
    "df_months_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save_glw = cfg.dataPath + '/GLAMOS/distributed_MB_grids/MBM/glamos_dems/'\n",
    "\n",
    "hydro_months = [\n",
    "    'sep', 'oct', 'nov', 'dec', 'jan', 'feb', 'mar', 'apr', 'may', 'jun',\n",
    "    'jul', 'aug'\n",
    "]\n",
    "# Initialize final storage for all glacier data\n",
    "all_glacier_data = []\n",
    "\n",
    "# Loop over glaciers\n",
    "for glacier_name in tqdm(glaciers):\n",
    "    glacier_path = os.path.join(path_save_glw, glacier_name)\n",
    "    if not os.path.isdir(glacier_path):\n",
    "        continue  # skip non-directories\n",
    "\n",
    "    # Regex pattern adapted for current glacier name\n",
    "    pattern = re.compile(rf'{glacier_name}_(\\d{{4}})_[a-z]{{3}}\\.zarr')\n",
    "\n",
    "    # Extract available years\n",
    "    years = set()\n",
    "    for fname in os.listdir(glacier_path):\n",
    "        match = pattern.match(fname)\n",
    "        if match:\n",
    "            years.add(int(match.group(1)))\n",
    "    years = sorted(years)\n",
    "\n",
    "    # Collect all year-month data\n",
    "    all_years_data = []\n",
    "    for year in years:\n",
    "        monthly_data = {}\n",
    "        for month in hydro_months:\n",
    "            zarr_path = os.path.join(glacier_path,\n",
    "                                     f'{glacier_name}_{year}_{month}.zarr')\n",
    "            if not os.path.exists(zarr_path):\n",
    "                continue\n",
    "\n",
    "            ds = xr.open_dataset(zarr_path)\n",
    "            df = ds.pred_masked.to_dataframe().drop(['x', 'y'],\n",
    "                                                    axis=1).reset_index()\n",
    "            df_pred_months = df[df.pred_masked.notna()]\n",
    "\n",
    "            df_el = ds.masked_elev.to_dataframe().drop(['x', 'y'],\n",
    "                                                       axis=1).reset_index()\n",
    "            df_elv_months = df_el[df.pred_masked.notna()]\n",
    "\n",
    "            df_pred_months['elevation'] = df_elv_months.masked_elev.values\n",
    "\n",
    "            monthly_data[month] = df_pred_months.pred_masked.values\n",
    "\n",
    "        if monthly_data:\n",
    "            df_months = pd.DataFrame(monthly_data)\n",
    "            df_months['year'] = year\n",
    "            df_months['glacier'] = glacier_name  # add glacier name\n",
    "            df_months['elevation'] = df_pred_months.elevation.values\n",
    "            all_years_data.append(df_months)\n",
    "\n",
    "    # Concatenate this glacier's data\n",
    "    if all_years_data:\n",
    "        df_glacier = pd.concat(all_years_data, axis=0, ignore_index=True)\n",
    "        all_glacier_data.append(df_glacier)\n",
    "\n",
    "# Final full DataFrame for all glaciers\n",
    "df_months_XGB = pd.concat(all_glacier_data, axis=0, ignore_index=True)\n",
    "df_months_XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Count entries per (glacier, year)\n",
    "counts_XGB = df_months_XGB.groupby(['glacier',\n",
    "                                    'year']).size().rename('count_XGB')\n",
    "counts_NN = df_months_NN.groupby(['glacier', 'year']).size().rename('count_NN')\n",
    "\n",
    "# Step 2: Merge counts\n",
    "comparison = pd.merge(counts_XGB,\n",
    "                      counts_NN,\n",
    "                      how='outer',\n",
    "                      left_index=True,\n",
    "                      right_index=True)\n",
    "\n",
    "# Step 3: Fill missing with 0 (if a pair is present in one but not the other)\n",
    "comparison = comparison.fillna(0).astype(int)\n",
    "\n",
    "# Step 4: Find where counts differ\n",
    "discrepancies = comparison[comparison['count_XGB'] != comparison['count_NN']]\n",
    "\n",
    "# Show the result\n",
    "discrepancies.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glacier-wide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get glacier-wide MB for every year\n",
    "glwd_months_NN = df_months_NN.groupby(['glacier', 'year']).mean().reset_index()\n",
    "glwd_months_XGB = df_months_XGB.groupby(['glacier',\n",
    "                                         'year']).mean().reset_index()\n",
    "glwd_months_XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_nn, array_xgb, months = [], [], []\n",
    "month_order = [\n",
    "    'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct',\n",
    "    'nov', 'dec'\n",
    "]\n",
    "cat_month = CategoricalDtype(month_order, ordered=True)\n",
    "\n",
    "df_months_xgb = glwd_months_XGB[month_order]\n",
    "df_months_nn = glwd_months_NN[month_order]\n",
    "\n",
    "for col in df_months_nn.columns:\n",
    "    array_nn.append(df_months_nn[col].values)\n",
    "    array_xgb.append(df_months_xgb[col].values)\n",
    "    months.append(np.tile(col, len(df_months_nn[col])))\n",
    "\n",
    "df_months_nn_long = pd.DataFrame(\n",
    "    data={\n",
    "        'mb_nn': np.concatenate(np.array(array_nn)),\n",
    "        'mb_xgb': np.concatenate(np.array(array_xgb)),\n",
    "        'Month': np.concatenate(np.array(months))\n",
    "    })\n",
    "\n",
    "# order df_months_nn_long\n",
    "df_months_nn_long['Month'] = df_months_nn_long['Month'].astype(cat_month)\n",
    "df_months_nn_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_colors = [color_annual, color_winter]\n",
    "alpha = 1\n",
    "\n",
    "cm = 1 / 2.54\n",
    "ax, fig = joypy.joyplot(df_months_nn_long,\n",
    "                        by='Month',\n",
    "                        column=['mb_xgb', 'mb_nn'],\n",
    "                        alpha=0.8,\n",
    "                        overlap=0,\n",
    "                        fill=False,\n",
    "                        linewidth=1.5,\n",
    "                        xlabelsize=8.5,\n",
    "                        ylabelsize=8.5,\n",
    "                        x_range=[-2.2, 2.2],\n",
    "                        grid=False,\n",
    "                        color=model_colors,\n",
    "                        figsize=(12 * cm, 14 * cm),\n",
    "                        ylim='own')\n",
    "\n",
    "vline_alpha = 0.5\n",
    "plt.axvline(x=0, color='grey', alpha=vline_alpha, linewidth=1)\n",
    "\n",
    "plt.xlabel('Mass balance (m w.e.)', fontsize=8.5)\n",
    "plt.yticks(ticks=range(1, 13), labels=month_order, fontsize=8.5)\n",
    "plt.gca().set_yticklabels(month_order)\n",
    "\n",
    "legend_patches = [\n",
    "    Patch(facecolor=color, label=model, alpha=alpha, edgecolor='k')\n",
    "    for model, color in zip(['XGB', 'NN'], model_colors)\n",
    "]\n",
    "plt.legend(handles=legend_patches,\n",
    "           loc='upper center',\n",
    "           bbox_to_anchor=(0.48, -0.1),\n",
    "           ncol=4,\n",
    "           fontsize=8.5,\n",
    "           handletextpad=0.5,\n",
    "           columnspacing=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geodetic MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PREDICTIONS_NN = os.path.join(\n",
    "    cfg.dataPath, 'GLAMOS', 'distributed_MB_grids',\n",
    "    'MBM/testing_combis/glamos_dems_NN_SEB_full_OGGM')\n",
    "PATH_PREDICTIONS_XGB = cfg.dataPath + path_distributed_MB_glamos + 'MBM/glamos_dems/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers_in_glamos = os.listdir(PATH_PREDICTIONS_NN)\n",
    "\n",
    "geodetic_mb = get_geodetic_MB(cfg)\n",
    "\n",
    "# get years per glacier\n",
    "years_start_per_gl = geodetic_mb.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodetic_mb.groupby('glacier_name')['Aend'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "periods_per_glacier, geoMB_per_glacier = build_periods_per_glacier(geodetic_mb)\n",
    "\n",
    "# Glaciers with geodetic MB data:\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area(cfg)\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list, gl_area):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "\n",
    "glacier_list = [\n",
    "    f for f in list(periods_per_glacier.keys()) if f in glaciers_in_glamos\n",
    "]\n",
    "glacier_list = sort_by_area(glacier_list, gl_area)\n",
    "print('Number of glaciers:', len(glacier_list))\n",
    "print('Glaciers:', glacier_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_xgb = process_geodetic_mass_balance_comparison(\n",
    "    glacier_list=os.listdir(PATH_PREDICTIONS_XGB),\n",
    "    path_SMB_GLAMOS_csv=cfg.dataPath + path_SMB_GLAMOS_csv,\n",
    "    periods_per_glacier=periods_per_glacier,\n",
    "    geoMB_per_glacier=geoMB_per_glacier,\n",
    "    gl_area=gl_area,\n",
    "    test_glaciers=TEST_GLACIERS,\n",
    "    path_predictions=PATH_PREDICTIONS_XGB,  # or another path if needed\n",
    "    cfg=cfg)\n",
    "\n",
    "df_all_nn = process_geodetic_mass_balance_comparison(\n",
    "    glacier_list=glacier_list,\n",
    "    path_SMB_GLAMOS_csv=cfg.dataPath + path_SMB_GLAMOS_csv,\n",
    "    periods_per_glacier=periods_per_glacier,\n",
    "    geoMB_per_glacier=geoMB_per_glacier,\n",
    "    gl_area=gl_area,\n",
    "    test_glaciers=TEST_GLACIERS,\n",
    "    path_predictions=PATH_PREDICTIONS_NN,  # or another path if needed\n",
    "    cfg=cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where any required columns are NaN\n",
    "df_all_nn = df_all_nn.dropna(subset=['Geodetic MB', 'MBM MB'])\n",
    "\n",
    "# Compute RMSE and Pearson correlation\n",
    "rmse_nn = mean_squared_error(df_all_nn[\"Geodetic MB\"],\n",
    "                             df_all_nn[\"MBM MB\"],\n",
    "                             squared=False)\n",
    "corr_nn = np.corrcoef(df_all_nn[\"Geodetic MB\"], df_all_nn[\"MBM MB\"])[0, 1]\n",
    "\n",
    "# Drop rows where any required columns are NaN\n",
    "df_all_xgb = df_all_xgb.dropna(subset=['Geodetic MB', 'MBM MB'])\n",
    "\n",
    "# Compute RMSE and Pearson correlation\n",
    "rmse_xgb = mean_squared_error(df_all_xgb[\"Geodetic MB\"],\n",
    "                              df_all_xgb[\"MBM MB\"],\n",
    "                              squared=False)\n",
    "corr_xgb = np.corrcoef(df_all_xgb[\"Geodetic MB\"], df_all_xgb[\"MBM MB\"])[0, 1]\n",
    "\n",
    "# Define figure and axes\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 8), sharex=True, sharey=True)\n",
    "\n",
    "# Plot MBM MB vs Geodetic MB\n",
    "plot_scatter_geodetic_MB(df_all_xgb, 'GLACIER', True, axs[1], \"MBM MB\",\n",
    "                         rmse_xgb, corr_xgb)\n",
    "\n",
    "axs[1].set_title('XGB predictions', fontsize=20)\n",
    "axs[1].set_ylabel('Predicted mass balance [m w.e.]', fontsize=18)\n",
    "axs[1].set_xlabel('Geodetic mass balance [m w.e.]', fontsize=18)\n",
    "\n",
    "plot_scatter_geodetic_MB(df_all_nn, 'GLACIER', True, axs[0], \"MBM MB\", rmse_nn,\n",
    "                         corr_nn)\n",
    "axs[0].set_ylabel('Predicted mass balance [m w.e.]', fontsize=18)\n",
    "axs[0].set_xlabel('Geodetic mass balance [m w.e.]', fontsize=18)\n",
    "axs[0].set_title('NN predictions', fontsize=20)\n",
    "\n",
    "# Adjust legend outside of plot\n",
    "handles, labels = axs[1].get_legend_handles_labels()\n",
    "fig.legend(handles,\n",
    "           labels,\n",
    "           bbox_to_anchor=(1.05, 1),\n",
    "           loc=\"upper left\",\n",
    "           borderaxespad=0.,\n",
    "           ncol=2,\n",
    "           fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stake data ONCE instead of for every glacier\n",
    "stake_file = os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv,\n",
    "                          \"CH_wgms_dataset_all.csv\")\n",
    "df_stakes = pd.read_csv(stake_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "GLACIER_NAME = 'aletsch'\n",
    "df_xgb = df_all_xgb[df_all_xgb.GLACIER == GLACIER_NAME]\n",
    "df_nn = df_all_nn[df_all_nn.GLACIER == GLACIER_NAME]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5), sharex=True, sharey=True)\n",
    "\n",
    "plot_scatter_comparison(axs[0],\n",
    "                        df_xgb,\n",
    "                        GLACIER_NAME,\n",
    "                        color_mbm=color_annual,\n",
    "                        color_glamos=color_winter,\n",
    "                        title_suffix=\"(XGB)\")\n",
    "plot_scatter_comparison(axs[1],\n",
    "                        df_nn,\n",
    "                        GLACIER_NAME,\n",
    "                        color_mbm=color_annual,\n",
    "                        color_glamos=color_winter,\n",
    "                        title_suffix=\"(NN)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GLAMOS data\n",
    "GLAMOS_glwmb = get_GLAMOS_glwmb(GLACIER_NAME, cfg)\n",
    "\n",
    "MBM_glwmb_nn = mbm_glwd_pred(PATH_PREDICTIONS_NN, GLACIER_NAME)\n",
    "MBM_glwmb_nn.rename(columns={\"MBM Balance\": \"MBM Balance NN\"}, inplace=True)\n",
    "MBM_glwmb_xgb = mbm_glwd_pred(PATH_PREDICTIONS_XGB, GLACIER_NAME)\n",
    "MBM_glwmb_xgb.rename(columns={\"MBM Balance\": \"MBM Balance XGB\"}, inplace=True)\n",
    "\n",
    "# Merge with GLAMOS data\n",
    "MBM_glwmb_nn = MBM_glwmb_nn.join(GLAMOS_glwmb)\n",
    "\n",
    "# Drop NaN values to avoid plotting errors\n",
    "MBM_glwmb_nn = MBM_glwmb_nn.dropna()\n",
    "\n",
    "MBM_glwmb = MBM_glwmb_nn.join(MBM_glwmb_xgb)\n",
    "# Plot the data\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "MBM_glwmb.plot(ax=axs[0],\n",
    "               y=['MBM Balance XGB', 'GLAMOS Balance'],\n",
    "               marker=\"o\",\n",
    "               color=[color_annual, color_winter])\n",
    "MBM_glwmb.plot(ax=axs[1],\n",
    "               y=['MBM Balance NN', 'GLAMOS Balance'],\n",
    "               marker=\"o\",\n",
    "               color=[color_annual, color_winter])\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_title(f\"{GLACIER_NAME.capitalize()} Glacier\", fontsize=24)\n",
    "    ax.set_ylabel(\"Mass Balance [m w.e.]\", fontsize=18)\n",
    "    ax.set_xlabel(\"Year\", fontsize=18)\n",
    "    ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "    ax.legend(fontsize=14)\n",
    "\n",
    "axs[0].set_title(f\"{GLACIER_NAME.capitalize()} Glacier (XGB)\", fontsize=16)\n",
    "axs[1].set_title(f\"{GLACIER_NAME.capitalize()} Glacier (NN)\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in MBM_glwmb_nn.index:\n",
    "    plot_mass_balance_comparison_annual(\n",
    "        glacier_name=GLACIER_NAME,\n",
    "        year=year,\n",
    "        cfg=cfg,\n",
    "        df_stakes=df_stakes,\n",
    "        path_distributed_mb=path_distributed_MB_glamos,\n",
    "        path_pred_xgb=PATH_PREDICTIONS_XGB,\n",
    "        path_pred_nn=PATH_PREDICTIONS_NN,\n",
    "        get_glamos_func=get_GLAMOS_glwmb,\n",
    "        get_pred_func=get_predicted_mb,\n",
    "        get_glamos_pred_func=get_predicted_mb_glamos,\n",
    "        load_grid_func=load_grid_file,\n",
    "        to_wgs84_func=transform_xarray_coords_lv95_to_wgs84,\n",
    "        apply_filter_func=apply_gaussian_filter,\n",
    "        get_colormaps_func=get_color_maps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_name = 'gorner'\n",
    "year = 2008\n",
    "stake_loc = df_stakes[df_stakes.GLACIER == glacier_name][[\n",
    "    'POINT_LAT', 'POINT_LON'\n",
    "]]\n",
    "\n",
    "df_sgi_grid = pd.read_parquet(\n",
    "    cfg.dataPath +\n",
    "    '../data/GLAMOS/topo/gridded_topo_inputs/SGI_grid/gorner/gorner_grid_2023.parquet'\n",
    ")\n",
    "\n",
    "sns.scatterplot(df_sgi_grid, x='POINT_LON', y='POINT_LAT', s=1, color='black')\n",
    "sns.scatterplot(stake_loc,\n",
    "                x='POINT_LON',\n",
    "                y='POINT_LAT',\n",
    "                s=20,\n",
    "                color='red',\n",
    "                label='Stake location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
