{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "import traceback\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from skorch.helper import SliceDataset\n",
    "from datetime import datetime\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "import itertools\n",
    "import random\n",
    "import csv\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.geodata import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.set_warn_always(False)\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'\n",
    "\n",
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "\n",
    "    # # Try to limit CPU usage of random search\n",
    "    # torch.set_num_threads(2)  # or 1\n",
    "    # os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "    # os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = pd.read_csv(cfg.dataPath + path_PMB_GLAMOS_csv +\n",
    "                          'CH_wgms_dataset_all.csv')\n",
    "# drop taelliboden and plainemorte if in there\n",
    "if 'taelliboden' in data_glamos['GLACIER'].unique():\n",
    "    data_glamos = data_glamos[data_glamos['GLACIER'] != 'taelliboden']\n",
    "if 'plainemorte' in data_glamos['GLACIER'].unique():\n",
    "    data_glamos = data_glamos[data_glamos['GLACIER'] != 'plainemorte']\n",
    "\n",
    "# Glaciers with data of potential clear sky radiation\n",
    "# Format to same names as stakes:\n",
    "glDirect = np.sort([\n",
    "    re.search(r'xr_direct_(.*?)\\.zarr', f).group(1)\n",
    "    for f in os.listdir(cfg.dataPath + path_pcsr + 'zarr/')\n",
    "])\n",
    "\n",
    "restgl = np.sort(Diff(list(glDirect), list(data_glamos.GLACIER.unique())))\n",
    "\n",
    "print('Glaciers with potential clear sky radiation data:\\n', glDirect)\n",
    "print('Number of glaciers:', len(glDirect))\n",
    "print('Glaciers without potential clear sky radiation data:\\n', restgl)\n",
    "\n",
    "# Filter out glaciers without data:\n",
    "data_glamos = data_glamos[data_glamos.GLACIER.isin(glDirect)]\n",
    "\n",
    "print('-------------------')\n",
    "print('Number of glaciers:', len(data_glamos['GLACIER'].unique()))\n",
    "print('Number of winter and annual samples:', len(data_glamos))\n",
    "print('Number of annual samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'winter']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:\n",
    "### Input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_NN.csv')\n",
    "data_monthly = dataloader_gl.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocking on glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in TEST_GLACIERS if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in TEST_GLACIERS]\n",
    "\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(TEST_GLACIERS)]\n",
    "print('Size of monthly test data:', len(data_test))\n",
    "\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train))\n",
    "\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "# Number of annual versus winter measurements:\n",
    "print('-------------\\nTrain:')\n",
    "print('Number of monthly winter and annual samples:', len(data_train))\n",
    "print('Number of monthly annual samples:',\n",
    "      len(data_train[data_train.PERIOD == 'annual']))\n",
    "print('Number of monthly winter samples:',\n",
    "      len(data_train[data_train.PERIOD == 'winter']))\n",
    "\n",
    "# Same for test\n",
    "data_test_annual = data_test[data_test.PERIOD == 'annual']\n",
    "data_test_winter = data_test[data_test.PERIOD == 'winter']\n",
    "\n",
    "print('Test:')\n",
    "print('Number of monthly winter and annual samples:', len(data_test))\n",
    "print('Number of monthly annual samples:', len(data_test_annual))\n",
    "print('Number of monthly winter samples:', len(data_test_winter))\n",
    "\n",
    "print('Total:')\n",
    "print('Number of monthly rows:', len(dataloader_gl.data))\n",
    "print('Number of annual rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'annual']))\n",
    "print('Number of winter rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'winter']))\n",
    "\n",
    "# same for original data:\n",
    "print('-------------\\nIn annual format:')\n",
    "print('Number of annual train rows:',\n",
    "      len(data_glamos[data_glamos.GLACIER.isin(train_glaciers)]))\n",
    "print('Number of annual test rows:',\n",
    "      len(data_glamos[data_glamos.GLACIER.isin(TEST_GLACIERS)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=TEST_GLACIERS,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "dataloader = mbm.dataloader.DataLoader(cfg, data=data_train)\n",
    "\n",
    "train_itr, val_itr = dataloader.set_train_test_split(test_size=0.2)\n",
    "\n",
    "# Get all indices of the training and valing dataset at once from the iterators. Once called, the iterators are empty.\n",
    "train_indices, val_indices = list(train_itr), list(val_itr)\n",
    "\n",
    "df_X_train = data_train.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Get val set\n",
    "df_X_val = data_train.iloc[val_indices]\n",
    "y_val = df_X_val['POINT_BALANCE'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_init = {'device': 'cpu'}  # Use CPU for training, apparently faster\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='valid_loss',\n",
    "    patience=15,\n",
    "    threshold=1e-4,  # Optional: stop only when improvement is very small\n",
    ")\n",
    "\n",
    "lr_scheduler_cb = LRScheduler(policy=ReduceLROnPlateau,\n",
    "                              monitor='valid_loss',\n",
    "                              mode='min',\n",
    "                              factor=0.5,\n",
    "                              patience=5,\n",
    "                              threshold=0.01,\n",
    "                              threshold_mode='rel',\n",
    "                              verbose=True)\n",
    "\n",
    "dataset = dataset_val = None  # Initialized hereafter\n",
    "\n",
    "\n",
    "def my_train_split(ds, y=None, **fit_params):\n",
    "    return dataset, dataset_val\n",
    "\n",
    "\n",
    "params = {\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 128,\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optimizer__weight_decay': 1e-05,\n",
    "    'module__hidden_layers': [128, 128, 64, 32],\n",
    "    'module__dropout': 0.2,\n",
    "    'module__use_batchnorm': True,\n",
    "}\n",
    "\n",
    "features_topo = [\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "    'pcsr',\n",
    "] + list(vois_topographical)\n",
    "\n",
    "feature_columns = features_topo + list(vois_climate)\n",
    "\n",
    "cfg.setFeatures(feature_columns)\n",
    "\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "df_X_train_subset = df_X_train[all_columns]\n",
    "df_X_val_subset = df_X_val[all_columns]\n",
    "df_X_test_subset = test_set['df_X'][all_columns]\n",
    "\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of validation dataset:', df_X_val_subset.shape)\n",
    "print('Shape of testing dataset:', test_set['df_X'][all_columns].shape)\n",
    "print('Running with features:', feature_columns)\n",
    "nInp = len(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and set to CPU\n",
    "model_filename = \"nn_model_2025-08-06.pt\"  # Replace with actual date if needed\n",
    "\n",
    "# read pickle with params\n",
    "params_filename = \"nn_params_2025-08-06.pkl\"  # Replace with actual date if needed\n",
    "with open(f\"models/{params_filename}\", \"rb\") as f:\n",
    "    custom_params = pickle.load(f)\n",
    "\n",
    "params = custom_params\n",
    "\n",
    "args = {\n",
    "    'module': FlexibleNetwork,\n",
    "    'nbFeatures': nInp,\n",
    "    'module__input_dim': nInp,\n",
    "    'module__dropout': params['module__dropout'],\n",
    "    'module__hidden_layers': params['module__hidden_layers'],\n",
    "    'train_split': my_train_split,\n",
    "    'batch_size': params['batch_size'],\n",
    "    'verbose': 1,\n",
    "    'iterator_train__shuffle': True,\n",
    "    'lr': params['lr'],\n",
    "    'max_epochs': 300,\n",
    "    'optimizer': params['optimizer'],\n",
    "    'optimizer__weight_decay': params['optimizer__weight_decay'],\n",
    "    'module__use_batchnorm': params['module__use_batchnorm'],\n",
    "    'callbacks': [\n",
    "        ('early_stop', early_stop),\n",
    "        ('lr_scheduler', lr_scheduler_cb),\n",
    "    ]\n",
    "}\n",
    "\n",
    "loaded_model = mbm.models.CustomNeuralNetRegressor.load_model(\n",
    "    cfg,\n",
    "    model_filename,\n",
    "    **{\n",
    "        **args,\n",
    "        **param_init\n",
    "    },\n",
    ")\n",
    "loaded_model = loaded_model.set_params(device='cpu')\n",
    "loaded_model = loaded_model.to('cpu')\n",
    "\n",
    "grouped_ids, scores_NN, ids_NN, y_pred_NN = evaluate_model_and_group_predictions(\n",
    "    loaded_model, df_X_test_subset, test_set['y'], cfg, mbm)\n",
    "\n",
    "baseline_score = scores_NN['rmse']\n",
    "print('Baseline RMSE:', baseline_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(cfg.seed)\n",
    "importances = {col: [] for col in feature_columns}\n",
    "\n",
    "# Compute baseline\n",
    "_, scores_baseline, _, _ = evaluate_model_and_group_predictions(\n",
    "    loaded_model, df_X_test_subset, test_set['y'], cfg, mbm)\n",
    "\n",
    "baseline_score = scores_baseline['rmse']\n",
    "print(f\"Baseline RMSE: {baseline_score:.4f}\")\n",
    "\n",
    "n_repeats = 10\n",
    "for col in tqdm(feature_columns):\n",
    "    for _ in range(n_repeats):\n",
    "        df_permuted = df_X_test_subset.copy()\n",
    "        df_permuted[col] = rng.permutation(df_permuted[col].values)\n",
    "\n",
    "        # Evaluate model on permuted data\n",
    "        _, scores_perm, _, _ = evaluate_model_and_group_predictions(\n",
    "            loaded_model, df_permuted, test_set['y'], cfg, mbm)\n",
    "        perm_score = scores_perm['rmse']\n",
    "        importance = perm_score - baseline_score  # Positive = worse performance\n",
    "        importances[col].append(importance)\n",
    "\n",
    "# Aggregate results\n",
    "df_importances = pd.DataFrame({\n",
    "    \"feature\":\n",
    "    feature_columns,\n",
    "    \"mean_importance\": [np.mean(importances[col]) for col in feature_columns],\n",
    "    \"std_importance\": [np.std(importances[col]) for col in feature_columns],\n",
    "}).sort_values(by=\"mean_importance\", ascending=False)\n",
    "\n",
    "plot_permutation_importance(df_importances, top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importances[df_importances.mean_importance > 0.02].sort_values(\n",
    "    by='mean_importance').feature.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependence per glacier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_per_el = data_glamos[data_glamos.PERIOD == 'annual'].groupby(\n",
    "    ['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "\n",
    "test_gl_per_el = gl_per_el[TEST_GLACIERS].sort_values().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up RNG and storage\n",
    "rng = np.random.default_rng(cfg.seed)\n",
    "n_repeats = 10\n",
    "top_n = 20  # Top N features to show\n",
    "\n",
    "# Prepare subplot layout\n",
    "n_glaciers = len(TEST_GLACIERS)\n",
    "ncols = 3  # or any layout you prefer\n",
    "nrows = int(np.ceil(n_glaciers / ncols))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(6 * ncols, 5 * nrows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, glacier_name in enumerate(test_gl_per_el):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Subset data\n",
    "    glacier_test_set = df_X_test_subset[df_X_test_subset.GLACIER == glacier_name].reset_index(drop=True)\n",
    "    glacier_y = test_set['y'][glacier_test_set.index]\n",
    "\n",
    "    # Initialize importances dictionary\n",
    "    importances = {col: [] for col in feature_columns}\n",
    "\n",
    "    # Compute baseline RMSE\n",
    "    _, scores_baseline, _, _ = evaluate_model_and_group_predictions(\n",
    "        loaded_model, glacier_test_set, glacier_y, cfg, mbm)\n",
    "    baseline_score = scores_baseline['rmse']\n",
    "\n",
    "    # Permutation importance\n",
    "    for col in tqdm(feature_columns, desc=f\"Processing {glacier_name}\"):\n",
    "        for _ in range(n_repeats):\n",
    "            df_permuted = glacier_test_set.copy()\n",
    "            df_permuted[col] = rng.permutation(df_permuted[col].values)\n",
    "            _, scores_perm, _, _ = evaluate_model_and_group_predictions(\n",
    "                loaded_model, df_permuted, glacier_y, cfg, mbm)\n",
    "            perm_score = scores_perm['rmse']\n",
    "            importance = perm_score - baseline_score\n",
    "            importances[col].append(importance)\n",
    "\n",
    "    # Aggregate\n",
    "    df_importances = pd.DataFrame({\n",
    "        \"feature\": feature_columns,\n",
    "        \"mean_importance\": [np.mean(importances[col]) for col in feature_columns],\n",
    "        \"std_importance\": [np.std(importances[col]) for col in feature_columns],\n",
    "    }).sort_values(by=\"mean_importance\", ascending=False).head(top_n)\n",
    "\n",
    "    # Plot in subplot\n",
    "    ax.barh(df_importances[\"feature\"], df_importances[\"mean_importance\"],\n",
    "            xerr=df_importances[\"std_importance\"], align=\"center\")\n",
    "    ax.set_title(f\"{glacier_name} (RMSE baseline: {baseline_score:.2f})\")\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel(\"Permutation Importance (↑ = more important)\")\n",
    "\n",
    "# Turn off unused subplots\n",
    "for i in range(len(TEST_GLACIERS), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial dependence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_init = {'device': 'cpu'}  # Use CPU for training, apparently faster\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='valid_loss',\n",
    "    patience=15,\n",
    "    threshold=1e-4,  # Optional: stop only when improvement is very small\n",
    ")\n",
    "\n",
    "lr_scheduler_cb = LRScheduler(policy=ReduceLROnPlateau,\n",
    "                              monitor='valid_loss',\n",
    "                              mode='min',\n",
    "                              factor=0.5,\n",
    "                              patience=5,\n",
    "                              threshold=0.01,\n",
    "                              threshold_mode='rel',\n",
    "                              verbose=True)\n",
    "\n",
    "dataset = dataset_val = None  # Initialized hereafter\n",
    "\n",
    "\n",
    "def my_train_split(ds, y=None, **fit_params):\n",
    "    return dataset, dataset_val\n",
    "\n",
    "\n",
    "params = {\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 128,\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optimizer__weight_decay': 1e-05,\n",
    "    'module__hidden_layers': [128, 128, 64, 32],\n",
    "    'module__dropout': 0.2,\n",
    "    'module__use_batchnorm': True,\n",
    "}\n",
    "\n",
    "features_topo = [\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "    'pcsr',\n",
    "] + list(vois_topographical)\n",
    "\n",
    "feature_columns = features_topo + list(vois_climate)\n",
    "\n",
    "cfg.setFeatures(feature_columns)\n",
    "\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "df_X_train_subset = df_X_train[all_columns]\n",
    "df_X_val_subset = df_X_val[all_columns]\n",
    "df_X_test_subset = test_set['df_X'][all_columns]\n",
    "\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of validation dataset:', df_X_val_subset.shape)\n",
    "print('Shape of testing dataset:', test_set['df_X'][all_columns].shape)\n",
    "print('Running with features:', feature_columns)\n",
    "nInp = len(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and set to CPU\n",
    "model_filename = \"nn_model_2025-08-06.pt\"  # Replace with actual date if needed\n",
    "# read pickle with params\n",
    "params_filename = \"nn_params_2025-08-06.pkl\"  # Replace with actual date if needed\n",
    "with open(f\"models/{params_filename}\", \"rb\") as f:\n",
    "    custom_params = pickle.load(f)\n",
    "\n",
    "params = custom_params\n",
    "\n",
    "args = {\n",
    "    'module': FlexibleNetwork,\n",
    "    'nbFeatures': nInp,\n",
    "    'module__input_dim': nInp,\n",
    "    'module__dropout': params['module__dropout'],\n",
    "    'module__hidden_layers': params['module__hidden_layers'],\n",
    "    'train_split': my_train_split,\n",
    "    'batch_size': params['batch_size'],\n",
    "    'verbose': 1,\n",
    "    'iterator_train__shuffle': True,\n",
    "    'lr': params['lr'],\n",
    "    'max_epochs': 300,\n",
    "    'optimizer': params['optimizer'],\n",
    "    'optimizer__weight_decay': params['optimizer__weight_decay'],\n",
    "    'module__use_batchnorm': params['module__use_batchnorm'],\n",
    "    'callbacks': [\n",
    "        ('early_stop', early_stop),\n",
    "        ('lr_scheduler', lr_scheduler_cb),\n",
    "    ]\n",
    "}\n",
    "\n",
    "loaded_model = mbm.models.CustomNeuralNetRegressor.load_model(\n",
    "    cfg,\n",
    "    model_filename,\n",
    "    **{\n",
    "        **args,\n",
    "        **param_init\n",
    "    },\n",
    ")\n",
    "loaded_model = loaded_model.set_params(device='cpu')\n",
    "loaded_model = loaded_model.to('cpu')\n",
    "\n",
    "grouped_ids, scores_NN, ids_NN, y_pred_NN = evaluate_model_and_group_predictions(\n",
    "    loaded_model, df_X_test_subset, test_set['y'], cfg, mbm)\n",
    "scores_annual, scores_winter = compute_seasonal_scores(grouped_ids,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "baseline_score = scores_NN['rmse']\n",
    "print('Baseline RMSE:', baseline_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_pdp(model, df_X, y, cfg, mbm, feature, grid_resolution=20):\n",
    "    df_X = df_X.copy()\n",
    "    feature_values = np.linspace(df_X[feature].min(), df_X[feature].max(),\n",
    "                                 grid_resolution)\n",
    "    pdp = []\n",
    "\n",
    "    for val in feature_values:\n",
    "        df_temp = df_X.copy()\n",
    "        df_temp[feature] = val  # Fix this feature at 'val'\n",
    "        grouped_preds, _, _, _ = evaluate_model_and_group_predictions(\n",
    "            model, df_temp, y, cfg, mbm)\n",
    "        pdp.append(\n",
    "            grouped_preds['pred'].mean())  # Average across grouped predictions\n",
    "\n",
    "    return feature_values, pdp\n",
    "\n",
    "\n",
    "# Subset the DataFrame where PERIOD == 'annual'\n",
    "df_X_test_subset_annual = df_X_test_subset[df_X_test_subset['PERIOD'] ==\n",
    "                                           'annual'].reset_index()\n",
    "annual_idx = df_X_test_subset_annual.index\n",
    "y_test_subset_annual = test_set['y'][annual_idx]\n",
    "\n",
    "fig, axs = plt.subplots(4, 4, figsize=(20, 10), sharex=False, sharey=False)\n",
    "axs = axs.flatten()\n",
    "i = 0\n",
    "for feature in tqdm(feature_columns):\n",
    "    values, pdp_vals = custom_pdp(loaded_model, df_X_test_subset_annual,\n",
    "                                  test_set['y'], cfg, mbm, feature)\n",
    "    axs[i].plot(values, pdp_vals)\n",
    "    axs[i].set_xlabel(feature)\n",
    "    axs[i].set_ylabel(\"Mean pred. MB\")\n",
    "\n",
    "    i += 1\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN = True\n",
    "# if RUN:\n",
    "#     # Setup logging\n",
    "#     log_filename = f'logs/nn_vars_search_progress_{datetime.now().strftime(\"%Y-%m-%d\")}.csv'\n",
    "\n",
    "#     fieldnames = list(\n",
    "#         sampled_params[0].keys()) + ['valid_loss', 'status', 'error']\n",
    "\n",
    "#     with open(log_filename, mode='w', newline='') as log_file:\n",
    "#         writer = csv.DictWriter(log_file, fieldnames=fieldnames)\n",
    "#         writer.writeheader()\n",
    "\n",
    "#     results = []\n",
    "#     for i, sampled_feat in enumerate(sampled_params):\n",
    "#         try:\n",
    "#             print(f\"\\n--- Running config {i+1}/{len(sampled_params)} ---\")\n",
    "#             print(sampled_feat)\n",
    "\n",
    "#             features_topo = [\n",
    "#                 'ELEVATION_DIFFERENCE',\n",
    "#             ] + list(sampled_feat['topographical'])\n",
    "\n",
    "#             features_climate = list(sampled_feat['climate'])\n",
    "\n",
    "#             feature_columns = features_topo + features_climate\n",
    "\n",
    "#             cfg.setFeatures(feature_columns)\n",
    "\n",
    "#             all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "#             df_X_train_subset = df_X_train[all_columns]\n",
    "#             df_X_val_subset = df_X_val[all_columns]\n",
    "#             df_X_test_subset = test_set['df_X'][all_columns]\n",
    "\n",
    "#             nInp = len(feature_columns)\n",
    "\n",
    "#             # Initialize network\n",
    "#             args = {\n",
    "#                 'module':\n",
    "#                 FlexibleNetwork,\n",
    "#                 'nbFeatures':\n",
    "#                 nInp,\n",
    "#                 'module__input_dim':\n",
    "#                 nInp,\n",
    "#                 'module__dropout':\n",
    "#                 params['module__dropout'],\n",
    "#                 'module__hidden_layers':\n",
    "#                 params['module__hidden_layers'],\n",
    "#                 'train_split':\n",
    "#                 my_train_split,\n",
    "#                 'batch_size':\n",
    "#                 params['batch_size'],\n",
    "#                 'verbose':\n",
    "#                 1,\n",
    "#                 'iterator_train__shuffle':\n",
    "#                 True,\n",
    "#                 'lr':\n",
    "#                 params['lr'],\n",
    "#                 'max_epochs':\n",
    "#                 200,\n",
    "#                 'optimizer':\n",
    "#                 params['optimizer'],\n",
    "#                 'optimizer__weight_decay':\n",
    "#                 params['optimizer__weight_decay'],\n",
    "#                 'module__use_batchnorm':\n",
    "#                 params['module__use_batchnorm'],\n",
    "#                 'callbacks': [\n",
    "#                     ('early_stop', early_stop),\n",
    "#                     ('lr_scheduler', lr_scheduler_cb),\n",
    "#                 ]\n",
    "#             }\n",
    "\n",
    "#             custom_nn = mbm.models.CustomNeuralNetRegressor(\n",
    "#                 cfg, **args, **param_init)\n",
    "#             custom_nn.seed_all()\n",
    "\n",
    "#             features, metadata = custom_nn._create_features_metadata(\n",
    "#                 df_X_train_subset)\n",
    "\n",
    "#             features_val, metadata_val = custom_nn._create_features_metadata(\n",
    "#                 df_X_val_subset)\n",
    "\n",
    "#             # Define the dataset for the NN\n",
    "#             dataset = mbm.data_processing.AggregatedDataset(cfg,\n",
    "#                                                             features=features,\n",
    "#                                                             metadata=metadata,\n",
    "#                                                             targets=y_train)\n",
    "#             dataset = mbm.data_processing.SliceDatasetBinding(\n",
    "#                 SliceDataset(dataset, idx=0), SliceDataset(dataset, idx=1))\n",
    "\n",
    "#             dataset_val = mbm.data_processing.AggregatedDataset(\n",
    "#                 cfg,\n",
    "#                 features=features_val,\n",
    "#                 metadata=metadata_val,\n",
    "#                 targets=y_val)\n",
    "#             dataset_val = mbm.data_processing.SliceDatasetBinding(\n",
    "#                 SliceDataset(dataset_val, idx=0),\n",
    "#                 SliceDataset(dataset_val, idx=1))\n",
    "\n",
    "#             custom_nn.fit(dataset.X, dataset.y)\n",
    "#             # Extract final validation loss\n",
    "#             valid_loss_best = custom_nn.history[-1]['valid_loss']\n",
    "\n",
    "#             grouped_ids, scores_NN, ids_NN, y_pred_NN = evaluate_model_and_group_predictions(\n",
    "#                 custom_nn, df_X_test_subset, test_set['y'], cfg, mbm)\n",
    "\n",
    "#             scores_annual, scores_winter = compute_seasonal_scores(\n",
    "#                 grouped_ids, target_col='target', pred_col='pred')\n",
    "\n",
    "#             row = {\n",
    "#                 **sampled_feat, 'valid_loss': valid_loss_best,\n",
    "#                 'status': 'success',\n",
    "#                 'error': ''\n",
    "#             }\n",
    "#         except Exception:\n",
    "#             err_msg = traceback.format_exc()\n",
    "#             print(err_msg)\n",
    "#             row = {\n",
    "#                 **sampled_feat, 'valid_loss': None,\n",
    "#                 'status': 'fail',\n",
    "#                 'error': err_msg\n",
    "#             }\n",
    "#         # Append result to log\n",
    "#         with open(log_filename, mode='a', newline='') as log_file:\n",
    "#             writer = csv.DictWriter(log_file, fieldnames=fieldnames)\n",
    "#             writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
