{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from skorch.helper import SliceDataset\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.xgb_helpers import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'\n",
    "\n",
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "rgi_df = pd.read_csv(path_glacier_ids, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    # \"aspect\", # OGGM\n",
    "    # \"slope\", # OGGM\n",
    "    \"aspect_sgi\",  # SGI\n",
    "    \"slope_sgi\",  # SGI\n",
    "    \"hugonnet_dhdt\",  # OGGM\n",
    "    \"consensus_ice_thickness\",  # OGGM\n",
    "    \"millan_v\",  # OGGM\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Try to limit CPU usage of random search\n",
    "torch.set_num_threads(2)  # or 1\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = pd.read_csv(path_PMB_GLAMOS_csv + 'CH_wgms_dataset_all.csv')\n",
    "\n",
    "print('Number of glaciers:', len(data_glamos['GLACIER'].unique()))\n",
    "print('Number of winter and annual samples:', len(data_glamos))\n",
    "print('Number of annual samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'winter']))\n",
    "\n",
    "# Capitalize glacier names:\n",
    "glacierCap = {}\n",
    "for gl in data_glamos['GLACIER'].unique():\n",
    "    if isinstance(gl, str):  # Ensure the glacier name is a string\n",
    "        if gl.lower() == 'claridenu':\n",
    "            glacierCap[gl] = 'Clariden_U'\n",
    "        elif gl.lower() == 'claridenl':\n",
    "            glacierCap[gl] = 'Clariden_L'\n",
    "        else:\n",
    "            glacierCap[gl] = gl.capitalize()\n",
    "    else:\n",
    "        print(f\"Warning: Non-string glacier name encountered: {gl}\")\n",
    "\n",
    "data_glamos.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glaciers with pot. radiadation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glaciers with data of potential clear sky radiation\n",
    "# Format to same names as stakes:\n",
    "glDirect = np.sort([\n",
    "    re.search(r'xr_direct_(.*?)\\.zarr', f).group(1)\n",
    "    for f in os.listdir(path_pcsr + 'zarr/')\n",
    "])\n",
    "\n",
    "restgl = np.sort(Diff(list(glDirect), list(data_glamos.GLACIER.unique())))\n",
    "\n",
    "print('Glaciers with potential clear sky radiation data:\\n', glDirect)\n",
    "print('Number of glaciers:', len(glDirect))\n",
    "print('Glaciers without potential clear sky radiation data:\\n', restgl)\n",
    "\n",
    "# Filter out glaciers without data:\n",
    "data_glamos = data_glamos[data_glamos.GLACIER.isin(glDirect)]\n",
    "\n",
    "# Look at the data of the ERA5 dataset:\n",
    "xr.open_dataset(path_ERA5_raw + 'era5_monthly_averaged_data.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:\n",
    "### Input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data': path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data': path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data(run_flag=RUN,\n",
    "                                     data_glamos=data_glamos,\n",
    "                                     paths=paths,\n",
    "                                     cfg=cfg,\n",
    "                                     vois_climate=vois_climate,\n",
    "                                     vois_topographical=vois_topographical)\n",
    "data_monthly = dataloader_gl.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocking on glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_glaciers = [\n",
    "    'tortin', 'plattalva', 'sanktanna', 'schwarzberg', 'hohlaub', 'pizol',\n",
    "    'corvatsch', 'tsanfleuron', 'forno'\n",
    "]\n",
    "\n",
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in test_glaciers]\n",
    "\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(test_glaciers)]\n",
    "print('Size of test data:', len(data_test))\n",
    "\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of train data:', len(data_train))\n",
    "\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "# Number of annual versus winter measurements:\n",
    "print('Train:')\n",
    "print('Number of winter and annual samples:', len(data_train))\n",
    "print('Number of annual samples:',\n",
    "      len(data_train[data_train.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_train[data_train.PERIOD == 'winter']))\n",
    "\n",
    "# Same for test\n",
    "data_test_annual = data_test[data_test.PERIOD == 'annual']\n",
    "data_test_winter = data_test[data_test.PERIOD == 'winter']\n",
    "\n",
    "print('Test:')\n",
    "print('Number of winter and annual samples:', len(data_test))\n",
    "print('Number of annual samples:', len(data_test_annual))\n",
    "print('Number of winter samples:', len(data_test_winter))\n",
    "\n",
    "print('Total:')\n",
    "print('Number of monthly rows:', len(dataloader_gl.data))\n",
    "print('Number of annual rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'annual']))\n",
    "print('Number of winter rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'winter']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_columns = train_set['df_X'].columns.difference(cfg.metaData)\n",
    "# feature_columns = list(feature_columns)\n",
    "\n",
    "# Grid search\n",
    "# For each of the XGBoost parameter, define the grid range\n",
    "param_grid = {'lr': [0.001, 0.01], 'max_epochs': [1000, 2000]}\n",
    "\n",
    "feature_columns = [\n",
    "    'ELEVATION_DIFFERENCE'\n",
    "] + list(vois_climate) + list(vois_topographical) + ['pcsr']\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "df_X_train_subset = train_set['df_X'][all_columns]\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of testing dataset:', test_set['df_X'][all_columns].shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "param_init = {}\n",
    "param_init['device'] = 'cuda:0'\n",
    "\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]\n",
    "\n",
    "nInp = len(feature_columns)\n",
    "network = nn.Sequential(\n",
    "    nn.Linear(nInp, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4, 1),\n",
    ")\n",
    "\n",
    "custom_nn = mbm.models.CustomNeuralNetRegressor(\n",
    "    cfg,\n",
    "    network,\n",
    "    nbFeatures=nInp,\n",
    "    train_split=\n",
    "    False,  # train_split is disabled since cross validation is handled by the splits variable hereafter\n",
    "    batch_size=16,\n",
    "    verbose=0,\n",
    "    iterator_train__shuffle=True,\n",
    "    **param_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, metadata = custom_nn._create_features_metadata(df_X_train_subset, )\n",
    "\n",
    "# Normalize the features\n",
    "bounds_features = {\n",
    "    k:\n",
    "    (np.min(train_set['df_X'][k].values), np.max(train_set['df_X'][k].values))\n",
    "    for k in feature_columns\n",
    "}\n",
    "norm = mbm.data_processing.Normalizer(bounds_features)\n",
    "norm_features = norm.normalize(features)\n",
    "\n",
    "# Define the dataset for the NN\n",
    "dataset = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                features=norm_features,\n",
    "                                                metadata=metadata,\n",
    "                                                targets=train_set['y'])\n",
    "splits = dataset.mapSplitsToDataset(splits)\n",
    "\n",
    "# Use SliceDataset to make the dataset accessible as a numpy array for scikit learn\n",
    "dataset = [SliceDataset(dataset, idx=0), SliceDataset(dataset, idx=1)]\n",
    "\n",
    "print(dataset[0].shape, dataset[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_GRIDSEARCH = False\n",
    "if RUN_GRIDSEARCH:\n",
    "    # GridSearch\n",
    "    # custom_nn.gridsearch(parameters=parameters, splits=splits, dataset=dataset, num_jobs=-1)\n",
    "\n",
    "    # RandomisedSearch, with n_iter the number of parameter settings that are sampled. Trade-off between goodness of the solution\n",
    "    # versus runtime.\n",
    "    custom_nn.randomsearch(\n",
    "        parameters=param_grid,\n",
    "        n_iter=20,\n",
    "        splits=splits,\n",
    "        dataset=dataset,\n",
    "    )\n",
    "    best_params = params = custom_nn.param_search.best_params_\n",
    "    best_estimator = custom_nn.param_search.best_estimator_\n",
    "    print(\"Best parameters:\\n\", best_params)\n",
    "    print(\"Best score:\\n\", custom_nn.param_search.best_score_)\n",
    "\n",
    "    # Save the model\n",
    "    custom_nn.save_model('gs_model.pkl')\n",
    "\n",
    "    # Create a folder to save figures (optional)\n",
    "    save_dir = \"figures\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Plot training and test scores\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(custom_nn.param_search.cv_results_['mean_train_score'], label='Train score')\n",
    "    plt.plot(custom_nn.param_search.cv_results_['mean_test_score'], label='Test score')\n",
    "    plt.xlabel('Hyperparameter Set Index')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('RandomizedSearchCV Results')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(os.path.join(save_dir, \"param_search_scores.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close()  # closes the plot to avoid display in notebooks/scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train custom model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(dataset[0]), dataset[0].shape)\n",
    "print(type(dataset[1]), dataset[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "\n",
    "if TRAIN:\n",
    "    param_grid = {'lr': [0.001, 0.01], 'max_epochs': [1000, 2000]}\n",
    "\n",
    "    custom_nn.set_params(lr=0.01, max_epochs=1000)\n",
    "\n",
    "    custom_nn.fit(dataset[0], dataset[1])\n",
    "\n",
    "    # Save the model\n",
    "    custom_nn.save_model('nn_model.pkl')\n",
    "else:\n",
    "    # Load model and set to CPU\n",
    "    loaded_model = mbm.models.CustomNeuralNetRegressor.load_model(\"nn_model.pkl\")\n",
    "    loaded_model = loaded_model.set_params(device='cpu')\n",
    "    loaded_model = loaded_model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features and metadata\n",
    "features_test, metadata_test = loaded_model._create_features_metadata(\n",
    "    test_set['df_X'][all_columns]\n",
    ")\n",
    "\n",
    "# Normalize features\n",
    "norm_features_test = norm.normalize(features_test)\n",
    "\n",
    "# Ensure all tensors are on CPU if they are torch tensors\n",
    "if hasattr(norm_features_test, 'cpu'):\n",
    "    norm_features_test = norm_features_test.cpu()\n",
    "if hasattr(metadata_test, 'cpu'):\n",
    "    metadata_test = metadata_test.cpu()\n",
    "\n",
    "# Ensure targets are also on CPU\n",
    "targets_test = test_set['y']\n",
    "if hasattr(targets_test, 'cpu'):\n",
    "    targets_test = targets_test.cpu()\n",
    "\n",
    "# Create the dataset\n",
    "dataset_test = mbm.data_processing.AggregatedDataset(\n",
    "    cfg,\n",
    "    features=norm_features_test,\n",
    "    metadata=metadata_test,\n",
    "    targets=targets_test\n",
    ")\n",
    "\n",
    "\n",
    "dataset_test = [SliceDataset(dataset_test, idx=0), SliceDataset(dataset_test, idx=1)]\n",
    "\n",
    "# Make predictions aggr to meas ID\n",
    "y_pred = loaded_model.predict(dataset_test[0])\n",
    "y_pred_agg = loaded_model.aggrPredict(dataset_test[0])\n",
    "\n",
    "batchIndex = np.arange(len(y_pred_agg))\n",
    "y_true = np.array([e for e in dataset_test[1][batchIndex]])\n",
    "\n",
    "# Calculate scores\n",
    "score = loaded_model.score(dataset_test[0], dataset_test[1])\n",
    "mse, rmse, mae, pearson = loaded_model.evalMetrics(y_pred, y_true)\n",
    "\n",
    "# Aggregate predictions\n",
    "id = dataset_test[0].dataset.indexToId(batchIndex)\n",
    "data = {'target': [e[0] for e in dataset_test[1]], 'ID': id, 'pred': y_pred_agg}\n",
    "grouped_ids = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predVSTruth(grouped_ids, mae, rmse, title):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    legend_nn = \"\\n\".join(\n",
    "        (r\"$\\mathrm{MAE_{nn}}=%.3f, \\mathrm{RMSE_{nn}}=%.3f$ \" % (\n",
    "            mae,\n",
    "            rmse,\n",
    "        ), ))\n",
    "\n",
    "    marker_nn = 'o'\n",
    "    sns.scatterplot(grouped_ids,\n",
    "                    x=\"target\",\n",
    "                    y=\"pred\",\n",
    "                    ax=ax,\n",
    "                    alpha=0.5,\n",
    "                    marker=marker_nn)\n",
    "\n",
    "    ax.set_ylabel('Predicted PMB [m w.e.]', fontsize=20)\n",
    "    ax.set_xlabel('Observed PMB [m w.e.]', fontsize=20)\n",
    "\n",
    "    ax.text(0.03,\n",
    "            0.98,\n",
    "            legend_nn,\n",
    "            transform=ax.transAxes,\n",
    "            verticalalignment=\"top\",\n",
    "            fontsize=20)\n",
    "    ax.legend([], [], frameon=False)\n",
    "    # diagonal line\n",
    "    pt = (0, 0)\n",
    "    ax.axline(pt, slope=1, color=\"grey\", linestyle=\"-\", linewidth=0.2)\n",
    "    ax.axvline(0, color=\"grey\", linestyle=\"-\", linewidth=0.2)\n",
    "    ax.axhline(0, color=\"grey\", linestyle=\"-\", linewidth=0.2)\n",
    "    ax.grid()\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predVSTruth(grouped_ids, mae, rmse, title='NN on test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
