{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from skorch.helper import SliceDataset\n",
    "from datetime import datetime\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "import pickle \n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'\n",
    "\n",
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "\n",
    "vois_climate = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "# Capitalize glacier names:\n",
    "glacierCap = {}\n",
    "for gl in data_glamos['GLACIER'].unique():\n",
    "    if isinstance(gl, str):  # Ensure the glacier name is a string\n",
    "        if gl.lower() == 'claridenu':\n",
    "            glacierCap[gl] = 'Clariden_U'\n",
    "        elif gl.lower() == 'claridenl':\n",
    "            glacierCap[gl] = 'Clariden_L'\n",
    "        else:\n",
    "            glacierCap[gl] = gl.capitalize()\n",
    "    else:\n",
    "        print(f\"Warning: Non-string glacier name encountered: {gl}\")\n",
    "\n",
    "# drop taelliboden and plainemorte if in there\n",
    "if 'taelliboden' in data_glamos['GLACIER'].unique():\n",
    "    data_glamos = data_glamos[data_glamos['GLACIER'] != 'taelliboden']\n",
    "if 'plainemorte' in data_glamos['GLACIER'].unique():\n",
    "    data_glamos = data_glamos[data_glamos['GLACIER'] != 'plainemorte']\n",
    "\n",
    "print('-------------------')\n",
    "print('Number of glaciers:', len(data_glamos['GLACIER'].unique()))\n",
    "print('Number of winter and annual samples:', len(data_glamos))\n",
    "print('Number of annual samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'winter']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:\n",
    "### Input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_NN.csv')\n",
    "\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils.build_head_tail_pads_from_monthly_df(\n",
    "    data_monthly)\n",
    "_, month_pos = mbm.data_processing.utils._rebuild_month_index(\n",
    "    months_head_pad, months_tail_pad)\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos[\"FROM_DATE\"] = pd.to_datetime(data_glamos[\"FROM_DATE\"].astype(str),\n",
    "                                          format=\"%Y%m%d\")\n",
    "data_glamos[\"TO_DATE\"] = pd.to_datetime(data_glamos[\"TO_DATE\"].astype(str),\n",
    "                                        format=\"%Y%m%d\")\n",
    "\n",
    "# Extract first and last months as numbers (1â€“12)\n",
    "data_glamos[\"FIRST_MONTH_NUM\"] = data_glamos[[\"FROM_DATE\"\n",
    "                                              ]].min(axis=1).dt.month\n",
    "data_glamos[\"LAST_MONTH_NUM\"] = data_glamos[[\"TO_DATE\"]].max(axis=1).dt.month\n",
    "\n",
    "# Compute min of all FIRST and max of all LAST\n",
    "global_first = data_glamos[\"FIRST_MONTH_NUM\"].min()\n",
    "global_last = data_glamos[\"LAST_MONTH_NUM\"].max()\n",
    "\n",
    "# Convert back to abbreviations if needed\n",
    "month_abbr = {\n",
    "    i: pd.to_datetime(str(i), format=\"%m\").strftime(\"%b\").lower()\n",
    "    for i in range(1, 13)\n",
    "}\n",
    "global_first_abbr = month_abbr[global_first]\n",
    "global_last_abbr = month_abbr[global_last]\n",
    "\n",
    "# print(\"Global earliest first month:\", global_first_abbr)\n",
    "# print(\"Global latest last month:\",global_last_abbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_annual = dataloader_gl.data[dataloader_gl.data.PERIOD == 'annual']\n",
    "\n",
    "# print mean and std of N_MONTHS\n",
    "print('Mean number of months:', data_annual.N_MONTHS.mean())\n",
    "print('Std number of months:', data_annual.N_MONTHS.std())\n",
    "\n",
    "# same for winter\n",
    "data_winter = dataloader_gl.data[dataloader_gl.data.PERIOD == 'winter']\n",
    "print('Mean number of months (winter):', data_winter.N_MONTHS.mean())\n",
    "print('Std number of months (winter):', data_winter.N_MONTHS.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocking on glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in TEST_GLACIERS if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in TEST_GLACIERS]\n",
    "\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(TEST_GLACIERS)]\n",
    "print('Size of monthly test data:', len(data_test))\n",
    "\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train))\n",
    "\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "# Number of annual versus winter measurements:\n",
    "print('-------------\\nTrain:')\n",
    "print('Number of monthly winter and annual samples:', len(data_train))\n",
    "print('Number of monthly annual samples:',\n",
    "      len(data_train[data_train.PERIOD == 'annual']))\n",
    "print('Number of monthly winter samples:',\n",
    "      len(data_train[data_train.PERIOD == 'winter']))\n",
    "\n",
    "# Same for test\n",
    "data_test_annual = data_test[data_test.PERIOD == 'annual']\n",
    "data_test_winter = data_test[data_test.PERIOD == 'winter']\n",
    "\n",
    "print('Test:')\n",
    "print('Number of monthly winter and annual samples:', len(data_test))\n",
    "print('Number of monthly annual samples:', len(data_test_annual))\n",
    "print('Number of monthly winter samples:', len(data_test_winter))\n",
    "\n",
    "print('Total:')\n",
    "print('Number of monthly rows:', len(dataloader_gl.data))\n",
    "print('Number of annual rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'annual']))\n",
    "print('Number of winter rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'winter']))\n",
    "\n",
    "# same for original data:\n",
    "print('-------------\\nIn annual format:')\n",
    "print('Number of annual train rows:',\n",
    "      len(data_glamos[data_glamos.GLACIER.isin(train_glaciers)]))\n",
    "print('Number of annual test rows:',\n",
    "      len(data_glamos[data_glamos.GLACIER.isin(TEST_GLACIERS)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=TEST_GLACIERS,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "dataloader = mbm.dataloader.DataLoader(cfg, data=data_train)\n",
    "\n",
    "train_itr, val_itr = dataloader.set_train_test_split(test_size=0.2)\n",
    "\n",
    "# Get all indices of the training and valing dataset at once from the iterators. Once called, the iterators are empty.\n",
    "train_indices, val_indices = list(train_itr), list(val_itr)\n",
    "\n",
    "df_X_train = data_train.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Get val set\n",
    "df_X_val = data_train.iloc[val_indices]\n",
    "y_val = df_X_val['POINT_BALANCE'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter grid search results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open grid_search results\n",
    "gs_results = pd.read_csv(\n",
    "    'logs/nn_param_search_progress_2025-09-19.csv').sort_values(\n",
    "        by='valid_loss', ascending=True)\n",
    "best_params = gs_results.iloc[0].to_dict()\n",
    "\n",
    "print('Best parameters from grid search:')\n",
    "for key, value in best_params.items():\n",
    "    if key not in ['valid_loss', 'train_loss']:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "gs_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_topo = [\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "    'pcsr',\n",
    "] + list(vois_topographical)\n",
    "\n",
    "feature_columns = features_topo + list(vois_climate)\n",
    "\n",
    "cfg.setFeatures(feature_columns)\n",
    "\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "# Because CH has some extra columns, we need to cut those\n",
    "df_X_train_subset = df_X_train[all_columns]\n",
    "df_X_val_subset = df_X_val[all_columns]\n",
    "df_X_test_subset = test_set['df_X'][all_columns]\n",
    "\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of validation dataset:', df_X_val_subset.shape)\n",
    "print('Shape of testing dataset:', df_X_test_subset.shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "assert all(train_set['df_X'].POINT_BALANCE == train_set['y'])\n",
    "\n",
    "# create the same but for winter only:\n",
    "df_X_train_subset_winter = df_X_train_subset[df_X_train_subset.PERIOD ==\n",
    "                                             'winter']\n",
    "df_X_val_subset_winter = df_X_val_subset[df_X_val_subset.PERIOD == 'winter']\n",
    "y_train_w = df_X_train_subset_winter['POINT_BALANCE'].values\n",
    "y_val_w = df_X_val_subset_winter['POINT_BALANCE'].values\n",
    "print('Shape of training dataset only winter:', df_X_train_subset_winter.shape)\n",
    "print('Shape of validation dataset only winter:', df_X_val_subset_winter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='valid_loss',\n",
    "    patience=15,\n",
    "    threshold=1e-4,  # Optional: stop only when improvement is very small\n",
    ")\n",
    "\n",
    "lr_scheduler_cb = LRScheduler(policy=ReduceLROnPlateau,\n",
    "                              monitor='valid_loss',\n",
    "                              mode='min',\n",
    "                              factor=0.5,\n",
    "                              patience=5,\n",
    "                              threshold=0.01,\n",
    "                              threshold_mode='rel',\n",
    "                              verbose=True)\n",
    "\n",
    "dataset = dataset_val = None  # Initialized hereafter\n",
    "\n",
    "\n",
    "def my_train_split(ds, y=None, **fit_params):\n",
    "    return dataset, dataset_val\n",
    "\n",
    "\n",
    "# param_init = {'device': 'cuda:0'}\n",
    "param_init = {'device': 'cpu'}  # Use CPU for training\n",
    "nInp = len(feature_columns)\n",
    "print('Number of input features:', nInp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'lr': 0.001,\n",
    "#     'batch_size': 128,\n",
    "#     'optimizer': torch.optim.Adam,\n",
    "#     'optimizer__weight_decay': 1e-05,\n",
    "#     'module__hidden_layers': [128, 128, 64, 32],\n",
    "#     'module__dropout': 0.2,\n",
    "#     'module__use_batchnorm': True,\n",
    "# }\n",
    "\n",
    "params = {\n",
    "    'lr': 0.0005,\n",
    "    'batch_size': 128,\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optimizer__weight_decay': 0.0,\n",
    "    'module__hidden_layers': [128, 128, 64, 64, 32],\n",
    "    'module__dropout': 0.2,\n",
    "    'module__use_batchnorm': False,\n",
    "}\n",
    "\n",
    "args = {\n",
    "    'module': FlexibleNetwork,\n",
    "    'nbFeatures': nInp,\n",
    "    'module__input_dim': nInp,\n",
    "    'module__dropout': params['module__dropout'],\n",
    "    'module__hidden_layers': params['module__hidden_layers'],\n",
    "    'train_split': my_train_split,\n",
    "    'batch_size': params['batch_size'],\n",
    "    'verbose': 1,\n",
    "    'iterator_train__shuffle': True,\n",
    "    'lr': params['lr'],\n",
    "    'max_epochs': 200,\n",
    "    'optimizer': params['optimizer'],\n",
    "    'optimizer__weight_decay': params['optimizer__weight_decay'],\n",
    "    'module__use_batchnorm': params['module__use_batchnorm'],\n",
    "    'callbacks': [\n",
    "        ('early_stop', early_stop),\n",
    "        ('lr_scheduler', lr_scheduler_cb),\n",
    "    ]\n",
    "}\n",
    "\n",
    "custom_nn = mbm.models.CustomNeuralNetRegressor(cfg, **args, **param_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, metadata = mbm.data_processing.utils.create_features_metadata(\n",
    "    cfg, df_X_train_subset)\n",
    "\n",
    "features_val, metadata_val = mbm.data_processing.utils.create_features_metadata(\n",
    "    cfg, df_X_val_subset)\n",
    "\n",
    "# Define the dataset for the NN\n",
    "dataset = mbm.data_processing.AggregatedDataset(\n",
    "    cfg,\n",
    "    features=features,\n",
    "    metadata=metadata,\n",
    "    months_head_pad=months_head_pad,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    targets=y_train)\n",
    "dataset = mbm.data_processing.SliceDatasetBinding(SliceDataset(dataset, idx=0),\n",
    "                                                  SliceDataset(dataset, idx=1))\n",
    "print(\"train:\", dataset.X.shape, dataset.y.shape)\n",
    "\n",
    "dataset_val = mbm.data_processing.AggregatedDataset(\n",
    "    cfg,\n",
    "    features=features_val,\n",
    "    metadata=metadata_val,\n",
    "    months_head_pad=months_head_pad,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    targets=y_val)\n",
    "dataset_val = mbm.data_processing.SliceDatasetBinding(\n",
    "    SliceDataset(dataset_val, idx=0), SliceDataset(dataset_val, idx=1))\n",
    "print(\"validation:\", dataset_val.X.shape, dataset_val.y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train custom model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "if TRAIN:\n",
    "    custom_nn.seed_all()\n",
    "\n",
    "    print(\"Training the model...\")\n",
    "    print('Model parameters:')\n",
    "    for key, value in args.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    custom_nn.fit(dataset.X, dataset.y)\n",
    "    # The dataset provided in fit is not used as the datasets are overwritten in the provided train_split function\n",
    "\n",
    "    # Generate filename with current date\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    model_filename = f\"nn_model_{current_date}\"\n",
    "\n",
    "    plot_training_history(custom_nn.history, skip_first_n=5)\n",
    "\n",
    "    # After Training: Best weights are already loaded\n",
    "    # Save the model\n",
    "    custom_nn.save_model(model_filename)\n",
    "\n",
    "    # save params dic\n",
    "    params_filename = f\"nn_params_{current_date}.pkl\"\n",
    "\n",
    "    with open(f\"models/{params_filename}\", \"wb\") as f:\n",
    "        pickle.dump(args, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and set to CPU\n",
    "model_filename = \"nn_model_2025-09-22.pt\"  # Replace with actual date if needed\n",
    "\n",
    "# read pickle with params\n",
    "params_filename = \"nn_params_2025-09-22.pkl\"  # Replace with actual date if needed\n",
    "with open(f\"models/{params_filename}\", \"rb\") as f:\n",
    "    custom_params = pickle.load(f)\n",
    "\n",
    "params = custom_params\n",
    "\n",
    "args = {\n",
    "    'module': FlexibleNetwork,\n",
    "    'nbFeatures': nInp,\n",
    "    'module__input_dim': nInp,\n",
    "    'module__dropout': params['module__dropout'],\n",
    "    'module__hidden_layers': params['module__hidden_layers'],\n",
    "    'train_split': my_train_split,\n",
    "    'batch_size': params['batch_size'],\n",
    "    'verbose': 1,\n",
    "    'iterator_train__shuffle': True,\n",
    "    'lr': params['lr'],\n",
    "    'max_epochs': 300,\n",
    "    'optimizer': params['optimizer'],\n",
    "    'optimizer__weight_decay': params['optimizer__weight_decay'],\n",
    "    'module__use_batchnorm': params['module__use_batchnorm'],\n",
    "    'callbacks': [\n",
    "        ('early_stop', early_stop),\n",
    "        ('lr_scheduler', lr_scheduler_cb),\n",
    "    ]\n",
    "}\n",
    "\n",
    "loaded_model = mbm.models.CustomNeuralNetRegressor.load_model(\n",
    "    cfg,\n",
    "    model_filename,\n",
    "    **{\n",
    "        **args,\n",
    "        **param_init\n",
    "    },\n",
    ")\n",
    "loaded_model = loaded_model.set_params(device='cpu')\n",
    "loaded_model = loaded_model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_ids, scores_NN, ids_NN, y_pred_NN = evaluate_model_and_group_predictions(\n",
    "    loaded_model, df_X_test_subset, test_set['y'], cfg, months_head_pad,\n",
    "    months_tail_pad)\n",
    "\n",
    "baseline_score = scores_NN['rmse']\n",
    "print('Baseline RMSE:', baseline_score)\n",
    "\n",
    "scores_annual, scores_winter = compute_seasonal_scores(grouped_ids,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "fig = plot_predictions_summary(grouped_ids=grouped_ids,\n",
    "                               scores_annual=scores_annual,\n",
    "                               scores_winter=scores_winter,\n",
    "                               predVSTruth=predVSTruth,\n",
    "                               plotMeanPred=plotMeanPred,\n",
    "                               ax_xlim=(-8, 6),\n",
    "                               ax_ylim=(-8, 6))\n",
    "\n",
    "# calculate RMSE\n",
    "root_mean_squared_error(grouped_ids['target'], grouped_ids['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_per_el = data_glamos[data_glamos.PERIOD == 'annual'].groupby(\n",
    "    ['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "\n",
    "test_gl_per_el = gl_per_el[TEST_GLACIERS].sort_values().index\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(20, 15), sharex=True)\n",
    "\n",
    "PlotIndividualGlacierPredVsTruth(grouped_ids,\n",
    "                                 axs=axs,\n",
    "                                 color_annual=color_dark_blue,\n",
    "                                 color_winter=color_pink,\n",
    "                                 custom_order=test_gl_per_el)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_ids_NN_train, scores_NN_train, ids_train, y_pred_train = evaluate_model_and_group_predictions(\n",
    "    loaded_model, data_train[all_columns], data_train['POINT_BALANCE'].values,\n",
    "    cfg, months_head_pad, months_tail_pad)\n",
    "scores_annual_NN, scores_winter_NN = compute_seasonal_scores(\n",
    "    grouped_ids_NN_train, target_col='target', pred_col='pred')\n",
    "fig = plot_predictions_summary(grouped_ids=grouped_ids_NN_train,\n",
    "                               scores_annual=scores_annual_NN,\n",
    "                               scores_winter=scores_winter_NN,\n",
    "                               predVSTruth=predVSTruth,\n",
    "                               plotMeanPred=plotMeanPred,\n",
    "                               ax_xlim=(-14, 8),\n",
    "                               ax_ylim=(-14, 8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gl_per_el = gl_per_el[train_glaciers].sort_values().index\n",
    "\n",
    "fig, axs = plt.subplots(7, 3, figsize=(20, 30), sharex=False)\n",
    "\n",
    "PlotIndividualGlacierPredVsTruth(grouped_ids_NN_train,\n",
    "                                 axs=axs,\n",
    "                                 color_annual=color_dark_blue,\n",
    "                                 color_winter=color_pink,\n",
    "                                 custom_order=train_gl_per_el,\n",
    "                                 ax_xlim=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrapolate in space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodetic_mb = get_geodetic_MB(cfg)\n",
    "\n",
    "# get years per glacier\n",
    "years_start_per_gl = geodetic_mb.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodetic_mb.groupby('glacier_name')['Aend'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "periods_per_glacier, geoMB_per_glacier = build_periods_per_glacier(geodetic_mb)\n",
    "\n",
    "glacier_list = list(data_glamos.GLACIER.unique())\n",
    "print('Number of glaciers with pcsr:', len(glacier_list))\n",
    "\n",
    "geodetic_glaciers = periods_per_glacier.keys()\n",
    "print('Number of glaciers with geodetic MB:', len(geodetic_glaciers))\n",
    "\n",
    "# Intersection of both\n",
    "common_glaciers = list(set(geodetic_glaciers) & set(glacier_list))\n",
    "print('Number of common glaciers:', len(common_glaciers))\n",
    "\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area(cfg)\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list, gl_area):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "\n",
    "glacier_list = sort_by_area(common_glaciers, gl_area)\n",
    "glacier_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GLAMOS grids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "path_save_glw = os.path.join(\n",
    "    cfg.dataPath, 'GLAMOS', 'distributed_MB_grids',\n",
    "    'MBM/testing_combis/glamos_dems_NN_SEB_full_OGGM')\n",
    "os.makedirs(path_save_glw, exist_ok=True)\n",
    "path_xr_grids = os.path.join(cfg.dataPath, 'GLAMOS', 'topo', 'GLAMOS_DEM',\n",
    "                             'xr_masked_grids')\n",
    "\n",
    "RUN = True\n",
    "if RUN:\n",
    "    emptyfolder(path_save_glw)\n",
    "    for glacier_name in glacier_list:\n",
    "        glacier_path = os.path.join(cfg.dataPath + path_glacier_grid_glamos,\n",
    "                                    glacier_name)\n",
    "\n",
    "        if not os.path.exists(glacier_path):\n",
    "            print(f\"Folder not found for {glacier_name}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        glacier_files = sorted(\n",
    "            [f for f in os.listdir(glacier_path) if glacier_name in f])\n",
    "\n",
    "        geodetic_range = range(np.min(periods_per_glacier[glacier_name]),\n",
    "                               np.max(periods_per_glacier[glacier_name]) + 1)\n",
    "\n",
    "        years = [\n",
    "            int(file_name.split('_')[2].split('.')[0])\n",
    "            for file_name in glacier_files\n",
    "        ]\n",
    "        years = [y for y in years if y in geodetic_range]\n",
    "\n",
    "        print(\n",
    "            f\"Processing {glacier_name} ({len(years)} files): {geodetic_range}\"\n",
    "        )\n",
    "\n",
    "        for year in tqdm(years, desc=f\"Processing {glacier_name}\",\n",
    "                         leave=False):\n",
    "            file_name = f\"{glacier_name}_grid_{year}.parquet\"\n",
    "\n",
    "            # Load parquet input glacier grid file in monthly format (pre-processed)\n",
    "            df_grid_monthly = pd.read_parquet(\n",
    "                os.path.join(cfg.dataPath + path_glacier_grid_glamos,\n",
    "                             glacier_name, file_name))\n",
    "\n",
    "            df_grid_monthly.drop_duplicates(inplace=True)\n",
    "\n",
    "            # Keep only necessary columns, avoiding missing columns issues\n",
    "            df_grid_monthly = df_grid_monthly[[\n",
    "                col for col in all_columns if col in df_grid_monthly.columns\n",
    "            ]]\n",
    "            df_grid_monthly = df_grid_monthly.dropna()\n",
    "\n",
    "            # Create geodata object\n",
    "            geoData = mbm.geodata.GeoData(df_grid_monthly,\n",
    "                                          months_head_pad=months_head_pad,\n",
    "                                          months_tail_pad=months_tail_pad)\n",
    "\n",
    "            # Computes and saves gridded MB for a year and glacier\n",
    "            path_glacier_dem = os.path.join(cfg.dataPath, path_xr_grids,\n",
    "                                            f\"{glacier_name}_{year}.zarr\")\n",
    "            geoData.gridded_MB_pred(\n",
    "                df_grid_monthly,\n",
    "                loaded_model,\n",
    "                glacier_name,\n",
    "                year,\n",
    "                all_columns,\n",
    "                path_glacier_dem,\n",
    "                path_save_glw,\n",
    "                save_monthly_pred=True,\n",
    "                type_model='NN',\n",
    "            )\n",
    "\n",
    "glacier_name = 'aletsch'\n",
    "year = 2008\n",
    "# open xarray\n",
    "xr.open_dataset(\n",
    "    os.path.join(\n",
    "        path_save_glw,\n",
    "        f'{glacier_name}/{glacier_name}_{year}_annual.zarr')).pred_masked.plot(\n",
    "            cmap='RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers_in_glamos = os.listdir(path_save_glw)\n",
    "\n",
    "geodetic_mb = get_geodetic_MB(cfg)\n",
    "\n",
    "# get years per glacier\n",
    "years_start_per_gl = geodetic_mb.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodetic_mb.groupby('glacier_name')['Aend'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "periods_per_glacier, geoMB_per_glacier = build_periods_per_glacier(geodetic_mb)\n",
    "\n",
    "# Glaciers with geodetic MB data:\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area(cfg)\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list, gl_area):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "\n",
    "glacier_list = [\n",
    "    f for f in list(periods_per_glacier.keys()) if f in glaciers_in_glamos\n",
    "]\n",
    "glacier_list = sort_by_area(glacier_list, gl_area)\n",
    "print('Number of glaciers:', len(glacier_list))\n",
    "print('Glaciers:', glacier_list)\n",
    "\n",
    "df_all_nn = process_geodetic_mass_balance_comparison(\n",
    "    glacier_list=glacier_list,\n",
    "    path_SMB_GLAMOS_csv=cfg.dataPath + path_SMB_GLAMOS_csv,\n",
    "    periods_per_glacier=periods_per_glacier,\n",
    "    geoMB_per_glacier=geoMB_per_glacier,\n",
    "    gl_area=gl_area,\n",
    "    test_glaciers=TEST_GLACIERS,\n",
    "    path_predictions=path_save_glw,  # or another path if needed\n",
    "    cfg=cfg)\n",
    "\n",
    "# Drop rows where any required columns are NaN\n",
    "df_all_nn = df_all_nn.dropna(subset=['Geodetic MB', 'MBM MB'])\n",
    "df_all_nn = df_all_nn.sort_values(by='Area')\n",
    "df_all_nn['GLACIER'] = df_all_nn['GLACIER'].apply(lambda x: x.capitalize())\n",
    "\n",
    "# Compute RMSE and Pearson correlation\n",
    "rmse_nn = root_mean_squared_error(df_all_nn[\"Geodetic MB\"],\n",
    "                             df_all_nn[\"MBM MB\"])\n",
    "corr_nn = np.corrcoef(df_all_nn[\"Geodetic MB\"], df_all_nn[\"MBM MB\"])[0, 1]\n",
    "\n",
    "plot_mbm_vs_geodetic_by_area_bin(df_all_nn,\n",
    "                                 bins=[0, 1, 5, 10, 100, np.inf],\n",
    "                                 labels=['<1', '1-5', '5â€“10', '>10', '>100'],\n",
    "                                 max_bins=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(cfg.seed)\n",
    "importances = {col: [] for col in feature_columns}\n",
    "\n",
    "# Compute baseline\n",
    "_, scores_baseline, _, _ = evaluate_model_and_group_predictions(\n",
    "    loaded_model, df_X_test_subset, test_set['y'], cfg, months_head_pad,\n",
    "    months_tail_pad)\n",
    "\n",
    "baseline_score = scores_baseline['rmse']\n",
    "print(f\"Baseline RMSE: {baseline_score:.4f}\")\n",
    "\n",
    "n_repeats = 10\n",
    "for col in tqdm(feature_columns):\n",
    "    for _ in range(n_repeats):\n",
    "        df_permuted = df_X_test_subset.copy()\n",
    "        df_permuted[col] = rng.permutation(df_permuted[col].values)\n",
    "\n",
    "        # Evaluate model on permuted data\n",
    "        _, scores_perm, _, _ = evaluate_model_and_group_predictions(\n",
    "            loaded_model, df_permuted, test_set['y'], cfg, months_head_pad,\n",
    "            months_tail_pad)\n",
    "        perm_score = scores_perm['rmse']\n",
    "        importance = perm_score - baseline_score  # Positive = worse performance\n",
    "        importances[col].append(importance)\n",
    "\n",
    "# Aggregate results\n",
    "df_importances = pd.DataFrame({\n",
    "    \"feature\":\n",
    "    feature_columns,\n",
    "    \"mean_importance\": [np.mean(importances[col]) for col in feature_columns],\n",
    "    \"std_importance\": [np.std(importances[col]) for col in feature_columns],\n",
    "}).sort_values(by=\"mean_importance\", ascending=False)\n",
    "plot_permutation_importance(df_importances, top_n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_nn = process_geodetic_mass_balance_comparison(\n",
    "    glacier_list=glacier_list,\n",
    "    path_SMB_GLAMOS_csv=cfg.dataPath + path_SMB_GLAMOS_csv,\n",
    "    periods_per_glacier=periods_per_glacier,\n",
    "    geoMB_per_glacier=geoMB_per_glacier,\n",
    "    gl_area=gl_area,\n",
    "    test_glaciers=TEST_GLACIERS,\n",
    "    path_predictions=path_save_glw,  # or another path if needed\n",
    "    cfg=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stake data ONCE instead of for every glacier\n",
    "stake_file = os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv,\n",
    "                          \"CH_wgms_dataset_all.csv\")\n",
    "df_stakes = pd.read_csv(stake_file)\n",
    "\n",
    "# Example usage\n",
    "GLACIER_NAME = 'silvretta'\n",
    "df_nn = df_all_nn[df_all_nn.GLACIER == GLACIER_NAME]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "plot_scatter_comparison(axs[0],\n",
    "                        df_nn,\n",
    "                        GLACIER_NAME,\n",
    "                        color_mbm=color_annual,\n",
    "                        color_glamos=color_winter,\n",
    "                        title_suffix=\"(NN)\")\n",
    "\n",
    "# Load GLAMOS data\n",
    "GLAMOS_glwmb = get_GLAMOS_glwmb(GLACIER_NAME, cfg)\n",
    "\n",
    "MBM_glwmb_nn = mbm_glwd_pred(path_save_glw, GLACIER_NAME)\n",
    "MBM_glwmb_nn.rename(columns={\"MBM Balance\": \"MBM Balance NN\"}, inplace=True)\n",
    "\n",
    "# Merge with GLAMOS data\n",
    "MBM_glwmb_nn = MBM_glwmb_nn.join(GLAMOS_glwmb)\n",
    "\n",
    "# Drop NaN values to avoid plotting errors\n",
    "MBM_glwmb_nn = MBM_glwmb_nn.dropna()\n",
    "\n",
    "MBM_glwmb_nn.plot(ax=axs[1],\n",
    "                  y=['MBM Balance NN', 'GLAMOS Balance'],\n",
    "                  marker=\"o\",\n",
    "                  color=[color_annual, color_winter])\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_title(f\"{GLACIER_NAME.capitalize()} Glacier\", fontsize=24)\n",
    "    ax.set_ylabel(\"Mass Balance [m w.e.]\", fontsize=18)\n",
    "    ax.set_xlabel(\"Year\", fontsize=18)\n",
    "    ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "    ax.legend(fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in MBM_glwmb_nn.index:\n",
    "    plot_mass_balance_comparison_annual_glamos_nn(\n",
    "        glacier_name=GLACIER_NAME,\n",
    "        year=year,\n",
    "        cfg=cfg,\n",
    "        df_stakes=df_stakes,\n",
    "        path_distributed_mb=path_distributed_MB_glamos,\n",
    "        path_pred_nn=path_save_glw,\n",
    "        get_glamos_func=get_GLAMOS_glwmb,\n",
    "        get_pred_func=get_predicted_mb,\n",
    "        get_glamos_pred_func=get_predicted_mb_glamos,\n",
    "        load_grid_func=load_grid_file,\n",
    "        to_wgs84_func=transform_xarray_coords_lv95_to_wgs84,\n",
    "        apply_filter_func=apply_gaussian_filter,\n",
    "        get_colormaps_func=get_color_maps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
