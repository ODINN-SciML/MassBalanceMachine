{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glacier grids from RGI:\n",
    "\n",
    "Creates monthly grid files for the MBM to make PMB predictions over the whole glacier grid. The files come from the RGI grid with OGGM topography. Computing takes a long time because of the conversion to monthly format.\n",
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import massbalancemachine as mbm\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from cmcrameri import cm\n",
    "from oggm import utils, workflow\n",
    "from oggm import cfg as oggmCfg\n",
    "import geopandas as gpd\n",
    "import geopandas as gpd\n",
    "import traceback\n",
    "import salem\n",
    "import oggm\n",
    "import pickle\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from skorch.helper import SliceDataset\n",
    "from cartopy import crs as ccrs, feature as cfeature\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "from collections import Counter\n",
    "\n",
    "# scripts\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.geodata import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.nn_helpers import *\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()  # in case no memory\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "# Climate columns\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "# Topographical columns\n",
    "vois_topographical = [\n",
    "    \"aspect\",\n",
    "    \"slope\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "    \"topo\",\n",
    "]\n",
    "\n",
    "glacier_outline_rgi = gpd.read_file(cfg.dataPath + path_rgi_outlines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdirs, rgidf = initialize_oggm_glacier_directories(\n",
    "    cfg,\n",
    "    rgi_region=\"11\",\n",
    "    rgi_version=\"6\",\n",
    "    base_url=\n",
    "    \"https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L3-L5_files/2023.1/elev_bands/W5E5_w_data/\",\n",
    "    log_level='WARNING',\n",
    "    task_list=None,\n",
    ")\n",
    "# Save OGGM xr for all needed glaciers in RGI region 11.6:\n",
    "export_oggm_grids(cfg, gdirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGI Ids:\n",
    "# Read glacier ids:\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "rgi_df.loc['rhone']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RGI grids for all glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_RGIs = cfg.dataPath + path_OGGM + 'xr_grids/'\n",
    "glaciers = os.listdir(path_RGIs)\n",
    "\n",
    "print(f\"Found {len(glaciers)} glaciers in RGI region 11.6\")\n",
    "\n",
    "# Open an example\n",
    "# rgi_gl = gdirs[0].rgi_id\n",
    "rgi_gl = 'RGI60-11.01238'\n",
    "\n",
    "ds = xr.open_dataset(path_RGIs + rgi_gl + '.zarr')\n",
    "glacier_mask = np.where(ds['glacier_mask'].values == 0, np.nan,\n",
    "                        ds['glacier_mask'].values)\n",
    "\n",
    "# Create glacier mask\n",
    "ds = ds.assign(masked_slope=glacier_mask * ds['slope'])\n",
    "ds = ds.assign(masked_elev=glacier_mask * ds['topo'])\n",
    "ds = ds.assign(masked_aspect=glacier_mask * ds['aspect'])\n",
    "ds = ds.assign(masked_dis=glacier_mask * ds['dis_from_border'])\n",
    "\n",
    "# Assign other variables only if available\n",
    "if 'hugonnet_dhdt' in ds:\n",
    "    ds = ds.assign(masked_hug=glacier_mask * ds['hugonnet_dhdt'])\n",
    "if 'consensus_ice_thickness' in ds:\n",
    "    ds = ds.assign(masked_cit=glacier_mask * ds['consensus_ice_thickness'])\n",
    "if 'millan_v' in ds:\n",
    "    ds = ds.assign(masked_miv=glacier_mask * ds['millan_v'])\n",
    "\n",
    "glacier_indices = np.where(ds['glacier_mask'].values == 1)\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 8), sharey=True)\n",
    "\n",
    "ds.masked_aspect.plot(ax=axs[0], cmap='twilight_shifted', add_colorbar=False)\n",
    "ds.masked_slope.plot(ax=axs[1], cmap='cividis', add_colorbar=False)\n",
    "ds.masked_elev.plot(ax=axs[2], cmap='terrain', add_colorbar=False)\n",
    "ds.glacier_mask.plot(ax=axs[3], cmap='binary', add_colorbar=False)\n",
    "\n",
    "axs[0].set_title(\"Aspect OGGM\")\n",
    "axs[1].set_title(\"Slope OGGM\")\n",
    "axs[2].set_title(\"DEM OGGM\")\n",
    "axs[3].set_title(\"Glacier mask OGGM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_glacier(path_RGIs, rgi_gl):\n",
    "    # Load dataset\n",
    "    ds = xr.open_dataset(path_RGIs + rgi_gl + '.zarr')\n",
    "\n",
    "    # Check if 'glacier_mask' exists\n",
    "    if 'glacier_mask' not in ds:\n",
    "        raise ValueError(\n",
    "            f\"'glacier_mask' variable not found in dataset {rgi_gl}\")\n",
    "\n",
    "    # Create glacier mask\n",
    "    glacier_mask = np.where(ds['glacier_mask'].values == 0, np.nan,\n",
    "                            ds['glacier_mask'].values)\n",
    "\n",
    "    # Apply mask to core variables\n",
    "    ds = ds.assign(masked_slope=glacier_mask * ds['slope'])\n",
    "    ds = ds.assign(masked_elev=glacier_mask * ds['topo'])\n",
    "    ds = ds.assign(masked_aspect=glacier_mask * ds['aspect'])\n",
    "    ds = ds.assign(masked_dis=glacier_mask * ds['dis_from_border'])\n",
    "\n",
    "    # Apply mask to optional variables if present\n",
    "    if 'hugonnet_dhdt' in ds:\n",
    "        ds = ds.assign(masked_hug=glacier_mask * ds['hugonnet_dhdt'])\n",
    "    if 'consensus_ice_thickness' in ds:\n",
    "        ds = ds.assign(masked_cit=glacier_mask * ds['consensus_ice_thickness'])\n",
    "    if 'millan_v' in ds:\n",
    "        ds = ds.assign(masked_miv=glacier_mask * ds['millan_v'])\n",
    "\n",
    "    # Indices where glacier_mask == 1\n",
    "    glacier_indices = np.where(ds['glacier_mask'].values == 1)\n",
    "\n",
    "    return ds, glacier_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create masked grids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_xr_grids = os.path.join(cfg.dataPath, 'GLAMOS/topo/RGI_v6_11/',\n",
    "                             'xr_masked_grids/')\n",
    "RUN = False\n",
    "if RUN:\n",
    "    emptyfolder(path_xr_grids)\n",
    "\n",
    "    for gdir in tqdm(gdirs):\n",
    "        rgi_gl = gdir.rgi_id\n",
    "\n",
    "        try:\n",
    "            # Create masked glacier dataset\n",
    "            ds, glacier_indices = create_masked_glacier(path_RGIs, rgi_gl)\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping {rgi_gl}: {e}\")\n",
    "            continue  # Skip to next glacier\n",
    "\n",
    "        dx_m, dy_m = get_res_from_projected(ds)\n",
    "\n",
    "        # Coarsen to 50 m resolution if needed\n",
    "        if 20 < dx_m < 50:\n",
    "            ds = coarsenDS_mercator(ds, target_res_m=50)\n",
    "            dx_m, dy_m = get_res_from_projected(ds)\n",
    "        else:\n",
    "            ds = ds\n",
    "\n",
    "        # Change coordinates to Lat/Lon projection\n",
    "        original_proj = ds.pyproj_srs\n",
    "        ds = ds.rio.write_crs(original_proj)\n",
    "        ds_latlon = ds.rio.reproject(\"EPSG:4326\")\n",
    "        ds_latlon = ds_latlon.rename({'x': 'lon', 'y': 'lat'})\n",
    "\n",
    "        # Save xarray dataset\n",
    "        save_path = os.path.join(path_xr_grids, f\"{rgi_gl}.zarr\")\n",
    "        ds_latlon.to_zarr(save_path)\n",
    "\n",
    "# open example\n",
    "for gdir in gdirs:\n",
    "    if gdir.rgi_id == 'RGI60-11.01238':\n",
    "        gdir_rhone = gdir\n",
    "\n",
    "rgi_gl_rhone = gdir_rhone.rgi_id\n",
    "ds = xr.open_dataset(path_xr_grids + rgi_gl_rhone + '.zarr')\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 6))\n",
    "ds.masked_aspect.plot(ax=axs[0], cmap='twilight_shifted', add_colorbar=True)\n",
    "ds.masked_slope.plot(ax=axs[1], cmap='cividis', add_colorbar=True)\n",
    "ds.masked_elev.plot(ax=axs[2], cmap='terrain', add_colorbar=True)\n",
    "ds.glacier_mask.plot(ax=axs[3], cmap='binary', add_colorbar=False)\n",
    "\n",
    "axs[0].set_title(\"Aspect\")\n",
    "axs[1].set_title(\"Slope\")\n",
    "axs[2].set_title(\"DEM\")\n",
    "axs[3].set_title(\"Glacier mask\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open example\n",
    "for gdir in gdirs:\n",
    "    if gdir.rgi_id == 'RGI60-11.00878':\n",
    "        gdir_rhone = gdir\n",
    "\n",
    "rgi_gl_rhone = gdir_rhone.rgi_id\n",
    "ds = xr.open_dataset(path_xr_grids + rgi_gl_rhone + '.zarr')\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 6))\n",
    "ds.masked_aspect.plot(ax=axs[0], cmap='twilight_shifted', add_colorbar=True)\n",
    "ds.masked_slope.plot(ax=axs[1], cmap='cividis', add_colorbar=True)\n",
    "ds.masked_elev.plot(ax=axs[2], cmap='terrain', add_colorbar=True)\n",
    "ds.glacier_mask.plot(ax=axs[3], cmap='binary', add_colorbar=False)\n",
    "\n",
    "axs[0].set_title(\"Aspect\")\n",
    "axs[1].set_title(\"Slope\")\n",
    "axs[2].set_title(\"DEM\")\n",
    "axs[3].set_title(\"Glacier mask\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create monthly dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = False\n",
    "path_rgi_alps = os.path.join(cfg.dataPath,\n",
    "                             'GLAMOS/topo/gridded_topo_inputs/RGI_v6_11/')\n",
    "\n",
    "if RUN:\n",
    "    years = range(2000, 2024)\n",
    "\n",
    "    #os.makedirs(path_rgi_alps, exist_ok=True)\n",
    "    #emptyfolder(path_rgi_alps)\n",
    "\n",
    "    valid_rgis = [\n",
    "        f.replace('.zarr', '') for f in os.listdir(path_xr_grids)\n",
    "        if f.endswith('.zarr')\n",
    "    ]\n",
    "    \n",
    "    processed_rgis = os.listdir(path_rgi_alps)\n",
    "    rest_rgis = list(set(valid_rgis) - set(processed_rgis))\n",
    "    print(f\"Number of glaciers to process: {len(rest_rgis)}\")\n",
    "\n",
    "    for gdir in tqdm(gdirs, desc=\"Processing glaciers\"):\n",
    "    # for gdir in [gdir_rhone]:  # For testing, only process one glacier\n",
    "        rgi_gl = gdir.rgi_id\n",
    "\n",
    "        if rgi_gl not in valid_rgis:\n",
    "            print(f\"Skipping {rgi_gl}: not found in valid RGI glaciers\")\n",
    "            continue\n",
    "        if rgi_gl in processed_rgis:\n",
    "            continue\n",
    "        try:\n",
    "            file_path = os.path.join(path_xr_grids, f\"{rgi_gl}.zarr\")\n",
    "            if not os.path.exists(file_path):\n",
    "                raise FileNotFoundError(f\"Missing file: {file_path}\")\n",
    "\n",
    "            try:\n",
    "                ds = xr.open_zarr(file_path, consolidated=True)\n",
    "            except Exception:\n",
    "                ds = xr.open_zarr(file_path)\n",
    "\n",
    "            # Create glacier grid\n",
    "            try:\n",
    "                df_grid = create_glacier_grid_RGI(ds, years, rgi_gl)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed creating glacier grid for {rgi_gl}: {e}\")\n",
    "                continue\n",
    "\n",
    "            df_grid.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # Add GLWD_ID\n",
    "            df_grid['GLWD_ID'] = [\n",
    "                mbm.data_processing.utils.get_hash(f\"{r}_{y}\") for r, y in zip(\n",
    "                    df_grid['RGIId'].astype(str), df_grid['YEAR'].astype(str))\n",
    "            ]\n",
    "            df_grid['GLWD_ID'] = df_grid['GLWD_ID'].astype(str)\n",
    "            df_grid['GLACIER'] = df_grid['RGIId']\n",
    "\n",
    "            # Prepare output folder\n",
    "            folder_path = os.path.join(path_rgi_alps, rgi_gl)\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "            # Process each year\n",
    "            for year in years:\n",
    "                try:\n",
    "                    df_grid_y = df_grid[df_grid.YEAR == year].copy()\n",
    "                    if df_grid_y.empty:\n",
    "                        continue\n",
    "\n",
    "                    # Wrap Dataset creation & climate feature extraction\n",
    "                    try:\n",
    "                        dataset_grid_yearly = mbm.data_processing.Dataset(\n",
    "                            cfg=cfg,\n",
    "                            data=df_grid_y,\n",
    "                            region_name='CH',\n",
    "                            data_path=os.path.join(cfg.dataPath,\n",
    "                                                   path_PMB_GLAMOS_csv))\n",
    "\n",
    "                        era5_climate_data = os.path.join(\n",
    "                            cfg.dataPath, path_ERA5_raw,\n",
    "                            'era5_monthly_averaged_data_Alps.nc')\n",
    "                        geopotential_data = os.path.join(\n",
    "                            cfg.dataPath, path_ERA5_raw,\n",
    "                            'era5_geopotential_pressure_Alps.nc')\n",
    "\n",
    "                        dataset_grid_yearly.get_climate_features(\n",
    "                            climate_data=era5_climate_data,\n",
    "                            geopotential_data=geopotential_data,\n",
    "                            change_units=True,\n",
    "                            smoothing_vois={\n",
    "                                'vois_climate': vois_climate,\n",
    "                                'vois_other': ['ALTITUDE_CLIMATE']\n",
    "                            })\n",
    "                    except Exception as e:\n",
    "                        print(\n",
    "                            f\"Failed adding climate features for {rgi_gl}: {e}\"\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                    vois_topographical_sub = [\n",
    "                        voi for voi in vois_topographical\n",
    "                        if voi in df_grid_y.columns\n",
    "                    ]\n",
    "\n",
    "                    dataset_grid_yearly.convert_to_monthly(\n",
    "                        meta_data_columns=cfg.metaData,\n",
    "                        vois_climate=vois_climate,\n",
    "                        vois_topographical=vois_topographical_sub)\n",
    "\n",
    "                    save_path = os.path.join(folder_path,\n",
    "                                             f\"{rgi_gl}_grid_{year}.parquet\")\n",
    "                    dataset_grid_yearly.data.to_parquet(save_path,\n",
    "                                                        engine=\"pyarrow\",\n",
    "                                                        compression=\"snappy\")\n",
    "                    #print(f\"Saved: {save_path}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed processing {rgi_gl} for year {year}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with glacier {rgi_gl}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gdir in gdirs:\n",
    "    if gdir.rgi_id == 'RGI60-11.01238':\n",
    "        gdir_rhone = gdir\n",
    "\n",
    "# Look at one example\n",
    "# load the dataset\n",
    "rgi_gl = gdir_rhone.rgi_id\n",
    "\n",
    "year = 2000\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == 'sep']\n",
    "print(df['t2m'].unique())\n",
    "\n",
    "year = 2008\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == 'sep']\n",
    "print(df['t2m'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gdir in gdirs:\n",
    "    if gdir.rgi_id == 'RGI60-11.01238':\n",
    "        gdir_rhone = gdir\n",
    "\n",
    "# Look at one example\n",
    "# load the dataset\n",
    "year = 2008\n",
    "rgi_gl = gdir_rhone.rgi_id\n",
    "\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == 'sep']\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "voi = [\n",
    "    't2m', 'tp', 'ALTITUDE_CLIMATE', 'ELEVATION_DIFFERENCE', 'hugonnet_dhdt',\n",
    "    'consensus_ice_thickness'\n",
    "]\n",
    "axs = axs.flatten()\n",
    "for i, var in enumerate(voi):\n",
    "    sns.scatterplot(df,\n",
    "                    x='POINT_LON',\n",
    "                    y='POINT_LAT',\n",
    "                    hue=var,\n",
    "                    s=5,\n",
    "                    alpha=0.5,\n",
    "                    palette='twilight_shifted',\n",
    "                    ax=axs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location of all glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_ids = os.listdir(path_rgi_alps)\n",
    "pos_gl = []\n",
    "for rgi_gl in tqdm(rgi_ids):\n",
    "    df = pd.read_parquet(\n",
    "        os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "    pos_gl.append((df.POINT_LAT.mean(), df.POINT_LON.mean()))\n",
    "df_pos_all = pd.DataFrame(pos_gl, columns=['lat', 'lon'])\n",
    "df_pos_all['rgi_id'] = rgi_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of glaciers in RGI region 11.6:', len(df_pos_all))\n",
    "\n",
    "# ---- 2. Create figure and base map ----\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "\n",
    "latN, latS = 48, 44\n",
    "lonW, lonE = 4, 14\n",
    "projPC = ccrs.PlateCarree()\n",
    "ax2 = plt.axes(projection=projPC)\n",
    "ax2.set_extent([lonW, lonE, latS, latN], crs=ccrs.Geodetic())\n",
    "\n",
    "ax2.add_feature(cfeature.COASTLINE)\n",
    "ax2.add_feature(cfeature.LAKES)\n",
    "ax2.add_feature(cfeature.RIVERS)\n",
    "ax2.add_feature(cfeature.BORDERS, linestyle='-', linewidth=1)\n",
    "\n",
    "g = sns.scatterplot(\n",
    "    data=df_pos_all,\n",
    "    x='lon',\n",
    "    y='lat',\n",
    "    alpha=0.6,\n",
    "    transform=projPC,\n",
    "    ax=ax2,\n",
    "    zorder=10,\n",
    "    legend=True  # custom legend added below\n",
    ")\n",
    "\n",
    "glacier_outline_rgi.plot(ax=ax2, transform=projPC, color='black')\n",
    "\n",
    "# ---- 4. Gridlines ----\n",
    "gl = ax2.gridlines(draw_labels=True,\n",
    "                   linewidth=1,\n",
    "                   color='gray',\n",
    "                   alpha=0.5,\n",
    "                   linestyle='--')\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "gl.xlabel_style = {'size': 16, 'color': 'black'}\n",
    "gl.ylabel_style = {'size': 16, 'color': 'black'}\n",
    "gl.top_labels = gl.right_labels = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train NN model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Climate columns\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str',\n",
    "]\n",
    "# Topographical columns\n",
    "vois_topographical = [\n",
    "    \"aspect\",\n",
    "    \"slope\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "# drop taelliboden if in there\n",
    "if 'taelliboden' in data_glamos['GLACIER'].unique():\n",
    "    data_glamos = data_glamos[data_glamos['GLACIER'] != 'taelliboden']\n",
    "\n",
    "print('-------------------')\n",
    "print('Number of glaciers:', len(data_glamos['GLACIER'].unique()))\n",
    "print('Number of winter and annual samples:', len(data_glamos))\n",
    "print('Number of annual samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'winter']))\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_central_alps.csv')\n",
    "data_monthly = dataloader_gl.data\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils.build_head_tail_pads_from_monthly_df(data_monthly)\n",
    "\n",
    "data_monthly['GLWD_ID'] = data_monthly.apply(\n",
    "    lambda x: mbm.data_processing.utils.get_hash(f\"{x.GLACIER}_{x.YEAR}\"),\n",
    "    axis=1)\n",
    "data_monthly['GLWD_ID'] = data_monthly['GLWD_ID'].astype(str)\n",
    "\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in TEST_GLACIERS if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in TEST_GLACIERS]\n",
    "\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(TEST_GLACIERS)]\n",
    "print('Size of monthly test data:', len(data_test))\n",
    "\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train))\n",
    "\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "# Number of annual versus winter measurements:\n",
    "print('-------------\\nTrain:')\n",
    "print('Number of monthly winter and annual samples:', len(data_train))\n",
    "print('Number of monthly annual samples:',\n",
    "      len(data_train[data_train.PERIOD == 'annual']))\n",
    "print('Number of monthly winter samples:',\n",
    "      len(data_train[data_train.PERIOD == 'winter']))\n",
    "\n",
    "# Same for test\n",
    "data_test_annual = data_test[data_test.PERIOD == 'annual']\n",
    "data_test_winter = data_test[data_test.PERIOD == 'winter']\n",
    "\n",
    "print('Test:')\n",
    "print('Number of monthly winter and annual samples:', len(data_test))\n",
    "print('Number of monthly annual samples:', len(data_test_annual))\n",
    "print('Number of monthly winter samples:', len(data_test_winter))\n",
    "\n",
    "print('Total:')\n",
    "print('Number of monthly rows:', len(dataloader_gl.data))\n",
    "print('Number of annual rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'annual']))\n",
    "print('Number of winter rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'winter']))\n",
    "\n",
    "# same for original data:\n",
    "print('-------------\\nIn annual format:')\n",
    "print('Number of annual train rows:',\n",
    "      len(data_glamos[data_glamos.GLACIER.isin(train_glaciers)]))\n",
    "print('Number of annual test rows:',\n",
    "      len(data_glamos[data_glamos.GLACIER.isin(TEST_GLACIERS)]))\n",
    "\n",
    "print('---------------\\n CV splits:')\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=TEST_GLACIERS,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "dataloader = mbm.dataloader.DataLoader(cfg, data=data_train)\n",
    "\n",
    "train_itr, val_itr = dataloader.set_train_test_split(test_size=0.2)\n",
    "\n",
    "# Get all indices of the training and valing dataset at once from the iterators. Once called, the iterators are empty.\n",
    "train_indices, val_indices = list(train_itr), list(val_itr)\n",
    "\n",
    "df_X_train = data_train.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Get val set\n",
    "df_X_val = data_train.iloc[val_indices]\n",
    "y_val = df_X_val['POINT_BALANCE'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_topo = [\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "    # 'pcsr',\n",
    "] + list(vois_topographical)\n",
    "\n",
    "feature_columns = features_topo + list(vois_climate)\n",
    "\n",
    "cfg.setFeatures(feature_columns)\n",
    "\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "# Because CH has some extra columns, we need to cut those\n",
    "df_X_train_subset = df_X_train[all_columns]\n",
    "df_X_val_subset = df_X_val[all_columns]\n",
    "df_X_test_subset = test_set['df_X'][all_columns]\n",
    "\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of validation dataset:', df_X_val_subset.shape)\n",
    "print('Shape of testing dataset:', df_X_test_subset.shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "assert all(train_set['df_X'].POINT_BALANCE == train_set['y'])\n",
    "\n",
    "# create the same but for winter only:\n",
    "df_X_train_subset_winter = df_X_train_subset[df_X_train_subset.PERIOD ==\n",
    "                                             'winter']\n",
    "df_X_val_subset_winter = df_X_val_subset[df_X_val_subset.PERIOD == 'winter']\n",
    "y_train_w = df_X_train_subset_winter['POINT_BALANCE'].values\n",
    "y_val_w = df_X_val_subset_winter['POINT_BALANCE'].values\n",
    "print('Shape of training dataset only winter:', df_X_train_subset_winter.shape)\n",
    "print('Shape of validation dataset only winter:', df_X_val_subset_winter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='valid_loss',\n",
    "    patience=15,\n",
    "    threshold=1e-4,  # Optional: stop only when improvement is very small\n",
    ")\n",
    "\n",
    "lr_scheduler_cb = LRScheduler(policy=ReduceLROnPlateau,\n",
    "                              monitor='valid_loss',\n",
    "                              mode='min',\n",
    "                              factor=0.5,\n",
    "                              patience=5,\n",
    "                              threshold=0.01,\n",
    "                              threshold_mode='rel',\n",
    "                              verbose=True)\n",
    "\n",
    "dataset = dataset_val = None  # Initialized hereafter\n",
    "\n",
    "\n",
    "def my_train_split(ds, y=None, **fit_params):\n",
    "    return dataset, dataset_val\n",
    "\n",
    "\n",
    "# param_init = {'device': 'cuda:0'}\n",
    "param_init = {'device': 'cpu'}  # Use CPU for training\n",
    "nInp = len(feature_columns)\n",
    "print('Number of input features:', nInp)\n",
    "\n",
    "params = {\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 256,\n",
    "    'optimizer': torch.optim.AdamW,\n",
    "    'optimizer__weight_decay': 1e-05,\n",
    "    'module__hidden_layers': [256, 128, 64, 32],\n",
    "    'module__dropout': 0.2,\n",
    "    'module__use_batchnorm': True,\n",
    "}\n",
    "\n",
    "args = {\n",
    "    'module': FlexibleNetwork,\n",
    "    'nbFeatures': nInp,\n",
    "    'module__input_dim': nInp,\n",
    "    'module__dropout': params['module__dropout'],\n",
    "    'module__hidden_layers': params['module__hidden_layers'],\n",
    "    'train_split': my_train_split,\n",
    "    'batch_size': params['batch_size'],\n",
    "    'verbose': 1,\n",
    "    'iterator_train__shuffle': True,\n",
    "    'lr': params['lr'],\n",
    "    'max_epochs': 200,\n",
    "    'optimizer': params['optimizer'],\n",
    "    'optimizer__weight_decay': params['optimizer__weight_decay'],\n",
    "    'module__use_batchnorm': params['module__use_batchnorm'],\n",
    "    'callbacks': [\n",
    "        ('early_stop', early_stop),\n",
    "        ('lr_scheduler', lr_scheduler_cb),\n",
    "    ]\n",
    "}\n",
    "\n",
    "custom_nn = mbm.models.CustomNeuralNetRegressor(cfg, **args, **param_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, metadata = mbm.data_processing.utils.create_features_metadata(cfg, df_X_train_subset)\n",
    "\n",
    "features_val, metadata_val = mbm.data_processing.utils.create_features_metadata(\n",
    "    cfg, df_X_val_subset)\n",
    "\n",
    "# Define the dataset for the NN\n",
    "dataset = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                features=features,\n",
    "                                                metadata=metadata,\n",
    "                                                months_head_pad=months_head_pad,\n",
    "                                                months_tail_pad=months_tail_pad,\n",
    "                                                targets=y_train)\n",
    "dataset = mbm.data_processing.SliceDatasetBinding(SliceDataset(dataset, idx=0),\n",
    "                                                  SliceDataset(dataset, idx=1))\n",
    "print(\"train:\", dataset.X.shape, dataset.y.shape)\n",
    "\n",
    "dataset_val = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                    features=features_val,\n",
    "                                                    metadata=metadata_val,\n",
    "                                                    months_head_pad=months_head_pad,\n",
    "                                                    months_tail_pad=months_tail_pad,\n",
    "                                                    targets=y_val)\n",
    "dataset_val = mbm.data_processing.SliceDatasetBinding(\n",
    "    SliceDataset(dataset_val, idx=0), SliceDataset(dataset_val, idx=1))\n",
    "print(\"validation:\", dataset_val.X.shape, dataset_val.y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train custom NN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = False\n",
    "if TRAIN:\n",
    "    custom_nn.seed_all()\n",
    "\n",
    "    print(\"Training the model...\")\n",
    "    print('Model parameters:')\n",
    "    for key, value in args.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    custom_nn.fit(dataset.X, dataset.y)\n",
    "    # The dataset provided in fit is not used as the datasets are overwritten in the provided train_split function\n",
    "\n",
    "    # Generate filename with current date\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    model_filename = f\"nn_model_{current_date}_CA\"\n",
    "\n",
    "    plot_training_history(custom_nn.history, skip_first_n=5)\n",
    "\n",
    "    # After Training: Best weights are already loaded\n",
    "    # Save the model\n",
    "    custom_nn.save_model(model_filename)\n",
    "\n",
    "    # save params dic\n",
    "    params_filename = f\"nn_params_{current_date}_CA.pkl\"\n",
    "\n",
    "    with open(f\"models/{params_filename}\", \"wb\") as f:\n",
    "        pickle.dump(args, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and set to CPU\n",
    "model_filename = \"nn_model_2025-08-26_CA.pt\"  # Replace with actual date if needed\n",
    "# read pickle with params\n",
    "params_filename = \"nn_params_2025-08-26_CA.pkl\"  # Replace with actual date if needed\n",
    "with open(f\"models/{params_filename}\", \"rb\") as f:\n",
    "    custom_params = pickle.load(f)\n",
    "\n",
    "params = custom_params\n",
    "\n",
    "args = {\n",
    "    'module': FlexibleNetwork,\n",
    "    'nbFeatures': nInp,\n",
    "    'module__input_dim': nInp,\n",
    "    'module__dropout': params['module__dropout'],\n",
    "    'module__hidden_layers': params['module__hidden_layers'],\n",
    "    'train_split': my_train_split,\n",
    "    'batch_size': params['batch_size'],\n",
    "    'verbose': 1,\n",
    "    'iterator_train__shuffle': True,\n",
    "    'lr': params['lr'],\n",
    "    'max_epochs': 300,\n",
    "    'optimizer': params['optimizer'],\n",
    "    'optimizer__weight_decay': params['optimizer__weight_decay'],\n",
    "    'module__use_batchnorm': params['module__use_batchnorm'],\n",
    "    'callbacks': [\n",
    "        ('early_stop', early_stop),\n",
    "        ('lr_scheduler', lr_scheduler_cb),\n",
    "    ]\n",
    "}\n",
    "\n",
    "loaded_model = mbm.models.CustomNeuralNetRegressor.load_model(\n",
    "    cfg,\n",
    "    model_filename,\n",
    "    **{\n",
    "        **args,\n",
    "        **param_init\n",
    "    },\n",
    ")\n",
    "loaded_model = loaded_model.set_params(device='cpu')\n",
    "loaded_model = loaded_model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_ids, scores_NN, ids_NN, y_pred_NN = evaluate_model_and_group_predictions(\n",
    "    loaded_model, df_X_test_subset, test_set['y'], cfg, months_head_pad, months_tail_pad)\n",
    "scores_annual, scores_winter = compute_seasonal_scores(grouped_ids,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "fig = plot_predictions_summary(grouped_ids=grouped_ids,\n",
    "                               scores_annual=scores_annual,\n",
    "                               scores_winter=scores_winter,\n",
    "                               predVSTruth=predVSTruth,\n",
    "                               plotMeanPred=plotMeanPred,\n",
    "                               ax_xlim=(-8, 6),\n",
    "                               ax_ylim=(-8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrapolate in space to RGI glaciers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check glaciers with missing topo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_ids = os.listdir(path_rgi_alps)\n",
    "year = 2000  # Example year, change as needed\n",
    "\n",
    "incomplete_rgis = {}\n",
    "total_area_rgi, area_incomplete_rgi = 0, 0\n",
    "for rgi_gl in tqdm(rgi_ids):\n",
    "    df = pd.read_parquet(\n",
    "        os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "\n",
    "    total_area_rgi += glacier_outline_rgi[glacier_outline_rgi.RGIId ==\n",
    "                                          rgi_gl].Area.values[0]\n",
    "\n",
    "    # check if all vois_topographical in df.columns\n",
    "    missing_vois = [voi for voi in vois_topographical if voi not in df.columns]\n",
    "    if len(missing_vois) > 0:\n",
    "        incomplete_rgis[rgi_gl] = missing_vois\n",
    "        area_incomplete_rgi += glacier_outline_rgi[glacier_outline_rgi.RGIId ==\n",
    "                                                   rgi_gl].Area.values[0]\n",
    "\n",
    "df_pos_all['incomplete_topo'] = df_pos_all['rgi_id'].apply(\n",
    "    lambda x: x in incomplete_rgis)\n",
    "\n",
    "print('Number of incomplete RGI glaciers:', len(incomplete_rgis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Total area of glaciers with missing topo: {area_incomplete_rgi:.2f} km²\")\n",
    "print(\n",
    "    f\"Total area of all glaciers in RGI region 11.6: {total_area_rgi:.2f} km²\")\n",
    "# percentage\n",
    "perc_incomplete = (area_incomplete_rgi / total_area_rgi) * 100\n",
    "print(\n",
    "    f\"Percentage of glaciers with missing topo: {perc_incomplete:.2f}% ({len(incomplete_rgis)} glaciers)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Flatten the lists into a single list\n",
    "all_vars = [var for sublist in incomplete_rgis.values() for var in sublist]\n",
    "\n",
    "# Step 2: Count occurrences\n",
    "var_counts = Counter(all_vars)\n",
    "\n",
    "# Step 3: Convert to DataFrame for easier plotting (optional)\n",
    "var_counts_df = pd.DataFrame.from_dict(var_counts,\n",
    "                                       orient='index',\n",
    "                                       columns=['count'\n",
    "                                                ]).sort_values(by='count',\n",
    "                                                               ascending=False)\n",
    "\n",
    "# Step 4: Plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "var_counts_df.plot(kind='bar', legend=False)\n",
    "plt.title('Frequency of topo variables missing in RGI glaciers')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_outline_rgi = gpd.read_file(cfg.dataPath + path_rgi_outlines)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "\n",
    "latN, latS = 48, 44\n",
    "lonW, lonE = 4, 14\n",
    "projPC = ccrs.PlateCarree()\n",
    "ax2 = plt.axes(projection=projPC)\n",
    "ax2.set_extent([lonW, lonE, latS, latN], crs=ccrs.Geodetic())\n",
    "\n",
    "ax2.add_feature(cfeature.COASTLINE)\n",
    "ax2.add_feature(cfeature.LAKES)\n",
    "ax2.add_feature(cfeature.RIVERS)\n",
    "ax2.add_feature(cfeature.BORDERS, linestyle='-', linewidth=1)\n",
    "\n",
    "g = sns.scatterplot(\n",
    "    data=df_pos_all,\n",
    "    x='lon',\n",
    "    y='lat',\n",
    "    alpha=0.6,\n",
    "    hue='incomplete_topo',\n",
    "    transform=projPC,\n",
    "    ax=ax2,\n",
    "    zorder=10,\n",
    "    legend=True  # custom legend added below\n",
    ")\n",
    "\n",
    "# glacier_outline_rgi.plot(ax=ax2, transform=projPC, color='black')\n",
    "\n",
    "gl = ax2.gridlines(draw_labels=True,\n",
    "                   linewidth=1,\n",
    "                   color='gray',\n",
    "                   alpha=0.5,\n",
    "                   linestyle='--')\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "gl.xlabel_style = {'size': 16, 'color': 'black'}\n",
    "gl.ylabel_style = {'size': 16, 'color': 'black'}\n",
    "gl.top_labels = gl.right_labels = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_gl = []\n",
    "for rgi_gl in incomplete_rgis:\n",
    "    area_gl.append(glacier_outline_rgi[glacier_outline_rgi.RGIId ==\n",
    "                                       rgi_gl].Area.values[0])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(area_gl, bins=30,\n",
    "         edgecolor='black')  # You can adjust 'bins' as needed\n",
    "plt.xlabel('Glacier Area (km²)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of glacier areas with missing topo')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrapolate (only on full glaciers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_rgis = [rgi_gl for rgi_gl in rgi_ids if rgi_gl not in incomplete_rgis]\n",
    "print('Number of glaciers with complete topo data:', len(full_rgis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "path_save_glw = cfg.dataPath + '/GLAMOS/distributed_MB_grids/MBM/central_europe/'\n",
    "path_xr_grids = os.path.join(cfg.dataPath, 'GLAMOS/topo/RGI_v6_11/',\n",
    "                             'xr_masked_grids/')\n",
    "RUN = False\n",
    "if RUN:\n",
    "    emptyfolder(path_save_glw)\n",
    "\n",
    "    # Define output CSV path after clearing\n",
    "    output_file = os.path.join(path_save_glw, \"glacier_mean_MB.csv\")\n",
    "\n",
    "    # Start with header\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"Index,RGIId,Year,Mean_MB\\n\")\n",
    "\n",
    "    index_counter = 0  # Initialize index\n",
    "\n",
    "    output_df = pd.read_csv(output_file)\n",
    "    missing_rgis = [\n",
    "        rgi_gl for rgi_gl in full_rgis\n",
    "        if rgi_gl not in output_df.RGIId.unique()\n",
    "    ]\n",
    "\n",
    "    for rgi_gl in tqdm(missing_rgis):\n",
    "        glacier_path = os.path.join(path_rgi_alps, rgi_gl)\n",
    "\n",
    "        if not os.path.exists(glacier_path):\n",
    "            print(f\"Folder not found for {rgi_gl}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        glacier_files = sorted(\n",
    "            [f for f in os.listdir(glacier_path) if rgi_gl in f])\n",
    "        years = [\n",
    "            int(file_name.split('_')[2].split('.')[0])\n",
    "            for file_name in glacier_files\n",
    "        ]\n",
    "\n",
    "        for year in years:\n",
    "            file_name = f\"{rgi_gl}_grid_{year}.parquet\"\n",
    "            file_path = os.path.join(glacier_path, file_name)\n",
    "\n",
    "            try:\n",
    "                # Load parquet file\n",
    "                df_grid_monthly = pd.read_parquet(file_path)\n",
    "                df_grid_monthly.drop_duplicates(inplace=True)\n",
    "\n",
    "                df_grid_monthly = df_grid_monthly[[\n",
    "                    col for col in all_columns\n",
    "                    if col in df_grid_monthly.columns\n",
    "                ]]\n",
    "                df_grid_monthly = df_grid_monthly.dropna()\n",
    "\n",
    "                path_glacier_dem = os.path.join(cfg.dataPath, path_xr_grids,\n",
    "                                                f\"{rgi_gl}.zarr\")\n",
    "\n",
    "                # Predict annual MB\n",
    "                pred_annual, df_pred_months_annual = loaded_model.glacier_wide_pred(\n",
    "                    df_grid_monthly[all_columns], type_pred='annual')\n",
    "                pred_y_annual = pred_annual.drop(columns=['YEAR'],\n",
    "                                                 errors='ignore')\n",
    "\n",
    "                # Compute mean MB\n",
    "                mean_MB = pred_y_annual.pred.mean()\n",
    "\n",
    "                # Write row to CSV\n",
    "                with open(output_file, 'a') as f:\n",
    "                    f.write(f\"{index_counter},{rgi_gl},{year},{mean_MB:.4f}\\n\")\n",
    "\n",
    "                index_counter += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {rgi_gl} {year}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open output file\n",
    "output_file = os.path.join(path_save_glw, \"glacier_mean_MB.csv\")\n",
    "output_file = pd.read_csv(output_file)\n",
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rgis = [\n",
    "    rgi_gl for rgi_gl in full_rgis if rgi_gl not in output_file.RGIId.unique()\n",
    "]\n",
    "missing_rgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_rgis) * len(range(2000, 2024))  # Total number of rows expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean predicted MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open output file\n",
    "output_df = os.path.join(path_save_glw, \"glacier_mean_MB.csv\")\n",
    "output_df = pd.read_csv(output_df)\n",
    "\n",
    "output_df.groupby('Year').agg({'Mean_MB': 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
