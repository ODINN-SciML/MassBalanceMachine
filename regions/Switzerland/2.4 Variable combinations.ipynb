{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import massbalancemachine as mbm\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, LineString, Point\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import GroupKFold, KFold, train_test_split, GroupShuffleSplit\n",
    "import itertools\n",
    "import cupy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "from oggm import cfg, utils, workflow, tasks\n",
    "import logging\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "from ast import literal_eval\n",
    "\n",
    "import config\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.xgb_helpers import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(config.SEED)\n",
    "\n",
    "# in case no memory\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "custom_working_dir = '../../../data/OGGM/'\n",
    "\n",
    "# Specify the short names of the climate variables available in the dataset\n",
    "vois_climate = ['t2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str']\n",
    "voi_topographical = ['aspect', 'slope', 'dis_from_border', 'topo']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "rgi_df = pd.read_csv(path_rgi, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = pd.read_csv(path_PMB_GLAMOS_csv + 'CH_wgms_dataset.csv')\n",
    "data_glamos.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glacier grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacierName = 'gries'\n",
    "rgi_gl = rgi_df.loc[glacierName]['rgi_id.v6']\n",
    "data_gl = data_glamos[data_glamos.RGIId == rgi_gl]\n",
    "dataset_gl = mbm.Dataset(data=data_gl,\n",
    "                         region_name='CH',\n",
    "                         data_path=path_PMB_GLAMOS_csv)\n",
    "\n",
    "ds, glacier_indices, gdir = dataset_gl.get_glacier_mask(custom_working_dir)\n",
    "\n",
    "# Create pandas dataframe of glacier grid\n",
    "years = data_gl['YEAR'].unique()\n",
    "df_grid_annual = dataset_gl.create_glacier_grid(custom_working_dir)\n",
    "# Add metadata that is not in WGMS dataset\n",
    "df_grid_annual[\"PERIOD\"] = \"annual\"\n",
    "df_grid_annual['GLACIER'] = glacierName\n",
    "\n",
    "# Load monthly glacier grid (preprocessed in other notebooks)\n",
    "df_grid_monthly = pd.read_csv(path_glacier_grid + f'{glacierName}_grid.csv')\n",
    "dataloader = mbm.DataLoader(data=df_grid_monthly,\n",
    "                            random_seed=config.SEED,\n",
    "                            meta_data_columns=config.META_DATA)\n",
    "print('\\nNumber of years: {}, from {} to {}'.format(len(years), years[0],\n",
    "                                                    years[-1]))\n",
    "print('\\nNumber of total (yearly) measurements:', len(df_grid_annual))\n",
    "df_grid_monthly.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot glacier attributes of oggm:\n",
    "plotGlAttr(ds, cmap=sns.color_palette(\"viridis\", as_cmap=True))\n",
    "\n",
    "# Plot glacier grid with stakes:\n",
    "plotGlGrid(df_grid_annual, data_gl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stakes data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gl = data_glamos[data_glamos.RGIId == rgi_gl]\n",
    "# change mm w.e. to m w.e.\n",
    "data_gl['POINT_BALANCE'] = data_gl['POINT_BALANCE'] / 1000\n",
    "dataset_gl = mbm.Dataset(data=data_gl,\n",
    "                         region_name='CH',\n",
    "                         data_path=path_PMB_GLAMOS_csv)\n",
    "print('Number of winter and annual samples:', len(data_gl))\n",
    "print('Number of annual samples:', len(data_gl[data_gl.PERIOD == 'annual']))\n",
    "print('Number of winter samples:', len(data_gl[data_gl.PERIOD == 'winter']))\n",
    "plotNumMeasPerYear(data_gl, glacierName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add climate features and transform to monthly format\n",
    "dataloader_gl = getMonthlyDataLoader(glacierName, vois_climate,\n",
    "                                     voi_topographical)\n",
    "\n",
    "# Get train, test and validation data\n",
    "splits, test_set, train_set = getCVSplits(dataloader_gl)\n",
    "\n",
    "print('Train years:', train_set['years'])\n",
    "print('Test years:', test_set['years'])\n",
    "\n",
    "# Plot splits\n",
    "visualiseSplits(test_set['y'], train_set['y'], splits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All combinations of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['ELEVATION_DIFFERENCE'] + voi_topographical + vois_climate\n",
    "print('Feature columns:', feature_columns)\n",
    "iterable_voi = list(powerset(vois_climate, min_length=3))\n",
    "iterable_topo = list(powerset(voi_topographical, min_length=2))\n",
    "\n",
    "# combinations for t2m and tp\n",
    "combinations_voi_topo = list(itertools.product(iterable_voi, iterable_topo))\n",
    "\n",
    "print('Number of combinations:', len(combinations_voi_topo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom XGB regressor\n",
    "custom_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 4,\n",
    "    'learning_rate': 0.01,\n",
    "    'gamma': 1\n",
    "}\n",
    "param_init = {}\n",
    "param_init['device'] = 'cuda:0'\n",
    "param_init['tree_method'] = 'hist'\n",
    "param_init[\"random_state\"] = config.SEED\n",
    "# merge two dictionaries\n",
    "params = {**custom_params, **param_init}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "glaciersToRun = ['aletsch', 'gries', 'silvretta']\n",
    "\n",
    "for glacierName in glaciersToRun:\n",
    "    # If this glacier has already been run, skip it\n",
    "    if os.path.exists(f'results/combinations_climate_topo_{glacierName}.csv'):\n",
    "        print(f'{glacierName} already run, skipping...')\n",
    "        continue\n",
    "    print('Running for', glacierName)\n",
    "\n",
    "    # Add climate features and transform to monthly format\n",
    "    dataloader_gl = getMonthlyDataLoader(glacierName, vois_climate,\n",
    "                                         voi_topographical)\n",
    "\n",
    "    # Get train, test and validation data\n",
    "    splits, test_set, train_set = getCVSplits(dataloader_gl)\n",
    "\n",
    "    dfcombi = pd.DataFrame(combinations_voi_topo, columns=['voi', 'topo'])\n",
    "    val_score = []\n",
    "    for (voi, topo) in tqdm(combinations_voi_topo, desc='Number of comb.'):\n",
    "        feature_columns = ['ELEVATION_DIFFERENCE'] + voi + topo\n",
    "\n",
    "        # Make a cross-validation split\n",
    "        splits_cv = dataloader_gl.get_cv_split(n_splits=3,\n",
    "                                               type_fold='group-meas-id')\n",
    "\n",
    "        # select those feature in the dataset\n",
    "        df_X_train_subset = train_set['df_X'][feature_columns +\n",
    "                                              config.META_DATA +\n",
    "                                              config.NOT_METADATA_NOT_FEATURES]\n",
    "        groups_subset = df_X_train_subset['ID'].values\n",
    "\n",
    "        # Fit the model\n",
    "        custom_xgboost = mbm.models.CustomXGBoostRegressor(**params)\n",
    "        custom_xgboost.fit(df_X_train_subset, train_set['y'])\n",
    "\n",
    "        # Evaluate the model with cross-validation\n",
    "        scores_cv = cross_val_score(custom_xgboost,\n",
    "                                    df_X_train_subset,\n",
    "                                    y=train_set['y'],\n",
    "                                    groups=groups_subset,\n",
    "                                    cv=splits_cv)\n",
    "        val_score.append(scores_cv.mean())\n",
    "\n",
    "    dfcombi['val_score'] = val_score\n",
    "    dfcombi.sort_values(by='val_score', ascending=False, inplace=True)\n",
    "    # Save the results\n",
    "    dfcombi.to_csv(f'results/combinations_climate_topo_{glacierName}.csv',\n",
    "                   index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcombi = pd.DataFrame()\n",
    "for glacierName in glaciersToRun:\n",
    "    dfcombi_ = pd.read_csv(\n",
    "        f'results/combinations_climate_topo_{glacierName}.csv',\n",
    "        converters={\n",
    "            \"voi\": literal_eval,\n",
    "            \"topo\": literal_eval\n",
    "        })\n",
    "    dfcombi_.rename(columns={'voi': 'climate'}, inplace=True)\n",
    "    # Give a hash to each combination of voi and topo:\n",
    "    dfcombi_['climate-topo-hash'] = [\n",
    "        makeCombNum(dfcombi_['climate'].iloc[i], dfcombi_['topo'].iloc[i])\n",
    "        for i in range(len(dfcombi_))\n",
    "    ]\n",
    "    # Make val_score positive\n",
    "    dfcombi_['val_score'] = dfcombi_['val_score'].abs()\n",
    "    dfcombi_['glacier'] = glacierName\n",
    "    dfcombi = pd.concat([dfcombi, dfcombi_])\n",
    "\n",
    "# Find 50 best combinations\n",
    "N = 50\n",
    "dfWeights_climate_all, dfWeights_topo_all = pd.DataFrame(), pd.DataFrame()\n",
    "for glacier in dfcombi.glacier.unique():\n",
    "    N_best = dfcombi[dfcombi.glacier == glacier].sort_values(\n",
    "        by='val_score')[:N]\n",
    "    mean_score = N_best['val_score'].mean()\n",
    "\n",
    "    topo_weight_map = np.zeros(len(voi_topographical))\n",
    "    climate_weight_map = np.zeros(len(vois_climate))\n",
    "\n",
    "    count_climate = N_best.explode('climate').groupby(\n",
    "        'climate').count().reset_index()\n",
    "    count_topo = N_best.explode('topo').groupby('topo').count().reset_index()\n",
    "\n",
    "    for i, var in enumerate(count_topo['topo']):\n",
    "        topo_weight_map[i] = count_topo.iloc[i]['glacier']\n",
    "\n",
    "    for i, var in enumerate(count_climate['climate']):\n",
    "        climate_weight_map[i] = count_climate.iloc[i]['glacier']\n",
    "\n",
    "    dfWeights = pd.DataFrame({\n",
    "        'weight':\n",
    "        np.concatenate([climate_weight_map, topo_weight_map], axis=0),\n",
    "        'feature_type':\n",
    "        np.concatenate([\n",
    "            np.tile('climate', len(vois_climate)),\n",
    "            np.tile('topo', len(voi_topographical))\n",
    "        ]),\n",
    "        'feature':\n",
    "        np.concatenate([vois_climate, voi_topographical])\n",
    "    })\n",
    "    dfWeights['freq_var'] = dfWeights['weight'] / 50\n",
    "    dfWeights['glacier'] = glacier\n",
    "    dfWeights['mean_score'] = mean_score\n",
    "    dfWeights_climate = dfWeights[dfWeights.feature_type ==\n",
    "                                  'climate'].sort_values(by='freq_var',\n",
    "                                                         ascending=False)\n",
    "    dfWeights_topo = dfWeights[dfWeights.feature_type == 'topo'].sort_values(\n",
    "        by='freq_var', ascending=False)\n",
    "\n",
    "    dfWeights_climate_all = pd.concat(\n",
    "        [dfWeights_climate_all, dfWeights_climate])\n",
    "    dfWeights_topo_all = pd.concat([dfWeights_topo_all, dfWeights_topo])\n",
    "dfWeights_all = pd.concat([dfWeights_climate_all, dfWeights_topo_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the weights\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot frequence of variables so that each row is a glacier and each column a type\n",
    "g = sns.FacetGrid(\n",
    "    dfWeights_all,\n",
    "    col=\"glacier\",\n",
    "    row='feature_type',\n",
    ")\n",
    "g.map(sns.barplot, \"feature\", \"freq_var\", orient='v', alpha=0.5)\n",
    "\n",
    "for col_val, ax in g.axes_dict.items():\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.tick_params(axis=\"x\", rotation=90)\n",
    "    ax.set_title(col_val)\n",
    "    mean_score = dfWeights_all[dfWeights_all.glacier ==\n",
    "                               col_val[1]]['mean_score'].iloc[0]\n",
    "    legend_text = \"\\n\".join((r\"$\\mathrm{val\\ MSE}=%.1f$\" % (mean_score, ), ))\n",
    "    ax.text(\n",
    "        0.05,\n",
    "        0.88,\n",
    "        legend_text,\n",
    "        transform=ax.transAxes,\n",
    "        verticalalignment=\"bottom\",\n",
    "        fontsize=16,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWeights_all[(dfWeights_all.feature_type == 'topo')\n",
    "              & (dfWeights_all.glacier == 'gries') &\n",
    "              (dfWeights_all.freq_var > 0.5)].feature.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test best combination for a glacier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacierName = 'silvretta'\n",
    "\n",
    "# Add climate features and transform to monthly format\n",
    "dataloader_gl = getMonthlyDataLoader(glacierName, vois_climate,\n",
    "                                     voi_topographical)\n",
    "\n",
    "# Get train, test and validation data\n",
    "splits, test_set, train_set = getCVSplits(dataloader_gl)\n",
    "\n",
    "voi = dfWeights_all[(dfWeights_all.feature_type == 'climate')\n",
    "                    & (dfWeights_all.glacier == glacierName) &\n",
    "                    (dfWeights_all.freq_var > 0.5)].feature.values\n",
    "topo = dfWeights_all[(dfWeights_all.feature_type == 'topo')\n",
    "                     & (dfWeights_all.glacier == glacierName) &\n",
    "                     (dfWeights_all.freq_var > 0.5)].feature.values\n",
    "feature_columns = ['ELEVATION_DIFFERENCE'] + list(voi) + list(topo)\n",
    "all_columns = feature_columns + config.META_DATA + config.NOT_METADATA_NOT_FEATURES\n",
    "\n",
    "# Grid search\n",
    "# For each of the XGBoost parameter, define the grid range\n",
    "parameters = {\n",
    "    'max_depth': [\n",
    "        3,\n",
    "        4,\n",
    "        5,\n",
    "        6,\n",
    "    ],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'gamma': [0, 1]\n",
    "}\n",
    "\n",
    "param_init = {}\n",
    "param_init['device'] = 'cuda:0'\n",
    "param_init['tree_method'] = 'hist'\n",
    "param_init[\"random_state\"] = config.SEED\n",
    "\n",
    "# custom variables\n",
    "df_X_train_subset = train_set['df_X'][all_columns]\n",
    "print('Shape of the dataset:', df_X_train_subset.shape)\n",
    "\n",
    "# Create a CustomXGBoostRegressor instance\n",
    "custom_xgboost = mbm.models.CustomXGBoostRegressor(**param_init)\n",
    "custom_xgboost.randomsearch(\n",
    "    parameters=parameters,\n",
    "    n_iter=20,\n",
    "    splits=splits,\n",
    "    features=df_X_train_subset,\n",
    "    targets=train_set['y'],\n",
    "    num_jobs=-1,\n",
    "    random_seed=config.SEED,\n",
    ")\n",
    "\n",
    "best_params = params = custom_xgboost.param_search.best_params_\n",
    "best_estimator = custom_xgboost.param_search.best_estimator_\n",
    "print(\"Best parameters:\\n\", best_params)\n",
    "print(\"Best score:\\n\", custom_xgboost.param_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to CPU for predictions:\n",
    "xgb = best_estimator.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on test\n",
    "features_test, metadata_test = xgb._create_features_metadata(\n",
    "    test_set['df_X'][all_columns], config.META_DATA)\n",
    "y_pred = xgb.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = xgb.aggrPredict(metadata_test, config.META_DATA, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = xgb.score(test_set['df_X'][all_columns],\n",
    "                  test_set['y'])  # negative\n",
    "mse, rmse, mae, pearson_corr = xgb.evalMetrics(metadata_test, y_pred,\n",
    "                                               test_set['y'])\n",
    "\n",
    "# Aggregate predictions to annual or winter:\n",
    "df_pred = test_set['df_X'][all_columns].copy()\n",
    "df_pred['target'] = test_set['y']\n",
    "grouped_ids = df_pred.groupby('ID').agg({'target': 'mean', 'YEAR': 'first'})\n",
    "grouped_ids['pred'] = y_pred_agg\n",
    "grouped_ids['PERIOD'] = test_set['df_X'][\n",
    "    feature_columns + config.META_DATA +\n",
    "    config.NOT_METADATA_NOT_FEATURES].groupby('ID')['PERIOD'].first()\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "predVSTruth(ax, grouped_ids, mae, rmse, pearson_corr)\n",
    "ax.set_title('Target vs prediction')\n",
    "\n",
    "mean = grouped_ids.groupby('YEAR')['target'].mean().values\n",
    "std = grouped_ids.groupby('YEAR')['target'].std().values\n",
    "years = grouped_ids.YEAR.unique()\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.fill_between(\n",
    "    years,\n",
    "    mean - std,\n",
    "    mean + std,\n",
    "    color=\"orange\",\n",
    "    alpha=0.3,\n",
    ")\n",
    "ax.plot(years, mean, color=\"orange\", label=\"mean target\")\n",
    "ax.plot(years,\n",
    "        grouped_ids.groupby('YEAR')['pred'].mean().values,\n",
    "        color=\"blue\",\n",
    "        label=\"mean pred\",\n",
    "        linestyle='--')\n",
    "ax.legend()\n",
    "ax.set_title('Mean yearly target and prediction')\n",
    "plt.suptitle(f'XGBoost on {glacierName.title()} (split years)', fontsize=20)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FI = best_estimator.feature_importances_\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "feature_importdf = pd.DataFrame(data={\n",
    "    \"variables\": feature_columns,\n",
    "    \"feat_imp\": FI\n",
    "})\n",
    "\n",
    "feature_importdf['variables'] = feature_importdf['variables'].apply(\n",
    "    lambda x: vois_climate_long_name[x] + f' ({x})'\n",
    "    if x in vois_climate else x)\n",
    "\n",
    "feature_importdf.sort_values(by=\"feat_imp\", ascending=True, inplace=True)\n",
    "# feature_importdf = feature_importdf[feature_importdf.feat_imp > 0.02]\n",
    "sns.barplot(feature_importdf, x='feat_imp', y='variables', dodge=False, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Whole grid:\n",
    "# Make predictions:\n",
    "df_grid_monthly = pd.read_csv(path_glacier_grid + f'{glacierName}_grid.csv')\n",
    "# Select only the subset of features\n",
    "df_grid_monthly = df_grid_monthly[all_columns]\n",
    "\n",
    "# Set to CPU for predictions:\n",
    "xgb = xgb.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on whole glacier grid\n",
    "features_grid, metadata_grid = xgb._create_features_metadata(\n",
    "    df_grid_monthly, config.META_DATA)\n",
    "print('Shape of the dataset:', features_grid.shape)\n",
    "y_pred_grid = xgb.predict(features_grid)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_grid_agg = xgb.aggrPredict(metadata_grid, config.META_DATA,\n",
    "                                  features_grid)\n",
    "\n",
    "# Aggregate predictions to annual or winter:\n",
    "grouped_ids = df_grid_monthly.groupby('ID').agg({'YEAR': 'mean'})\n",
    "grouped_ids['pred'] = y_pred_grid_agg\n",
    "\n",
    "# Sum over all points of a glacier to get glacier wide SMB\n",
    "grouped_ids = grouped_ids.groupby('YEAR').mean()\n",
    "\n",
    "df_target = pd.read_csv(path_SMB_GLAMOS_csv + 'fix/' +\n",
    "                        f'{glacierName}_fix.csv')\n",
    "df_target = transformDates(df_target)\n",
    "# Remove obvious duplicates:\n",
    "df_target = df_target.drop_duplicates()\n",
    "df_target['YEAR'] = df_target['date1'].apply(lambda x: pd.to_datetime(x).year)\n",
    "df_target['Annual Balance'] = df_target['Annual Balance'] / (1000)\n",
    "df_target = df_target[['YEAR', 'Annual Balance']].set_index('YEAR')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
    "grouped_ids.plot(y='pred', label='Predicted SMB', ax=ax, color=color_xgb)\n",
    "\n",
    "df_target[df_target.index>1960].plot(y='Annual Balance', label='GLAMOS SMB', ax=ax, color=color_tim)\n",
    "\n",
    "ax.set_title(f'{glacierName.title()} SMB')\n",
    "ax.set_ylabel('SMB (m w.e.)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to GeoB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodetic_csv = pd.read_csv(\n",
    "    '../../../data/GLAMOS/glacier-wide/volumechange_2023_r2023/volumechange_2023_r2023_old.csv',\n",
    "    sep=';')\n",
    "geodetic_csv = geodetic_csv.iloc[2:]  # remove unit rows\n",
    "geodetic_csv['glacier id'] = geodetic_csv['glacier id'].apply(\n",
    "    lambda x: x.split('-')[0].upper().strip() + '/' + x.split('-')[1].strip())\n",
    "geodetic_csv.rename(columns={\n",
    "    'glacier id': 'sgi-id',\n",
    "    'start date of observation': 'FROM_DATE',\n",
    "    'end date of observation': 'TO_DATE',\n",
    "    'annual geodetic mass balance': 'Bgeod'\n",
    "},\n",
    "                    inplace=True)\n",
    "geodetic_csv.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgi_id = rgi_df.loc[glacierName]['sgi-id'].strip()\n",
    "gl_geoMB = geodetic_csv[geodetic_csv['sgi-id'] == sgi_id]\n",
    "\n",
    "# assign hydr. year\n",
    "def assignHydrYear(date):\n",
    "    date = pd.to_datetime(date)\n",
    "    return date.year\n",
    "\n",
    "gl_geoMB['FROM_YEAR'] = pd.to_datetime(\n",
    "    gl_geoMB['FROM_DATE']).apply(assignHydrYear) + 1\n",
    "gl_geoMB['TO_YEAR'] = pd.to_datetime(\n",
    "    gl_geoMB['TO_DATE']).apply(assignHydrYear)\n",
    "gl_geoMB = gl_geoMB[gl_geoMB['FROM_YEAR'] > 1961]\n",
    "gl_geoMB['B-Period'] = gl_geoMB['FROM_YEAR'].astype(\n",
    "    str) + '-' + gl_geoMB['TO_YEAR'].astype(str)\n",
    "gl_geoMB['Bgeod'] = gl_geoMB['Bgeod'].astype(float)\n",
    "gl_geoMB['volume change'] = gl_geoMB['volume change'].astype(\n",
    "    float)\n",
    "\n",
    "geodPred_ML, geodPred_TIM = [], []\n",
    "for i, row in gl_geoMB.iterrows():\n",
    "    geodPred_ML.append(\n",
    "        grouped_ids.loc[row.FROM_YEAR:row.TO_YEAR].mean().values[0])\n",
    "    geodPred_TIM.append(\n",
    "        df_target.loc[row.FROM_YEAR:row.TO_YEAR].mean().values[0])\n",
    "\n",
    "geodPred_df = pd.DataFrame({\n",
    "    'Bgeod':\n",
    "    np.concatenate(\n",
    "        [gl_geoMB['Bgeod'].values, geodPred_ML, geodPred_TIM]),\n",
    "    'Type':\n",
    "    np.concatenate([\n",
    "        np.tile('Bgeod', len(gl_geoMB)),\n",
    "        np.tile('ML', len(gl_geoMB)),\n",
    "        np.tile('PDD', len(gl_geoMB))\n",
    "    ]),\n",
    "    'Period':\n",
    "    np.concatenate([\n",
    "        gl_geoMB['B-Period'], gl_geoMB['B-Period'],\n",
    "        gl_geoMB['B-Period']\n",
    "    ])\n",
    "})\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "sns.barplot(geodPred_df,\n",
    "            x='Period',\n",
    "            y='Bgeod',\n",
    "            hue='Type',\n",
    "            ax=ax,\n",
    "            orient='v',\n",
    "            alpha=0.5,\n",
    "            palette = sns.color_palette([\"green\", color_xgb, color_tim]))\n",
    "plt.tight_layout()\n",
    "ax.set_title(glacierName.title() + ' GeoMB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
