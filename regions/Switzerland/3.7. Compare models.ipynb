{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import joypy\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from skorch.helper import SliceDataset\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.geodata_plots import *\n",
    "from scripts.NN_networks import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'\n",
    "\n",
    "# Glacier outlines:\n",
    "glacier_outline_sgi = gpd.read_file(\n",
    "    os.path.join(cfg.dataPath, path_SGI_topo, 'inventory_sgi2016_r2020',\n",
    "                 'SGI_2016_glaciers_copy.shp'))  # Load the shapefile\n",
    "glacier_outline_rgi = gpd.read_file(cfg.dataPath + path_rgi_outlines)\n",
    "\n",
    "test_glaciers = [\n",
    "    'tortin', 'plattalva', 'sanktanna', 'schwarzberg', 'hohlaub', 'pizol',\n",
    "    'corvatsch', 'tsanfleuron', 'forno'\n",
    "]\n",
    "\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "print('-------------------')\n",
    "print('Number of glaciers:', len(data_glamos['GLACIER'].unique()))\n",
    "print('Number of winter and annual samples:', len(data_glamos))\n",
    "print('Number of annual samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'winter']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of measurements per glacier:\n",
    "glacier_info = data_glamos.groupby('GLACIER').size().sort_values(\n",
    "    ascending=False).reset_index()\n",
    "glacier_info.rename(columns={0: 'Nb. measurements'}, inplace=True)\n",
    "glacier_info.set_index('GLACIER', inplace=True)\n",
    "\n",
    "glacier_loc = data_glamos.groupby('GLACIER')[['POINT_LAT', 'POINT_LON']].mean()\n",
    "\n",
    "glacier_info = glacier_loc.merge(glacier_info, on='GLACIER')\n",
    "\n",
    "glacier_period = data_glamos.groupby(['GLACIER', 'PERIOD'\n",
    "                                      ]).size().unstack().fillna(0).astype(int)\n",
    "\n",
    "glacier_info = glacier_info.merge(glacier_period, on='GLACIER')\n",
    "\n",
    "glacier_info['Train/Test glacier'] = glacier_info.apply(\n",
    "    lambda x: 'Test' if x.name in test_glaciers else 'Train', axis=1)\n",
    "glacier_info.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign glaciers to river basin names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load RGI glacier IDs ===\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids)\n",
    "rgi_df.columns = rgi_df.columns.str.strip()\n",
    "rgi_df = rgi_df.sort_values(by='short_name').set_index('short_name')\n",
    "\n",
    "# === Load SGI region geometries ===\n",
    "SGI_regions = gpd.read_file(\n",
    "    os.path.join(cfg.dataPath, path_SGI_topo, 'inventory_sgi2016_r2020',\n",
    "                 'sgi_regions.geojson'))\n",
    "\n",
    "# Clean object columns\n",
    "SGI_regions[SGI_regions.select_dtypes(include='object').columns] = \\\n",
    "    SGI_regions.select_dtypes(include='object').apply(lambda col: col.str.strip())\n",
    "\n",
    "SGI_regions = SGI_regions.drop_duplicates().dropna()\n",
    "SGI_regions = SGI_regions.set_index('pk_sgi_region')\n",
    "\n",
    "# === Map to Level 0 river basins ===\n",
    "catchment_lv0 = {\n",
    "    'A': 'Rhine',\n",
    "    'B': 'Rhone',\n",
    "    'C': 'Po',\n",
    "    'D': 'Adige',\n",
    "    'E': 'Danube'\n",
    "}\n",
    "rgi_df['rvr_lv0'] = rgi_df['sgi-id'].str[0].map(catchment_lv0)\n",
    "\n",
    "\n",
    "# === Map to Level 1 river basins using SGI regions ===\n",
    "def get_river_basin(sgi_id):\n",
    "    key = sgi_id.split('-')[0]\n",
    "    if key not in SGI_regions.index:\n",
    "        return None\n",
    "    basin = SGI_regions.loc[key, 'river_basin_name']\n",
    "    if isinstance(basin, pd.Series):\n",
    "        return basin.dropna().unique()[0] if not basin.dropna().empty else None\n",
    "    return basin if pd.notna(basin) else None\n",
    "\n",
    "\n",
    "rgi_df['rvr_lv1'] = rgi_df['sgi-id'].apply(get_river_basin)\n",
    "\n",
    "# Final formatting\n",
    "rgi_df = rgi_df.reset_index().rename(columns={\n",
    "    'short_name': 'GLACIER'\n",
    "}).set_index('GLACIER')\n",
    "\n",
    "glacier_info = glacier_info.merge(rgi_df[['rvr_lv0', 'rvr_lv1']],\n",
    "                                  on='GLACIER',\n",
    "                                  how='left')\n",
    "glacier_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_NN.csv')\n",
    "data_monthly = dataloader_gl.data\n",
    "\n",
    "data_monthly['GLWD_ID'] = data_monthly.apply(\n",
    "    lambda x: mbm.data_processing.utils.get_hash(f\"{x.GLACIER}_{x.YEAR}\"),\n",
    "    axis=1)\n",
    "data_monthly['GLWD_ID'] = data_monthly['GLWD_ID'].astype(str)\n",
    "\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "dataloader = mbm.dataloader.DataLoader(cfg, data=data_train)\n",
    "\n",
    "train_itr, val_itr = dataloader.set_train_test_split(test_size=0.2)\n",
    "\n",
    "# Get all indices of the training and valing dataset at once from the iterators. Once called, the iterators are empty.\n",
    "train_indices, val_indices = list(train_itr), list(val_itr)\n",
    "\n",
    "df_X_train = data_train.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Get val set\n",
    "df_X_val = data_train.iloc[val_indices]\n",
    "y_val = df_X_val['POINT_BALANCE'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_topo = [\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "    'pcsr',\n",
    "] + list(vois_topographical)\n",
    "\n",
    "feature_columns = features_topo + list(vois_climate)\n",
    "\n",
    "cfg.setFeatures(feature_columns)\n",
    "\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "# Because CH has some extra columns, we need to cut those\n",
    "df_X_train_subset = df_X_train[all_columns]\n",
    "df_X_val_subset = df_X_val[all_columns]\n",
    "df_X_test_subset = test_set['df_X'][all_columns]\n",
    "\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of validation dataset:', df_X_val_subset.shape)\n",
    "print('Shape of testing dataset:', df_X_test_subset.shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "assert all(train_set['df_X'].POINT_BALANCE == train_set['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models:\n",
    "### XGBoost model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_init_xgb = {\n",
    "    # 'device': 'cuda:0',\n",
    "    'device': 'cpu',\n",
    "    'tree_method': 'hist',\n",
    "    \"random_state\": cfg.seed,\n",
    "    \"n_jobs\": cfg.numJobs\n",
    "}\n",
    "\n",
    "custom_params = {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 800}\n",
    "\n",
    "params = {**param_init_xgb, **custom_params}\n",
    "print(params)\n",
    "custom_xgb_model = mbm.models.CustomXGBoostRegressor(cfg, **params)\n",
    "\n",
    "# Fit on train data:\n",
    "custom_xgb_model.fit(df_X_train_subset, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test\n",
    "custom_xgb_model = custom_xgb_model.set_params(device='cpu')\n",
    "features_test, metadata_test = custom_xgb_model._create_features_metadata(\n",
    "    test_set['df_X'][all_columns])\n",
    "y_pred = custom_xgb_model.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = custom_xgb_model.aggrPredict(metadata_test, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = custom_xgb_model.score(test_set['df_X'][all_columns],\n",
    "                               test_set['y'])  # negative\n",
    "print('Overall score:', np.abs(score))\n",
    "\n",
    "grouped_ids_xgb = getDfAggregatePred(test_set, y_pred_agg, all_columns)\n",
    "PlotPredictions(grouped_ids_xgb, y_pred, metadata_test, test_set,\n",
    "                custom_xgb_model)\n",
    "plt.suptitle(f'MBM tested on {test_glaciers}', fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_xgb = pd.DataFrame(metadata_test, columns=cfg.metaData)\n",
    "df_pred_xgb['pred'] = y_pred\n",
    "\n",
    "pred_per_id = pd.DataFrame(df_pred_xgb.groupby('ID').pred.unique())\n",
    "pred_per_id['MONTHS'] = df_pred_xgb.groupby('ID').MONTHS.unique()\n",
    "pred_per_id.reset_index(inplace=True)\n",
    "\n",
    "# df_pred_months_annual = df_pred_months[df_pred_months['PERIOD'] == 'annual']\n",
    "months_extended = [\n",
    "    'sep', 'oct', 'nov', 'dec', 'jan', 'feb', 'mar', 'apr', 'may', 'jun',\n",
    "    'jul', 'aug', 'sep_', 'oct_'\n",
    "]\n",
    "\n",
    "df_months_xgb = pd.DataFrame(columns=months_extended)\n",
    "\n",
    "for i, row in pred_per_id.iterrows():\n",
    "    dic = {}\n",
    "    for i, month in enumerate(row.MONTHS):\n",
    "        if month in dic.keys():\n",
    "            month = month + '_'\n",
    "        dic[month] = row.pred[i]\n",
    "\n",
    "    # add missing months from months extended\n",
    "    for month in months_extended:\n",
    "        if month not in dic.keys():\n",
    "            dic[month] = np.nan\n",
    "\n",
    "    df_months_xgb = pd.concat(\n",
    "        [df_months_xgb, pd.DataFrame([dic])], ignore_index=True)\n",
    "df_months_xgb = df_months_xgb.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and set to CPU\n",
    "model_filename = \"nn_model_2025-06-06.pt\"  # Replace with actual date if needed\n",
    "\n",
    "features_topo = [\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "    'pcsr',\n",
    "] + list(vois_topographical)\n",
    "\n",
    "feature_columns = features_topo + list(vois_climate)\n",
    "\n",
    "cfg.setFeatures(feature_columns)\n",
    "\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='valid_loss',\n",
    "    patience=10,\n",
    "    threshold=1e-4,  # Optional: stop only when improvement is very small\n",
    ")\n",
    "\n",
    "lr_scheduler_cb = LRScheduler(policy=ReduceLROnPlateau,\n",
    "                              monitor='valid_loss',\n",
    "                              mode='min',\n",
    "                              factor=0.5,\n",
    "                              patience=5,\n",
    "                              threshold=0.01,\n",
    "                              threshold_mode='rel',\n",
    "                              verbose=True)\n",
    "\n",
    "custom_params = {\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 128,\n",
    "    'module__layer0': 64,\n",
    "    'module__layer1': 64,\n",
    "    'module__layer2': 32,\n",
    "    'module__layer3': 32,\n",
    "    'module__dropout': 0.2,\n",
    "    'optimizer': torch.optim.Adam\n",
    "}\n",
    "\n",
    "params = custom_params\n",
    "param_init_NN = {'device': 'cpu'}  # Use CPU for training\n",
    "nInp = len(feature_columns)\n",
    "\n",
    "dataset = dataset_val = None  # Initialized hereafter\n",
    "\n",
    "\n",
    "def my_train_split(ds, y=None, **fit_params):\n",
    "    return dataset, dataset_val\n",
    "\n",
    "\n",
    "args_NN = {\n",
    "    'module': BiggerNetwork,\n",
    "    'nbFeatures': nInp,\n",
    "    'module__input_dim': nInp,\n",
    "    'module__dropout': params['module__dropout'],\n",
    "    'module__layer0': params['module__layer0'],\n",
    "    'module__layer1': params['module__layer1'],\n",
    "    'module__layer2': params['module__layer2'],\n",
    "    'module__layer3': params['module__layer3'],\n",
    "    'train_split': my_train_split,\n",
    "    'batch_size': params['batch_size'],\n",
    "    'verbose': 1,\n",
    "    'iterator_train__shuffle': True,\n",
    "    'lr': params['lr'],\n",
    "    'max_epochs': 300,\n",
    "    'optimizer': params['optimizer'],\n",
    "    'callbacks': [\n",
    "        ('early_stop', early_stop),\n",
    "        ('lr_scheduler', lr_scheduler_cb),\n",
    "    ]\n",
    "}\n",
    "\n",
    "custom_NN_model = mbm.models.CustomNeuralNetRegressor.load_model(\n",
    "    cfg,\n",
    "    model_filename,\n",
    "    **{\n",
    "        **args_NN,\n",
    "        **param_init_NN\n",
    "    },\n",
    ")\n",
    "custom_NN_model = custom_NN_model.set_params(device='cpu')\n",
    "custom_NN_model = custom_NN_model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features and metadata\n",
    "features_test, metadata_test = custom_NN_model._create_features_metadata(\n",
    "    df_X_test_subset)\n",
    "\n",
    "# Ensure all tensors are on CPU if they are torch tensors\n",
    "if hasattr(features_test, 'cpu'):\n",
    "    features_test = features_test.cpu()\n",
    "\n",
    "# Ensure targets are also on CPU\n",
    "targets_test = test_set['y']\n",
    "if hasattr(targets_test, 'cpu'):\n",
    "    targets_test = targets_test.cpu()\n",
    "\n",
    "# Create the dataset\n",
    "dataset_test = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                     features=features_test,\n",
    "                                                     metadata=metadata_test,\n",
    "                                                     targets=targets_test)\n",
    "\n",
    "dataset_test = [\n",
    "    SliceDataset(dataset_test, idx=0),\n",
    "    SliceDataset(dataset_test, idx=1)\n",
    "]\n",
    "\n",
    "# Make predictions aggr to meas ID\n",
    "y_pred = custom_NN_model.predict(dataset_test[0])\n",
    "y_pred_agg = custom_NN_model.aggrPredict(dataset_test[0])\n",
    "\n",
    "batchIndex = np.arange(len(y_pred_agg))\n",
    "y_true = np.array([e for e in dataset_test[1][batchIndex]])\n",
    "\n",
    "# Calculate scores\n",
    "score = custom_NN_model.score(dataset_test[0], dataset_test[1])\n",
    "mse, rmse, mae, pearson = custom_NN_model.evalMetrics(y_pred, y_true)\n",
    "\n",
    "# Aggregate predictions\n",
    "id = dataset_test[0].dataset.indexToId(batchIndex)\n",
    "data = {\n",
    "    'target': [e[0] for e in dataset_test[1]],\n",
    "    'ID': id,\n",
    "    'pred': y_pred_agg\n",
    "}\n",
    "grouped_ids_NN = pd.DataFrame(data)\n",
    "\n",
    "# Add period\n",
    "periods_per_ids = df_X_test_subset.groupby('ID')['PERIOD'].first()\n",
    "grouped_ids_NN = grouped_ids_NN.merge(periods_per_ids, on='ID')\n",
    "\n",
    "# Add glacier name\n",
    "glacier_per_ids = df_X_test_subset.groupby('ID')['GLACIER'].first()\n",
    "grouped_ids_NN = grouped_ids_NN.merge(glacier_per_ids, on='ID')\n",
    "\n",
    "# Add YEAR\n",
    "years_per_ids = df_X_test_subset.groupby('ID')['YEAR'].first()\n",
    "grouped_ids_NN = grouped_ids_NN.merge(years_per_ids, on='ID')\n",
    "\n",
    "# Add MONTHS\n",
    "months_per_id = test_set['df_X'][all_columns].groupby('ID')['MONTHS'].unique()\n",
    "grouped_ids_NN = grouped_ids_NN.merge(months_per_id, on='ID')\n",
    "\n",
    "grouped_ids_NN.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPredictions_NN(grouped_ids_NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_months = pd.DataFrame(y_pred)\n",
    "df_pred_months['ID'] = id\n",
    "df_pred_months['MONTHS'] = grouped_ids_NN['MONTHS']\n",
    "df_pred_months['PERIOD'] = grouped_ids_NN['PERIOD']\n",
    "\n",
    "months_extended = [\n",
    "    'sep', 'oct', 'nov', 'dec', 'jan', 'feb', 'mar', 'apr', 'may', 'jun',\n",
    "    'jul', 'aug', 'sep_', 'oct_'\n",
    "]\n",
    "\n",
    "df_months_nn = pd.DataFrame(columns=months_extended)\n",
    "\n",
    "for i, row in df_pred_months.iterrows():\n",
    "    dic = {}\n",
    "    for i, month in enumerate(row.MONTHS):\n",
    "        if month in dic.keys():\n",
    "            month = month + '_'\n",
    "        dic[month] = row[i]\n",
    "\n",
    "    # add missing months from months extended\n",
    "    for month in months_extended:\n",
    "        if month not in dic.keys():\n",
    "            dic[month] = np.nan\n",
    "    df_months_nn = pd.concat([df_months_nn, pd.DataFrame([dic])],\n",
    "                             ignore_index=True)\n",
    "\n",
    "df_months_nn = df_months_nn.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_nn, array_xgb, months = [], [], []\n",
    "\n",
    "month_order = [\n",
    "    'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct',\n",
    "    'nov', 'dec'\n",
    "]\n",
    "cat_month = CategoricalDtype(month_order, ordered=True)\n",
    "\n",
    "df_months_xgb = df_months_xgb[month_order]\n",
    "df_months_nn = df_months_nn[month_order]\n",
    "\n",
    "for col in df_months_nn.columns:\n",
    "    array_nn.append(df_months_nn[col].values)\n",
    "    array_xgb.append(df_months_xgb[col].values)\n",
    "    months.append(np.tile(col, len(df_months_nn[col])))\n",
    "\n",
    "df_months_nn_long = pd.DataFrame(\n",
    "    data={\n",
    "        'mb_nn': np.concatenate(np.array(array_nn)),\n",
    "        'mb_xgb': np.concatenate(np.array(array_xgb)),\n",
    "        'Month': np.concatenate(np.array(months))\n",
    "    })\n",
    "\n",
    "# order df_months_nn_long\n",
    "df_months_nn_long['Month'] = df_months_nn_long['Month'].astype(cat_month)\n",
    "\n",
    "model_colors = [color_xgb, color_tim]\n",
    "alpha = 1\n",
    "\n",
    "cm = 1 / 2.54\n",
    "ax, fig = joypy.joyplot(df_months_nn_long,\n",
    "                        by='Month',\n",
    "                        column=['mb_xgb', 'mb_nn'],\n",
    "                        alpha=0.8,\n",
    "                        overlap=0,\n",
    "                        fill=False,\n",
    "                        linewidth=1.5,\n",
    "                        xlabelsize=8.5,\n",
    "                        ylabelsize=8.5,\n",
    "                        x_range=[-2.2, 2.2],\n",
    "                        grid=False,\n",
    "                        color=model_colors,\n",
    "                        figsize=(12 * cm, 14 * cm),\n",
    "                        ylim='own')\n",
    "\n",
    "vline_alpha = 0.5\n",
    "plt.axvline(x=0, color='grey', alpha=vline_alpha, linewidth=1)\n",
    "\n",
    "plt.xlabel('Mass balance (m w.e.)', fontsize=8.5)\n",
    "plt.yticks(ticks=range(1, 13), labels=month_order, fontsize=8.5)\n",
    "plt.gca().set_yticklabels(month_order)\n",
    "\n",
    "legend_patches = [\n",
    "    Patch(facecolor=color, label=model, alpha=alpha, edgecolor='k')\n",
    "    for model, color in zip(\n",
    "        ['XGB', 'NN'], model_colors)\n",
    "]\n",
    "plt.legend(handles=legend_patches,\n",
    "           loc='upper center',\n",
    "           bbox_to_anchor=(0.48, -0.1),\n",
    "           ncol=4,\n",
    "           fontsize=8.5,\n",
    "           handletextpad=0.5,\n",
    "           columnspacing=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter on test glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "grouped_ids_annual_NN = grouped_ids_NN[grouped_ids_NN.PERIOD == 'annual']\n",
    "y_true_mean_NN = grouped_ids_annual_NN['target']\n",
    "y_pred_agg_NN = grouped_ids_annual_NN['pred']\n",
    "scores_annual_NN = {\n",
    "    'mse': mean_squared_error(y_true_mean_NN, y_pred_agg_NN),\n",
    "    'rmse': root_mean_squared_error(y_true_mean_NN, y_pred_agg_NN),\n",
    "    'mae': mean_absolute_error(y_true_mean_NN, y_pred_agg_NN),\n",
    "    'pearson_corr': np.corrcoef(y_true_mean_NN, y_pred_agg_NN)[0, 1]\n",
    "}\n",
    "\n",
    "grouped_ids_annual_xgb = grouped_ids_xgb[grouped_ids_xgb.PERIOD == 'annual']\n",
    "y_true_mean_xgb = grouped_ids_annual_xgb['target']\n",
    "y_pred_agg_xgb = grouped_ids_annual_xgb['pred']\n",
    "scores_annual_xgb = {\n",
    "    'mse': mean_squared_error(y_true_mean_xgb, y_pred_agg_xgb),\n",
    "    'rmse': root_mean_squared_error(y_true_mean_xgb, y_pred_agg_xgb),\n",
    "    'mae': mean_absolute_error(y_true_mean_xgb, y_pred_agg_xgb),\n",
    "    'pearson_corr': np.corrcoef(y_true_mean_xgb, y_pred_agg_xgb)[0, 1]\n",
    "}\n",
    "\n",
    "# Winter\n",
    "grouped_ids_winter_NN = grouped_ids_NN[grouped_ids_NN.PERIOD == 'winter']\n",
    "y_true_mean_NN_w = grouped_ids_winter_NN['target']\n",
    "y_pred_agg_NN_w = grouped_ids_winter_NN['pred']\n",
    "scores_winter_NN = {\n",
    "    'mse': mean_squared_error(y_true_mean_NN_w, y_pred_agg_NN_w),\n",
    "    'rmse': root_mean_squared_error(y_true_mean_NN_w, y_pred_agg_NN_w),\n",
    "    'mae': mean_absolute_error(y_true_mean_NN_w, y_pred_agg_NN_w),\n",
    "    'pearson_corr': np.corrcoef(y_true_mean_NN_w, y_pred_agg_NN_w)[0, 1]\n",
    "}\n",
    "\n",
    "grouped_ids_winter_xgb = grouped_ids_xgb[grouped_ids_xgb.PERIOD == 'winter']\n",
    "y_true_mean_xgb_w = grouped_ids_winter_xgb['target']\n",
    "y_pred_agg_xgb_w = grouped_ids_winter_xgb['pred']\n",
    "scores_winter_xgb = {\n",
    "    'mse': mean_squared_error(y_true_mean_xgb_w, y_pred_agg_xgb_w),\n",
    "    'rmse': root_mean_squared_error(y_true_mean_xgb_w, y_pred_agg_xgb_w),\n",
    "    'mae': mean_absolute_error(y_true_mean_xgb_w, y_pred_agg_xgb_w),\n",
    "    'pearson_corr': np.corrcoef(y_true_mean_xgb_w, y_pred_agg_xgb_w)[0, 1]\n",
    "}\n",
    "\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "ax1.set_title('XGB predictions', fontsize=20)\n",
    "predVSTruth(ax1,\n",
    "            grouped_ids_xgb,\n",
    "            scores_annual_xgb,\n",
    "            hue='PERIOD',\n",
    "            add_legend=False,\n",
    "            palette=[color_dark_blue, color_pink])\n",
    "\n",
    "legend_xgb = \"\\n\".join(\n",
    "    ((r\"$\\mathrm{RMSE_a}=%.3f$, $\\mathrm{RMSE_w}=%.3f$,\" %\n",
    "      (scores_annual_xgb[\"rmse\"], scores_winter_xgb[\"rmse\"])),\n",
    "     (r\"$\\mathrm{\\rho_a}=%.3f$, $\\mathrm{\\rho_w}=%.3f$\" %\n",
    "      (scores_annual_xgb[\"pearson_corr\"], scores_winter_xgb[\"pearson_corr\"]))))\n",
    "ax1.text(0.03,\n",
    "         0.98,\n",
    "         legend_xgb,\n",
    "         transform=ax1.transAxes,\n",
    "         verticalalignment=\"top\",\n",
    "         fontsize=20,\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "ax2.set_title('NN predictions', fontsize=20)\n",
    "predVSTruth(ax2,\n",
    "            grouped_ids_NN,\n",
    "            scores_annual_NN,\n",
    "            hue='PERIOD',\n",
    "            add_legend=False,\n",
    "            palette=[color_dark_blue, color_pink])\n",
    "\n",
    "legend_NN = \"\\n\".join(\n",
    "    ((r\"$\\mathrm{RMSE_a}=%.3f$, $\\mathrm{RMSE_w}=%.3f$,\" %\n",
    "      (scores_annual_NN[\"rmse\"], scores_winter_NN[\"rmse\"])),\n",
    "     (r\"$\\mathrm{\\rho_a}=%.3f$, $\\mathrm{\\rho_w}=%.3f$\" %\n",
    "      (scores_annual_NN[\"pearson_corr\"], scores_winter_NN[\"pearson_corr\"]))))\n",
    "ax2.text(0.03,\n",
    "         0.98,\n",
    "         legend_NN,\n",
    "         transform=ax2.transAxes,\n",
    "         verticalalignment=\"top\",\n",
    "         fontsize=20,\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geodetic MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PREDICTIONS_NN = cfg.dataPath + path_distributed_MB_glamos + 'MBM/glamos_dems_NN/'\n",
    "PATH_PREDICTIONS_XGB = cfg.dataPath + path_distributed_MB_glamos + 'MBM/glamos_dems/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers_in_glamos = os.listdir(PATH_PREDICTIONS_NN)\n",
    "\n",
    "geodetic_mb = get_geodetic_MB(cfg)\n",
    "\n",
    "# get years per glacier\n",
    "years_start_per_gl = geodetic_mb.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodetic_mb.groupby('glacier_name')['Aend'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "periods_per_glacier, geoMB_per_glacier = build_periods_per_glacier(geodetic_mb)\n",
    "\n",
    "# Glaciers with geodetic MB data:\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area(cfg)\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list, gl_area):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "\n",
    "glacier_list = [\n",
    "    f for f in list(periods_per_glacier.keys()) if f in glaciers_in_glamos\n",
    "]\n",
    "glacier_list = sort_by_area(glacier_list, gl_area)\n",
    "# print len and list\n",
    "print('Number of glaciers:', len(glacier_list))\n",
    "print('Glaciers:', glacier_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_xgb = process_geodetic_mass_balance_comparison(\n",
    "    glacier_list=glacier_list,\n",
    "    path_SMB_GLAMOS_csv=cfg.dataPath + path_SMB_GLAMOS_csv,\n",
    "    periods_per_glacier=periods_per_glacier,\n",
    "    geoMB_per_glacier=geoMB_per_glacier,\n",
    "    gl_area=gl_area,\n",
    "    test_glaciers=test_glaciers,\n",
    "    path_predictions=PATH_PREDICTIONS_XGB,  # or another path if needed\n",
    "    cfg=cfg)\n",
    "\n",
    "df_all_nn = process_geodetic_mass_balance_comparison(\n",
    "    glacier_list=glacier_list,\n",
    "    path_SMB_GLAMOS_csv=cfg.dataPath + path_SMB_GLAMOS_csv,\n",
    "    periods_per_glacier=periods_per_glacier,\n",
    "    geoMB_per_glacier=geoMB_per_glacier,\n",
    "    gl_area=gl_area,\n",
    "    test_glaciers=test_glaciers,\n",
    "    path_predictions=PATH_PREDICTIONS_NN,  # or another path if needed\n",
    "    cfg=cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where any required columns are NaN\n",
    "df_all_nn = df_all_nn.dropna(subset=['Geodetic MB', 'MBM MB', 'GLAMOS MB'])\n",
    "\n",
    "# Compute RMSE and Pearson correlation\n",
    "rmse_nn = mean_squared_error(df_all_nn[\"Geodetic MB\"],\n",
    "                             df_all_nn[\"MBM MB\"],\n",
    "                             squared=False)\n",
    "corr_nn = np.corrcoef(df_all_nn[\"Geodetic MB\"], df_all_nn[\"MBM MB\"])[0, 1]\n",
    "\n",
    "# Drop rows where any required columns are NaN\n",
    "df_all_xgb = df_all_xgb.dropna(subset=['Geodetic MB', 'MBM MB', 'GLAMOS MB'])\n",
    "\n",
    "# Compute RMSE and Pearson correlation\n",
    "rmse_xgb = mean_squared_error(df_all_xgb[\"Geodetic MB\"],\n",
    "                              df_all_xgb[\"MBM MB\"],\n",
    "                              squared=False)\n",
    "corr_xgb = np.corrcoef(df_all_xgb[\"Geodetic MB\"], df_all_xgb[\"MBM MB\"])[0, 1]\n",
    "\n",
    "# Define figure and axes\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 8), sharex=True, sharey = True)\n",
    "\n",
    "# Plot MBM MB vs Geodetic MB\n",
    "plot_scatter(df_all_xgb, 'GLACIER', True, axs[0], \"MBM MB\", rmse_xgb, corr_xgb)\n",
    "\n",
    "axs[0].set_title('XGB predictions', fontsize=20)\n",
    "axs[0].set_ylabel('Predicted mass balance [m w.e.]', fontsize=18)\n",
    "axs[0].set_xlabel('Geodetic mass balance [m w.e.]', fontsize=18)\n",
    "\n",
    "plot_scatter(df_all_nn, 'GLACIER', True, axs[1], \"MBM MB\", rmse_nn, corr_nn)\n",
    "axs[1].set_ylabel('Predicted mass balance [m w.e.]', fontsize=18)\n",
    "axs[1].set_xlabel('Geodetic mass balance [m w.e.]', fontsize=18)\n",
    "axs[1].set_title('NN predictions', fontsize=20)\n",
    "\n",
    "# Adjust legend outside of plot\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "fig.legend(handles,\n",
    "           labels,\n",
    "           bbox_to_anchor=(1.05, 1),\n",
    "           loc=\"upper left\",\n",
    "           borderaxespad=0.,\n",
    "           ncol=2,\n",
    "           fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stake data ONCE instead of for every glacier\n",
    "stake_file = os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv,\n",
    "                          \"CH_wgms_dataset_all.csv\")\n",
    "df_stakes = pd.read_csv(stake_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "GLACIER_NAME = 'gietro'\n",
    "df_xgb = df_all_xgb[df_all_xgb.GLACIER == GLACIER_NAME]\n",
    "df_nn = df_all_nn[df_all_nn.GLACIER == GLACIER_NAME]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5), sharex=True, sharey=True)\n",
    "\n",
    "plot_scatter_comparison(axs[0],\n",
    "                        df_xgb,\n",
    "                        GLACIER_NAME,\n",
    "                        color_mbm=color_xgb,\n",
    "                        color_glamos=color_tim,\n",
    "                        title_suffix=\"(XGB)\")\n",
    "plot_scatter_comparison(axs[1],\n",
    "                        df_nn,\n",
    "                        GLACIER_NAME,\n",
    "                        color_mbm=color_xgb,\n",
    "                        color_glamos=color_tim,\n",
    "                        title_suffix=\"(NN)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GLAMOS data\n",
    "GLAMOS_glwmb = get_GLAMOS_glwmb(GLACIER_NAME, cfg)\n",
    "\n",
    "MBM_glwmb_nn = mbm_glwd_pred(PATH_PREDICTIONS_NN, GLACIER_NAME)\n",
    "MBM_glwmb_nn.rename(columns={\"MBM Balance\": \"MBM Balance NN\"}, inplace=True)\n",
    "MBM_glwmb_xgb = mbm_glwd_pred(PATH_PREDICTIONS_XGB, GLACIER_NAME)\n",
    "MBM_glwmb_xgb.rename(columns={\"MBM Balance\": \"MBM Balance XGB\"}, inplace=True)\n",
    "\n",
    "# Merge with GLAMOS data\n",
    "MBM_glwmb_nn = MBM_glwmb_nn.join(GLAMOS_glwmb)\n",
    "\n",
    "# Drop NaN values to avoid plotting errors\n",
    "MBM_glwmb_nn = MBM_glwmb_nn.dropna()\n",
    "\n",
    "MBM_glwmb = MBM_glwmb_nn.join(MBM_glwmb_xgb)\n",
    "# Plot the data\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "MBM_glwmb.plot(ax=axs[0],\n",
    "               y=['MBM Balance XGB', 'GLAMOS Balance'],\n",
    "               marker=\"o\",\n",
    "               color=[color_xgb, color_tim])\n",
    "MBM_glwmb.plot(ax=axs[1],\n",
    "               y=['MBM Balance NN', 'GLAMOS Balance'],\n",
    "               marker=\"o\",\n",
    "               color=[color_xgb, color_tim])\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_title(f\"{GLACIER_NAME.capitalize()} Glacier\", fontsize=24)\n",
    "    ax.set_ylabel(\"Mass Balance [m w.e.]\", fontsize=18)\n",
    "    ax.set_xlabel(\"Year\", fontsize=18)\n",
    "    ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "    ax.legend(fontsize=14)\n",
    "\n",
    "axs[0].set_title(f\"{GLACIER_NAME.capitalize()} Glacier (XGB)\", fontsize=16)\n",
    "axs[1].set_title(f\"{GLACIER_NAME.capitalize()} Glacier (NN)\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in MBM_glwmb_nn.index:\n",
    "    plot_mass_balance_comparison_annual(\n",
    "        glacier_name=GLACIER_NAME,\n",
    "        year=year,\n",
    "        cfg=cfg,\n",
    "        df_stakes=df_stakes,\n",
    "        path_distributed_mb=path_distributed_MB_glamos,\n",
    "        path_pred_xgb=PATH_PREDICTIONS_XGB,\n",
    "        path_pred_nn=PATH_PREDICTIONS_NN,\n",
    "        get_glamos_func=get_GLAMOS_glwmb,\n",
    "        get_pred_func=get_predicted_mb,\n",
    "        get_glamos_pred_func=get_predicted_mb_glamos,\n",
    "        load_grid_func=load_grid_file,\n",
    "        to_wgs84_func=transform_xarray_coords_lv95_to_wgs84,\n",
    "        apply_filter_func=apply_gaussian_filter,\n",
    "        get_colormaps_func=get_color_maps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
