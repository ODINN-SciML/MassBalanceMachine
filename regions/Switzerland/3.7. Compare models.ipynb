{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from skorch.helper import SliceDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'\n",
    "\n",
    "# Glacier outlines:\n",
    "glacier_outline_sgi = gpd.read_file(\n",
    "    os.path.join(cfg.dataPath, path_SGI_topo, 'inventory_sgi2016_r2020',\n",
    "                 'SGI_2016_glaciers_copy.shp'))  # Load the shapefile\n",
    "glacier_outline_rgi = gpd.read_file(cfg.dataPath + path_rgi_outlines)\n",
    "\n",
    "test_glaciers = [\n",
    "    'tortin', 'plattalva', 'sanktanna', 'schwarzberg', 'hohlaub', 'pizol',\n",
    "    'corvatsch', 'tsanfleuron', 'forno'\n",
    "]\n",
    "\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = pd.read_csv(cfg.dataPath + path_PMB_GLAMOS_csv +\n",
    "                          'CH_wgms_dataset_all.csv')\n",
    "data_glamos = data_glamos.dropna()\n",
    "\n",
    "# Glaciers with data of potential clear sky radiation\n",
    "# Format to same names as stakes:\n",
    "glDirect = np.sort([\n",
    "    re.search(r'xr_direct_(.*?)\\.zarr', f).group(1)\n",
    "    for f in os.listdir(cfg.dataPath + path_pcsr + 'zarr/')\n",
    "])\n",
    "\n",
    "restgl = np.sort(Diff(list(glDirect), list(data_glamos.GLACIER.unique())))\n",
    "\n",
    "# Filter out glaciers without data:\n",
    "data_glamos = data_glamos[data_glamos.GLACIER.isin(glDirect)]\n",
    "\n",
    "print('Number of glaciers:', len(data_glamos['GLACIER'].unique()))\n",
    "print('Number of winter and annual samples:', len(data_glamos))\n",
    "print('Number of annual samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'winter']))\n",
    "\n",
    "data_glamos.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of measurements per glacier:\n",
    "glacier_info = data_glamos.groupby('GLACIER').size().sort_values(\n",
    "    ascending=False).reset_index()\n",
    "glacier_info.rename(columns={0: 'Nb. measurements'}, inplace=True)\n",
    "glacier_info.set_index('GLACIER', inplace=True)\n",
    "\n",
    "glacier_loc = data_glamos.groupby('GLACIER')[['POINT_LAT', 'POINT_LON']].mean()\n",
    "\n",
    "glacier_info = glacier_loc.merge(glacier_info, on='GLACIER')\n",
    "\n",
    "glacier_period = data_glamos.groupby(['GLACIER', 'PERIOD'\n",
    "                                      ]).size().unstack().fillna(0).astype(int)\n",
    "\n",
    "glacier_info = glacier_info.merge(glacier_period, on='GLACIER')\n",
    "\n",
    "glacier_info['Train/Test glacier'] = glacier_info.apply(\n",
    "    lambda x: 'Test' if x.name in test_glaciers else 'Train', axis=1)\n",
    "glacier_info.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign glaciers to river basin names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load RGI glacier IDs ===\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids)\n",
    "rgi_df.columns = rgi_df.columns.str.strip()\n",
    "rgi_df = rgi_df.sort_values(by='short_name').set_index('short_name')\n",
    "\n",
    "# === Load SGI region geometries ===\n",
    "SGI_regions = gpd.read_file(\n",
    "    os.path.join(cfg.dataPath, path_SGI_topo, 'inventory_sgi2016_r2020',\n",
    "                 'sgi_regions.geojson'))\n",
    "\n",
    "# Clean object columns\n",
    "SGI_regions[SGI_regions.select_dtypes(include='object').columns] = \\\n",
    "    SGI_regions.select_dtypes(include='object').apply(lambda col: col.str.strip())\n",
    "\n",
    "SGI_regions = SGI_regions.drop_duplicates().dropna()\n",
    "SGI_regions = SGI_regions.set_index('pk_sgi_region')\n",
    "\n",
    "# === Map to Level 0 river basins ===\n",
    "catchment_lv0 = {\n",
    "    'A': 'Rhine',\n",
    "    'B': 'Rhone',\n",
    "    'C': 'Po',\n",
    "    'D': 'Adige',\n",
    "    'E': 'Danube'\n",
    "}\n",
    "rgi_df['rvr_lv0'] = rgi_df['sgi-id'].str[0].map(catchment_lv0)\n",
    "\n",
    "\n",
    "# === Map to Level 1 river basins using SGI regions ===\n",
    "def get_river_basin(sgi_id):\n",
    "    key = sgi_id.split('-')[0]\n",
    "    if key not in SGI_regions.index:\n",
    "        return None\n",
    "    basin = SGI_regions.loc[key, 'river_basin_name']\n",
    "    if isinstance(basin, pd.Series):\n",
    "        return basin.dropna().unique()[0] if not basin.dropna().empty else None\n",
    "    return basin if pd.notna(basin) else None\n",
    "\n",
    "\n",
    "rgi_df['rvr_lv1'] = rgi_df['sgi-id'].apply(get_river_basin)\n",
    "\n",
    "# Final formatting\n",
    "rgi_df = rgi_df.reset_index().rename(columns={\n",
    "    'short_name': 'GLACIER'\n",
    "}).set_index('GLACIER')\n",
    "rgi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_info = glacier_info.merge(rgi_df[['rvr_lv0', 'rvr_lv1']],\n",
    "                                  on='GLACIER',\n",
    "                                  how='left')\n",
    "glacier_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models:\n",
    "### XGBoost model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = True\n",
    "dataloader_gl = process_or_load_data(run_flag=RUN,\n",
    "                                     data_glamos=data_glamos,\n",
    "                                     paths=paths,\n",
    "                                     cfg=cfg,\n",
    "                                     vois_climate=vois_climate,\n",
    "                                     vois_topographical=vois_topographical)\n",
    "data_monthly = dataloader_gl.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_init_xgb = {\n",
    "    'device': 'cuda:0',\n",
    "    'tree_method': 'hist',\n",
    "    \"random_state\": cfg.seed,\n",
    "    \"n_jobs\": cfg.numJobs\n",
    "}\n",
    "\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_params = {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 800}\n",
    "\n",
    "# Feature columns:\n",
    "feature_columns = [\n",
    "    'ELEVATION_DIFFERENCE'\n",
    "] + list(vois_climate) + list(vois_topographical) + ['pcsr']\n",
    "# feature_columns = ['ELEVATION_DIFFERENCE'\n",
    "#                    ] + list(vois_climate) + list(vois_topographical)\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "df_X_train_subset = train_set['df_X'][all_columns]\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of testing dataset:', test_set['df_X'][all_columns].shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "params = {**param_init_xgb, **custom_params}\n",
    "print(params)\n",
    "custom_xgb_model = mbm.models.CustomXGBoostRegressor(cfg, **params)\n",
    "\n",
    "# Fit on train data:\n",
    "custom_xgb_model.fit(train_set['df_X'][all_columns], train_set['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test\n",
    "custom_xgb_model = custom_xgb_model.set_params(device='cpu')\n",
    "features_test, metadata_test = custom_xgb_model._create_features_metadata(\n",
    "    test_set['df_X'][all_columns])\n",
    "y_pred = custom_xgb_model.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = custom_xgb_model.aggrPredict(metadata_test, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = custom_xgb_model.score(test_set['df_X'][all_columns],\n",
    "                               test_set['y'])  # negative\n",
    "print('Overall score:', np.abs(score))\n",
    "\n",
    "grouped_ids_xgb = getDfAggregatePred(test_set, y_pred_agg, all_columns)\n",
    "PlotPredictions(grouped_ids_xgb, y_pred, metadata_test, test_set,\n",
    "                custom_xgb_model)\n",
    "plt.suptitle(f'MBM tested on {test_glaciers}', fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns that are metadata or neither used in metadata or features\n",
    "feature_columns = list(train_set['df_X'].columns.difference(cfg.metaData).drop(\n",
    "    cfg.notMetaDataNotFeatures))\n",
    "\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "nInp = len(feature_columns)\n",
    "cfg.setFeatures(feature_columns)\n",
    "\n",
    "params_NN = {\n",
    "    'lr': 0.01,\n",
    "    'batch_size': 256,\n",
    "    'optimizer': torch.optim.SGD,\n",
    "    'module__0__out_features': 16,\n",
    "    'module__2__out_features': 8\n",
    "}\n",
    "\n",
    "network = nn.Sequential(\n",
    "    nn.Linear(nInp, params_NN['module__0__out_features']),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(params_NN['module__0__out_features'],\n",
    "              params_NN['module__2__out_features']),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(params_NN['module__2__out_features'], 1),\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='valid_loss',\n",
    "    patience=10,\n",
    "    threshold=1e-4,  # Optional: stop only when improvement is very small\n",
    ")\n",
    "\n",
    "lr_scheduler_cb = LRScheduler(policy=ReduceLROnPlateau,\n",
    "                              monitor='valid_loss',\n",
    "                              mode='min',\n",
    "                              factor=0.5,\n",
    "                              patience=5,\n",
    "                              threshold=0.01,\n",
    "                              threshold_mode='rel',\n",
    "                              verbose=True)\n",
    "\n",
    "param_init_NN = {'device': 'cuda:0'}\n",
    "\n",
    "dataset = dataset_val = None  # Initialized hereafter\n",
    "\n",
    "\n",
    "def my_train_split(ds, y=None, **fit_params):\n",
    "    return dataset, dataset_val\n",
    "\n",
    "\n",
    "args_NN = {\n",
    "    'module': network,\n",
    "    'nbFeatures': nInp,\n",
    "    'train_split': my_train_split,\n",
    "    'batch_size': params_NN['batch_size'],\n",
    "    'verbose': 1,\n",
    "    'iterator_train__shuffle': True,\n",
    "    'lr': params_NN['lr'],\n",
    "    'max_epochs': 300,\n",
    "    'optimizer': params_NN['optimizer'],\n",
    "    'callbacks': [\n",
    "        ('early_stop', early_stop),\n",
    "        ('lr_scheduler', lr_scheduler_cb),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and set to CPU\n",
    "model_filename = \"nn_model_2025-06-02.pt\"  # Replace with actual date if needed\n",
    "\n",
    "# current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "# model_filename = f\"nn_model_{current_date}.pt\"\n",
    "\n",
    "custom_NN_model = mbm.models.CustomNeuralNetRegressor.load_model(\n",
    "    cfg,\n",
    "    model_filename,\n",
    "    **{\n",
    "        **args_NN,\n",
    "        **param_init_NN\n",
    "    },\n",
    ")\n",
    "custom_NN_model = custom_NN_model.set_params(device='cpu')\n",
    "custom_NN_model = custom_NN_model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features and metadata\n",
    "features_test, metadata_test = custom_NN_model._create_features_metadata(\n",
    "    test_set['df_X'][all_columns])\n",
    "\n",
    "# Ensure all tensors are on CPU if they are torch tensors\n",
    "if hasattr(features_test, 'cpu'):\n",
    "    features_test = features_test.cpu()\n",
    "\n",
    "# Ensure targets are also on CPU\n",
    "targets_test = test_set['y']\n",
    "if hasattr(targets_test, 'cpu'):\n",
    "    targets_test = targets_test.cpu()\n",
    "\n",
    "# Create the dataset\n",
    "dataset_test = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                     features=features_test,\n",
    "                                                     metadata=metadata_test,\n",
    "                                                     targets=targets_test)\n",
    "\n",
    "dataset_test = [\n",
    "    SliceDataset(dataset_test, idx=0),\n",
    "    SliceDataset(dataset_test, idx=1)\n",
    "]\n",
    "\n",
    "# Make predictions aggr to meas ID\n",
    "y_pred = custom_NN_model.predict(dataset_test[0])\n",
    "y_pred_agg = custom_NN_model.aggrPredict(dataset_test[0])\n",
    "\n",
    "batchIndex = np.arange(len(y_pred_agg))\n",
    "y_true = np.array([e for e in dataset_test[1][batchIndex]])\n",
    "\n",
    "# Calculate scores\n",
    "score = custom_NN_model.score(dataset_test[0], dataset_test[1])\n",
    "mse, rmse, mae, pearson = custom_NN_model.evalMetrics(y_pred, y_true)\n",
    "\n",
    "# Aggregate predictions\n",
    "id = dataset_test[0].dataset.indexToId(batchIndex)\n",
    "data = {\n",
    "    'target': [e[0] for e in dataset_test[1]],\n",
    "    'ID': id,\n",
    "    'pred': y_pred_agg\n",
    "}\n",
    "grouped_ids_NN = pd.DataFrame(data)\n",
    "\n",
    "# Add period\n",
    "periods_per_ids = test_set['df_X'][all_columns].groupby('ID')['PERIOD'].first()\n",
    "grouped_ids_NN = grouped_ids_NN.merge(periods_per_ids, on='ID')\n",
    "\n",
    "# Add glacier name\n",
    "glacier_per_ids = test_set['df_X'][all_columns].groupby(\n",
    "    'ID')['GLACIER'].first()\n",
    "grouped_ids_NN = grouped_ids_NN.merge(glacier_per_ids, on='ID')\n",
    "\n",
    "# Add YEAR\n",
    "years_per_ids = test_set['df_X'][all_columns].groupby('ID')['YEAR'].first()\n",
    "grouped_ids_NN = grouped_ids_NN.merge(years_per_ids, on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPredictions_NN(grouped_ids_NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter on test glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "grouped_ids_annual_NN = grouped_ids_NN[grouped_ids_NN.PERIOD == 'annual']\n",
    "y_true_mean_NN = grouped_ids_annual_NN['target']\n",
    "y_pred_agg_NN = grouped_ids_annual_NN['pred']\n",
    "scores_annual_NN = {\n",
    "    'mse': mean_squared_error(y_true_mean_NN, y_pred_agg_NN),\n",
    "    'rmse': root_mean_squared_error(y_true_mean_NN, y_pred_agg_NN),\n",
    "    'mae': mean_absolute_error(y_true_mean_NN, y_pred_agg_NN),\n",
    "    'pearson_corr': np.corrcoef(y_true_mean_NN, y_pred_agg_NN)[0, 1]\n",
    "}\n",
    "\n",
    "grouped_ids_annual_xgb = grouped_ids_xgb[grouped_ids_xgb.PERIOD == 'annual']\n",
    "y_true_mean_xgb = grouped_ids_annual_xgb['target']\n",
    "y_pred_agg_xgb = grouped_ids_annual_xgb['pred']\n",
    "scores_annual_xgb = {\n",
    "    'mse': mean_squared_error(y_true_mean_xgb, y_pred_agg_xgb),\n",
    "    'rmse': root_mean_squared_error(y_true_mean_xgb, y_pred_agg_xgb),\n",
    "    'mae': mean_absolute_error(y_true_mean_xgb, y_pred_agg_xgb),\n",
    "    'pearson_corr': np.corrcoef(y_true_mean_xgb, y_pred_agg_xgb)[0, 1]\n",
    "}\n",
    "\n",
    "# Winter\n",
    "grouped_ids_winter_NN = grouped_ids_NN[grouped_ids_NN.PERIOD == 'winter']\n",
    "y_true_mean_NN_w = grouped_ids_winter_NN['target']\n",
    "y_pred_agg_NN_w = grouped_ids_winter_NN['pred']\n",
    "scores_winter_NN = {\n",
    "    'mse': mean_squared_error(y_true_mean_NN_w, y_pred_agg_NN_w),\n",
    "    'rmse': root_mean_squared_error(y_true_mean_NN_w, y_pred_agg_NN_w),\n",
    "    'mae': mean_absolute_error(y_true_mean_NN_w, y_pred_agg_NN_w),\n",
    "    'pearson_corr': np.corrcoef(y_true_mean_NN_w, y_pred_agg_NN_w)[0, 1]\n",
    "}\n",
    "\n",
    "grouped_ids_winter_xgb = grouped_ids_xgb[grouped_ids_xgb.PERIOD == 'winter']\n",
    "y_true_mean_xgb_w = grouped_ids_winter_xgb['target']\n",
    "y_pred_agg_xgb_w = grouped_ids_winter_xgb['pred']\n",
    "scores_winter_xgb = {\n",
    "    'mse': mean_squared_error(y_true_mean_xgb_w, y_pred_agg_xgb_w),\n",
    "    'rmse': root_mean_squared_error(y_true_mean_xgb_w, y_pred_agg_xgb_w),\n",
    "    'mae': mean_absolute_error(y_true_mean_xgb_w, y_pred_agg_xgb_w),\n",
    "    'pearson_corr': np.corrcoef(y_true_mean_xgb_w, y_pred_agg_xgb_w)[0, 1]\n",
    "}\n",
    "\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "ax1.set_title('XGB predictions', fontsize=20)\n",
    "predVSTruth(ax1,\n",
    "            grouped_ids_xgb,\n",
    "            scores_annual_xgb,\n",
    "            hue='PERIOD',\n",
    "            add_legend=False,\n",
    "            palette=[color_dark_blue, color_pink])\n",
    "\n",
    "legend_xgb = \"\\n\".join(\n",
    "    ((r\"$\\mathrm{RMSE_a}=%.3f$, $\\mathrm{RMSE_w}=%.3f$,\" %\n",
    "      (scores_annual_xgb[\"rmse\"], scores_winter_xgb[\"rmse\"])),\n",
    "     (r\"$\\mathrm{\\rho_a}=%.3f$, $\\mathrm{\\rho_w}=%.3f$\" %\n",
    "      (scores_annual_xgb[\"pearson_corr\"], scores_winter_xgb[\"pearson_corr\"]))))\n",
    "ax1.text(0.03,\n",
    "         0.98,\n",
    "         legend_xgb,\n",
    "         transform=ax1.transAxes,\n",
    "         verticalalignment=\"top\",\n",
    "         fontsize=20,\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "ax2.set_title('NN predictions', fontsize=20)\n",
    "predVSTruth(ax2,\n",
    "            grouped_ids_NN,\n",
    "            scores_annual_NN,\n",
    "            hue='PERIOD',\n",
    "            add_legend=False,\n",
    "            palette=[color_dark_blue, color_pink])\n",
    "\n",
    "legend_NN = \"\\n\".join(\n",
    "    ((r\"$\\mathrm{RMSE_a}=%.3f$, $\\mathrm{RMSE_w}=%.3f$,\" %\n",
    "      (scores_annual_NN[\"rmse\"], scores_winter_NN[\"rmse\"])),\n",
    "     (r\"$\\mathrm{\\rho_a}=%.3f$, $\\mathrm{\\rho_w}=%.3f$\" %\n",
    "      (scores_annual_NN[\"pearson_corr\"], scores_winter_NN[\"pearson_corr\"]))))\n",
    "ax2.text(0.03,\n",
    "         0.98,\n",
    "         legend_NN,\n",
    "         transform=ax2.transAxes,\n",
    "         verticalalignment=\"top\",\n",
    "         fontsize=20,\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geodetic MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PREDICTIONS_NN = cfg.dataPath + path_distributed_MB_glamos + 'MBM/glamos_dems_NN/'\n",
    "PATH_PREDICTIONS_XGB = cfg.dataPath + path_distributed_MB_glamos + 'MBM/glamos_dems/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers_in_glamos = os.listdir(PATH_PREDICTIONS_NN)\n",
    "\n",
    "geodetic_mb = get_geodetic_MB(cfg)\n",
    "\n",
    "# filter to glaciers with potential clear sky radiation data\n",
    "geodetic_mb = geodetic_mb[geodetic_mb.glacier_name.isin(glDirect)]\n",
    "\n",
    "# get years per glacier\n",
    "years_start_per_gl = geodetic_mb.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodetic_mb.groupby('glacier_name')['Aend'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "periods_per_glacier, _ = build_periods_per_glacier(geodetic_mb)\n",
    "\n",
    "# Glaciers with geodetic MB data:\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area(cfg)\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list, gl_area):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "glacier_list = [\n",
    "    f for f in list(periods_per_glacier.keys()) if f in glaciers_in_glamos\n",
    "]\n",
    "glacier_list = sort_by_area(glacier_list, gl_area)\n",
    "# print len and list\n",
    "print('Number of glaciers:', len(glacier_list))\n",
    "print('Glaciers:', glacier_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_xgb = process_geodetic_mass_balance_comparison(\n",
    "    glacier_list=glacier_list,\n",
    "    path_SMB_GLAMOS_csv=cfg.dataPath+path_SMB_GLAMOS_csv,\n",
    "    periods_per_glacier=periods_per_glacier,\n",
    "    geoMB_per_glacier=geoMB_per_glacier,\n",
    "    gl_area=gl_area,\n",
    "    test_glaciers=test_glaciers,\n",
    "    path_predictions=PATH_PREDICTIONS_XGB,  # or another path if needed\n",
    "    cfg = cfg\n",
    ")\n",
    "\n",
    "df_all_nn = process_geodetic_mass_balance_comparison(\n",
    "    glacier_list=glacier_list,\n",
    "    path_SMB_GLAMOS_csv=cfg.dataPath+path_SMB_GLAMOS_csv,\n",
    "    periods_per_glacier=periods_per_glacier,\n",
    "    geoMB_per_glacier=geoMB_per_glacier,\n",
    "    gl_area=gl_area,\n",
    "    test_glaciers=test_glaciers,\n",
    "    path_predictions=PATH_PREDICTIONS_NN,  # or another path if needed\n",
    "    cfg = cfg\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where any required columns are NaN\n",
    "df_all_nn = df_all_nn.dropna(subset=['Geodetic MB', 'MBM MB', 'GLAMOS MB'])\n",
    "\n",
    "# Compute RMSE and Pearson correlation\n",
    "rmse_nn = mean_squared_error(df_all_nn[\"Geodetic MB\"],\n",
    "                             df_all_nn[\"MBM MB\"],\n",
    "                             squared=False)\n",
    "corr_nn = np.corrcoef(df_all_nn[\"Geodetic MB\"], df_all_nn[\"MBM MB\"])[0, 1]\n",
    "\n",
    "# Drop rows where any required columns are NaN\n",
    "df_all_xgb = df_all_xgb.dropna(subset=['Geodetic MB', 'MBM MB', 'GLAMOS MB'])\n",
    "\n",
    "# Compute RMSE and Pearson correlation\n",
    "rmse_xgb = mean_squared_error(df_all_xgb[\"Geodetic MB\"],\n",
    "                              df_all_xgb[\"MBM MB\"],\n",
    "                              squared=False)\n",
    "corr_xgb = np.corrcoef(df_all_xgb[\"Geodetic MB\"], df_all_xgb[\"MBM MB\"])[0, 1]\n",
    "\n",
    "# Define figure and axes\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 8), sharex=True)\n",
    "\n",
    "# Plot MBM MB vs Geodetic MB\n",
    "plot_scatter(df_all_xgb, 'GLACIER', True, axs[0], \"MBM MB\", rmse_xgb, corr_xgb)\n",
    "\n",
    "axs[0].set_title('XGB predictions', fontsize=20)\n",
    "axs[0].set_ylabel('Predicted mass balance [m w.e.]', fontsize=18)\n",
    "axs[0].set_xlabel('Geodetic mass balance [m w.e.]', fontsize=18)\n",
    "\n",
    "plot_scatter(df_all_nn, 'GLACIER', True, axs[1], \"MBM MB\", rmse_nn, corr_nn)\n",
    "axs[1].set_ylabel('Predicted mass balance [m w.e.]', fontsize=18)\n",
    "axs[1].set_xlabel('Geodetic mass balance [m w.e.]', fontsize=18)\n",
    "axs[1].set_title('NN predictions', fontsize=20)\n",
    "\n",
    "# Adjust legend outside of plot\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "fig.legend(handles,\n",
    "           labels,\n",
    "           bbox_to_anchor=(1.05, 1),\n",
    "           loc=\"upper left\",\n",
    "           borderaxespad=0.,\n",
    "           ncol=2,\n",
    "           fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
