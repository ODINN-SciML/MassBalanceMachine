{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(),\n",
    "                             '../../'))  # Add root of repo to import MBM\n",
    "import csv\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "from skorch.helper import SliceDataset\n",
    "from datetime import datetime\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "import itertools\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import WeightedRandomSampler, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'\n",
    "\n",
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "\n",
    "vois_climate = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(data_monthly.GLACIER.unique())\n",
    "missing_glaciers = [g for g in TEST_GLACIERS if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in TEST_GLACIERS]\n",
    "\n",
    "data_test = data_monthly[data_monthly.GLACIER.isin(TEST_GLACIERS)]\n",
    "print('Size of monthly test data:', len(data_test))\n",
    "\n",
    "data_train = data_monthly[data_monthly.GLACIER.isin(train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train))\n",
    "\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=TEST_GLACIERS,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))\n",
    "\n",
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "\n",
    "data_test = test_set['df_X']\n",
    "data_test['y'] = test_set['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'pcsr',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = [\n",
    "    'aspect_sgi', 'slope_sgi', 'hugonnet_dhdt', 'consensus_ice_thickness',\n",
    "    'millan_v'\n",
    "]\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "\n",
    "df_train = data_train.copy()\n",
    "df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "df_test = data_test.copy()\n",
    "df_test['PERIOD'] = df_test['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "# --- build train dataset from dataframe ---\n",
    "ds_train = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_train,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "ds_test = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_test,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train), val_ratio=0.2, seed=cfg.seed)\n",
    "\n",
    "Fm = ds_train.Xm.shape[-1]\n",
    "Fs = ds_train.Xs.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_params = {\n",
    "    'Fm': 9,\n",
    "    'Fs': 5,\n",
    "    'hidden_size': 128,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': False,\n",
    "    'dropout': 0.0,\n",
    "    'static_layers': 0,\n",
    "    'static_hidden': None,\n",
    "    'static_dropout': None,\n",
    "    'lr': 0.0001,\n",
    "    'weight_decay': 0.0,\n",
    "    'loss_name': 'neutral',\n",
    "    'loss_spec': None,\n",
    "    'two_heads': False,\n",
    "    'head_dropout': 0\n",
    "}\n",
    "\n",
    "# Build dataloaders:\n",
    "ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train)\n",
    "\n",
    "ds_test_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_test)\n",
    "\n",
    "# --- make train/val loaders and get split indices (scalers fit on TRAIN, applied to whole ds) ---\n",
    "train_dl, val_dl = ds_train_copy.make_loaders(\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- build test dataset and loader (reuses train scalers) ---\n",
    "test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_copy, ds_train_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "#model_filename = f\"models/lstm_model_{current_date}_simple.pt\"\n",
    "model_filename = f\"models/lstm_model_2025-09-19_simple.pt\"\n",
    "model = mbm.models.LSTM_MB.build_model_from_params(cfg, custom_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(custom_params)\n",
    "\n",
    "# Load the best weights\n",
    "best_state = torch.load(model_filename, map_location=device)\n",
    "model.load_state_dict(best_state)\n",
    "\n",
    "# Evaluate on test\n",
    "test_metrics_LSTM_simple, grouped_ids_LSTM_simple = model.evaluate_with_preds(\n",
    "    device, test_dl, ds_test_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics_LSTM_simple[\n",
    "    'RMSE_annual'], test_metrics_LSTM_simple['RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two heads LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- pick the most recent grid-search log ---\n",
    "custom_params = {\n",
    "    'Fm': 9,\n",
    "    'Fs': 5,\n",
    "    'hidden_size': 128,\n",
    "    'num_layers': 2,\n",
    "    'bidirectional': False,\n",
    "    'dropout': 0.0,\n",
    "    'static_layers': 2,\n",
    "    'static_hidden': [128, 64],\n",
    "    'static_dropout': 0.1,\n",
    "    'lr': 0.0005,\n",
    "    'weight_decay': 0.0001,\n",
    "    'loss_name': 'neutral',\n",
    "    'loss_spec': None,\n",
    "    'two_heads': True,\n",
    "    'head_dropout': 0.0\n",
    "}\n",
    "\n",
    "# Build dataloaders:\n",
    "ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train)\n",
    "\n",
    "ds_test_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_test)\n",
    "\n",
    "# --- make train/val loaders and get split indices (scalers fit on TRAIN, applied to whole ds) ---\n",
    "train_dl, val_dl = ds_train_copy.make_loaders(\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- build test dataset and loader (reuses train scalers) ---\n",
    "test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_copy, ds_train_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "model_filename = f\"models/lstm_model_{current_date}_two_heads.pt\"\n",
    "model_filename = f\"models/lstm_model_2025-09-19_two_heads.pt\"\n",
    "model = mbm.models.LSTM_MB.build_model_from_params(cfg, custom_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(custom_params)\n",
    "\n",
    "# Load the best weights\n",
    "best_state = torch.load(model_filename, map_location=device)\n",
    "model.load_state_dict(best_state)\n",
    "\n",
    "# Evaluate on test\n",
    "test_metrics_LSTM_2heads, grouped_ids_LSTM_2heads = model.evaluate_with_preds(\n",
    "    device, test_dl, ds_test_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics_LSTM_2heads[\n",
    "    'RMSE_annual'], test_metrics_LSTM_2heads['RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_val = None  # Initialized hereafter\n",
    "\n",
    "\n",
    "def my_train_split(ds, y=None, **fit_params):\n",
    "    return dataset, dataset_val\n",
    "\n",
    "\n",
    "# --- features ---\n",
    "features_topo = ['ELEVATION_DIFFERENCE', 'pcsr'] + list(vois_topographical)\n",
    "feature_columns = features_topo + list(vois_climate)\n",
    "cfg.setFeatures(feature_columns)\n",
    "\n",
    "# keep only features + any required non-feature fields your code needs later\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "# --- test subset (no blanket dropna here; keep what your model expects) ---\n",
    "df_X_test_subset_MLP = test_set['df_X'][all_columns].copy()\n",
    "y_test = test_set['y']\n",
    "\n",
    "# --- load saved model (no callbacks/scheduler/early stop needed) ---\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "model_filename = \"nn_model_2025-09-22.pt\"  # adjust if needed\n",
    "params_filename = \"nn_params_2025-09-22.pkl\"  # adjust if needed\n",
    "\n",
    "with open(f\"models/{params_filename}\", \"rb\") as f:\n",
    "    params = pickle.load(f)\n",
    "\n",
    "nInp = len(feature_columns)\n",
    "\n",
    "args = {\n",
    "    'module': FlexibleNetwork,\n",
    "    'nbFeatures': nInp,\n",
    "    'module__input_dim': nInp,\n",
    "    'module__dropout': params['module__dropout'],\n",
    "    'module__hidden_layers': params['module__hidden_layers'],\n",
    "    'module__use_batchnorm': params['module__use_batchnorm'],\n",
    "    # the rest is irrelevant for inference but harmless if present:\n",
    "    'batch_size': params.get('batch_size', 128),\n",
    "    'verbose': 0,\n",
    "}\n",
    "\n",
    "loaded_MLP = mbm.models.CustomNeuralNetRegressor.load_model(\n",
    "    cfg,\n",
    "    model_filename,\n",
    "    **args,\n",
    "    device='cpu',\n",
    ").to('cpu')\n",
    "\n",
    "# --- evaluate on test ---\n",
    "grouped_ids_NN, scores_NN, ids_NN, y_pred_NN = evaluate_model_and_group_predictions(\n",
    "    loaded_MLP, df_X_test_subset_MLP, y_test, cfg, months_head_pad,\n",
    "    months_tail_pad)\n",
    "\n",
    "print(\"Test scores:\", scores_NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter on test glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 8))\n",
    "\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "ax1.set_title('LSTM simple', fontsize=20)\n",
    "predVSTruth(ax1,\n",
    "            grouped_ids_LSTM_simple,\n",
    "            hue='PERIOD',\n",
    "            palette=[color_dark_blue, color_pink])\n",
    "\n",
    "legend = \"\\n\".join((\n",
    "    (r\"$\\mathrm{RMSE_a}=%.3f$, $\\mathrm{RMSE_w}=%.3f$,\" %\n",
    "     (test_metrics_LSTM_simple[\"RMSE_annual\"],\n",
    "      test_metrics_LSTM_simple[\"RMSE_winter\"])),\n",
    "    (r\"$\\mathrm{R^2_a}=%.3f$, $\\mathrm{R^2_w}=%.3f$\" %\n",
    "     (test_metrics_LSTM_simple[\"R2_annual\"],\n",
    "      test_metrics_LSTM_simple[\"R2_winter\"])),\n",
    "    r\"$\\mathrm{B_a}=%.3f$, $\\mathrm{B_w}=%.3f$\" %\n",
    "    (test_metrics_LSTM_simple[\"Bias_annual\"],\n",
    "     test_metrics_LSTM_simple[\"Bias_winter\"]),\n",
    "))\n",
    "ax1.text(0.03,\n",
    "         0.96,\n",
    "         legend,\n",
    "         transform=ax1.transAxes,\n",
    "         verticalalignment=\"top\",\n",
    "         fontsize=18,\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0))\n",
    "\n",
    "ax2 = plt.subplot(1, 3, 2)\n",
    "ax2.set_title('LSTM two heads', fontsize=20)\n",
    "predVSTruth(ax2,\n",
    "            grouped_ids_LSTM_2heads,\n",
    "            hue='PERIOD',\n",
    "            palette=[color_dark_blue, color_pink])\n",
    "\n",
    "legend = \"\\n\".join((\n",
    "    (r\"$\\mathrm{RMSE_a}=%.3f$, $\\mathrm{RMSE_w}=%.3f$,\" %\n",
    "     (test_metrics_LSTM_2heads[\"RMSE_annual\"],\n",
    "      test_metrics_LSTM_2heads[\"RMSE_winter\"])),\n",
    "    (r\"$\\mathrm{R^2_a}=%.3f$, $\\mathrm{R^2_w}=%.3f$\" %\n",
    "     (test_metrics_LSTM_2heads[\"R2_annual\"],\n",
    "      test_metrics_LSTM_2heads[\"R2_winter\"])),\n",
    "    r\"$\\mathrm{B_a}=%.3f$, $\\mathrm{B_w}=%.3f$\" %\n",
    "    (test_metrics_LSTM_2heads[\"Bias_annual\"],\n",
    "     test_metrics_LSTM_2heads[\"Bias_winter\"]),\n",
    "))\n",
    "ax2.text(0.03,\n",
    "         0.96,\n",
    "         legend,\n",
    "         transform=ax2.transAxes,\n",
    "         verticalalignment=\"top\",\n",
    "         fontsize=18,\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0))\n",
    "\n",
    "ax3 = plt.subplot(1, 3, 3)\n",
    "scores_annual_NN, scores_winter_NN = compute_seasonal_scores(\n",
    "    grouped_ids_NN, target_col='target', pred_col='pred')\n",
    "ax3.set_title('MLP', fontsize=20)\n",
    "predVSTruth(ax3,\n",
    "            grouped_ids_NN,\n",
    "            hue='PERIOD',\n",
    "            palette=[color_dark_blue, color_pink])\n",
    "\n",
    "legend_NN = \"\\n\".join((\n",
    "    (r\"$\\mathrm{RMSE_a}=%.3f$, $\\mathrm{RMSE_w}=%.3f$,\" %\n",
    "     (scores_annual_NN[\"rmse\"], scores_winter_NN[\"rmse\"])),\n",
    "    (r\"$\\mathrm{R^2_a}=%.3f$, $\\mathrm{R^2_w}=%.3f$\" %\n",
    "     (scores_annual_NN[\"R2\"], scores_winter_NN[\"R2\"])),\n",
    "    r\"$\\mathrm{B_a}=%.3f$, $\\mathrm{B_w}=%.3f$\" %\n",
    "    (scores_annual_NN[\"Bias\"], scores_winter_NN[\"Bias\"]),\n",
    "))\n",
    "ax3.text(0.03,\n",
    "         0.96,\n",
    "         legend_NN,\n",
    "         transform=ax3.transAxes,\n",
    "         verticalalignment=\"top\",\n",
    "         fontsize=18,\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geodetic MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PREDICTIONS_NN = os.path.join(\n",
    "    cfg.dataPath, 'GLAMOS', 'distributed_MB_grids',\n",
    "    'MBM/testing_combis/glamos_dems_NN_SEB_full_OGGM')\n",
    "PATH_PREDICTIONS_LSTM_simple = os.path.join(cfg.dataPath, 'GLAMOS',\n",
    "                                            'distributed_MB_grids',\n",
    "                                            'MBM/testing_LSTM/LSTM_simple')\n",
    "PATH_PREDICTIONS_LSTM_two_heads = os.path.join(\n",
    "    cfg.dataPath, 'GLAMOS', 'distributed_MB_grids',\n",
    "    'MBM/testing_LSTM/LSTM_two_heads_best')\n",
    "\n",
    "PATH_PREDICTIONS_LSTM_no_oggm = os.path.join(\n",
    "    cfg.dataPath, 'GLAMOS', 'distributed_MB_grids',\n",
    "    'MBM/testing_LSTM/LSTM_no_oggm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers_in_glamos = os.listdir(PATH_PREDICTIONS_NN)\n",
    "\n",
    "geodetic_mb = get_geodetic_MB(cfg)\n",
    "\n",
    "# get years per glacier\n",
    "years_start_per_gl = geodetic_mb.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodetic_mb.groupby('glacier_name')['Aend'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "periods_per_glacier, geoMB_per_glacier = build_periods_per_glacier(geodetic_mb)\n",
    "\n",
    "# Glaciers with geodetic MB data:\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area(cfg)\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list, gl_area):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "\n",
    "glacier_list = [\n",
    "    f for f in list(periods_per_glacier.keys()) if f in glaciers_in_glamos\n",
    "]\n",
    "glacier_list = sort_by_area(glacier_list, gl_area)\n",
    "print('Number of glaciers:', len(glacier_list))\n",
    "print('Glaciers:', glacier_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_nn = process_geodetic_mass_balance_comparison(\n",
    "    glacier_list=os.listdir(PATH_PREDICTIONS_NN),\n",
    "    path_SMB_GLAMOS_csv=cfg.dataPath + path_SMB_GLAMOS_csv,\n",
    "    periods_per_glacier=periods_per_glacier,\n",
    "    geoMB_per_glacier=geoMB_per_glacier,\n",
    "    gl_area=gl_area,\n",
    "    test_glaciers=TEST_GLACIERS,\n",
    "    path_predictions=PATH_PREDICTIONS_NN,  # or another path if needed\n",
    "    cfg=cfg)\n",
    "\n",
    "df_lstm_simple = process_geodetic_mass_balance_comparison(\n",
    "    glacier_list=glacier_list,\n",
    "    path_SMB_GLAMOS_csv=cfg.dataPath + path_SMB_GLAMOS_csv,\n",
    "    periods_per_glacier=periods_per_glacier,\n",
    "    geoMB_per_glacier=geoMB_per_glacier,\n",
    "    gl_area=gl_area,\n",
    "    test_glaciers=TEST_GLACIERS,\n",
    "    path_predictions=PATH_PREDICTIONS_LSTM_simple,  # or another path if needed\n",
    "    cfg=cfg)\n",
    "\n",
    "df_lstm_two_heads = process_geodetic_mass_balance_comparison(\n",
    "    glacier_list=glacier_list,\n",
    "    path_SMB_GLAMOS_csv=cfg.dataPath + path_SMB_GLAMOS_csv,\n",
    "    periods_per_glacier=periods_per_glacier,\n",
    "    geoMB_per_glacier=geoMB_per_glacier,\n",
    "    gl_area=gl_area,\n",
    "    test_glaciers=TEST_GLACIERS,\n",
    "    path_predictions=\n",
    "    PATH_PREDICTIONS_LSTM_two_heads,  # or another path if needed\n",
    "    cfg=cfg)\n",
    "\n",
    "df_lstm_no_oggm = process_geodetic_mass_balance_comparison(\n",
    "    glacier_list=glacier_list,\n",
    "    path_SMB_GLAMOS_csv=cfg.dataPath + path_SMB_GLAMOS_csv,\n",
    "    periods_per_glacier=periods_per_glacier,\n",
    "    geoMB_per_glacier=geoMB_per_glacier,\n",
    "    gl_area=gl_area,\n",
    "    test_glaciers=TEST_GLACIERS,\n",
    "    path_predictions=\n",
    "    PATH_PREDICTIONS_LSTM_two_heads,  # or another path if needed\n",
    "    cfg=cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where any required columns are NaN\n",
    "df_all_nn = df_all_nn.dropna(subset=['Geodetic MB', 'MBM MB'])\n",
    "\n",
    "# Compute RMSE and Pearson correlation\n",
    "rmse_nn = root_mean_squared_error(df_all_nn[\"Geodetic MB\"],\n",
    "                                  df_all_nn[\"MBM MB\"])\n",
    "corr_nn = np.corrcoef(df_all_nn[\"Geodetic MB\"], df_all_nn[\"MBM MB\"])[0, 1]\n",
    "\n",
    "# Drop rows where any required columns are NaN\n",
    "df_lstm_simple = df_lstm_simple.dropna(subset=['Geodetic MB', 'MBM MB'])\n",
    "\n",
    "# Compute RMSE and Pearson correlation\n",
    "rmse_lstm_simple = root_mean_squared_error(df_lstm_simple[\"Geodetic MB\"],\n",
    "                                           df_lstm_simple[\"MBM MB\"])\n",
    "corr_lstm_simple = np.corrcoef(df_lstm_simple[\"Geodetic MB\"],\n",
    "                               df_lstm_simple[\"MBM MB\"])[0, 1]\n",
    "\n",
    "# Drop rows where any required columns are NaN\n",
    "df_lstm_two_heads = df_lstm_two_heads.dropna(subset=['Geodetic MB', 'MBM MB'])\n",
    "\n",
    "# Compute RMSE and Pearson correlation\n",
    "rmse_lstm_two_heads = root_mean_squared_error(df_lstm_two_heads[\"Geodetic MB\"],\n",
    "                                              df_lstm_two_heads[\"MBM MB\"])\n",
    "corr_lstm_two_heads = np.corrcoef(df_lstm_two_heads[\"Geodetic MB\"],\n",
    "                                  df_lstm_two_heads[\"MBM MB\"])[0, 1]\n",
    "\n",
    "# Drop rows where any required columns are NaN\n",
    "df_lstm_no_oggm = df_lstm_no_oggm.dropna(subset=['Geodetic MB', 'MBM MB'])\n",
    "\n",
    "# Compute RMSE and Pearson correlation\n",
    "rmse_lstm_no_oggm = root_mean_squared_error(df_lstm_no_oggm[\"Geodetic MB\"],\n",
    "                                              df_lstm_no_oggm[\"MBM MB\"])\n",
    "corr_lstm_no_oggm = np.corrcoef(df_lstm_no_oggm[\"Geodetic MB\"],\n",
    "                                  df_lstm_no_oggm[\"MBM MB\"])[0, 1]\n",
    "\n",
    "print('NN MLP: RMSE = {:.3f}, Corr = {:.3f}'.format(rmse_nn, corr_nn))\n",
    "plot_mbm_vs_geodetic_by_area_bin(df_all_nn,\n",
    "                                 bins=[0, 1, 5, 10, 100, np.inf],\n",
    "                                 labels=['<1', '1-5', '5–10', '>10', '>100'],\n",
    "                                 max_bins=4)\n",
    "print('LSTM simple: RMSE = {:.3f}, Corr = {:.3f}'.format(\n",
    "    rmse_lstm_simple, corr_lstm_simple))\n",
    "plot_mbm_vs_geodetic_by_area_bin(df_lstm_simple,\n",
    "                                 bins=[0, 1, 5, 10, 100, np.inf],\n",
    "                                 labels=['<1', '1-5', '5–10', '>10', '>100'],\n",
    "                                 max_bins=4)\n",
    "print('LSTM two heads: RMSE = {:.3f}, Corr = {:.3f}'.format(\n",
    "    rmse_lstm_two_heads, corr_lstm_two_heads))\n",
    "plot_mbm_vs_geodetic_by_area_bin(df_lstm_two_heads,\n",
    "                                 bins=[0, 1, 5, 10, 100, np.inf],\n",
    "                                 labels=['<1', '1-5', '5–10', '>10', '>100'],\n",
    "                                 max_bins=4)\n",
    "\n",
    "print('LSTM no_oggm: RMSE = {:.3f}, Corr = {:.3f}'.format(\n",
    "    rmse_lstm_no_oggm, corr_lstm_no_oggm))\n",
    "plot_mbm_vs_geodetic_by_area_bin(df_lstm_no_oggm,\n",
    "                                 bins=[0, 1, 5, 10, 100, np.inf],\n",
    "                                 labels=['<1', '1-5', '5–10', '>10', '>100'],\n",
    "                                 max_bins=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
