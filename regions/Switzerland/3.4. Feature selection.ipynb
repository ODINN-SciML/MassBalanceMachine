{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from calendar import month_abbr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import copy\n",
    "import concurrent.futures\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.xgb_helpers import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig(numJobs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'\n",
    "\n",
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "rgi_df = pd.read_csv(path_glacier_ids, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    # \"aspect\", # OGGM\n",
    "    # \"slope\", # OGGM\n",
    "    \"aspect_sgi\",  # SGI\n",
    "    \"slope_sgi\",  # SGI\n",
    "    \"hugonnet_dhdt\",  # OGGM\n",
    "    \"consensus_ice_thickness\",  # OGGM\n",
    "    \"millan_v\",  # OGGM\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = pd.read_csv(path_PMB_GLAMOS_csv + 'CH_wgms_dataset_all.csv')\n",
    "\n",
    "print('Number of glaciers:', len(data_glamos['GLACIER'].unique()))\n",
    "print('Number of winter and annual samples:', len(data_glamos))\n",
    "print('Number of annual samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'winter']))\n",
    "\n",
    "# Capitalize glacier names:\n",
    "glacierCap = {}\n",
    "for gl in data_glamos['GLACIER'].unique():\n",
    "    if isinstance(gl, str):  # Ensure the glacier name is a string\n",
    "        if gl.lower() == 'claridenu':\n",
    "            glacierCap[gl] = 'Clariden_U'\n",
    "        elif gl.lower() == 'claridenl':\n",
    "            glacierCap[gl] = 'Clariden_L'\n",
    "        else:\n",
    "            glacierCap[gl] = gl.capitalize()\n",
    "    else:\n",
    "        print(f\"Warning: Non-string glacier name encountered: {gl}\")\n",
    "\n",
    "data_glamos.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glaciers with data of potential clear sky radiation\n",
    "# Format to same names as stakes:\n",
    "glDirect = np.sort([\n",
    "    re.search(r'xr_direct_(.*?)\\.nc', f).group(1)\n",
    "    for f in os.listdir(path_pcsr + 'csv/')\n",
    "])\n",
    "\n",
    "restgl = np.sort(Diff(list(glDirect), list(data_glamos.GLACIER.unique())))\n",
    "\n",
    "print('Glaciers with potential clear sky radiation data:\\n', glDirect)\n",
    "print('Number of glaciers:', len(glDirect))\n",
    "print('Glaciers without potential clear sky radiation data:\\n', restgl)\n",
    "\n",
    "# Filter out glaciers without data:\n",
    "data_glamos = data_glamos[data_glamos.GLACIER.isin(glDirect)]\n",
    "\n",
    "# Look at the data of the ERA5 dataset:\n",
    "xr.open_dataset(path_ERA5_raw + 'era5_monthly_averaged_data.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:\n",
    "### Input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data': path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data': path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': path_pcsr + 'csv/'\n",
    "}\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data(run_flag=RUN,\n",
    "                                     data_glamos=data_glamos,\n",
    "                                     paths=paths,\n",
    "                                     cfg=cfg,\n",
    "                                     vois_climate=vois_climate,\n",
    "                                     vois_topographical=vois_topographical)\n",
    "data_monthly = dataloader_gl.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_glaciers = [\n",
    "#     'tortin', 'plattalva', 'sanktanna', 'schwarzberg', 'hohlaub', 'pizol',\n",
    "#     'corvatsch', 'tsanfleuron', 'forno'\n",
    "# ]\n",
    "\n",
    "test_glaciers = ['']\n",
    "\n",
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in test_glaciers]\n",
    "\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(test_glaciers)]\n",
    "print('Size of test data:', len(data_test))\n",
    "\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of train data:', len(data_train))\n",
    "\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "# Number of annual versus winter measurements:\n",
    "print('Train:')\n",
    "print('Number of winter and annual samples:', len(data_train))\n",
    "print('Number of annual samples:',\n",
    "      len(data_train[data_train.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_train[data_train.PERIOD == 'winter']))\n",
    "\n",
    "# Same for test\n",
    "data_test_annual = data_test[data_test.PERIOD == 'annual']\n",
    "data_test_winter = data_test[data_test.PERIOD == 'winter']\n",
    "\n",
    "print('Test:')\n",
    "print('Number of winter and annual samples:', len(data_test))\n",
    "print('Number of annual samples:', len(data_test_annual))\n",
    "print('Number of winter samples:', len(data_test_winter))\n",
    "\n",
    "print('Total:')\n",
    "print('Number of monthly rows:', len(dataloader_gl.data))\n",
    "print('Number of annual rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'annual']))\n",
    "print('Number of winter rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'winter']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CV splits:\n",
    "\n",
    "Split on IDs during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_splits, test_set, train_set = get_cv_splits(dataloader_gl,\n",
    "                                             test_split_on='GLACIER',\n",
    "                                             test_splits=test_glaciers,\n",
    "                                             random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))\n",
    "\n",
    "visualiseSplits(test_set['y'], train_set['y'], cv_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]\n",
    "\n",
    "param_init = {}\n",
    "param_init['device'] = 'cuda:0'\n",
    "param_init['tree_method'] = 'hist'\n",
    "param_init[\"random_state\"] = cfg.seed\n",
    "param_init[\"n_jobs\"] = cfg.numJobs\n",
    "custom_params = {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 800}\n",
    "\n",
    "params = {**param_init, **custom_params}\n",
    "custom_model = mbm.models.CustomXGBoostRegressor(cfg, **params)\n",
    "\n",
    "# Feature columns:\n",
    "feature_columns = [\n",
    "    'ELEVATION_DIFFERENCE'\n",
    "] + list(vois_climate) + list(vois_topographical) + ['pcsr']\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "print('Shape of training dataset:', train_set['df_X'][all_columns].shape)\n",
    "print('Shape of testing dataset:', test_set['df_X'][all_columns].shape)\n",
    "print('Running with features:', feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_predict(custom_model, train_set, cv_splits, all_columns):\n",
    "    \"\"\"\n",
    "    Fits the given model on training splits and evaluates on validation splits.\n",
    "\n",
    "    Parameters:\n",
    "    - custom_model: The machine learning model to fit and evaluate.\n",
    "    - train_set: Dictionary containing 'y' (target) and 'df_X' (features).\n",
    "    - cv_splits: List of (train_idx, val_idx) tuples for cross-validation.\n",
    "    - all_columns: List of feature column names to use.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary containing mean and standard deviation of validation and training scores.\n",
    "    \"\"\"\n",
    "    val_score, train_score = [], []\n",
    "\n",
    "    for i, (train_idx, val_idx) in enumerate(cv_splits):\n",
    "        if i > 3: # Only run on first 3 splits for speed\n",
    "            break\n",
    "        target_train = train_set['y'][train_idx]\n",
    "        target_val = train_set['y'][val_idx]\n",
    "        input_train = train_set['df_X'].iloc[train_idx][all_columns]\n",
    "        input_val = train_set['df_X'].iloc[val_idx][all_columns]\n",
    "\n",
    "        # Fit on train folds:\n",
    "        custom_model.fit(input_train, target_train)\n",
    "\n",
    "        # Ensure model is set to CPU\n",
    "        custom_model = custom_model.set_params(device='cpu')\n",
    "\n",
    "        # Calculate scores on validation and training folds:\n",
    "        val_score.append(np.abs(custom_model.score(input_val, target_val)))\n",
    "        train_score.append(\n",
    "            np.abs(custom_model.score(input_train, target_train)))\n",
    "\n",
    "    return {\n",
    "        \"mean_val_score\": np.mean(val_score),\n",
    "        \"std_val_score\": np.std(val_score),\n",
    "        \"mean_train_score\": np.mean(train_score),\n",
    "        \"std_train_score\": np.std(train_score),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One round only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_analysis(custom_model,\n",
    "                                train_set,\n",
    "                                cv_splits,\n",
    "                                feature_columns,\n",
    "                                grouped_features=None):\n",
    "    \"\"\"\n",
    "    Evaluates the importance of each feature by removing it one at a time and calculating the impact on model performance.\n",
    "\n",
    "    Parameters:\n",
    "    - custom_model: The machine learning model to fit and evaluate.\n",
    "    - train_set: Dictionary containing 'y' (target) and 'df_X' (features).\n",
    "    - cv_splits: List of (train_idx, val_idx) tuples for cross-validation.\n",
    "    - feature_columns: List of feature column names.\n",
    "    - grouped_features: List of feature names that must always be included/excluded together.\n",
    "    - parallel: Boolean, whether to run evaluations in parallel.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with scores for each feature removed.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    features_to_remove = [\n",
    "        f for f in feature_columns\n",
    "        if not grouped_features or f not in grouped_features\n",
    "    ]\n",
    "\n",
    "    def evaluate_without_feature(feature):\n",
    "        \"\"\"Helper function to evaluate the model without a specific feature or feature group.\"\"\"\n",
    "        reduced_features = [f for f in feature_columns if f != feature]\n",
    "\n",
    "        # Ensure grouped features are removed together\n",
    "        if grouped_features and feature in grouped_features:\n",
    "            reduced_features = [\n",
    "                f for f in feature_columns if f not in grouped_features\n",
    "            ]\n",
    "\n",
    "        reduced_columns = reduced_features + cfg.fieldsNotFeatures\n",
    "\n",
    "        # Clone the model to avoid contamination\n",
    "        model_copy = copy.deepcopy(custom_model)\n",
    "        model_copy = model_copy.set_params(\n",
    "            device=\"cuda:0\")  # Ensure it remains on GPU\n",
    "\n",
    "        # Evaluate model performance with reduced feature set\n",
    "        scores = cross_val_predict(model_copy, train_set, cv_splits,\n",
    "                                   reduced_columns)\n",
    "        scores[\n",
    "            \"removed_feature\"] = feature if feature not in grouped_features else \"Group: \" + \", \".join(\n",
    "                grouped_features)\n",
    "\n",
    "        # Explicitly delete model copy to free memory\n",
    "        del model_copy\n",
    "        gc.collect()  # Trigger garbage collection\n",
    "\n",
    "        return scores\n",
    "\n",
    "    print(\"\\nStarting feature importance analysis...\\n\")\n",
    "\n",
    "    # Run evaluations in parallel or sequentially\n",
    "    features_to_evaluate = features_to_remove + ([grouped_features[0]]\n",
    "                                                 if grouped_features else [])\n",
    "\n",
    "    for feature in tqdm(features_to_evaluate, desc=\"Evaluating Features\"):\n",
    "        results.append(evaluate_without_feature(feature))\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    # Identify the feature/group whose removal led to the highest validation score (least useful)\n",
    "    least_important_feature = df_results.loc[\n",
    "        df_results[\"mean_val_score\"].idxmax(), \"removed_feature\"]\n",
    "    print(\n",
    "        f\"\\nLeast useful feature (highest validation score when removed): {least_important_feature}\"\n",
    "    )\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define grouped features that should always be removed together\n",
    "# grouped_features = [\"hugonnet_dhdt\", \"consensus_ice_thickness\", \"millan_v\"]\n",
    "\n",
    "# # Run feature importance analysis\n",
    "# df_feature_importance = feature_importance_analysis(custom_model, train_set,\n",
    "#                                                     cv_splits, feature_columns,\n",
    "#                                                     grouped_features)\n",
    "\n",
    "# # Save results:\n",
    "# df_feature_importance.to_csv('results/feature_importance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_feature_elimination(custom_model,\n",
    "                                  train_set,\n",
    "                                  cv_splits,\n",
    "                                  feature_columns,\n",
    "                                  grouped_features=None,\n",
    "                                  min_features=5):\n",
    "    \"\"\"\n",
    "    Iteratively removes the least important feature (or group of features) until only `min_features` remain.\n",
    "\n",
    "    Parameters:\n",
    "    - custom_model: The machine learning model to fit and evaluate.\n",
    "    - train_set: Dictionary containing 'y' (target) and 'df_X' (features).\n",
    "    - cv_splits: List of (train_idx, val_idx) tuples for cross-validation.\n",
    "    - feature_columns: List of feature column names.\n",
    "    - grouped_features: List of feature names that must always be included/excluded together.\n",
    "    - parallel: Boolean, whether to run evaluations in parallel.\n",
    "    - min_features: The minimum number of features to retain.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with scores and feature elimination history.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a working copy of feature columns\n",
    "    remaining_features = feature_columns.copy()\n",
    "    elimination_results = []\n",
    "\n",
    "    print(\"\\nStarting iterative feature elimination...\\n\")\n",
    "\n",
    "    round_nb = 1\n",
    "    while len(remaining_features) > min_features:\n",
    "        results = []\n",
    "        features_to_remove = [\n",
    "            f for f in remaining_features\n",
    "            if not grouped_features or f not in grouped_features\n",
    "        ]\n",
    "        print('\\nRemaining features:', remaining_features)\n",
    "\n",
    "        def evaluate_without_feature(feature):\n",
    "            print('Evaluate without ', feature)\n",
    "            \"\"\"Evaluates the model without a specific feature or feature group.\"\"\"\n",
    "            reduced_features = [f for f in remaining_features if f != feature]\n",
    "\n",
    "            # Ensure grouped features are removed together\n",
    "            if grouped_features and feature in grouped_features:\n",
    "                reduced_features = [\n",
    "                    f for f in remaining_features if f not in grouped_features\n",
    "                ]\n",
    "\n",
    "            reduced_columns = reduced_features + cfg.fieldsNotFeatures\n",
    "\n",
    "            # Clone and reset model to ensure fresh training\n",
    "            model_copy = copy.deepcopy(custom_model)\n",
    "            model_copy = model_copy.set_params(\n",
    "                device=\"cuda:0\")  # Ensure it stays on GPU\n",
    "\n",
    "            # Evaluate model performance with reduced feature set\n",
    "            scores = cross_val_predict(model_copy, train_set, cv_splits,\n",
    "                                    reduced_columns)\n",
    "            scores[\n",
    "                \"removed_feature\"] = feature if feature not in grouped_features else \"Group: \" + \", \".join(\n",
    "                    grouped_features)\n",
    "\n",
    "            # Explicitly delete model copy to free memory\n",
    "            del model_copy\n",
    "            gc.collect()  # Trigger garbage collection\n",
    "\n",
    "            return scores\n",
    "\n",
    "        features_to_evaluate = features_to_remove + ([grouped_features[0]] if\n",
    "                                                     grouped_features else [])\n",
    "        for feature in tqdm(\n",
    "                    features_to_evaluate,\n",
    "                    desc=\n",
    "                    f\"Evaluating Features (Remaining: {len(remaining_features)})\"\n",
    "            ):\n",
    "                results.append(evaluate_without_feature(feature))\n",
    "\n",
    "        # Convert results to DataFrame\n",
    "        df_results = pd.DataFrame(results)\n",
    "        df_results['remaining_feature'] = remaining_features\n",
    "        df_results['Round'] = round_nb\n",
    "\n",
    "        # Identify the least important feature (highest validation score when removed)\n",
    "        least_important_feature = df_results.loc[\n",
    "            df_results[\"mean_val_score\"].idxmax(), \"removed_feature\"]\n",
    "        print(f\"\\nRemoving least important feature: {least_important_feature}\")\n",
    "\n",
    "        # Store elimination history\n",
    "        elimination_results.append(df_results)\n",
    "\n",
    "        # Remove feature(s) from the remaining feature list\n",
    "        if least_important_feature.startswith(\"Group: \"):\n",
    "            removed_features = least_important_feature.replace(\"Group: \",\n",
    "                                                               \"\").split(\", \")\n",
    "            remaining_features = [\n",
    "                f for f in remaining_features if f not in removed_features\n",
    "            ]\n",
    "        else:\n",
    "            remaining_features.remove(least_important_feature)\n",
    "            \n",
    "        round_nb += 1\n",
    "\n",
    "    # Compile full elimination history\n",
    "    full_elimination_df = pd.concat(elimination_results, ignore_index=True)\n",
    "\n",
    "    return full_elimination_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grouped features that should always be removed together\n",
    "grouped_features = [\"hugonnet_dhdt\", \"consensus_ice_thickness\", \"millan_v\"]\n",
    "\n",
    "# Run iterative feature elimination\n",
    "df_elimination_history = iterative_feature_elimination(custom_model, train_set,\n",
    "                                                       cv_splits,\n",
    "                                                       feature_columns,\n",
    "                                                       grouped_features)\n",
    "\n",
    "df_elimination_history.to_csv('results/df_elimination_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    'ELEVATION_DIFFERENCE'\n",
    "] + list(vois_climate) + list(vois_topographical) + ['pcsr']\n",
    "grouped_features = [\"hugonnet_dhdt\", \"consensus_ice_thickness\", \"millan_v\"]\n",
    "\n",
    "df_elimination_history = pd.read_csv('results/df_elimination_history.csv')\n",
    "df_elimination_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
