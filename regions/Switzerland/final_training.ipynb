{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Notebook for final training of models\n",
    "\n",
    "Model combinations:\n",
    "Features: Temperature+precipitation and basic energy balance\n",
    "CV: Random 5-fold cross validation and blocking-by-glacier\n",
    "\n",
    "Each of the four combinations above are trained with four different targets: Annual+seasonal, annual, summer and winter\n",
    "Total of 16 model combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 0.1 Import libraries and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from matplotlib import pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "from model_functions import select_variables\n",
    "from model_functions import train_xgb_model\n",
    "from plotting_functions import plot_gsearch_results\n",
    "from plotting_functions import plot_prediction_per_fold\n",
    "from plotting_functions import plot_prediction\n",
    "from plotting_functions import plot_prediction_subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_subplot(y1, y2, data_type:str, ax, n_toplot=10**10, fold=False):\n",
    "    \"\"\"\n",
    "    Plot model predictions y1 vs. actual observations y2 and show\n",
    "    calculated error metrics.\n",
    "\n",
    "    Parameters:\n",
    "    y1 : np.array\n",
    "        Predicted labels.\n",
    "    y2 : np.array\n",
    "        Actual labels.\n",
    "    data_type : str\n",
    "        Type of data, e.g. \"Validation\" or \"Test\".\n",
    "    ax : array\n",
    "        Axis object\n",
    "    n_toplot : int\n",
    "        Number of points to plot. \n",
    "    \"\"\"\n",
    "    \n",
    "    from scipy.stats import gaussian_kde\n",
    "    from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "    if fold:\n",
    "        figsize=(5,5)\n",
    "        fontsize=12\n",
    "        s= 15\n",
    "    else:\n",
    "        figsize=(8,8)\n",
    "        fontsize=16\n",
    "        s= 20\n",
    "    \n",
    "    idxs = np.arange(len(y1))\n",
    "    np.random.shuffle(idxs)\n",
    "\n",
    "    y_max = 8#7 #max(max(y1), max(y2))[0] + 1\n",
    "    y_min = -15#1 #min(min(y1), min(y2))[0] - 1\n",
    "    \n",
    "    y_expected = y1.reshape(-1)[idxs[:n_toplot]]\n",
    "    y_predicted = y2.reshape(-1)[idxs[:n_toplot]]\n",
    "\n",
    "    xy = np.vstack([y_expected, y_predicted])\n",
    "    z = gaussian_kde(xy)(xy)\n",
    "    # Sort the points by density, so that the densest points are plotted last\n",
    "    idx = z.argsort()\n",
    "    y_plt, ann_plt, z = y_expected[idx], y_predicted[idx], z[idx]\n",
    "    \n",
    "    #fig = plt.figure(figsize=figsize)\n",
    "    sc = ax.scatter(y_plt, ann_plt, c=z, s=s)\n",
    "    sc.set_clim(0,0.2)\n",
    "    #plt.tick_params(labelsize=14)\n",
    "    plt.colorbar(sc,ax=ax,fraction=0.046) \n",
    "    lineStart = y_min\n",
    "    lineEnd = y_max\n",
    "    ax.plot([lineStart, lineEnd], [lineStart, lineEnd], 'k-')\n",
    "    ax.set_title(\"Model Evaluation \" + data_type, fontsize=fontsize)\n",
    "    ax.set_ylabel('Modeled SMB (m.w.e)', fontsize=fontsize)\n",
    "    ax.set_xlabel('Reference SMB (m.w.e)', fontsize=fontsize)\n",
    "    ax.axvline(0.0, ls='-.', c='k')\n",
    "    ax.axhline(0.0, ls='-.', c='k')\n",
    "    ax.set_xlim(lineStart, lineEnd)\n",
    "    ax.set_ylim(lineStart, lineEnd)\n",
    "    ax.set_box_aspect(1)\n",
    "    \n",
    "    textstr = '\\n'.join((\n",
    "    r'$RMSE=%.2f$' % (mean_squared_error(y_expected, y_predicted, squared=False), ),\n",
    "    r'$MSE=%.2f$' % (mean_squared_error(y_expected, y_predicted, squared=True), ),\n",
    "    r'$R^2=%.2f$' % (r2_score(y_expected, y_predicted), )))\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    # place a text box in upper left in axes coords\n",
    "    ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n",
    "            verticalalignment='top', bbox=props)\n",
    "    \n",
    "    return ax\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_per_season(X_train_s, y_train_s, splits_s, best_model, months=12):\n",
    "    y_pred_list = []\n",
    "    y_test_list = []\n",
    "    i=0\n",
    "\n",
    "    for train_index, test_index in splits_s:\n",
    "        # Loops over n_splits iterations and gets train and test splits in each fold\n",
    "        X_train, X_test = X_train_s[train_index], X_train_s[test_index]\n",
    "        y_train, y_test = y_train_s[train_index], y_train_s[test_index]\n",
    "        best_model.fit(X_train, y_train)\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        indices = np.argwhere((X_test == months))[:,0]\n",
    "        y_test_crop = y_test[indices]\n",
    "        y_pred_crop = y_pred[indices]\n",
    "\n",
    "        y_test_list.extend(y_test_crop)\n",
    "        y_pred_list.extend(y_pred_crop)\n",
    "\n",
    "        i=i+1\n",
    "\n",
    "    # Arrays of predictions and observations for each fold\n",
    "    y_test_all = np.hstack([*y_test_list])\n",
    "    y_pred_all = np.hstack([*y_pred_list])\n",
    "\n",
    "    return y_test_all, y_pred_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_per_season_test(X_test, y_test, best_model, months=12):\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    indices = np.argwhere((X_test == months))[:,0]\n",
    "    y_test_crop = y_test[indices]\n",
    "    y_pred_crop = y_pred[indices]\n",
    "\n",
    "    return y_test_crop, y_pred_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_per_season(y_test_all, y_pred_all, season='Annual'):\n",
    "    \n",
    "    figsize=(5,5)\n",
    "    fontsize=16\n",
    "    s= 20\n",
    "    n_toplot=10**10\n",
    "    \n",
    "    idxs = np.arange(len(y_test_all))\n",
    "    np.random.shuffle(idxs)\n",
    "\n",
    "    y_max = 8#7 #max(max(y1), max(y2))[0] + 1\n",
    "    y_min = -15#1 #min(min(y1), min(y2))[0] - 1\n",
    "    \n",
    "    y_expected = y_test_all.reshape(-1)[idxs[:n_toplot]]\n",
    "    y_predicted = y_pred_all.reshape(-1)[idxs[:n_toplot]]\n",
    "\n",
    "    xy = np.vstack([y_expected, y_predicted])\n",
    "    z = gaussian_kde(xy)(xy)\n",
    "    # Sort the points by density, so that the densest points are plotted last\n",
    "    idx = z.argsort()\n",
    "    y_plt, ann_plt, z = y_expected[idx], y_predicted[idx], z[idx]\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize, dpi=100)\n",
    "    plt.title(season + \" mass balance\", fontsize=20)\n",
    "    plt.ylabel('Modeled mass balance (m w.e)', fontsize=fontsize)\n",
    "    plt.xlabel('Observed mass balance (m w.e)', fontsize=fontsize)\n",
    "    sc = plt.scatter(y_plt, ann_plt, c=z, s=s)\n",
    "    plt.clim(0,0.4)\n",
    "    plt.tick_params(labelsize=14)\n",
    "    #plt.colorbar(sc) \n",
    "    lineStart = y_min\n",
    "    lineEnd = y_max\n",
    "    plt.plot([lineStart, lineEnd], [lineStart, lineEnd], 'k-')\n",
    "    plt.axvline(0.0, ls='-.', c='k')\n",
    "    plt.axhline(0.0, ls='-.', c='k')\n",
    "    plt.xlim(lineStart, lineEnd)\n",
    "    plt.ylim(lineStart, lineEnd)\n",
    "    plt.gca().set_box_aspect(1)\n",
    "    \n",
    "    textstr = '\\n'.join((\n",
    "    r'$RMSE=%.2f$' % (mean_squared_error(y_expected, y_predicted, squared=False), ),\n",
    "    r'$MSE=%.2f$' % (mean_squared_error(y_expected, y_predicted, squared=True), ),\n",
    "    r'$MAE=%.2f$' % (mean_absolute_error(y_expected, y_predicted), ),\n",
    "    r'$R^2=%.2f$' % (r2_score(y_expected, y_predicted), )))\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    # place a text box in upper left in axes coords\n",
    "    plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=14,\n",
    "            verticalalignment='top', bbox=props)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot permutation importance\n",
    "def plot_permutation_importance(df_train_X_s, X_train_s, y_train_s, splits_s, best_model, max_features_plot = 10):\n",
    "\n",
    "    fig, ax = plt.subplots(1,5, figsize=(30,10))\n",
    "    a = 0    \n",
    "    for train_index, test_index in splits_s:\n",
    "        # Loops over n_splits iterations and gets train and test splits in each fold\n",
    "        X_train, X_test = X_train_s[train_index], X_train_s[test_index]\n",
    "        y_train, y_test = y_train_s[train_index], y_train_s[test_index]\n",
    "\n",
    "        best_model.fit(X_train, y_train)\n",
    "    \n",
    "        result = permutation_importance(best_model, X_train, y_train, n_repeats=20, random_state=42, n_jobs=10)\n",
    "\n",
    "        sorted_idx = result.importances_mean.argsort()\n",
    "        labels = np.array(df_train_X_s.columns)[sorted_idx][-max_features_plot:]\n",
    "    \n",
    "        ax[a].boxplot(result.importances[sorted_idx].T[:,-max_features_plot:], vert=False, labels=labels)\n",
    "        ax[a].set_title(\"Permutation Importance Fold \" + str(a))\n",
    "\n",
    "        a=a+1\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 0.2 Import data and select test glaciers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify filepaths and filenames.\n",
    "loc = 'local'\n",
    "\n",
    "if loc == 'cryocloud':\n",
    "    filepath = '/home/jovyan/ML_MB_Norway_data/'\n",
    "elif loc == 'local':\n",
    "    filepath = 'Data/'\n",
    "\n",
    "filename = '2023-08-28_stake_mb_norway_cleaned_ids_latlon_wattributes_climate.csv'\n",
    "\n",
    "# Load data.\n",
    "data = pd.read_csv(filepath + filename)\n",
    "\n",
    "# Add year column\n",
    "data['year']=pd.to_datetime(data['dt_curr_year_max_date'].astype('string'), format=\"%d.%m.%Y %H:%M\")\n",
    "data['year'] = data.year.dt.year.astype('Int64')\n",
    "\n",
    "# Remove cells with nan in balance_netto.\n",
    "glacier_data_annual = data[data['balance_netto'].notna()]\n",
    "glacier_data_annual.reset_index(drop=True, inplace=True)\n",
    "\n",
    "glacier_data_winter = data[data['balance_winter'].notna()]\n",
    "glacier_data_winter.reset_index(drop=True, inplace=True)\n",
    "\n",
    "glacier_data_summer = data[data['balance_summer'].notna()]\n",
    "glacier_data_summer.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually select test glaciers (14 glaciers)\n",
    "test_glaciers = [54, 703, 941, 1135, 1280, 2085, 2320, 2478, 2768, 2769, 3133, 3137, 3138, 3141]\n",
    "\n",
    "# Get test dataset for each of annual, winter and summer mass balance\n",
    "df_test_annual = glacier_data_annual[glacier_data_annual['BREID'].isin(test_glaciers)]\n",
    "df_test_winter = glacier_data_winter[glacier_data_winter['BREID'].isin(test_glaciers)]\n",
    "df_test_summer = glacier_data_summer[glacier_data_summer['BREID'].isin(test_glaciers)]\n",
    "# 54 has 189 points\n",
    "# 703 has 30 points\n",
    "# 941 has 70 points\n",
    "# 1280 has 71 points\n",
    "# 2320 has 83 points\n",
    "# 2478 has 89 points\n",
    "# 2769 has 121 points\n",
    "# 3133 has 38 points\n",
    "# 3137 has 65 points\n",
    "# 3138 has 6 points\n",
    "# 3141 has 72 points\n",
    "\n",
    "# Get training dataset for each of annual, winter and summer mass balance\n",
    "df_train_annual = glacier_data_annual[~glacier_data_annual['BREID'].isin(test_glaciers)]\n",
    "df_train_winter = glacier_data_winter[~glacier_data_winter['BREID'].isin(test_glaciers)]\n",
    "df_train_summer = glacier_data_summer[~glacier_data_summer['BREID'].isin(test_glaciers)]\n",
    "\n",
    "# Add number of months to each dataframe\n",
    "df_train_annual['n_months']=12\n",
    "df_train_winter['n_months']=8\n",
    "df_train_summer['n_months']=6\n",
    "df_test_annual['n_months']=12\n",
    "df_test_winter['n_months']=8\n",
    "df_test_summer['n_months']=6\n",
    "\n",
    "print(f'Train/test annual: {len(df_train_annual)}/{len(df_test_annual)}')\n",
    "print(f'Train/test winter: {len(df_train_winter)}/{len(df_test_winter)}')\n",
    "print(f'Train/test summer: {len(df_train_summer)}/{len(df_test_summer)}')\n",
    "print(f'All train/test: {len(df_train_annual)+len(df_train_winter)+len(df_train_summer)} / {len(df_test_annual)+len(df_test_winter)+len(df_test_summer)}')\n",
    "print(f'Fraction train/test: {(len(df_train_annual)+len(df_train_winter)+len(df_train_summer)) / (len(df_test_annual)+len(df_test_winter)+len(df_test_summer)+len(df_train_annual)+len(df_train_winter)+len(df_train_summer))} / {(len(df_test_annual)+len(df_test_winter)+len(df_test_summer)) /(len(df_test_annual)+len(df_test_winter)+len(df_test_summer) + len(df_train_annual)+len(df_train_winter)+len(df_train_summer))}')\n",
    "print(f'Total entries: {len(df_test_annual)+len(df_test_winter)+len(df_test_summer) + len(df_train_annual)+len(df_train_winter)+len(df_train_summer)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_winter[df_test_winter['BREID'] == 2768].balance_winter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 0.3 Distribution of training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of measurements by topography\n",
    "temp_cols = ['t2m_oct','t2m_nov','t2m_dec','t2m_jan','t2m_feb','t2m_mar','t2m_apr','t2m_may','t2m_jun','t2m_jul','t2m_aug','t2m_sep']\n",
    "prec_cols = ['tp_oct','tp_nov','tp_dec','tp_jan','tp_feb','tp_mar','tp_apr','tp_may','tp_jun','tp_jul','tp_aug','tp_sep']\n",
    "\n",
    "f, ax = plt.subplots(2, 5, figsize=(12,6), sharey=True, sharex='col')\n",
    "df_train_annual['balance_netto'].plot.hist(bins=50, ax=ax[0,0])\n",
    "ax[0,0].set_title('mass balance')\n",
    "ax[0,0].set_ylabel('frequency (train)')\n",
    "df_train_annual['altitude'].plot.hist(bins=50, ax=ax[0,1])\n",
    "ax[0,1].set_title('altitude')\n",
    "df_train_annual['year'].plot.hist(bins=50, ax=ax[0,2])\n",
    "ax[0,2].set_title('years')\n",
    "df_train_annual[temp_cols].mean(axis=1).plot.hist(bins=50, ax=ax[0,3])\n",
    "ax[0,3].set_title('mean temp')\n",
    "df_train_annual[prec_cols].sum(axis=1).plot.hist(bins=50, ax=ax[0,4])\n",
    "ax[0,4].set_title('prec sum')\n",
    "\n",
    "df_test_annual['balance_netto'].plot.hist(bins=50, ax=ax[1,0])\n",
    "ax[1,0].set_ylabel('frequency (test)')\n",
    "ax[1,0].set_xlabel('mass_balance')\n",
    "df_test_annual['altitude'].plot.hist(bins=50, ax=ax[1,1])\n",
    "ax[1,1].set_xlabel('altitude')\n",
    "df_test_annual['year'].plot.hist(bins=50, ax=ax[1,2])\n",
    "ax[1,2].set_xlabel('year')\n",
    "df_test_annual[temp_cols].mean(axis=1).plot.hist(bins=50, ax=ax[1,3])\n",
    "ax[1,3].set_xlabel('temp (K)')\n",
    "df_test_annual[prec_cols].sum(axis=1).plot.hist(bins=50, ax=ax[1,4])\n",
    "ax[1,4].set_xlabel('prec sum (m?)')\n",
    "\n",
    "for row in ax:\n",
    "    for a in row:\n",
    "        a.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "T_0 = 273.15\n",
    "print(f\"Train mass balance: median {df_train_annual['balance_netto'].median()}, min {df_train_annual['balance_netto'].min()}, max {df_train_annual['balance_netto'].max()}.\")\n",
    "print(f\"Test mass balance: median {df_test_annual['balance_netto'].median()}, min {df_test_annual['balance_netto'].min()}, max {df_test_annual['balance_netto'].max()}.\")\n",
    "print(f\"Train altitude: mean {df_train_annual['altitude'].mean()}, min {df_train_annual['altitude'].min()}, max {df_train_annual['altitude'].max()}.\")\n",
    "print(f\"Test altitude: mean {df_test_annual['altitude'].mean()}, min {df_test_annual['altitude'].min()}, max {df_test_annual['altitude'].max()}.\")\n",
    "print(f\"Train mean temp: mean {df_train_annual[temp_cols].mean().mean()-T_0}, min {df_train_annual[temp_cols].mean(axis=1).min()-T_0}, max {df_train_annual[temp_cols].mean(axis=1).max()-T_0}.\")\n",
    "print(f\"Test mean temp: mean {df_test_annual[temp_cols].mean().mean()-T_0}, min {df_test_annual[temp_cols].mean(axis=1).min()-T_0}, max {df_test_annual[temp_cols].mean(axis=1).max()-T_0}.\")\n",
    "print(f\"Train prec sum: mean {df_train_annual[prec_cols].sum(axis=1).mean()}, min {df_train_annual[prec_cols].sum(axis=1).min()}, max {df_train_annual[prec_cols].sum(axis=1).max()}.\")\n",
    "print(f\"Test prec sum: mean {df_test_annual[prec_cols].sum(axis=1).mean()}, min {df_test_annual[prec_cols].sum(axis=1).min()}, max {df_test_annual[prec_cols].sum(axis=1).max()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### 0.4 Training set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter ranges.\n",
    "param_ranges = {'max_depth': [2, 3, 4, 5, 6, 7, 8],\n",
    "          'n_estimators': [50, 100, 200, 300, 400, 500], # number of trees (too many = overfitting, too few = underfitting)\n",
    "          'learning_rate': [0.01, 0.1, 0.15, 0.2, 0.25, 0.3]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 1.0 Model 1: Temp+prec with 5-fold random CV\n",
    "\n",
    "Features: Monthly temperature and precipitation <br>\n",
    "CV-strategy: 5-fold random cross-validation <br>\n",
    "Targets: <br>\n",
    "1.1: Annual and seasonal observations <br>\n",
    "1.2: Only annual mass balance <br>\n",
    "1.3: Only summer mass balance <br>\n",
    "1.4: Only winter mass balance <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "base_cols = ['BREID','altitude','aspect','slope','altitude_climate']\n",
    "temp_cols = ['t2m_oct','t2m_nov','t2m_dec','t2m_jan','t2m_feb','t2m_mar','t2m_apr','t2m_may','t2m_jun','t2m_jul','t2m_aug','t2m_sep']\n",
    "prec_cols = ['tp_oct','tp_nov','tp_dec','tp_jan','tp_feb','tp_mar','tp_apr','tp_may','tp_jun','tp_jul','tp_aug','tp_sep']\n",
    "label_cols = ['balance_netto']\n",
    "\n",
    "# Select features for training\n",
    "df_train_annual_clean = select_variables(df_train_annual, base_cols, temp_cols, prec_cols, ['balance_netto','n_months'])\n",
    "df_train_winter_clean = select_variables(df_train_winter, base_cols, temp_cols, prec_cols, ['balance_winter','n_months'])\n",
    "df_train_summer_clean = select_variables(df_train_summer, base_cols, temp_cols, prec_cols, ['balance_summer','n_months'])\n",
    "\n",
    "# Rename target columns to same name\n",
    "df_train_annual_clean.rename(columns={'balance_netto' : 'balance'}, inplace=True)\n",
    "df_train_winter_clean.rename(columns={'balance_winter' : 'balance'}, inplace=True)\n",
    "df_train_summer_clean.rename(columns={'balance_summer' : 'balance'}, inplace=True)\n",
    "\n",
    "# For summer balance, replace column values in accumulation months with NaN (oct, nov, dec, jan, feb, mar, apr, may)\n",
    "# For winter balance, replace column values in ablation months with NaN (may, jun, jul, aug, sep, oct)\n",
    "var = ['t2m', 'tp']\n",
    "mon_summer = ['may', 'jun', 'jul', 'aug', 'sep', 'oct']\n",
    "mon_winter = ['oct', 'nov', 'dec', 'jan', 'feb', 'mar', 'apr', 'may']\n",
    "\n",
    "for name in var:\n",
    "    for mon in mon_summer:\n",
    "        df_train_winter_clean[name+'_'+mon]= np.nan\n",
    "for name in var:   \n",
    "    for mon in mon_winter:\n",
    "        df_train_summer_clean[name+'_'+mon]= np.nan\n",
    "\n",
    "# Combine all annual, winter and summer data in one dataframe\n",
    "df_train_all = pd.concat([df_train_annual_clean, df_train_summer_clean, df_train_winter_clean])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test dataset\n",
    "\n",
    "# Select features for training\n",
    "df_test_annual_clean = select_variables(df_test_annual, base_cols, temp_cols, prec_cols, ['balance_netto','n_months'])\n",
    "df_test_winter_clean = select_variables(df_test_winter, base_cols, temp_cols, prec_cols, ['balance_winter','n_months'])\n",
    "df_test_summer_clean = select_variables(df_test_summer, base_cols, temp_cols, prec_cols, ['balance_summer','n_months'])\n",
    "\n",
    "# Rename target columns to same name\n",
    "df_test_annual_clean.rename(columns={'balance_netto' : 'balance'}, inplace=True)\n",
    "df_test_winter_clean.rename(columns={'balance_winter' : 'balance'}, inplace=True)\n",
    "df_test_summer_clean.rename(columns={'balance_summer' : 'balance'}, inplace=True)\n",
    "\n",
    "# For summer balance, replace column values in accumulation months with NaN (oct, nov, dec, jan, feb, mar, apr, may)\n",
    "# For winter balance, replace column values in ablation months with NaN (may, jun, jul, aug, sep, oct)\n",
    "for name in var:\n",
    "    for mon in mon_summer:\n",
    "        df_test_winter_clean[name+'_'+mon]= np.nan\n",
    "for name in var:   \n",
    "    for mon in mon_winter:\n",
    "        df_test_summer_clean[name+'_'+mon]= np.nan\n",
    "\n",
    "# Combine all annual, winter and summer data in one dataframe\n",
    "df_test_all = pd.concat([df_test_annual_clean, df_test_summer_clean, df_test_winter_clean])\n",
    "df_test_all.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### 1.1 Annual and seasonal mass balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training\n",
    "df_train_X = df_train_all.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_train_y = df_train_all[['balance']]\n",
    "\n",
    "# Get arrays of features and targets\n",
    "X_train, y_train = df_train_X.values, df_train_y.values\n",
    "\n",
    "# Set random seed for split\n",
    "rand_seed = 25\n",
    "\n",
    "# Use five folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=rand_seed)\n",
    "splits = list(kf.split(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# Set return_train to False to inhibit return training scores. Default is True.\n",
    "#cv_grid, best_model, cvl_scores = train_xgb_model(X_train, y_train, splits, param_ranges, \n",
    "#                                                  scorer='neg_mean_squared_error', return_train=True)\n",
    "\n",
    "# Save cv-object\n",
    "#joblib.dump(cv_grid, 'Models/Final_training/cv_rand_5fold_temp_prec_ann+seas_w_nmonths.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_grid = joblib.load('Models/Final_training/cv_rand_5fold_temp_prec_ann+seas_w_nmonths.pkl')\n",
    "best_model = cv_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, sharex=True, sharey=True, figsize=(25,25))\n",
    "a = 0    \n",
    "for train_index, test_index in splits:\n",
    "    # Loops over n_splits iterations and gets train and test splits in each fold\n",
    "    X_train_, X_test_ = X_train[train_index], X_train[test_index]\n",
    "    y_train_, y_test_ = y_train[train_index], y_train[test_index]\n",
    "    best_model.fit(X_train_, y_train_)\n",
    "    y_pred = best_model.predict(X_test_)\n",
    "\n",
    "    title = 'Validation fold ' + str(a)\n",
    "\n",
    "    plot_prediction_subplot(y_test_, y_pred, title, ax[a], n_toplot=5000, fold=True)\n",
    "\n",
    "    a=a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gsearch_results(cv_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_permutation_importance(df_train_X, X_train, y_train, splits, best_model, max_features_plot = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_annual, y_pred_annual = get_prediction_per_season(X_train, y_train, splits, best_model, months=12)\n",
    "y_test_winter, y_pred_winter = get_prediction_per_season(X_train, y_train, splits, best_model, months=8)\n",
    "y_test_summer, y_pred_summer = get_prediction_per_season(X_train, y_train, splits, best_model, months=6)\n",
    "\n",
    "print(y_test_annual.shape)\n",
    "print(y_test_winter.shape)\n",
    "print(y_test_summer.shape)\n",
    "\n",
    "y_test_all = np.hstack((y_test_annual, y_test_winter, y_test_summer))\n",
    "y_pred_all = np.hstack((y_pred_annual, y_pred_winter, y_pred_summer))\n",
    "\n",
    "print(y_test_all.shape)\n",
    "print(y_pred_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_per_season(y_test_winter, y_pred_winter, season='Winter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_prediction_per_fold(X_train, y_train, best_model, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "# Select features for training\n",
    "df_test_X = df_test_all.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_test_y = df_test_all[['balance']]\n",
    "\n",
    "X_test, y_test = df_test_X.values, df_test_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = cv_grid.best_estimator_\n",
    "test_model.fit(X_train, y_train)\n",
    "y_pred_test = test_model.predict(X_test)\n",
    "plot_prediction(y_test, y_pred_test, data_type='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_crop, y_pred_crop = get_prediction_per_season_test(X_test, y_test, test_model, months=6)\n",
    "plot_prediction_per_season(y_test_crop, y_pred_crop, season='Summer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### 1.2 Annual mass balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training\n",
    "df_train_X = df_train_annual_clean.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_train_y = df_train_annual_clean[['balance']]\n",
    "\n",
    "# Get arrays of features and targets\n",
    "X_train, y_train = df_train_X.values, df_train_y.values\n",
    "\n",
    "# Set random seed for split\n",
    "rand_seed = 25\n",
    "\n",
    "# Use five folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=rand_seed)\n",
    "splits = list(kf.split(X_train, y_train))\n",
    "\n",
    "# Train model\n",
    "# Set return_train to False to inhibit return training scores. Default is True.\n",
    "#cv_grid, best_model, cvl_scores = train_xgb_model(X_train, y_train, splits, param_ranges, \n",
    "#                                                  scorer='neg_mean_squared_error', return_train=True)\n",
    "\n",
    "# Save cv-object\n",
    "#joblib.dump(cv_grid, 'Models/Final_training/cv_rand_5fold_temp_prec_ann.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_grid = joblib.load('Models/Final_training/cv_rand_5fold_temp_prec_ann.pkl')\n",
    "best_model = cv_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_prediction_per_fold(X_train, y_train, best_model, splits)\n",
    "fig, ax = plt.subplots(1,5, sharex=True, sharey=True, figsize=(25,25))\n",
    "a = 0    \n",
    "for train_index, test_index in splits:\n",
    "    # Loops over n_splits iterations and gets train and test splits in each fold\n",
    "    X_train_, X_test_ = X_train[train_index], X_train[test_index]\n",
    "    y_train_, y_test_ = y_train[train_index], y_train[test_index]\n",
    "    best_model.fit(X_train_, y_train_)\n",
    "    y_pred = best_model.predict(X_test_)\n",
    "\n",
    "    title = 'Validation fold ' + str(a)\n",
    "\n",
    "    plot_prediction_subplot(y_test_, y_pred, title, ax[a], n_toplot=5000, fold=True)\n",
    "\n",
    "    a=a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gsearch_results(cv_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "# Select features for training\n",
    "df_test_X = df_test_annual_clean.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_test_y = df_test_annual_clean[['balance']]\n",
    "\n",
    "X_test_annual, y_test_annual = df_test_X.values, df_test_y.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = cv_grid.best_estimator_\n",
    "test_model.fit(X_train, y_train)\n",
    "y_pred_test_annual = test_model.predict(X_test_annual)\n",
    "plot_prediction(y_test_annual, y_pred_test_annual, data_type='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### 1.3 Summer mass balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training\n",
    "df_train_X = df_train_summer_clean.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_train_y = df_train_summer_clean[['balance']]\n",
    "\n",
    "# Get arrays of features and targets\n",
    "X_train, y_train = df_train_X.values, df_train_y.values\n",
    "\n",
    "# Set random seed for split\n",
    "rand_seed = 25\n",
    "\n",
    "# Use five folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=rand_seed)\n",
    "splits = list(kf.split(X_train, y_train))\n",
    "\n",
    "# Train model\n",
    "# Set return_train to False to inhibit return training scores. Default is True.\n",
    "#cv_grid, best_model, cvl_scores = train_xgb_model(X_train, y_train, splits, param_ranges, \n",
    "#                                                  scorer='neg_mean_squared_error', return_train=True)\n",
    "\n",
    "# Save cv-object\n",
    "#joblib.dump(cv_grid, 'Models/Final_training/cv_rand_5fold_temp_prec_summer.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_grid = joblib.load('Models/Final_training/cv_rand_5fold_temp_prec_summer.pkl')\n",
    "best_model = cv_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_prediction_per_fold(X_train, y_train, best_model, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, sharex=True, sharey=True, figsize=(25,25))\n",
    "a = 0    \n",
    "for train_index, test_index in splits:\n",
    "    # Loops over n_splits iterations and gets train and test splits in each fold\n",
    "    X_train_, X_test_ = X_train[train_index], X_train[test_index]\n",
    "    y_train_, y_test_ = y_train[train_index], y_train[test_index]\n",
    "    best_model.fit(X_train_, y_train_)\n",
    "    y_pred = best_model.predict(X_test_)\n",
    "\n",
    "    title = 'Validation fold ' + str(a)\n",
    "\n",
    "    plot_prediction_subplot(y_test_, y_pred, title, ax[a], n_toplot=5000, fold=True)\n",
    "\n",
    "    a=a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gsearch_results(cv_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "# Select features for training\n",
    "df_test_X = df_test_summer_clean.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_test_y = df_test_summer_clean[['balance']]\n",
    "\n",
    "X_test_summer, y_test_summer = df_test_X.values, df_test_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = cv_grid.best_estimator_\n",
    "test_model.fit(X_train, y_train)\n",
    "y_pred_test_summer = test_model.predict(X_test_summer)\n",
    "plot_prediction(y_test_summer, y_pred_test_summer, data_type='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### 1.4 Winter mass balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training\n",
    "df_train_X = df_train_winter_clean.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_train_y = df_train_winter_clean[['balance']]\n",
    "\n",
    "# Get arrays of features and targets\n",
    "X_train, y_train = df_train_X.values, df_train_y.values\n",
    "\n",
    "# Set random seed for split\n",
    "rand_seed = 25\n",
    "\n",
    "# Use five folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=rand_seed)\n",
    "splits = list(kf.split(X_train, y_train))\n",
    "\n",
    "# Train model\n",
    "# Set return_train to False to inhibit return training scores. Default is True.\n",
    "#cv_grid, best_model, cvl_scores = train_xgb_model(X_train, y_train, splits, param_ranges, \n",
    "#                                                  scorer='neg_mean_squared_error', return_train=True)\n",
    "\n",
    "# Save cv-object\n",
    "#joblib.dump(cv_grid, 'Models/Final_training/cv_rand_5fold_temp_prec_winter.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_grid = joblib.load('Models/Final_training/cv_rand_5fold_temp_prec_winter.pkl')\n",
    "best_model = cv_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_prediction_per_fold(X_train, y_train, best_model, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gsearch_results(cv_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, sharex=True, sharey=True, figsize=(25,25))\n",
    "a = 0    \n",
    "for train_index, test_index in splits:\n",
    "    # Loops over n_splits iterations and gets train and test splits in each fold\n",
    "    X_train_, X_test_ = X_train[train_index], X_train[test_index]\n",
    "    y_train_, y_test_ = y_train[train_index], y_train[test_index]\n",
    "    best_model.fit(X_train_, y_train_)\n",
    "    y_pred = best_model.predict(X_test_)\n",
    "\n",
    "    title = 'Validation fold ' + str(a)\n",
    "\n",
    "    plot_prediction_subplot(y_test_, y_pred, title, ax[a], n_toplot=5000, fold=True)\n",
    "\n",
    "    a=a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "# Select features for training\n",
    "df_test_X = df_test_winter_clean.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_test_y = df_test_winter_clean[['balance']]\n",
    "\n",
    "X_test_winter, y_test_winter = df_test_X.values, df_test_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = cv_grid.best_estimator_\n",
    "test_model.fit(X_train, y_train)\n",
    "y_pred_test_winter = test_model.predict(X_test_winter)\n",
    "plot_prediction(y_test_winter, y_pred_test_winter, data_type='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot annual,, summer and winter together\n",
    "y_test_all = np.hstack((y_test_annual[:,0], y_test_summer[:,0], y_test_winter[:,0]))\n",
    "y_pred_test_all = np.hstack((y_pred_test_annual, y_pred_test_summer, y_pred_test_winter))\n",
    "plot_prediction(y_test_all, y_pred_test_all, data_type='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "## 2.0 Model 2: Energy balance variables with 5-fold random CV\n",
    "Features: ERA5-Land meteorological variables: tp, t2m, fal, ssrd, str, sshf, slhf <br>\n",
    "CV-strategy: 5-fold random cross-validation <br>\n",
    "Targets: <br>\n",
    "2.1: Annual and seasonal observations <br>\n",
    "2.2: Only annual mass balance <br>\n",
    "2.3: Only summer mass balance <br>\n",
    "2.4: Only winter mass balance <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns to drop\n",
    "cols = ['RGIID','GLIMSID','utm_zone','utm_east_approx','utm_north_approx','altitude_approx',\n",
    "        'location_description','location_id','stake_no','utm_east','utm_north','dt_prev_year_min_date','dt_curr_year_max_date',\n",
    "        'dt_curr_year_min_date','stake_remark','flag_correction','approx_loc','approx_altitude',\n",
    "        'diff_north','diff_east','diff_altitude','diff_netto','lat_approx','lon_approx',\n",
    "        'topo','dis_from_border','year', 'lat','lon', 'slope_factor']\n",
    "\n",
    "snow_depth_m = ['sde_oct','sde_nov','sde_dec','sde_jan','sde_feb','sde_mar','sde_apr','sde_may','sde_jun','sde_jul','sde_aug','sde_sep']\n",
    "snow_density = ['rsn_oct','rsn_nov','rsn_dec','rsn_jan','rsn_feb','rsn_mar','rsn_apr','rsn_may','rsn_jun','rsn_jul','rsn_aug','rsn_sep']\n",
    "evaporation = ['es_oct','es_nov','es_dec','es_jan','es_feb','es_mar','es_apr','es_may','es_jun','es_jul','es_aug','es_sep']\n",
    "snow_cover = ['snowc_oct','snowc_nov','snowc_dec','snowc_jan','snowc_feb','snowc_mar','snowc_apr','snowc_may','snowc_jun','snowc_jul','snowc_aug','snowc_sep']\n",
    "snow_depth_we = ['sd_oct','sd_nov','sd_dec','sd_jan','sd_feb','sd_mar','sd_apr','sd_may','sd_jun','sd_jul','sd_aug','sd_sep']\n",
    "snow_temp = ['tsn_oct','tsn_nov','tsn_dec','tsn_jan','tsn_feb','tsn_mar','tsn_apr','tsn_may','tsn_jun','tsn_jul','tsn_aug','tsn_sep']\n",
    "snow_melt = ['smlt_oct','smlt_nov','smlt_dec','smlt_jan','smlt_feb','smlt_mar','smlt_apr','smlt_may','smlt_jun','smlt_jul','smlt_aug','smlt_sep']\n",
    "snowfall = ['sf_oct','sf_nov','sf_dec','sf_jan','sf_feb','sf_mar','sf_apr','sf_may','sf_jun','sf_jul','sf_aug','sf_sep']\n",
    "snow_albedo = ['asn_oct','asn_nov','asn_dec','asn_jan','asn_feb','asn_mar','asn_apr','asn_may','asn_jun','asn_jul','asn_aug','asn_sep']\n",
    "dewpt_temp = ['d2m_oct','d2m_nov','d2m_dec','d2m_jan','d2m_feb','d2m_mar','d2m_apr','d2m_may','d2m_jun','d2m_jul','d2m_aug','d2m_sep']\n",
    "surface_pressure = ['sp_oct','sp_nov','sp_dec','sp_jan','sp_feb','sp_mar','sp_apr','sp_may','sp_jun','sp_jul','sp_aug','sp_sep']\n",
    "sol_rad_net = ['ssr_oct','ssr_nov','ssr_dec','ssr_jan','ssr_feb','ssr_mar','ssr_apr','ssr_may','ssr_jun','ssr_jul','ssr_aug','ssr_sep']\n",
    "sol_therm_down = ['strd_oct','strd_nov','strd_dec','strd_jan','strd_feb','strd_mar','strd_apr','strd_may','strd_jun','strd_jul','strd_aug','strd_sep']\n",
    "#sol_rad_down = ['ssrd_oct','ssrd_nov','ssrd_dec','ssrd_jan','ssrd_feb','ssrd_mar','ssrd_apr','ssrd_may','ssrd_jun','ssrd_jul','ssrd_aug','ssrd_sep']\n",
    "u_wind = ['u10_oct','u10_nov','u10_dec','u10_jan','u10_feb','u10_mar','u10_apr','u10_may','u10_jun','u10_jul','u10_aug','u10_sep']\n",
    "v_wind = ['v10_oct','v10_nov','v10_dec','v10_jan','v10_feb','v10_mar','v10_apr','v10_may','v10_jun','v10_jul','v10_aug','v10_sep']\n",
    "#f_albedo = ['fal_oct','fal_nov','fal_dec','fal_jan','fal_feb','fal_mar','fal_apr','fal_may','fal_jun','fal_jul','fal_aug','fal_sep']\n",
    "\n",
    "drop_cols = [y for x in [cols, snow_depth_m, snow_density, evaporation, snow_cover, snow_depth_we, snow_temp, snow_melt, snowfall, snow_albedo, \n",
    "                         dewpt_temp, surface_pressure, sol_rad_net, sol_therm_down, u_wind, v_wind] for y in x]\n",
    "\n",
    "# Select features for training\n",
    "df_train_annual_clean = df_train_annual.drop(drop_cols, axis=1)\n",
    "df_train_winter_clean = df_train_winter.drop(drop_cols, axis=1)\n",
    "df_train_summer_clean = df_train_summer.drop(drop_cols, axis=1)\n",
    "df_train_annual_clean = df_train_annual_clean.drop(['balance_winter','balance_summer'], axis=1)\n",
    "df_train_winter_clean = df_train_winter_clean.drop(['balance_netto', 'balance_summer'], axis=1)\n",
    "df_train_summer_clean = df_train_summer_clean.drop(['balance_netto', 'balance_winter'], axis=1)\n",
    "\n",
    "# Rename target columns to same name\n",
    "df_train_annual_clean.rename(columns={'balance_netto' : 'balance'}, inplace=True)\n",
    "df_train_winter_clean.rename(columns={'balance_winter' : 'balance'}, inplace=True)\n",
    "df_train_summer_clean.rename(columns={'balance_summer' : 'balance'}, inplace=True)\n",
    "\n",
    "# df_train_X_... now contains columns of all chosen features and column with annual, winter or summer balance\n",
    "\n",
    "# For summer balance, replace column values in accumulation months with NaN (oct, nov, dec, jan, feb, mar, apr, may)\n",
    "# For winter balance, replace column values in ablation months with NaN (may, jun, jul, aug, sep, oct)\n",
    "var = ['t2m', 'sshf', 'slhf', 'ssrd', 'fal','str', 'tp']\n",
    "mon_summer = ['may', 'jun', 'jul', 'aug', 'sep', 'oct']\n",
    "mon_winter = ['oct', 'nov', 'dec', 'jan', 'feb', 'mar', 'apr', 'may']\n",
    "\n",
    "for name in var:\n",
    "    for mon in mon_summer:\n",
    "        df_train_winter_clean[name+'_'+mon]= np.nan\n",
    "for name in var:   \n",
    "    for mon in mon_winter:\n",
    "        df_train_summer_clean[name+'_'+mon]= np.nan\n",
    "\n",
    "# Combine all annual, winter and summer data in one dataframe\n",
    "df_train_all = pd.concat([df_train_annual_clean, df_train_summer_clean, df_train_winter_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "\n",
    "# Select features for training\n",
    "df_test_annual_clean = df_test_annual.drop(drop_cols, axis=1)\n",
    "df_test_winter_clean = df_test_winter.drop(drop_cols, axis=1)\n",
    "df_test_summer_clean = df_test_summer.drop(drop_cols, axis=1)\n",
    "df_test_annual_clean = df_test_annual_clean.drop(['balance_winter','balance_summer'], axis=1)\n",
    "df_test_winter_clean = df_test_winter_clean.drop(['balance_netto', 'balance_summer'], axis=1)\n",
    "df_test_summer_clean = df_test_summer_clean.drop(['balance_netto', 'balance_winter'], axis=1)\n",
    "\n",
    "# Rename target columns to same name\n",
    "df_test_annual_clean.rename(columns={'balance_netto' : 'balance'}, inplace=True)\n",
    "df_test_winter_clean.rename(columns={'balance_winter' : 'balance'}, inplace=True)\n",
    "df_test_summer_clean.rename(columns={'balance_summer' : 'balance'}, inplace=True)\n",
    "\n",
    "# For summer balance, replace column values in accumulation months with NaN (oct, nov, dec, jan, feb, mar, apr, may)\n",
    "# For winter balance, replace column values in ablation months with NaN (may, jun, jul, aug, sep, oct)\n",
    "for name in var:\n",
    "    for mon in mon_summer:\n",
    "        df_test_winter_clean[name+'_'+mon]= np.nan\n",
    "for name in var:   \n",
    "    for mon in mon_winter:\n",
    "        df_test_summer_clean[name+'_'+mon]= np.nan\n",
    "\n",
    "# Combine all annual, winter and summer data in one dataframe\n",
    "df_test_all = pd.concat([df_test_annual_clean, df_test_summer_clean, df_test_winter_clean])\n",
    "df_test_all.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "### 2.1 Annual and seasonal mass balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove instances where elevation is lower than 700 m a sl\n",
    "#df_train_all = df_train_all.loc[df_train_all['altitude'] >= 750]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training\n",
    "df_train_X = df_train_all.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_train_y = df_train_all[['balance']]\n",
    "\n",
    "# Get arrays of features and targets\n",
    "X_train, y_train = df_train_X.values, df_train_y.values\n",
    "\n",
    "# Set random seed for split\n",
    "rand_seed = 25\n",
    "\n",
    "# Use five folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=rand_seed)\n",
    "splits = list(kf.split(X_train, y_train))\n",
    "\n",
    "# Train model\n",
    "# Set return_train to False to inhibit return training scores. Default is True.\n",
    "cv_grid, best_model, cvl_scores = train_xgb_model(X_train, y_train, splits, param_ranges, \n",
    "                                                  scorer='neg_mean_squared_error', return_train=True)\n",
    "\n",
    "# Save cv-object\n",
    "joblib.dump(cv_grid, 'Models/Final_training/cv_rand_5fold_all_climate_ann+seas_new_may24.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_grid = joblib.load('Models/Final_training/cv_rand_5fold_all_climate_ann+seas_new_may24.pkl')\n",
    "#best_model = cv_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gsearch_results(cv_grid)\n",
    "#plot_prediction_per_fold(X_train, y_train, best_model, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, sharex=True, sharey=True, figsize=(25,25))\n",
    "a = 0    \n",
    "for train_index, test_index in splits:\n",
    "    # Loops over n_splits iterations and gets train and test splits in each fold\n",
    "    X_train_, X_test_ = X_train[train_index], X_train[test_index]\n",
    "    y_train_, y_test_ = y_train[train_index], y_train[test_index]\n",
    "    best_model.fit(X_train_, y_train_)\n",
    "    y_pred = best_model.predict(X_test_)\n",
    "\n",
    "    title = 'Validation fold ' + str(a)\n",
    "\n",
    "    plot_prediction_subplot(y_test_, y_pred, title, ax[a], n_toplot=5000, fold=True)\n",
    "\n",
    "    a=a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_permutation_importance(df_train_X, X_train, y_train, splits, best_model, max_features_plot = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_annual, y_pred_annual = get_prediction_per_season(X_train, y_train, splits, best_model, months=12)\n",
    "y_test_winter, y_pred_winter = get_prediction_per_season(X_train, y_train, splits, best_model, months=8)\n",
    "y_test_summer, y_pred_summer = get_prediction_per_season(X_train, y_train, splits, best_model, months=6)\n",
    "\n",
    "print(y_test_annual.shape)\n",
    "print(y_test_winter.shape)\n",
    "print(y_test_summer.shape)\n",
    "\n",
    "y_test_all = np.hstack((y_test_annual, y_test_winter, y_test_summer))\n",
    "y_pred_all = np.hstack((y_pred_annual, y_pred_winter, y_pred_summer))\n",
    "\n",
    "print(y_test_all.shape)\n",
    "print(y_pred_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_per_season(y_test_winter, y_pred_winter, season='Winter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "# Select features for training\n",
    "df_test_X = df_test_all.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_test_y = df_test_all[['balance']]\n",
    "\n",
    "X_test, y_test = df_test_X.values, df_test_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = cv_grid.best_estimator_\n",
    "test_model.fit(X_train, y_train)\n",
    "y_pred_test = test_model.predict(X_test)\n",
    "plot_prediction(y_test, y_pred_test, data_type='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_crop, y_pred_crop = get_prediction_per_season_test(X_test, y_test, test_model, months=6)\n",
    "plot_prediction_per_season(y_test_crop, y_pred_crop, season='Summer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "### 2.2 Annual mass balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training\n",
    "df_train_X = df_train_annual_clean.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_train_y = df_train_annual_clean[['balance']]\n",
    "\n",
    "# Get arrays of features and targets\n",
    "X_train, y_train = df_train_X.values, df_train_y.values\n",
    "\n",
    "# Set random seed for split\n",
    "rand_seed = 25\n",
    "\n",
    "# Use five folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=rand_seed)\n",
    "splits = list(kf.split(X_train, y_train))\n",
    "\n",
    "# Define parameter ranges.\n",
    "param_ranges = {'max_depth': [2, 3, 4, 5, 6, 7, 8],\n",
    "          'n_estimators': [50, 100, 200, 300, 400, 500], # number of trees (too many = overfitting, too few = underfitting)\n",
    "          'learning_rate': [0.01, 0.1, 0.15, 0.2, 0.25, 0.3]}\n",
    "\n",
    "# Train model\n",
    "# Set return_train to False to inhibit return training scores. Default is True.\n",
    "#cv_grid, best_model, cvl_scores = train_xgb_model(X_train, y_train, splits, param_ranges, \n",
    "#                                                  scorer='neg_mean_squared_error', return_train=True)\n",
    "\n",
    "# Save cv-object\n",
    "#joblib.dump(cv_grid, 'Models/Final_training/cv_rand_5fold_all_climate_annual.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_grid = joblib.load('Models/Final_training/cv_rand_5fold_all_climate_annual.pkl')\n",
    "best_model = cv_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gsearch_results(cv_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, sharex=True, sharey=True, figsize=(25,25))\n",
    "a = 0    \n",
    "for train_index, test_index in splits:\n",
    "    # Loops over n_splits iterations and gets train and test splits in each fold\n",
    "    X_train_, X_test_ = X_train[train_index], X_train[test_index]\n",
    "    y_train_, y_test_ = y_train[train_index], y_train[test_index]\n",
    "    best_model.fit(X_train_, y_train_)\n",
    "    y_pred = best_model.predict(X_test_)\n",
    "\n",
    "    title = 'Validation fold ' + str(a)\n",
    "\n",
    "    plot_prediction_subplot(y_test_, y_pred, title, ax[a], n_toplot=5000, fold=True)\n",
    "\n",
    "    a=a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "# Select features for training\n",
    "df_test_X = df_test_annual_clean.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_test_y = df_test_annual_clean[['balance']]\n",
    "\n",
    "X_test_annual, y_test_annual = df_test_X.values, df_test_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = cv_grid.best_estimator_\n",
    "test_model.fit(X_train, y_train)\n",
    "y_pred_test_annual = test_model.predict(X_test_annual)\n",
    "plot_prediction(y_test_annual, y_pred_test_annual, data_type='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "### 2.3 Summer mass balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training\n",
    "df_train_X = df_train_summer_clean.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_train_y = df_train_summer_clean[['balance']]\n",
    "\n",
    "# Get arrays of features and targets\n",
    "X_train, y_train = df_train_X.values, df_train_y.values\n",
    "\n",
    "# Set random seed for split\n",
    "rand_seed = 25\n",
    "\n",
    "# Use five folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=rand_seed)\n",
    "splits = list(kf.split(X_train, y_train))\n",
    "\n",
    "# Define parameter ranges.\n",
    "param_ranges = {'max_depth': [2, 3, 4, 5, 6, 7, 8],\n",
    "          'n_estimators': [50, 100, 200, 300, 400, 500], # number of trees (too many = overfitting, too few = underfitting)\n",
    "          'learning_rate': [0.01, 0.1, 0.15, 0.2, 0.25, 0.3]}\n",
    "\n",
    "# Train model\n",
    "# Set return_train to False to inhibit return training scores. Default is True.\n",
    "#cv_grid, best_model, cvl_scores = train_xgb_model(X_train, y_train, splits, param_ranges, \n",
    "#                                                  scorer='neg_mean_squared_error', return_train=True)\n",
    "\n",
    "# Save cv-object\n",
    "#joblib.dump(cv_grid, 'Models/Final_training/cv_rand_5fold_all_climate_summer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_grid = joblib.load('Models/Final_training/cv_rand_5fold_all_climate_summer.pkl')\n",
    "best_model = cv_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gsearch_results(cv_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, sharex=True, sharey=True, figsize=(25,25))\n",
    "a = 0    \n",
    "for train_index, test_index in splits:\n",
    "    # Loops over n_splits iterations and gets train and test splits in each fold\n",
    "    X_train_, X_test_ = X_train[train_index], X_train[test_index]\n",
    "    y_train_, y_test_ = y_train[train_index], y_train[test_index]\n",
    "    best_model.fit(X_train_, y_train_)\n",
    "    y_pred = best_model.predict(X_test_)\n",
    "\n",
    "    title = 'Validation fold ' + str(a)\n",
    "\n",
    "    plot_prediction_subplot(y_test_, y_pred, title, ax[a], n_toplot=5000, fold=True)\n",
    "\n",
    "    a=a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "# Select features for training\n",
    "df_test_X = df_test_summer_clean.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_test_y = df_test_summer_clean[['balance']]\n",
    "\n",
    "X_test_summer, y_test_summer = df_test_X.values, df_test_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = cv_grid.best_estimator_\n",
    "test_model.fit(X_train, y_train)\n",
    "y_pred_test_summer = test_model.predict(X_test_summer)\n",
    "plot_prediction(y_test_summer, y_pred_test_summer, data_type='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "### 2.4 Winter mass balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training\n",
    "df_train_X = df_train_winter_clean.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_train_y = df_train_winter_clean[['balance']]\n",
    "\n",
    "# Get arrays of features and targets\n",
    "X_train, y_train = df_train_X.values, df_train_y.values\n",
    "\n",
    "# Set random seed for split\n",
    "rand_seed = 25\n",
    "\n",
    "# Use five folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=rand_seed)\n",
    "splits = list(kf.split(X_train, y_train))\n",
    "\n",
    "# Define parameter ranges.\n",
    "param_ranges = {'max_depth': [2, 3, 4, 5, 6, 7, 8],\n",
    "          'n_estimators': [50, 100, 200, 300, 400, 500], # number of trees (too many = overfitting, too few = underfitting)\n",
    "          'learning_rate': [0.01, 0.1, 0.15, 0.2, 0.25, 0.3]}\n",
    "\n",
    "# Train model\n",
    "# Set return_train to False to inhibit return training scores. Default is True.\n",
    "#cv_grid, best_model, cvl_scores = train_xgb_model(X_train, y_train, splits, param_ranges, \n",
    "#                                                  scorer='neg_mean_squared_error', return_train=True)\n",
    "\n",
    "# Save cv-object\n",
    "#joblib.dump(cv_grid, 'Models/Final_training/cv_rand_5fold_all_climate_winter.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_grid = joblib.load('Models/Final_training/cv_rand_5fold_all_climate_winter.pkl')\n",
    "best_model = cv_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gsearch_results(cv_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, sharex=True, sharey=True, figsize=(25,25))\n",
    "a = 0    \n",
    "for train_index, test_index in splits:\n",
    "    # Loops over n_splits iterations and gets train and test splits in each fold\n",
    "    X_train_, X_test_ = X_train[train_index], X_train[test_index]\n",
    "    y_train_, y_test_ = y_train[train_index], y_train[test_index]\n",
    "    best_model.fit(X_train_, y_train_)\n",
    "    y_pred = best_model.predict(X_test_)\n",
    "\n",
    "    title = 'Validation fold ' + str(a)\n",
    "\n",
    "    plot_prediction_subplot(y_test_, y_pred, title, ax[a], n_toplot=5000, fold=True)\n",
    "\n",
    "    a=a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "# Select features for training\n",
    "df_test_X = df_test_winter_clean.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_test_y = df_test_winter_clean[['balance']]\n",
    "\n",
    "X_test_winter, y_test_winter = df_test_X.values, df_test_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = cv_grid.best_estimator_\n",
    "test_model.fit(X_train, y_train)\n",
    "y_pred_test_winter = test_model.predict(X_test_winter)\n",
    "plot_prediction(y_test_winter, y_pred_test_winter, data_type='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot annual,, summer and winter together\n",
    "y_test_all = np.hstack((y_test_annual[:,0], y_test_summer[:,0], y_test_winter[:,0]))\n",
    "y_pred_test_all = np.hstack((y_pred_test_annual, y_pred_test_summer, y_pred_test_winter))\n",
    "plot_prediction(y_test_all, y_pred_test_all, data_type='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "## 3.0 Model 3: Temperature and precipitation with 5-fold blocking-by-glacier CV\n",
    "Features: Temperature and precipitation <br>\n",
    "CV-strategy: 5-fold blocking-by-glacier cross validation <br>\n",
    "Targets: <br>\n",
    "3.1: Annual and seasonal observations <br>\n",
    "3.2: Only annual mass balance <br>\n",
    "3.3: Only summer mass balance <br>\n",
    "3.4: Only winter mass balance <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "base_cols = ['BREID','altitude','aspect','slope','altitude_climate','n_months']\n",
    "temp_cols = ['t2m_oct','t2m_nov','t2m_dec','t2m_jan','t2m_feb','t2m_mar','t2m_apr','t2m_may','t2m_jun','t2m_jul','t2m_aug','t2m_sep']\n",
    "prec_cols = ['tp_oct','tp_nov','tp_dec','tp_jan','tp_feb','tp_mar','tp_apr','tp_may','tp_jun','tp_jul','tp_aug','tp_sep']\n",
    "label_cols = ['balance_netto']\n",
    "\n",
    "# Select features for training\n",
    "df_train_annual_clean = select_variables(df_train_annual, base_cols, temp_cols, prec_cols, ['balance_netto'])\n",
    "df_train_winter_clean = select_variables(df_train_winter, base_cols, temp_cols, prec_cols, ['balance_winter'])\n",
    "df_train_summer_clean = select_variables(df_train_summer, base_cols, temp_cols, prec_cols, ['balance_summer'])\n",
    "\n",
    "# Rename target columns to same name\n",
    "df_train_annual_clean.rename(columns={'balance_netto' : 'balance'}, inplace=True)\n",
    "df_train_winter_clean.rename(columns={'balance_winter' : 'balance'}, inplace=True)\n",
    "df_train_summer_clean.rename(columns={'balance_summer' : 'balance'}, inplace=True)\n",
    "\n",
    "# For summer balance, replace column values in accumulation months with NaN (oct, nov, dec, jan, feb, mar, apr, may)\n",
    "# For winter balance, replace column values in ablation months with NaN (may, jun, jul, aug, sep, oct)\n",
    "var = ['t2m', 'tp']\n",
    "mon_summer = ['may', 'jun', 'jul', 'aug', 'sep', 'oct']\n",
    "mon_winter = ['oct', 'nov', 'dec', 'jan', 'feb', 'mar', 'apr', 'may']\n",
    "\n",
    "for name in var:\n",
    "    for mon in mon_summer:\n",
    "        df_train_winter_clean[name+'_'+mon]= np.nan\n",
    "for name in var:   \n",
    "    for mon in mon_winter:\n",
    "        df_train_summer_clean[name+'_'+mon]= np.nan\n",
    "\n",
    "# Combine all annual, winter and summer data in one dataframe\n",
    "df_train_all = pd.concat([df_train_annual_clean, df_train_summer_clean, df_train_winter_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test dataset\n",
    "\n",
    "# Select features for training\n",
    "df_test_annual_clean = select_variables(df_test_annual, base_cols, temp_cols, prec_cols, ['balance_netto'])\n",
    "df_test_winter_clean = select_variables(df_test_winter, base_cols, temp_cols, prec_cols, ['balance_winter'])\n",
    "df_test_summer_clean = select_variables(df_test_summer, base_cols, temp_cols, prec_cols, ['balance_summer'])\n",
    "\n",
    "# Rename target columns to same name\n",
    "df_test_annual_clean.rename(columns={'balance_netto' : 'balance'}, inplace=True)\n",
    "df_test_winter_clean.rename(columns={'balance_winter' : 'balance'}, inplace=True)\n",
    "df_test_summer_clean.rename(columns={'balance_summer' : 'balance'}, inplace=True)\n",
    "\n",
    "# For summer balance, replace column values in accumulation months with NaN (oct, nov, dec, jan, feb, mar, apr, may)\n",
    "# For winter balance, replace column values in ablation months with NaN (may, jun, jul, aug, sep, oct)\n",
    "for name in var:\n",
    "    for mon in mon_summer:\n",
    "        df_test_winter_clean[name+'_'+mon]= np.nan\n",
    "for name in var:   \n",
    "    for mon in mon_winter:\n",
    "        df_test_summer_clean[name+'_'+mon]= np.nan\n",
    "\n",
    "# Combine all annual, winter and summer data in one dataframe\n",
    "df_test_all = pd.concat([df_test_annual_clean, df_test_summer_clean, df_test_winter_clean])\n",
    "df_test_all.reset_index(drop=True, inplace=True)\n",
    "print(df_test_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "### 3.1 Annual and seasonal mass balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group together outlet glaciers of Folgefonna by renaming BREID to dummy ID 3000\n",
    "#df_train_all.loc[df_train_all['BREID'].isin([3126, 3127, 3128, 3129])]=3000\n",
    "\n",
    "# Shuffle df_train, get X and y values\n",
    "df_train_s = df_train_all.sample(frac=1, random_state=5)\n",
    "df_train_s.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Select features for training\n",
    "df_train_X_s = df_train_s.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_train_y_s = df_train_s[['balance']]\n",
    "\n",
    "# Get values\n",
    "X_train_s, y_train_s = df_train_X_s.values, df_train_y_s.values\n",
    "\n",
    "# Get glacier IDs from training dataset (in the order of which they appear in training dataset).\n",
    "# gp_s is an array with shape equal to the shape of X_train_s and y_train_s.\n",
    "gp_s = np.array(df_train_s['BREID'].values)\n",
    "\n",
    "# Use five folds\n",
    "group_kf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Split into folds according to group by glacier ID.\n",
    "# For each unique glacier ID, indices in gp_s indicate which rows in X_train_s and y_train_s belong to the glacier.\n",
    "splits_s = list(group_kf.split(X_train_s, y_train_s, gp_s))\n",
    "\n",
    "print('Train, fold 0: ', np.unique(gp_s[splits_s[0][0]]))\n",
    "print('Validation, fold 0: ', np.unique(gp_s[splits_s[0][1]]))\n",
    "print('Train, fold 1: ', np.unique(gp_s[splits_s[1][0]]))\n",
    "print('Validation, fold 1: ', np.unique(gp_s[splits_s[1][1]]))\n",
    "print('Train, fold 2: ', np.unique(gp_s[splits_s[2][0]]))\n",
    "print('Validation, fold 2: ', np.unique(gp_s[splits_s[2][1]]))\n",
    "print('Train, fold 3: ', np.unique(gp_s[splits_s[3][0]]))\n",
    "print('Validation, fold 3: ', np.unique(gp_s[splits_s[3][1]]))\n",
    "print('Train, fold 4: ', np.unique(gp_s[splits_s[4][0]]))\n",
    "print('Validation, fold 4: ', np.unique(gp_s[splits_s[4][1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# Set return_train to False to inhibit return training scores. Default is True.\n",
    "#cv_grid, best_model, cvl_scores = train_xgb_model(X_train_s, y_train_s, splits_s, param_ranges, \n",
    "#                                                  scorer='neg_mean_squared_error', return_train=True)\n",
    "\n",
    "# Save cv-object\n",
    "#joblib.dump(cv_grid, 'Models/Final_training/cv_block_glacier_temp_prec_ann+seas_w_nmonths.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_grid = joblib.load('Models/Final_training/cv_block_glacier_temp_prec_ann+seas_w_nmonths.pkl')\n",
    "best_model = cv_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_prediction_per_fold(X_train_s, y_train_s, best_model, splits_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_prediction_per_fold(X_train_s, y_train_s, best_model, splits_s)\n",
    "fig, ax = plt.subplots(1,5, sharex=True, sharey=True, figsize=(25,25))\n",
    "a = 0    \n",
    "for train_index, test_index in splits_s:\n",
    "    # Loops over n_splits iterations and gets train and test splits in each fold\n",
    "    X_train_, X_test_ = X_train_s[train_index], X_train_s[test_index]\n",
    "    y_train_, y_test_ = y_train_s[train_index], y_train_s[test_index]\n",
    "    best_model.fit(X_train_, y_train_)\n",
    "    y_pred = best_model.predict(X_test_)\n",
    "\n",
    "    title = 'Validation fold ' + str(a)\n",
    "\n",
    "    plot_prediction_subplot(y_test_, y_pred, title, ax[a], n_toplot=5000, fold=True)\n",
    "\n",
    "    a=a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gsearch_results(cv_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_permutation_importance(df_train_X_s, X_train_s, y_train_s, splits_s, best_model, max_features_plot = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_annual, y_pred_annual = get_prediction_per_season(X_train_s, y_train_s, splits_s, best_model, months=12)\n",
    "y_test_winter, y_pred_winter = get_prediction_per_season(X_train_s, y_train_s, splits_s, best_model, months=8)\n",
    "y_test_summer, y_pred_summer = get_prediction_per_season(X_train_s, y_train_s, splits_s, best_model, months=6)\n",
    "\n",
    "print(y_test_annual.shape)\n",
    "print(y_test_winter.shape)\n",
    "print(y_test_summer.shape)\n",
    "\n",
    "y_test_all = np.hstack((y_test_annual, y_test_winter, y_test_summer))\n",
    "y_pred_all = np.hstack((y_pred_annual, y_pred_winter, y_pred_summer))\n",
    "\n",
    "print(y_test_all.shape)\n",
    "print(y_pred_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_per_season(y_test_annual, y_pred_annual, season='Annual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "# Select features for training\n",
    "df_test_X = df_test_all.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_test_y = df_test_all[['balance']]\n",
    "\n",
    "X_test, y_test = df_test_X.values, df_test_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = cv_grid.best_estimator_\n",
    "test_model.fit(X_train_s, y_train_s)\n",
    "y_pred_test = test_model.predict(X_test)\n",
    "plot_prediction(y_test, y_pred_test, data_type='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_crop, y_pred_crop = get_prediction_per_season_test(X_test, y_test, test_model, months=12)\n",
    "plot_prediction_per_season(y_test_crop, y_pred_crop, season='Annual')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122",
   "metadata": {},
   "source": [
    "### 3.2 Annual mass balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group together outlet glaciers of Folgefonna by renaming BREID to dummy ID 3000\n",
    "#df_train_annual_clean.loc[df_train_annual_clean['BREID'].isin([3126, 3127, 3128, 3129])]=3000\n",
    "\n",
    "# Get annual data\n",
    "df_train_ann_s = df_train_s[df_train_s['n_months']==12]\n",
    "\n",
    "# Get order of glacier ID in shuffled dataframe\n",
    "sorter = list(df_train_s.BREID.unique())\n",
    "\n",
    "# Sort dataframe of annual mb according to list\n",
    "# This is to get same folds as when using all data\n",
    "df_train_ann_s = df_train_ann_s.iloc[df_train_ann_s['BREID'].map({v: k for k, v in enumerate(sorter)}).argsort()]\n",
    "df_train_ann_s.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Select features for training\n",
    "df_train_X_s = df_train_ann_s.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_train_y_s = df_train_ann_s[['balance']]\n",
    "\n",
    "# Get values\n",
    "X_train_s, y_train_s = df_train_X_s.values, df_train_y_s.values\n",
    "\n",
    "# Get glacier IDs from training dataset (in the order of which they appear in training dataset).\n",
    "# gp_s is an array with shape equal to the shape of X_train_s and y_train_s.\n",
    "gp_s = np.array(df_train_ann_s['BREID'].values)\n",
    "\n",
    "# Use five folds\n",
    "group_kf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Split into folds according to group by glacier ID.\n",
    "# For each unique glacier ID, indices in gp_s indicate which rows in X_train_s and y_train_s belong to the glacier.\n",
    "splits_s = list(group_kf.split(X_train_s, y_train_s, gp_s))\n",
    "\n",
    "print('Train, fold 0: ', np.unique(gp_s[splits_s[0][0]]))\n",
    "print('Validation, fold 0: ', np.unique(gp_s[splits_s[0][1]]))\n",
    "print('Train, fold 1: ', np.unique(gp_s[splits_s[1][0]]))\n",
    "print('Validation, fold 1: ', np.unique(gp_s[splits_s[1][1]]))\n",
    "print('Train, fold 2: ', np.unique(gp_s[splits_s[2][0]]))\n",
    "print('Validation, fold 2: ', np.unique(gp_s[splits_s[2][1]]))\n",
    "print('Train, fold 3: ', np.unique(gp_s[splits_s[3][0]]))\n",
    "print('Validation, fold 3: ', np.unique(gp_s[splits_s[3][1]]))\n",
    "print('Train, fold 4: ', np.unique(gp_s[splits_s[4][0]]))\n",
    "print('Validation, fold 4: ', np.unique(gp_s[splits_s[4][1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# Set return_train to False to inhibit return training scores. Default is True.\n",
    "#cv_grid, best_model, cvl_scores = train_xgb_model(X_train_s, y_train_s, splits_s, param_ranges, \n",
    "#                                                  scorer='neg_mean_squared_error', return_train=True)\n",
    "\n",
    "# Save cv-object\n",
    "#joblib.dump(cv_grid, 'Models/Final_training/cv_block_glacier_temp_prec_annual.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_grid = joblib.load('Models/Final_training/cv_block_glacier_temp_prec_annual.pkl')\n",
    "best_model = cv_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, sharex=True, sharey=True, figsize=(25,25))\n",
    "a = 0    \n",
    "for train_index, test_index in splits_s:\n",
    "    # Loops over n_splits iterations and gets train and test splits in each fold\n",
    "    X_train_, X_test_ = X_train_s[train_index], X_train_s[test_index]\n",
    "    y_train_, y_test_ = y_train_s[train_index], y_train_s[test_index]\n",
    "    best_model.fit(X_train_, y_train_)\n",
    "    y_pred = best_model.predict(X_test_)\n",
    "\n",
    "    title = 'Validation fold ' + str(a)\n",
    "\n",
    "    plot_prediction_subplot(y_test_, y_pred, title, ax[a], n_toplot=5000, fold=True)\n",
    "\n",
    "    a=a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gsearch_results(cv_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "# Select features for training\n",
    "df_test_X = df_test_annual_clean.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_test_y = df_test_annual_clean[['balance']]\n",
    "\n",
    "X_test_annual, y_test_annual = df_test_X.values, df_test_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = cv_grid.best_estimator_\n",
    "test_model.fit(X_train_s, y_train_s)\n",
    "y_pred_test_annual = test_model.predict(X_test_annual)\n",
    "plot_prediction(y_test_annual, y_pred_test_annual, data_type='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {},
   "source": [
    "### 3.3 Summer mass balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group together outlet glaciers of Folgefonna by renaming BREID to dummy ID 3000\n",
    "#df_train_summer_clean.loc[df_train_summer_clean['BREID'].isin([3126, 3127, 3128, 3129])]=3000\n",
    "\n",
    "# Get summer data\n",
    "df_train_summer_s = df_train_s[df_train_s['n_months']==6]\n",
    "\n",
    "# Get order of glacier ID in shuffled dataframe\n",
    "sorter = list(df_train_s.BREID.unique())\n",
    "\n",
    "# Sort dataframe of annual mb according to list\n",
    "# This is to get same folds as when using all data\n",
    "df_train_summer_s = df_train_summer_s.iloc[df_train_summer_s['BREID'].map({v: k for k, v in enumerate(sorter)}).argsort()]\n",
    "df_train_summer_s.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Shuffle df_train, get X and y values\n",
    "#df_train_s = df_train_summer_clean.sample(frac=1, random_state=5)\n",
    "#df_train_s.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Select features for training\n",
    "df_train_X_s = df_train_summer_s.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_train_y_s = df_train_summer_s[['balance']]\n",
    "\n",
    "# Get values\n",
    "X_train_s, y_train_s = df_train_X_s.values, df_train_y_s.values\n",
    "\n",
    "# Get glacier IDs from training dataset (in the order of which they appear in training dataset).\n",
    "# gp_s is an array with shape equal to the shape of X_train_s and y_train_s.\n",
    "gp_s = np.array(df_train_summer_s['BREID'].values)\n",
    "\n",
    "# Use five folds\n",
    "group_kf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Split into folds according to group by glacier ID.\n",
    "# For each unique glacier ID, indices in gp_s indicate which rows in X_train_s and y_train_s belong to the glacier.\n",
    "splits_s = list(group_kf.split(X_train_s, y_train_s, gp_s))\n",
    "\n",
    "print('Train, fold 0: ', np.unique(gp_s[splits_s[0][0]]))\n",
    "print('Validation, fold 0: ', np.unique(gp_s[splits_s[0][1]]))\n",
    "print('Train, fold 1: ', np.unique(gp_s[splits_s[1][0]]))\n",
    "print('Validation, fold 1: ', np.unique(gp_s[splits_s[1][1]]))\n",
    "print('Train, fold 2: ', np.unique(gp_s[splits_s[2][0]]))\n",
    "print('Validation, fold 2: ', np.unique(gp_s[splits_s[2][1]]))\n",
    "print('Train, fold 3: ', np.unique(gp_s[splits_s[3][0]]))\n",
    "print('Validation, fold 3: ', np.unique(gp_s[splits_s[3][1]]))\n",
    "print('Train, fold 4: ', np.unique(gp_s[splits_s[4][0]]))\n",
    "print('Validation, fold 4: ', np.unique(gp_s[splits_s[4][1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# Set return_train to False to inhibit return training scores. Default is True.\n",
    "#cv_grid, best_model, cvl_scores = train_xgb_model(X_train_s, y_train_s, splits_s, param_ranges, \n",
    "#                                                  scorer='neg_mean_squared_error', return_train=True)\n",
    "\n",
    "# Save cv-object\n",
    "#joblib.dump(cv_grid, 'Models/Final_training/cv_block_glacier_temp_prec_summer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_grid = joblib.load('Models/Final_training/cv_block_glacier_temp_prec_summer.pkl')\n",
    "best_model = cv_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, sharex=True, sharey=True, figsize=(25,25))\n",
    "a = 0    \n",
    "for train_index, test_index in splits_s:\n",
    "    # Loops over n_splits iterations and gets train and test splits in each fold\n",
    "    X_train_, X_test_ = X_train_s[train_index], X_train_s[test_index]\n",
    "    y_train_, y_test_ = y_train_s[train_index], y_train_s[test_index]\n",
    "    best_model.fit(X_train_, y_train_)\n",
    "    y_pred = best_model.predict(X_test_)\n",
    "\n",
    "    title = 'Validation fold ' + str(a)\n",
    "\n",
    "    plot_prediction_subplot(y_test_, y_pred, title, ax[a], n_toplot=5000, fold=True)\n",
    "\n",
    "    a=a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gsearch_results(cv_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "# Select features for training\n",
    "df_test_X = df_test_summer_clean.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_test_y = df_test_summer_clean[['balance']]\n",
    "\n",
    "X_test_summer, y_test_summer = df_test_X.values, df_test_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = cv_grid.best_estimator_\n",
    "test_model.fit(X_train_s, y_train_s)\n",
    "y_pred_test_summer = test_model.predict(X_test_summer)\n",
    "plot_prediction(y_test_summer, y_pred_test_summer, data_type='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138",
   "metadata": {},
   "source": [
    "### 3.4 Winter mass balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group together outlet glaciers of Folgefonna by renaming BREID to dummy ID 3000\n",
    "#df_train_summer_clean.loc[df_train_summer_clean['BREID'].isin([3126, 3127, 3128, 3129])]=3000\n",
    "\n",
    "# Get summer data\n",
    "df_train_winter_s = df_train_s[df_train_s['n_months']==8]\n",
    "\n",
    "# Get order of glacier ID in shuffled dataframe\n",
    "sorter = list(df_train_s.BREID.unique())\n",
    "\n",
    "# Sort dataframe of annual mb according to list\n",
    "# This is to get same folds as when using all data\n",
    "df_train_winter_s = df_train_winter_s.iloc[df_train_winter_s['BREID'].map({v: k for k, v in enumerate(sorter)}).argsort()]\n",
    "df_train_winter_s.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Shuffle df_train, get X and y values\n",
    "#df_train_s = df_train_summer_clean.sample(frac=1, random_state=5)\n",
    "#df_train_s.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Select features for training\n",
    "df_train_X_s = df_train_winter_s.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_train_y_s = df_train_winter_s[['balance']]\n",
    "\n",
    "# Get values\n",
    "X_train_s, y_train_s = df_train_X_s.values, df_train_y_s.values\n",
    "\n",
    "# Get glacier IDs from training dataset (in the order of which they appear in training dataset).\n",
    "# gp_s is an array with shape equal to the shape of X_train_s and y_train_s.\n",
    "gp_s = np.array(df_train_winter_s['BREID'].values)\n",
    "\n",
    "# Use five folds\n",
    "group_kf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Split into folds according to group by glacier ID.\n",
    "# For each unique glacier ID, indices in gp_s indicate which rows in X_train_s and y_train_s belong to the glacier.\n",
    "splits_s = list(group_kf.split(X_train_s, y_train_s, gp_s))\n",
    "\n",
    "print('Train, fold 0: ', np.unique(gp_s[splits_s[0][0]]))\n",
    "print('Validation, fold 0: ', np.unique(gp_s[splits_s[0][1]]))\n",
    "print('Train, fold 1: ', np.unique(gp_s[splits_s[1][0]]))\n",
    "print('Validation, fold 1: ', np.unique(gp_s[splits_s[1][1]]))\n",
    "print('Train, fold 2: ', np.unique(gp_s[splits_s[2][0]]))\n",
    "print('Validation, fold 2: ', np.unique(gp_s[splits_s[2][1]]))\n",
    "print('Train, fold 3: ', np.unique(gp_s[splits_s[3][0]]))\n",
    "print('Validation, fold 3: ', np.unique(gp_s[splits_s[3][1]]))\n",
    "print('Train, fold 4: ', np.unique(gp_s[splits_s[4][0]]))\n",
    "print('Validation, fold 4: ', np.unique(gp_s[splits_s[4][1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# Set return_train to False to inhibit return training scores. Default is True.\n",
    "#cv_grid, best_model, cvl_scores = train_xgb_model(X_train_s, y_train_s, splits_s, param_ranges, \n",
    "#                                                  scorer='neg_mean_squared_error', return_train=True)\n",
    "\n",
    "# Save cv-object\n",
    "#joblib.dump(cv_grid, 'Models/Final_training/cv_block_glacier_temp_prec_winter.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_grid = joblib.load('Models/Final_training/cv_block_glacier_temp_prec_winter.pkl')\n",
    "best_model = cv_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gsearch_results(cv_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, sharex=True, sharey=True, figsize=(25,25))\n",
    "a = 0    \n",
    "for train_index, test_index in splits_s:\n",
    "    # Loops over n_splits iterations and gets train and test splits in each fold\n",
    "    X_train_, X_test_ = X_train_s[train_index], X_train_s[test_index]\n",
    "    y_train_, y_test_ = y_train_s[train_index], y_train_s[test_index]\n",
    "    best_model.fit(X_train_, y_train_)\n",
    "    y_pred = best_model.predict(X_test_)\n",
    "\n",
    "    title = 'Validation fold ' + str(a)\n",
    "\n",
    "    plot_prediction_subplot(y_test_, y_pred, title, ax[a], n_toplot=5000, fold=True)\n",
    "\n",
    "    a=a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "# Select features for training\n",
    "df_test_X = df_test_winter_clean.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_test_y = df_test_winter_clean[['balance']]\n",
    "\n",
    "X_test_winter, y_test_winter = df_test_X.values, df_test_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = cv_grid.best_estimator_\n",
    "test_model.fit(X_train_s, y_train_s)\n",
    "y_pred_test_winter = test_model.predict(X_test_winter)\n",
    "plot_prediction(y_test_winter, y_pred_test_winter, data_type='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot annual,, summer and winter together\n",
    "y_test_all = np.hstack((y_test_annual[:,0], y_test_summer[:,0], y_test_winter[:,0]))\n",
    "y_pred_test_all = np.hstack((y_pred_test_annual, y_pred_test_summer, y_pred_test_winter))\n",
    "plot_prediction(y_test_all, y_pred_test_all, data_type='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147",
   "metadata": {},
   "source": [
    "## 4.0 Model 4: Energy balance variables with 5-fold blocking-by-glacier CV\n",
    "Features: ERA5-Land meteorological variables <br>\n",
    "CV-strategy: 5-fold blocking-by-glacier cross validation <br>\n",
    "Targets: <br>\n",
    "4.1: Annual and seasonal observations <br>\n",
    "4.2: Only annual mass balance <br>\n",
    "4.3: Only summer mass balance <br>\n",
    "4.4: Only winter mass balance <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns to drop\n",
    "cols = ['RGIID','GLIMSID','utm_zone','utm_east_approx','utm_north_approx','altitude_approx',\n",
    "        'location_description','location_id','stake_no','utm_east','utm_north','dt_prev_year_min_date','dt_curr_year_max_date',\n",
    "        'dt_curr_year_min_date','stake_remark','flag_correction','approx_loc','approx_altitude',\n",
    "        'diff_north','diff_east','diff_altitude','diff_netto','lat_approx','lon_approx',\n",
    "        'topo','dis_from_border','year', 'lat','lon', 'slope_factor']\n",
    "\n",
    "snow_depth_m = ['sde_oct','sde_nov','sde_dec','sde_jan','sde_feb','sde_mar','sde_apr','sde_may','sde_jun','sde_jul','sde_aug','sde_sep']\n",
    "snow_density = ['rsn_oct','rsn_nov','rsn_dec','rsn_jan','rsn_feb','rsn_mar','rsn_apr','rsn_may','rsn_jun','rsn_jul','rsn_aug','rsn_sep']\n",
    "evaporation = ['es_oct','es_nov','es_dec','es_jan','es_feb','es_mar','es_apr','es_may','es_jun','es_jul','es_aug','es_sep']\n",
    "snow_cover = ['snowc_oct','snowc_nov','snowc_dec','snowc_jan','snowc_feb','snowc_mar','snowc_apr','snowc_may','snowc_jun','snowc_jul','snowc_aug','snowc_sep']\n",
    "snow_depth_we = ['sd_oct','sd_nov','sd_dec','sd_jan','sd_feb','sd_mar','sd_apr','sd_may','sd_jun','sd_jul','sd_aug','sd_sep']\n",
    "snow_temp = ['tsn_oct','tsn_nov','tsn_dec','tsn_jan','tsn_feb','tsn_mar','tsn_apr','tsn_may','tsn_jun','tsn_jul','tsn_aug','tsn_sep']\n",
    "snow_melt = ['smlt_oct','smlt_nov','smlt_dec','smlt_jan','smlt_feb','smlt_mar','smlt_apr','smlt_may','smlt_jun','smlt_jul','smlt_aug','smlt_sep']\n",
    "snowfall = ['sf_oct','sf_nov','sf_dec','sf_jan','sf_feb','sf_mar','sf_apr','sf_may','sf_jun','sf_jul','sf_aug','sf_sep']\n",
    "snow_albedo = ['asn_oct','asn_nov','asn_dec','asn_jan','asn_feb','asn_mar','asn_apr','asn_may','asn_jun','asn_jul','asn_aug','asn_sep']\n",
    "dewpt_temp = ['d2m_oct','d2m_nov','d2m_dec','d2m_jan','d2m_feb','d2m_mar','d2m_apr','d2m_may','d2m_jun','d2m_jul','d2m_aug','d2m_sep']\n",
    "surface_pressure = ['sp_oct','sp_nov','sp_dec','sp_jan','sp_feb','sp_mar','sp_apr','sp_may','sp_jun','sp_jul','sp_aug','sp_sep']\n",
    "sol_rad_net = ['ssr_oct','ssr_nov','ssr_dec','ssr_jan','ssr_feb','ssr_mar','ssr_apr','ssr_may','ssr_jun','ssr_jul','ssr_aug','ssr_sep']\n",
    "sol_therm_down = ['strd_oct','strd_nov','strd_dec','strd_jan','strd_feb','strd_mar','strd_apr','strd_may','strd_jun','strd_jul','strd_aug','strd_sep']\n",
    "#sol_rad_down = ['ssrd_oct','ssrd_nov','ssrd_dec','ssrd_jan','ssrd_feb','ssrd_mar','ssrd_apr','ssrd_may','ssrd_jun','ssrd_jul','ssrd_aug','ssrd_sep']\n",
    "u_wind = ['u10_oct','u10_nov','u10_dec','u10_jan','u10_feb','u10_mar','u10_apr','u10_may','u10_jun','u10_jul','u10_aug','u10_sep']\n",
    "v_wind = ['v10_oct','v10_nov','v10_dec','v10_jan','v10_feb','v10_mar','v10_apr','v10_may','v10_jun','v10_jul','v10_aug','v10_sep']\n",
    "#f_albedo = ['fal_oct','fal_nov','fal_dec','fal_jan','fal_feb','fal_mar','fal_apr','fal_may','fal_jun','fal_jul','fal_aug','fal_sep']\n",
    "\n",
    "drop_cols = [y for x in [cols, snow_depth_m, snow_density, evaporation, snow_cover, snow_depth_we, snow_temp, snow_melt, snowfall, snow_albedo, \n",
    "                         dewpt_temp, surface_pressure, sol_rad_net, sol_therm_down, u_wind, v_wind] for y in x]\n",
    "\n",
    "# Select features for training\n",
    "df_train_annual_clean = df_train_annual.drop(drop_cols, axis=1)\n",
    "df_train_winter_clean = df_train_winter.drop(drop_cols, axis=1)\n",
    "df_train_summer_clean = df_train_summer.drop(drop_cols, axis=1)\n",
    "df_train_annual_clean = df_train_annual_clean.drop(['balance_winter','balance_summer'], axis=1)\n",
    "df_train_winter_clean = df_train_winter_clean.drop(['balance_netto', 'balance_summer'], axis=1)\n",
    "df_train_summer_clean = df_train_summer_clean.drop(['balance_netto', 'balance_winter'], axis=1)\n",
    "\n",
    "# Rename target columns to same name\n",
    "df_train_annual_clean.rename(columns={'balance_netto' : 'balance'}, inplace=True)\n",
    "df_train_winter_clean.rename(columns={'balance_winter' : 'balance'}, inplace=True)\n",
    "df_train_summer_clean.rename(columns={'balance_summer' : 'balance'}, inplace=True)\n",
    "\n",
    "# df_train_X_... now contains columns of all chosen features and column with annual, winter or summer balance\n",
    "\n",
    "# For summer balance, replace column values in accumulation months with NaN (oct, nov, dec, jan, feb, mar, apr, may)\n",
    "# For winter balance, replace column values in ablation months with NaN (may, jun, jul, aug, sep, oct)\n",
    "var = ['t2m', 'sshf', 'slhf', 'ssrd', 'fal','str', 'tp']\n",
    "mon_summer = ['may', 'jun', 'jul', 'aug', 'sep', 'oct']\n",
    "mon_winter = ['oct', 'nov', 'dec', 'jan', 'feb', 'mar', 'apr', 'may']\n",
    "\n",
    "for name in var:\n",
    "    for mon in mon_summer:\n",
    "        df_train_winter_clean[name+'_'+mon]= np.nan\n",
    "for name in var:   \n",
    "    for mon in mon_winter:\n",
    "        df_train_summer_clean[name+'_'+mon]= np.nan\n",
    "\n",
    "# Combine all annual, winter and summer data in one dataframe\n",
    "df_train_all = pd.concat([df_train_annual_clean, df_train_summer_clean, df_train_winter_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "\n",
    "# Select features for training\n",
    "df_test_annual_clean = df_test_annual.drop(drop_cols, axis=1)\n",
    "df_test_winter_clean = df_test_winter.drop(drop_cols, axis=1)\n",
    "df_test_summer_clean = df_test_summer.drop(drop_cols, axis=1)\n",
    "df_test_annual_clean = df_test_annual_clean.drop(['balance_winter','balance_summer'], axis=1)\n",
    "df_test_winter_clean = df_test_winter_clean.drop(['balance_netto', 'balance_summer'], axis=1)\n",
    "df_test_summer_clean = df_test_summer_clean.drop(['balance_netto', 'balance_winter'], axis=1)\n",
    "\n",
    "# Rename target columns to same name\n",
    "df_test_annual_clean.rename(columns={'balance_netto' : 'balance'}, inplace=True)\n",
    "df_test_winter_clean.rename(columns={'balance_winter' : 'balance'}, inplace=True)\n",
    "df_test_summer_clean.rename(columns={'balance_summer' : 'balance'}, inplace=True)\n",
    "\n",
    "# For summer balance, replace column values in accumulation months with NaN (oct, nov, dec, jan, feb, mar, apr, may)\n",
    "# For winter balance, replace column values in ablation months with NaN (may, jun, jul, aug, sep, oct)\n",
    "for name in var:\n",
    "    for mon in mon_summer:\n",
    "        df_test_winter_clean[name+'_'+mon]= np.nan\n",
    "for name in var:   \n",
    "    for mon in mon_winter:\n",
    "        df_test_summer_clean[name+'_'+mon]= np.nan\n",
    "\n",
    "# Combine all annual, winter and summer data in one dataframe\n",
    "df_test_all = pd.concat([df_test_annual_clean, df_test_summer_clean, df_test_winter_clean])\n",
    "df_test_all.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151",
   "metadata": {},
   "source": [
    "### 4.1 Ann + seas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with altitude_diff\n",
    "df_train_all['altitude_diff'] = df_train_all['altitude_climate']-df_train_all['altitude']\n",
    "df_train_all = df_train_all.drop(['altitude','altitude_climate'], axis=1)\n",
    "df_train_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group together outlet glaciers of Folgefonna by renaming BREID to dummy ID 3000\n",
    "#df_train_all.loc[df_train_all['BREID'].isin([3126, 3127, 3128, 3129])]=3000\n",
    "# Remove Nigardsbreen\n",
    "#df_train_all = df_train_all.loc[df_train_all['BREID'] != 2297]\n",
    "# Remove instances where elevation is lower than 700 m a sl\n",
    "#df_train_all = df_train_all.loc[df_train_all['altitude'] >= 750]\n",
    "\n",
    "# Shuffle df_train, get X and y values\n",
    "df_train_s = df_train_all.sample(frac=1, random_state=5)\n",
    "df_train_s.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Select features for training\n",
    "df_train_X_s = df_train_s.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_train_y_s = df_train_s[['balance']]\n",
    "\n",
    "# Get values\n",
    "X_train_s, y_train_s = df_train_X_s.values, df_train_y_s.values\n",
    "\n",
    "# Get glacier IDs from training dataset (in the order of which they appear in training dataset).\n",
    "# gp_s is an array with shape equal to the shape of X_train_s and y_train_s.\n",
    "gp_s = np.array(df_train_s['BREID'].values)\n",
    "\n",
    "# Use five folds\n",
    "group_kf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Split into folds according to group by glacier ID.\n",
    "# For each unique glacier ID, indices in gp_s indicate which rows in X_train_s and y_train_s belong to the glacier.\n",
    "splits_s = list(group_kf.split(X_train_s, y_train_s, gp_s))\n",
    "\n",
    "print('Train, fold 0: ', np.unique(gp_s[splits_s[0][0]]))\n",
    "print('Validation, fold 0: ', np.unique(gp_s[splits_s[0][1]]))\n",
    "print('Train, fold 1: ', np.unique(gp_s[splits_s[1][0]]))\n",
    "print('Validation, fold 1: ', np.unique(gp_s[splits_s[1][1]]))\n",
    "print('Train, fold 2: ', np.unique(gp_s[splits_s[2][0]]))\n",
    "print('Validation, fold 2: ', np.unique(gp_s[splits_s[2][1]]))\n",
    "print('Train, fold 3: ', np.unique(gp_s[splits_s[3][0]]))\n",
    "print('Validation, fold 3: ', np.unique(gp_s[splits_s[3][1]]))\n",
    "print('Train, fold 4: ', np.unique(gp_s[splits_s[4][0]]))\n",
    "print('Validation, fold 4: ', np.unique(gp_s[splits_s[4][1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# Set return_train to False to inhibit return training scores. Default is True.\n",
    "cv_grid, best_model, cvl_scores = train_xgb_model(X_train_s, y_train_s, splits_s, param_ranges, \n",
    "                                                  scorer='neg_mean_squared_error', return_train=True)\n",
    "\n",
    "# Save cv-object\n",
    "joblib.dump(cv_grid, 'Models/Final_training/cv_block_glacier_all_climate_ann+seas_altitude_diff.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_grid = joblib.load('Models/Final_training/cv_block_glacier_all_climate_ann+seas_altitude_diff.pkl')\n",
    "best_model = cv_grid.best_estimator_\n",
    "#best_model = xgb.XGBRegressor(n_estimators=400, max_depth=5, learning_rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gsearch_results(cv_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_permutation_importance(df_train_X_s, X_train_s, y_train_s, splits_s, best_model, max_features_plot = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_per_fold(X_train_s, y_train_s, best_model, splits_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, sharex=True, sharey=True, figsize=(25,25))\n",
    "a = 0    \n",
    "for train_index, test_index in splits_s:\n",
    "    # Loops over n_splits iterations and gets train and test splits in each fold\n",
    "    X_train_, X_test_ = X_train_s[train_index], X_train_s[test_index]\n",
    "    y_train_, y_test_ = y_train_s[train_index], y_train_s[test_index]\n",
    "    best_model.fit(X_train_, y_train_)\n",
    "    y_pred = best_model.predict(X_test_)\n",
    "\n",
    "    title = 'Validation fold ' + str(a)\n",
    "\n",
    "    plot_prediction_subplot(y_test_, y_pred, title, ax[a], n_toplot=5000, fold=True)\n",
    "\n",
    "    a=a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_annual, y_pred_annual = get_prediction_per_season(X_train_s, y_train_s, splits_s, best_model, months=12)\n",
    "y_test_winter, y_pred_winter = get_prediction_per_season(X_train_s, y_train_s, splits_s, best_model, months=8)\n",
    "y_test_summer, y_pred_summer = get_prediction_per_season(X_train_s, y_train_s, splits_s, best_model, months=6)\n",
    "\n",
    "print(y_test_annual.shape)\n",
    "print(y_test_winter.shape)\n",
    "print(y_test_summer.shape)\n",
    "\n",
    "y_test_all = np.hstack((y_test_annual, y_test_winter, y_test_summer))\n",
    "y_pred_all = np.hstack((y_pred_annual, y_pred_winter, y_pred_summer))\n",
    "\n",
    "print(y_test_all.shape)\n",
    "print(y_pred_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_per_season(y_test_winter, y_pred_winter, season='Winter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with elev diff\n",
    "df_test_all['altitude_diff'] = df_test_all['altitude_climate']-df_test_all['altitude']\n",
    "df_test_all = df_test_all.drop(['altitude','altitude_climate'], axis=1)\n",
    "#plt.hist(df_test_all['altitude_diff'])\n",
    "df_test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "# Select features for training\n",
    "df_test_X = df_test_all.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_test_y = df_test_all[['balance']]\n",
    "\n",
    "X_test, y_test = df_test_X.values, df_test_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = cv_grid.best_estimator_\n",
    "#test_model = xgb.XGBRegressor(n_estimators=400, max_depth=5, learning_rate=0.2)\n",
    "test_model.fit(X_train_s, y_train_s)\n",
    "y_pred_test = test_model.predict(X_test)\n",
    "plot_prediction(y_test, y_pred_test, data_type='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_crop, y_pred_crop = get_prediction_per_season_test(X_test, y_test, test_model, months=12)\n",
    "plot_prediction_per_season(y_test_crop, y_pred_crop, season='Annual')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166",
   "metadata": {},
   "source": [
    "### 4.1 Annual mass balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group together outlet glaciers of Folgefonna by renaming BREID to dummy ID 3000\n",
    "#df_train_annual_clean.loc[df_train_annual_clean['BREID'].isin([3126, 3127, 3128, 3129])]=3000\n",
    "\n",
    "# Get annual data\n",
    "df_train_ann_s = df_train_s[df_train_s['n_months']==12]\n",
    "\n",
    "# Get order of glacier ID in shuffled dataframe\n",
    "sorter = list(df_train_s.BREID.unique())\n",
    "\n",
    "# Sort dataframe of annual mb according to list\n",
    "# This is to get same folds as when using all data\n",
    "df_train_ann_s = df_train_ann_s.iloc[df_train_ann_s['BREID'].map({v: k for k, v in enumerate(sorter)}).argsort()]\n",
    "df_train_ann_s.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Select features for training\n",
    "df_train_X_s = df_train_ann_s.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_train_y_s = df_train_ann_s[['balance']]\n",
    "\n",
    "# Get values\n",
    "X_train_s, y_train_s = df_train_X_s.values, df_train_y_s.values\n",
    "\n",
    "# Get glacier IDs from training dataset (in the order of which they appear in training dataset).\n",
    "# gp_s is an array with shape equal to the shape of X_train_s and y_train_s.\n",
    "gp_s = np.array(df_train_ann_s['BREID'].values)\n",
    "\n",
    "# Use five folds\n",
    "group_kf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Split into folds according to group by glacier ID.\n",
    "# For each unique glacier ID, indices in gp_s indicate which rows in X_train_s and y_train_s belong to the glacier.\n",
    "splits_s = list(group_kf.split(X_train_s, y_train_s, gp_s))\n",
    "\n",
    "print('Train, fold 0: ', np.unique(gp_s[splits_s[0][0]]))\n",
    "print('Validation, fold 0: ', np.unique(gp_s[splits_s[0][1]]))\n",
    "print('Train, fold 1: ', np.unique(gp_s[splits_s[1][0]]))\n",
    "print('Validation, fold 1: ', np.unique(gp_s[splits_s[1][1]]))\n",
    "print('Train, fold 2: ', np.unique(gp_s[splits_s[2][0]]))\n",
    "print('Validation, fold 2: ', np.unique(gp_s[splits_s[2][1]]))\n",
    "print('Train, fold 3: ', np.unique(gp_s[splits_s[3][0]]))\n",
    "print('Validation, fold 3: ', np.unique(gp_s[splits_s[3][1]]))\n",
    "print('Train, fold 4: ', np.unique(gp_s[splits_s[4][0]]))\n",
    "print('Validation, fold 4: ', np.unique(gp_s[splits_s[4][1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# Set return_train to False to inhibit return training scores. Default is True.\n",
    "#cv_grid, best_model, cvl_scores = train_xgb_model(X_train_s, y_train_s, splits_s, param_ranges, \n",
    "#                                                  scorer='neg_mean_squared_error', return_train=True)\n",
    "\n",
    "# Save cv-object\n",
    "#joblib.dump(cv_grid, 'Models/Final_training/cv_block_glacier_all_climate_annual.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_grid = joblib.load('Models/Final_training/cv_block_glacier_all_climate_annual.pkl')\n",
    "best_model = cv_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gsearch_results(cv_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, sharex=True, sharey=True, figsize=(25,25))\n",
    "a = 0    \n",
    "for train_index, test_index in splits_s:\n",
    "    # Loops over n_splits iterations and gets train and test splits in each fold\n",
    "    X_train_, X_test_ = X_train_s[train_index], X_train_s[test_index]\n",
    "    y_train_, y_test_ = y_train_s[train_index], y_train_s[test_index]\n",
    "    best_model.fit(X_train_, y_train_)\n",
    "    y_pred = best_model.predict(X_test_)\n",
    "\n",
    "    title = 'Validation fold ' + str(a)\n",
    "\n",
    "    plot_prediction_subplot(y_test_, y_pred, title, ax[a], n_toplot=5000, fold=True)\n",
    "\n",
    "    a=a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "# Select features for training\n",
    "df_test_X = df_test_annual_clean.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_test_y = df_test_annual_clean[['balance']]\n",
    "\n",
    "X_test_annual, y_test_annual = df_test_X.values, df_test_y.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = cv_grid.best_estimator_\n",
    "test_model.fit(X_train_s, y_train_s)\n",
    "y_pred_test_annual = test_model.predict(X_test_annual)\n",
    "plot_prediction(y_test_annual, y_pred_test_annual, data_type='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174",
   "metadata": {},
   "source": [
    "### Summer mass balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group together outlet glaciers of Folgefonna by renaming BREID to dummy ID 3000\n",
    "#df_train_summer_clean.loc[df_train_summer_clean['BREID'].isin([3126, 3127, 3128, 3129])]=3000\n",
    "\n",
    "# Get summer data\n",
    "df_train_summer_s = df_train_s[df_train_s['n_months']==6]\n",
    "\n",
    "# Get order of glacier ID in shuffled dataframe\n",
    "sorter = list(df_train_s.BREID.unique())\n",
    "\n",
    "# Sort dataframe of annual mb according to list\n",
    "# This is to get same folds as when using all data\n",
    "df_train_summer_s = df_train_summer_s.iloc[df_train_summer_s['BREID'].map({v: k for k, v in enumerate(sorter)}).argsort()]\n",
    "df_train_summer_s.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Shuffle df_train, get X and y values\n",
    "#df_train_s = df_train_summer_clean.sample(frac=1, random_state=5)\n",
    "#df_train_s.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Select features for training\n",
    "df_train_X_s = df_train_summer_s.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_train_y_s = df_train_summer_s[['balance']]\n",
    "\n",
    "# Get values\n",
    "X_train_s, y_train_s = df_train_X_s.values, df_train_y_s.values\n",
    "\n",
    "# Get glacier IDs from training dataset (in the order of which they appear in training dataset).\n",
    "# gp_s is an array with shape equal to the shape of X_train_s and y_train_s.\n",
    "gp_s = np.array(df_train_summer_s['BREID'].values)\n",
    "\n",
    "# Use five folds\n",
    "group_kf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Split into folds according to group by glacier ID.\n",
    "# For each unique glacier ID, indices in gp_s indicate which rows in X_train_s and y_train_s belong to the glacier.\n",
    "splits_s = list(group_kf.split(X_train_s, y_train_s, gp_s))\n",
    "\n",
    "print('Train, fold 0: ', np.unique(gp_s[splits_s[0][0]]))\n",
    "print('Validation, fold 0: ', np.unique(gp_s[splits_s[0][1]]))\n",
    "print('Train, fold 1: ', np.unique(gp_s[splits_s[1][0]]))\n",
    "print('Validation, fold 1: ', np.unique(gp_s[splits_s[1][1]]))\n",
    "print('Train, fold 2: ', np.unique(gp_s[splits_s[2][0]]))\n",
    "print('Validation, fold 2: ', np.unique(gp_s[splits_s[2][1]]))\n",
    "print('Train, fold 3: ', np.unique(gp_s[splits_s[3][0]]))\n",
    "print('Validation, fold 3: ', np.unique(gp_s[splits_s[3][1]]))\n",
    "print('Train, fold 4: ', np.unique(gp_s[splits_s[4][0]]))\n",
    "print('Validation, fold 4: ', np.unique(gp_s[splits_s[4][1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# Set return_train to False to inhibit return training scores. Default is True.\n",
    "#cv_grid, best_model, cvl_scores = train_xgb_model(X_train_s, y_train_s, splits_s, param_ranges, \n",
    "#                                                  scorer='neg_mean_squared_error', return_train=True)\n",
    "\n",
    "# Save cv-object\n",
    "#joblib.dump(cv_grid, 'Models/Final_training/cv_block_glacier_all_climate_summer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_grid = joblib.load('Models/Final_training/cv_block_glacier_all_climate_summer.pkl')\n",
    "best_model = cv_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gsearch_results(cv_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, sharex=True, sharey=True, figsize=(25,25))\n",
    "a = 0    \n",
    "for train_index, test_index in splits_s:\n",
    "    # Loops over n_splits iterations and gets train and test splits in each fold\n",
    "    X_train_, X_test_ = X_train_s[train_index], X_train_s[test_index]\n",
    "    y_train_, y_test_ = y_train_s[train_index], y_train_s[test_index]\n",
    "    best_model.fit(X_train_, y_train_)\n",
    "    y_pred = best_model.predict(X_test_)\n",
    "\n",
    "    title = 'Validation fold ' + str(a)\n",
    "\n",
    "    plot_prediction_subplot(y_test_, y_pred, title, ax[a], n_toplot=5000, fold=True)\n",
    "\n",
    "    a=a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "# Select features for training\n",
    "df_test_X = df_test_summer_clean.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_test_y = df_test_summer_clean[['balance']]\n",
    "\n",
    "X_test_summer, y_test_summer = df_test_X.values, df_test_y.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = cv_grid.best_estimator_\n",
    "test_model.fit(X_train_s, y_train_s)\n",
    "y_pred_test_summer = test_model.predict(X_test_summer)\n",
    "plot_prediction(y_test_summer, y_pred_test_summer, data_type='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182",
   "metadata": {},
   "source": [
    "### 4.4 Winter mass balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group together outlet glaciers of Folgefonna by renaming BREID to dummy ID 3000\n",
    "#df_train_summer_clean.loc[df_train_summer_clean['BREID'].isin([3126, 3127, 3128, 3129])]=3000\n",
    "\n",
    "# Get summer data\n",
    "df_train_winter_s = df_train_s[df_train_s['n_months']==8]\n",
    "\n",
    "# Get order of glacier ID in shuffled dataframe\n",
    "sorter = list(df_train_s.BREID.unique())\n",
    "\n",
    "# Sort dataframe of annual mb according to list\n",
    "# This is to get same folds as when using all data\n",
    "df_train_winter_s = df_train_winter_s.iloc[df_train_winter_s['BREID'].map({v: k for k, v in enumerate(sorter)}).argsort()]\n",
    "df_train_winter_s.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Shuffle df_train, get X and y values\n",
    "#df_train_s = df_train_summer_clean.sample(frac=1, random_state=5)\n",
    "#df_train_s.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Select features for training\n",
    "df_train_X_s = df_train_winter_s.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_train_y_s = df_train_winter_s[['balance']]\n",
    "\n",
    "# Get values\n",
    "X_train_s, y_train_s = df_train_X_s.values, df_train_y_s.values\n",
    "\n",
    "# Get glacier IDs from training dataset (in the order of which they appear in training dataset).\n",
    "# gp_s is an array with shape equal to the shape of X_train_s and y_train_s.\n",
    "gp_s = np.array(df_train_winter_s['BREID'].values)\n",
    "\n",
    "# Use five folds\n",
    "group_kf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Split into folds according to group by glacier ID.\n",
    "# For each unique glacier ID, indices in gp_s indicate which rows in X_train_s and y_train_s belong to the glacier.\n",
    "splits_s = list(group_kf.split(X_train_s, y_train_s, gp_s))\n",
    "\n",
    "print('Train, fold 0: ', np.unique(gp_s[splits_s[0][0]]))\n",
    "print('Validation, fold 0: ', np.unique(gp_s[splits_s[0][1]]))\n",
    "print('Train, fold 1: ', np.unique(gp_s[splits_s[1][0]]))\n",
    "print('Validation, fold 1: ', np.unique(gp_s[splits_s[1][1]]))\n",
    "print('Train, fold 2: ', np.unique(gp_s[splits_s[2][0]]))\n",
    "print('Validation, fold 2: ', np.unique(gp_s[splits_s[2][1]]))\n",
    "print('Train, fold 3: ', np.unique(gp_s[splits_s[3][0]]))\n",
    "print('Validation, fold 3: ', np.unique(gp_s[splits_s[3][1]]))\n",
    "print('Train, fold 4: ', np.unique(gp_s[splits_s[4][0]]))\n",
    "print('Validation, fold 4: ', np.unique(gp_s[splits_s[4][1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# Set return_train to False to inhibit return training scores. Default is True.\n",
    "#cv_grid, best_model, cvl_scores = train_xgb_model(X_train_s, y_train_s, splits_s, param_ranges, \n",
    "#                                                  scorer='neg_mean_squared_error', return_train=True)\n",
    "\n",
    "# Save cv-object\n",
    "#joblib.dump(cv_grid, 'Models/Final_training/cv_block_glacier_all_climate_winter.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_grid = joblib.load('Models/Final_training/cv_block_glacier_all_climate_winter.pkl')\n",
    "best_model = cv_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gsearch_results(cv_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, sharex=True, sharey=True, figsize=(25,25))\n",
    "a = 0    \n",
    "for train_index, test_index in splits_s:\n",
    "    # Loops over n_splits iterations and gets train and test splits in each fold\n",
    "    X_train_, X_test_ = X_train_s[train_index], X_train_s[test_index]\n",
    "    y_train_, y_test_ = y_train_s[train_index], y_train_s[test_index]\n",
    "    best_model.fit(X_train_, y_train_)\n",
    "    y_pred = best_model.predict(X_test_)\n",
    "\n",
    "    title = 'Validation fold ' + str(a)\n",
    "\n",
    "    plot_prediction_subplot(y_test_, y_pred, title, ax[a], n_toplot=5000, fold=True)\n",
    "\n",
    "    a=a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "# Select features for training\n",
    "df_test_X = df_test_winter_clean.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_test_y = df_test_winter_clean[['balance']]\n",
    "\n",
    "X_test_winter, y_test_winter = df_test_X.values, df_test_y.values\n",
    "test_model = cv_grid.best_estimator_\n",
    "test_model.fit(X_train_s, y_train_s)\n",
    "y_pred_test_winter = test_model.predict(X_test_winter)\n",
    "plot_prediction(y_test_winter, y_pred_test_winter, data_type='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot annual,, summer and winter together\n",
    "y_test_all = np.hstack((y_test_annual[:,0], y_test_summer[:,0], y_test_winter[:,0]))\n",
    "y_pred_test_all = np.hstack((y_pred_test_annual, y_pred_test_summer, y_pred_test_winter))\n",
    "plot_prediction(y_test_all, y_pred_test_all, data_type='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190",
   "metadata": {},
   "source": [
    "# 5.0 using elevation diff instead of elevation and climate elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns to drop\n",
    "cols = ['RGIID','GLIMSID','utm_zone','utm_east_approx','utm_north_approx','altitude_approx',\n",
    "        'location_description','location_id','stake_no','utm_east','utm_north','dt_prev_year_min_date','dt_curr_year_max_date',\n",
    "        'dt_curr_year_min_date','stake_remark','flag_correction','approx_loc','approx_altitude',\n",
    "        'diff_north','diff_east','diff_altitude','diff_netto','lat_approx','lon_approx',\n",
    "        'topo','dis_from_border','year', 'lat','lon', 'slope_factor']\n",
    "\n",
    "snow_depth_m = ['sde_oct','sde_nov','sde_dec','sde_jan','sde_feb','sde_mar','sde_apr','sde_may','sde_jun','sde_jul','sde_aug','sde_sep']\n",
    "snow_density = ['rsn_oct','rsn_nov','rsn_dec','rsn_jan','rsn_feb','rsn_mar','rsn_apr','rsn_may','rsn_jun','rsn_jul','rsn_aug','rsn_sep']\n",
    "evaporation = ['es_oct','es_nov','es_dec','es_jan','es_feb','es_mar','es_apr','es_may','es_jun','es_jul','es_aug','es_sep']\n",
    "snow_cover = ['snowc_oct','snowc_nov','snowc_dec','snowc_jan','snowc_feb','snowc_mar','snowc_apr','snowc_may','snowc_jun','snowc_jul','snowc_aug','snowc_sep']\n",
    "snow_depth_we = ['sd_oct','sd_nov','sd_dec','sd_jan','sd_feb','sd_mar','sd_apr','sd_may','sd_jun','sd_jul','sd_aug','sd_sep']\n",
    "snow_temp = ['tsn_oct','tsn_nov','tsn_dec','tsn_jan','tsn_feb','tsn_mar','tsn_apr','tsn_may','tsn_jun','tsn_jul','tsn_aug','tsn_sep']\n",
    "snow_melt = ['smlt_oct','smlt_nov','smlt_dec','smlt_jan','smlt_feb','smlt_mar','smlt_apr','smlt_may','smlt_jun','smlt_jul','smlt_aug','smlt_sep']\n",
    "snowfall = ['sf_oct','sf_nov','sf_dec','sf_jan','sf_feb','sf_mar','sf_apr','sf_may','sf_jun','sf_jul','sf_aug','sf_sep']\n",
    "snow_albedo = ['asn_oct','asn_nov','asn_dec','asn_jan','asn_feb','asn_mar','asn_apr','asn_may','asn_jun','asn_jul','asn_aug','asn_sep']\n",
    "dewpt_temp = ['d2m_oct','d2m_nov','d2m_dec','d2m_jan','d2m_feb','d2m_mar','d2m_apr','d2m_may','d2m_jun','d2m_jul','d2m_aug','d2m_sep']\n",
    "surface_pressure = ['sp_oct','sp_nov','sp_dec','sp_jan','sp_feb','sp_mar','sp_apr','sp_may','sp_jun','sp_jul','sp_aug','sp_sep']\n",
    "sol_rad_net = ['ssr_oct','ssr_nov','ssr_dec','ssr_jan','ssr_feb','ssr_mar','ssr_apr','ssr_may','ssr_jun','ssr_jul','ssr_aug','ssr_sep']\n",
    "sol_therm_down = ['strd_oct','strd_nov','strd_dec','strd_jan','strd_feb','strd_mar','strd_apr','strd_may','strd_jun','strd_jul','strd_aug','strd_sep']\n",
    "#sol_rad_down = ['ssrd_oct','ssrd_nov','ssrd_dec','ssrd_jan','ssrd_feb','ssrd_mar','ssrd_apr','ssrd_may','ssrd_jun','ssrd_jul','ssrd_aug','ssrd_sep']\n",
    "u_wind = ['u10_oct','u10_nov','u10_dec','u10_jan','u10_feb','u10_mar','u10_apr','u10_may','u10_jun','u10_jul','u10_aug','u10_sep']\n",
    "v_wind = ['v10_oct','v10_nov','v10_dec','v10_jan','v10_feb','v10_mar','v10_apr','v10_may','v10_jun','v10_jul','v10_aug','v10_sep']\n",
    "#f_albedo = ['fal_oct','fal_nov','fal_dec','fal_jan','fal_feb','fal_mar','fal_apr','fal_may','fal_jun','fal_jul','fal_aug','fal_sep']\n",
    "\n",
    "drop_cols = [y for x in [cols, snow_depth_m, snow_density, evaporation, snow_cover, snow_depth_we, snow_temp, snow_melt, snowfall, snow_albedo, \n",
    "                         dewpt_temp, surface_pressure, sol_rad_net, sol_therm_down, u_wind, v_wind] for y in x]\n",
    "\n",
    "# Select features for training\n",
    "df_train_annual_clean = df_train_annual.drop(drop_cols, axis=1)\n",
    "df_train_winter_clean = df_train_winter.drop(drop_cols, axis=1)\n",
    "df_train_summer_clean = df_train_summer.drop(drop_cols, axis=1)\n",
    "df_train_annual_clean = df_train_annual_clean.drop(['balance_winter','balance_summer'], axis=1)\n",
    "df_train_winter_clean = df_train_winter_clean.drop(['balance_netto', 'balance_summer'], axis=1)\n",
    "df_train_summer_clean = df_train_summer_clean.drop(['balance_netto', 'balance_winter'], axis=1)\n",
    "\n",
    "# Rename target columns to same name\n",
    "df_train_annual_clean.rename(columns={'balance_netto' : 'balance'}, inplace=True)\n",
    "df_train_winter_clean.rename(columns={'balance_winter' : 'balance'}, inplace=True)\n",
    "df_train_summer_clean.rename(columns={'balance_summer' : 'balance'}, inplace=True)\n",
    "\n",
    "# df_train_X_... now contains columns of all chosen features and column with annual, winter or summer balance\n",
    "\n",
    "# For summer balance, replace column values in accumulation months with NaN (oct, nov, dec, jan, feb, mar, apr, may)\n",
    "# For winter balance, replace column values in ablation months with NaN (may, jun, jul, aug, sep, oct)\n",
    "var = ['t2m', 'sshf', 'slhf', 'ssrd', 'fal','str', 'tp']\n",
    "mon_summer = ['may', 'jun', 'jul', 'aug', 'sep', 'oct']\n",
    "mon_winter = ['oct', 'nov', 'dec', 'jan', 'feb', 'mar', 'apr', 'may']\n",
    "\n",
    "for name in var:\n",
    "    for mon in mon_summer:\n",
    "        df_train_winter_clean[name+'_'+mon]= np.nan\n",
    "for name in var:   \n",
    "    for mon in mon_winter:\n",
    "        df_train_summer_clean[name+'_'+mon]= np.nan\n",
    "\n",
    "# Combine all annual, winter and summer data in one dataframe\n",
    "df_train_all = pd.concat([df_train_annual_clean, df_train_summer_clean, df_train_winter_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all['altitude_diff'] = df_train_all['altitude_climate']-df_train_all['altitude']\n",
    "df_train_all = df_train_all.drop(['altitude','altitude_climate'], axis=1)\n",
    "df_train_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_train_all['altitude_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "\n",
    "# Select features for training\n",
    "df_test_annual_clean = df_test_annual.drop(drop_cols, axis=1)\n",
    "df_test_winter_clean = df_test_winter.drop(drop_cols, axis=1)\n",
    "df_test_summer_clean = df_test_summer.drop(drop_cols, axis=1)\n",
    "df_test_annual_clean = df_test_annual_clean.drop(['balance_winter','balance_summer'], axis=1)\n",
    "df_test_winter_clean = df_test_winter_clean.drop(['balance_netto', 'balance_summer'], axis=1)\n",
    "df_test_summer_clean = df_test_summer_clean.drop(['balance_netto', 'balance_winter'], axis=1)\n",
    "\n",
    "# Rename target columns to same name\n",
    "df_test_annual_clean.rename(columns={'balance_netto' : 'balance'}, inplace=True)\n",
    "df_test_winter_clean.rename(columns={'balance_winter' : 'balance'}, inplace=True)\n",
    "df_test_summer_clean.rename(columns={'balance_summer' : 'balance'}, inplace=True)\n",
    "\n",
    "# For summer balance, replace column values in accumulation months with NaN (oct, nov, dec, jan, feb, mar, apr, may)\n",
    "# For winter balance, replace column values in ablation months with NaN (may, jun, jul, aug, sep, oct)\n",
    "for name in var:\n",
    "    for mon in mon_summer:\n",
    "        df_test_winter_clean[name+'_'+mon]= np.nan\n",
    "for name in var:   \n",
    "    for mon in mon_winter:\n",
    "        df_test_summer_clean[name+'_'+mon]= np.nan\n",
    "\n",
    "# Combine all annual, winter and summer data in one dataframe\n",
    "df_test_all = pd.concat([df_test_annual_clean, df_test_summer_clean, df_test_winter_clean])\n",
    "df_test_all.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_all['altitude_diff'] = df_test_all['altitude_climate']-df_test_all['altitude']\n",
    "df_test_all = df_test_all.drop(['altitude','altitude_climate'], axis=1)\n",
    "plt.hist(df_test_all['altitude_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197",
   "metadata": {},
   "source": [
    "### 2.1 Annual and seasonal mass balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove instances where elevation is lower than 700 m a sl\n",
    "#df_train_all = df_train_all.loc[df_train_all['altitude'] >= 750]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter ranges.\n",
    "param_ranges = {'max_depth': [3, 4, 5, 6, 7, 8],\n",
    "          'n_estimators': [100, 200, 300, 400, 500, 600, 700], # number of trees (too many = overfitting, too few = underfitting)\n",
    "          'learning_rate': [0.01, 0.1, 0.15, 0.2, 0.25, 0.3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training\n",
    "df_train_X = df_train_all.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_train_y = df_train_all[['balance']]\n",
    "\n",
    "# Get arrays of features and targets\n",
    "X_train, y_train = df_train_X.values, df_train_y.values\n",
    "\n",
    "# Set random seed for split\n",
    "rand_seed = 25\n",
    "\n",
    "# Use five folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=rand_seed)\n",
    "splits = list(kf.split(X_train, y_train))\n",
    "\n",
    "# Train model\n",
    "# Set return_train to False to inhibit return training scores. Default is True.\n",
    "cv_grid, best_model, cvl_scores = train_xgb_model(X_train, y_train, splits, param_ranges, \n",
    "                                                  scorer='neg_mean_squared_error', return_train=True)\n",
    "\n",
    "# Save cv-object\n",
    "joblib.dump(cv_grid, 'Models/Final_training/cv_rand_5fold_all_climate_ann+seas_altitude_diff_more_trees.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_grid = joblib.load('Models/Final_training/cv_rand_5fold_all_climate_ann+seas_altitude_diff_more_trees.pkl')\n",
    "best_model = cv_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gsearch_results(cv_grid)\n",
    "#plot_prediction_per_fold(X_train, y_train, best_model, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, sharex=True, sharey=True, figsize=(25,25))\n",
    "a = 0    \n",
    "for train_index, test_index in splits:\n",
    "    # Loops over n_splits iterations and gets train and test splits in each fold\n",
    "    X_train_, X_test_ = X_train[train_index], X_train[test_index]\n",
    "    y_train_, y_test_ = y_train[train_index], y_train[test_index]\n",
    "    best_model.fit(X_train_, y_train_)\n",
    "    y_pred = best_model.predict(X_test_)\n",
    "\n",
    "    title = 'Validation fold ' + str(a)\n",
    "\n",
    "    plot_prediction_subplot(y_test_, y_pred, title, ax[a], n_toplot=5000, fold=True)\n",
    "\n",
    "    a=a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_permutation_importance(df_train_X, X_train, y_train, splits, best_model, max_features_plot = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_annual, y_pred_annual = get_prediction_per_season(X_train, y_train, splits, best_model, months=12)\n",
    "y_test_winter, y_pred_winter = get_prediction_per_season(X_train, y_train, splits, best_model, months=8)\n",
    "y_test_summer, y_pred_summer = get_prediction_per_season(X_train, y_train, splits, best_model, months=6)\n",
    "\n",
    "print(y_test_annual.shape)\n",
    "print(y_test_winter.shape)\n",
    "print(y_test_summer.shape)\n",
    "\n",
    "y_test_all = np.hstack((y_test_annual, y_test_winter, y_test_summer))\n",
    "y_pred_all = np.hstack((y_pred_annual, y_pred_winter, y_pred_summer))\n",
    "\n",
    "print(y_test_all.shape)\n",
    "print(y_pred_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_per_season(y_test_winter, y_pred_winter, season='Winter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "# Select features for training\n",
    "df_test_X = df_test_all.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_test_y = df_test_all[['balance']]\n",
    "\n",
    "X_test, y_test = df_test_X.values, df_test_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = cv_grid.best_estimator_\n",
    "test_model.fit(X_train, y_train)\n",
    "y_pred_test = test_model.predict(X_test)\n",
    "plot_prediction(y_test, y_pred_test, data_type='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_crop, y_pred_crop = get_prediction_per_season_test(X_test, y_test, test_model, months=8)\n",
    "plot_prediction_per_season(y_test_crop, y_pred_crop, season='Winter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211",
   "metadata": {},
   "source": [
    "# 5.1 using elevation diff instead of elevation and climate elevation and mae as objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns to drop\n",
    "cols = ['RGIID','GLIMSID','utm_zone','utm_east_approx','utm_north_approx','altitude_approx',\n",
    "        'location_description','location_id','stake_no','utm_east','utm_north','dt_prev_year_min_date','dt_curr_year_max_date',\n",
    "        'dt_curr_year_min_date','stake_remark','flag_correction','approx_loc','approx_altitude',\n",
    "        'diff_north','diff_east','diff_altitude','diff_netto','lat_approx','lon_approx',\n",
    "        'topo','dis_from_border','year', 'lat','lon', 'slope_factor']\n",
    "\n",
    "snow_depth_m = ['sde_oct','sde_nov','sde_dec','sde_jan','sde_feb','sde_mar','sde_apr','sde_may','sde_jun','sde_jul','sde_aug','sde_sep']\n",
    "snow_density = ['rsn_oct','rsn_nov','rsn_dec','rsn_jan','rsn_feb','rsn_mar','rsn_apr','rsn_may','rsn_jun','rsn_jul','rsn_aug','rsn_sep']\n",
    "evaporation = ['es_oct','es_nov','es_dec','es_jan','es_feb','es_mar','es_apr','es_may','es_jun','es_jul','es_aug','es_sep']\n",
    "snow_cover = ['snowc_oct','snowc_nov','snowc_dec','snowc_jan','snowc_feb','snowc_mar','snowc_apr','snowc_may','snowc_jun','snowc_jul','snowc_aug','snowc_sep']\n",
    "snow_depth_we = ['sd_oct','sd_nov','sd_dec','sd_jan','sd_feb','sd_mar','sd_apr','sd_may','sd_jun','sd_jul','sd_aug','sd_sep']\n",
    "snow_temp = ['tsn_oct','tsn_nov','tsn_dec','tsn_jan','tsn_feb','tsn_mar','tsn_apr','tsn_may','tsn_jun','tsn_jul','tsn_aug','tsn_sep']\n",
    "snow_melt = ['smlt_oct','smlt_nov','smlt_dec','smlt_jan','smlt_feb','smlt_mar','smlt_apr','smlt_may','smlt_jun','smlt_jul','smlt_aug','smlt_sep']\n",
    "snowfall = ['sf_oct','sf_nov','sf_dec','sf_jan','sf_feb','sf_mar','sf_apr','sf_may','sf_jun','sf_jul','sf_aug','sf_sep']\n",
    "snow_albedo = ['asn_oct','asn_nov','asn_dec','asn_jan','asn_feb','asn_mar','asn_apr','asn_may','asn_jun','asn_jul','asn_aug','asn_sep']\n",
    "dewpt_temp = ['d2m_oct','d2m_nov','d2m_dec','d2m_jan','d2m_feb','d2m_mar','d2m_apr','d2m_may','d2m_jun','d2m_jul','d2m_aug','d2m_sep']\n",
    "surface_pressure = ['sp_oct','sp_nov','sp_dec','sp_jan','sp_feb','sp_mar','sp_apr','sp_may','sp_jun','sp_jul','sp_aug','sp_sep']\n",
    "sol_rad_net = ['ssr_oct','ssr_nov','ssr_dec','ssr_jan','ssr_feb','ssr_mar','ssr_apr','ssr_may','ssr_jun','ssr_jul','ssr_aug','ssr_sep']\n",
    "sol_therm_down = ['strd_oct','strd_nov','strd_dec','strd_jan','strd_feb','strd_mar','strd_apr','strd_may','strd_jun','strd_jul','strd_aug','strd_sep']\n",
    "#sol_rad_down = ['ssrd_oct','ssrd_nov','ssrd_dec','ssrd_jan','ssrd_feb','ssrd_mar','ssrd_apr','ssrd_may','ssrd_jun','ssrd_jul','ssrd_aug','ssrd_sep']\n",
    "u_wind = ['u10_oct','u10_nov','u10_dec','u10_jan','u10_feb','u10_mar','u10_apr','u10_may','u10_jun','u10_jul','u10_aug','u10_sep']\n",
    "v_wind = ['v10_oct','v10_nov','v10_dec','v10_jan','v10_feb','v10_mar','v10_apr','v10_may','v10_jun','v10_jul','v10_aug','v10_sep']\n",
    "#f_albedo = ['fal_oct','fal_nov','fal_dec','fal_jan','fal_feb','fal_mar','fal_apr','fal_may','fal_jun','fal_jul','fal_aug','fal_sep']\n",
    "\n",
    "drop_cols = [y for x in [cols, snow_depth_m, snow_density, evaporation, snow_cover, snow_depth_we, snow_temp, snow_melt, snowfall, snow_albedo, \n",
    "                         dewpt_temp, surface_pressure, sol_rad_net, sol_therm_down, u_wind, v_wind] for y in x]\n",
    "\n",
    "# Select features for training\n",
    "df_train_annual_clean = df_train_annual.drop(drop_cols, axis=1)\n",
    "df_train_winter_clean = df_train_winter.drop(drop_cols, axis=1)\n",
    "df_train_summer_clean = df_train_summer.drop(drop_cols, axis=1)\n",
    "df_train_annual_clean = df_train_annual_clean.drop(['balance_winter','balance_summer'], axis=1)\n",
    "df_train_winter_clean = df_train_winter_clean.drop(['balance_netto', 'balance_summer'], axis=1)\n",
    "df_train_summer_clean = df_train_summer_clean.drop(['balance_netto', 'balance_winter'], axis=1)\n",
    "\n",
    "# Rename target columns to same name\n",
    "df_train_annual_clean.rename(columns={'balance_netto' : 'balance'}, inplace=True)\n",
    "df_train_winter_clean.rename(columns={'balance_winter' : 'balance'}, inplace=True)\n",
    "df_train_summer_clean.rename(columns={'balance_summer' : 'balance'}, inplace=True)\n",
    "\n",
    "# df_train_X_... now contains columns of all chosen features and column with annual, winter or summer balance\n",
    "\n",
    "# For summer balance, replace column values in accumulation months with NaN (oct, nov, dec, jan, feb, mar, apr, may)\n",
    "# For winter balance, replace column values in ablation months with NaN (may, jun, jul, aug, sep, oct)\n",
    "var = ['t2m', 'sshf', 'slhf', 'ssrd', 'fal','str', 'tp']\n",
    "mon_summer = ['may', 'jun', 'jul', 'aug', 'sep', 'oct']\n",
    "mon_winter = ['oct', 'nov', 'dec', 'jan', 'feb', 'mar', 'apr', 'may']\n",
    "\n",
    "for name in var:\n",
    "    for mon in mon_summer:\n",
    "        df_train_winter_clean[name+'_'+mon]= np.nan\n",
    "for name in var:   \n",
    "    for mon in mon_winter:\n",
    "        df_train_summer_clean[name+'_'+mon]= np.nan\n",
    "\n",
    "# Combine all annual, winter and summer data in one dataframe\n",
    "df_train_all = pd.concat([df_train_annual_clean, df_train_summer_clean, df_train_winter_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all['altitude_diff'] = df_train_all['altitude_climate']-df_train_all['altitude']\n",
    "df_train_all = df_train_all.drop(['altitude','altitude_climate'], axis=1)\n",
    "df_train_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_train_all['altitude_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "\n",
    "# Select features for training\n",
    "df_test_annual_clean = df_test_annual.drop(drop_cols, axis=1)\n",
    "df_test_winter_clean = df_test_winter.drop(drop_cols, axis=1)\n",
    "df_test_summer_clean = df_test_summer.drop(drop_cols, axis=1)\n",
    "df_test_annual_clean = df_test_annual_clean.drop(['balance_winter','balance_summer'], axis=1)\n",
    "df_test_winter_clean = df_test_winter_clean.drop(['balance_netto', 'balance_summer'], axis=1)\n",
    "df_test_summer_clean = df_test_summer_clean.drop(['balance_netto', 'balance_winter'], axis=1)\n",
    "\n",
    "# Rename target columns to same name\n",
    "df_test_annual_clean.rename(columns={'balance_netto' : 'balance'}, inplace=True)\n",
    "df_test_winter_clean.rename(columns={'balance_winter' : 'balance'}, inplace=True)\n",
    "df_test_summer_clean.rename(columns={'balance_summer' : 'balance'}, inplace=True)\n",
    "\n",
    "# For summer balance, replace column values in accumulation months with NaN (oct, nov, dec, jan, feb, mar, apr, may)\n",
    "# For winter balance, replace column values in ablation months with NaN (may, jun, jul, aug, sep, oct)\n",
    "for name in var:\n",
    "    for mon in mon_summer:\n",
    "        df_test_winter_clean[name+'_'+mon]= np.nan\n",
    "for name in var:   \n",
    "    for mon in mon_winter:\n",
    "        df_test_summer_clean[name+'_'+mon]= np.nan\n",
    "\n",
    "# Combine all annual, winter and summer data in one dataframe\n",
    "df_test_all = pd.concat([df_test_annual_clean, df_test_summer_clean, df_test_winter_clean])\n",
    "df_test_all.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_all['altitude_diff'] = df_test_all['altitude_climate']-df_test_all['altitude']\n",
    "df_test_all = df_test_all.drop(['altitude','altitude_climate'], axis=1)\n",
    "plt.hist(df_test_all['altitude_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218",
   "metadata": {},
   "source": [
    "### 2.1 Annual and seasonal mass balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove instances where elevation is lower than 700 m a sl\n",
    "#df_train_all = df_train_all.loc[df_train_all['altitude'] >= 750]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from plotting_functions import plot_prediction_per_fold\n",
    "\n",
    "def train_xgb_model(X, y, idc_list, params, scorer='neg_mean_absolute_error', return_train=True):\n",
    "\n",
    "    # Define model object.\n",
    "    xgb_model = xgb.XGBRegressor(objective='reg:absoluteerror')\n",
    "    \n",
    "    # Set up grid search. \n",
    "    clf = GridSearchCV(xgb_model, \n",
    "                       params, \n",
    "                       cv=idc_list, # Int or iterator (default for int is kfold)\n",
    "                       verbose=2, # Controls number of messages\n",
    "                       n_jobs=4, # No of parallell jobs\n",
    "                       scoring=scorer, # Can use multiple metrics\n",
    "                       refit=True, # Default True. For multiple metric evaluation, refit must be str denoting scorer to be used \n",
    "                       #to find the best parameters for refitting the estimator.\n",
    "                       return_train_score=return_train) # Default False. If False, cv_results_ will not include training scores.\n",
    "\n",
    "    # Fit model to folds\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Get results of grid search\n",
    "    print('Cross validation score: ', clf.best_score_)\n",
    "    print('Grid search best hyperparameters: ', clf.best_params_)\n",
    "\n",
    "    # Model object with best parameters (** to unpack parameter dict)\n",
    "    fitted_model = xgb.XGBRegressor(**clf.best_params_)\n",
    "    \n",
    "    cvl = cross_val_score(fitted_model, X, y, cv=idc_list, scoring='neg_mean_squared_error')\n",
    "\n",
    "    print('Cross validation scores per fold: ', cvl)\n",
    "    print('Mean cross validation score: ', cvl.mean())\n",
    "    print('Standard deviation: ', cvl.std())\n",
    "\n",
    "    plot_prediction_per_fold(X, y, fitted_model, idc_list)\n",
    "\n",
    "    return clf, fitted_model, cvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter ranges.\n",
    "param_ranges = {'max_depth': [4, 5, 6, 7],\n",
    "          'n_estimators': [200, 300, 400, 500, 600, 700], # number of trees (too many = overfitting, too few = underfitting)\n",
    "          'learning_rate': [0.1, 0.15, 0.2, 0.25, 0.3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training\n",
    "df_train_X = df_train_all.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_train_y = df_train_all[['balance']]\n",
    "\n",
    "# Get arrays of features and targets\n",
    "X_train, y_train = df_train_X.values, df_train_y.values\n",
    "\n",
    "# Set random seed for split\n",
    "rand_seed = 25\n",
    "\n",
    "# Use five folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=rand_seed)\n",
    "splits = list(kf.split(X_train, y_train))\n",
    "\n",
    "# Train model\n",
    "# Set return_train to False to inhibit return training scores. Default is True.\n",
    "cv_grid, best_model, cvl_scores = train_xgb_model(X_train, y_train, splits, param_ranges, \n",
    "                                                  scorer='neg_mean_absolute_error', return_train=True)\n",
    "\n",
    "# Save cv-object\n",
    "joblib.dump(cv_grid, 'Models/Final_training/cv_rand_5fold_all_climate_ann+seas_altitude_diff_more_trees_mae.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gsearch_results(cv_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, sharex=True, sharey=True, figsize=(25,25))\n",
    "a = 0    \n",
    "for train_index, test_index in splits:\n",
    "    # Loops over n_splits iterations and gets train and test splits in each fold\n",
    "    X_train_, X_test_ = X_train[train_index], X_train[test_index]\n",
    "    y_train_, y_test_ = y_train[train_index], y_train[test_index]\n",
    "    best_model.fit(X_train_, y_train_)\n",
    "    y_pred = best_model.predict(X_test_)\n",
    "\n",
    "    title = 'Validation fold ' + str(a)\n",
    "\n",
    "    plot_prediction_subplot(y_test_, y_pred, title, ax[a], n_toplot=5000, fold=True)\n",
    "\n",
    "    a=a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_permutation_importance(df_train_X, X_train, y_train, splits, best_model, max_features_plot = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_annual, y_pred_annual = get_prediction_per_season(X_train, y_train, splits, best_model, months=12)\n",
    "y_test_winter, y_pred_winter = get_prediction_per_season(X_train, y_train, splits, best_model, months=8)\n",
    "y_test_summer, y_pred_summer = get_prediction_per_season(X_train, y_train, splits, best_model, months=6)\n",
    "\n",
    "print(y_test_annual.shape)\n",
    "print(y_test_winter.shape)\n",
    "print(y_test_summer.shape)\n",
    "\n",
    "y_test_all = np.hstack((y_test_annual, y_test_winter, y_test_summer))\n",
    "y_pred_all = np.hstack((y_pred_annual, y_pred_winter, y_pred_summer))\n",
    "\n",
    "print(y_test_all.shape)\n",
    "print(y_pred_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_per_season(y_test_winter, y_pred_winter, season='Winter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "# Select features for training\n",
    "df_test_X = df_test_all.drop(['balance','BREID'], axis=1)\n",
    "\n",
    "# Select labels for training\n",
    "df_test_y = df_test_all[['balance']]\n",
    "\n",
    "X_test, y_test = df_test_X.values, df_test_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = cv_grid.best_estimator_\n",
    "test_model.fit(X_train, y_train)\n",
    "y_pred_test = test_model.predict(X_test)\n",
    "plot_prediction(y_test, y_pred_test, data_type='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_crop, y_pred_crop = get_prediction_per_season_test(X_test, y_test, test_model, months=12)\n",
    "plot_prediction_per_season(y_test_crop, y_pred_crop, season='Annual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
