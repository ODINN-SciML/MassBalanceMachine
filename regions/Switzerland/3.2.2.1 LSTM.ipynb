{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from skorch.helper import SliceDataset\n",
    "from datetime import datetime\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "import pickle \n",
    "from collections import Counter\n",
    "import ast\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'\n",
    "\n",
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "\n",
    "# vois_climate = [\n",
    "#     't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "# ]\n",
    "\n",
    "vois_climate = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "\n",
    "    # # Try to limit CPU usage of random search\n",
    "    # torch.set_num_threads(2)  # or 1\n",
    "    # os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "    # os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "# Capitalize glacier names:\n",
    "glacierCap = {}\n",
    "for gl in data_glamos['GLACIER'].unique():\n",
    "    if isinstance(gl, str):  # Ensure the glacier name is a string\n",
    "        if gl.lower() == 'claridenu':\n",
    "            glacierCap[gl] = 'Clariden_U'\n",
    "        elif gl.lower() == 'claridenl':\n",
    "            glacierCap[gl] = 'Clariden_L'\n",
    "        else:\n",
    "            glacierCap[gl] = gl.capitalize()\n",
    "    else:\n",
    "        print(f\"Warning: Non-string glacier name encountered: {gl}\")\n",
    "\n",
    "# drop taelliboden and plainemorte if in there\n",
    "if 'taelliboden' in data_glamos['GLACIER'].unique():\n",
    "    data_glamos = data_glamos[data_glamos['GLACIER'] != 'taelliboden']\n",
    "if 'plainemorte' in data_glamos['GLACIER'].unique():\n",
    "    data_glamos = data_glamos[data_glamos['GLACIER'] != 'plainemorte']\n",
    "\n",
    "print('-------------------')\n",
    "print('Number of glaciers:', len(data_glamos['GLACIER'].unique()))\n",
    "print('Number of winter and annual samples:', len(data_glamos))\n",
    "print('Number of annual samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'winter']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:\n",
    "### Input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_NN.csv')\n",
    "data_monthly = dataloader_gl.data\n",
    "\n",
    "data_monthly['GLWD_ID'] = data_monthly.apply(\n",
    "    lambda x: mbm.data_processing.utils.get_hash(f\"{x.GLACIER}_{x.YEAR}\"),\n",
    "    axis=1)\n",
    "data_monthly['GLWD_ID'] = data_monthly['GLWD_ID'].astype(str)\n",
    "\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_annual = dataloader_gl.data[dataloader_gl.data.PERIOD == 'annual']\n",
    "\n",
    "# print mean and std of N_MONTHS\n",
    "print('Mean number of months:', data_annual.N_MONTHS.mean())\n",
    "print('Std number of months:', data_annual.N_MONTHS.std())\n",
    "\n",
    "# same for winter\n",
    "data_winter = dataloader_gl.data[dataloader_gl.data.PERIOD == 'winter']\n",
    "print('Mean number of months (winter):', data_winter.N_MONTHS.mean())\n",
    "print('Std number of months (winter):', data_winter.N_MONTHS.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocking on glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in TEST_GLACIERS if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in TEST_GLACIERS]\n",
    "\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(TEST_GLACIERS)]\n",
    "print('Size of monthly test data:', len(data_test))\n",
    "\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train))\n",
    "\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "# Number of annual versus winter measurements:\n",
    "print('-------------\\nTrain:')\n",
    "print('Number of monthly winter and annual samples:', len(data_train))\n",
    "print('Number of monthly annual samples:',\n",
    "      len(data_train[data_train.PERIOD == 'annual']))\n",
    "print('Number of monthly winter samples:',\n",
    "      len(data_train[data_train.PERIOD == 'winter']))\n",
    "\n",
    "# Same for test\n",
    "data_test_annual = data_test[data_test.PERIOD == 'annual']\n",
    "data_test_winter = data_test[data_test.PERIOD == 'winter']\n",
    "\n",
    "print('Test:')\n",
    "print('Number of monthly winter and annual samples:', len(data_test))\n",
    "print('Number of monthly annual samples:', len(data_test_annual))\n",
    "print('Number of monthly winter samples:', len(data_test_winter))\n",
    "\n",
    "print('Total:')\n",
    "print('Number of monthly rows:', len(dataloader_gl.data))\n",
    "print('Number of annual rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'annual']))\n",
    "print('Number of winter rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'winter']))\n",
    "\n",
    "# same for original data:\n",
    "print('-------------\\nIn annual format:')\n",
    "print('Number of annual train rows:',\n",
    "      len(data_glamos[data_glamos.GLACIER.isin(train_glaciers)]))\n",
    "print('Number of annual test rows:',\n",
    "      len(data_glamos[data_glamos.GLACIER.isin(TEST_GLACIERS)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=TEST_GLACIERS,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "\n",
    "data_test = test_set['df_X']\n",
    "data_test['y'] = test_set['y']\n",
    "\n",
    "# dataloader = mbm.dataloader.DataLoader(cfg, data=data_train)\n",
    "\n",
    "# train_itr, val_itr = dataloader.set_train_test_split(test_size=0.2)\n",
    "\n",
    "# # Get all indices of the training and valing dataset at once from the iterators. Once called, the iterators are empty.\n",
    "# train_indices, val_indices = list(train_itr), list(val_itr)\n",
    "\n",
    "# df_X_train = data_train.iloc[train_indices]\n",
    "# y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# # Get val set\n",
    "# df_X_val = data_train.iloc[val_indices]\n",
    "# y_val = df_X_val['POINT_BALANCE'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = ['t2m','tp','slhf','sshf','ssrd','fal','str','pcsr']\n",
    "STATIC_COLS  = ['ELEVATION_DIFFERENCE','aspect_sgi','slope_sgi',\n",
    "                'hugonnet_dhdt','consensus_ice_thickness','millan_v']\n",
    "\n",
    "HYDRO_MONTHS = ['oct','nov','dec','jan','feb','mar','apr','may','jun','jul','aug','sep']\n",
    "HYDRO_POS = {m:i for i,m in enumerate(HYDRO_MONTHS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(\n",
    "    df: pd.DataFrame,\n",
    "    monthly_cols: List[str],\n",
    "    static_cols: List[str],\n",
    "    hydro_pos: Dict[str, int],\n",
    "    *,\n",
    "    show_progress: bool = True,\n",
    "    check_unique: bool = True\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Build 12-step LSTM sequences (Oct..Sep hydrological order) from a monthly table.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Must contain columns: GLACIER, YEAR, ID, PERIOD, MONTHS, and (optionally) POINT_BALANCE.\n",
    "        Assumes MONTHS are already normalized to {'oct','nov','dec','jan','feb','mar','apr','may','jun','jul','aug','sep'}.\n",
    "    monthly_cols : list[str]\n",
    "        Per-month feature columns (Fm).\n",
    "    static_cols : list[str]\n",
    "        Static feature columns (Fs), constant within a (GLACIER, YEAR, ID, PERIOD) group.\n",
    "    hydro_pos : dict[str,int]\n",
    "        Mapping from month token to 0..11 index (e.g., {'oct':0, ..., 'sep':11}).\n",
    "    expect_target : bool, default True\n",
    "        If True, expects POINT_BALANCE and returns 'y' (float32). If False, fills 'y' with NaN.\n",
    "    show_progress : bool, default True\n",
    "        Show tqdm progress bar.\n",
    "    check_unique : bool, default True\n",
    "        Verify that keys (GLACIER,YEAR,ID,PERIOD) are unique.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_dict : dict\n",
    "        {\n",
    "          'X_monthly': (B,12,Fm) float32,\n",
    "          'X_static':  (B,Fs)    float32,\n",
    "          'mask_valid':(B,12)    float32,\n",
    "          'mask_w':    (B,12)    float32 (Oct..Apr=1),\n",
    "          'mask_a':    (B,12)    float32 (all 1),\n",
    "          'y':         (B,)      float32 (NaN if expect_target=False),\n",
    "          'is_winter': (B,)      bool,\n",
    "          'is_annual': (B,)      bool,\n",
    "          'keys':      list[(GLACIER,YEAR,ID,PERIOD)]\n",
    "        }\n",
    "    \"\"\"\n",
    "    req = {'GLACIER','YEAR','ID','PERIOD','MONTHS', *monthly_cols, *static_cols}\n",
    "    missing = req - set(df.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns: {sorted(missing)}\")\n",
    "\n",
    "    # Normalize PERIOD just in case of stray whitespace/case\n",
    "    df = df.copy()\n",
    "    df['PERIOD'] = df['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "    mask_w_template = np.zeros(12, dtype=np.float32); mask_w_template[:7] = 1.0  # Oct..Apr\n",
    "    mask_a_template = np.ones(12, dtype=np.float32)\n",
    "\n",
    "    X_monthly, X_static = [], []\n",
    "    mask_valid, mask_w, mask_a = [], [], []\n",
    "    y, is_winter, is_annual, keys = [], [], [], []\n",
    "\n",
    "    groups = list(df.groupby(['GLACIER','YEAR','ID','PERIOD']))\n",
    "    iterator = tqdm(groups, desc=\"Building sequences\") if show_progress else groups\n",
    "\n",
    "    for (g, yr, mid, per), sub in iterator:\n",
    "        # Average duplicates within a month, if any\n",
    "        agg_cols = monthly_cols + static_cols + (['POINT_BALANCE'])\n",
    "        subm = (sub.groupby('MONTHS', as_index=False)[agg_cols]\n",
    "                  .mean(numeric_only=True))\n",
    "\n",
    "        # Build 12×Fm monthly matrix and valid mask\n",
    "        mat = np.zeros((12, len(monthly_cols)), dtype=np.float32)\n",
    "        mv  = np.zeros(12, dtype=np.float32)\n",
    "\n",
    "        for _, r in subm.iterrows():\n",
    "            m = r['MONTHS']\n",
    "            if m not in hydro_pos:\n",
    "                raise ValueError(f\"Unexpected month token '{m}'. Expected one of {list(hydro_pos.keys())}.\")\n",
    "            pos = hydro_pos[m]\n",
    "            mat[pos, :] = r[monthly_cols].to_numpy(np.float32)\n",
    "            mv[pos] = 1.0\n",
    "\n",
    "        # Static features: take from first row (should be identical within group)\n",
    "        s = subm.iloc[0][static_cols].to_numpy(np.float32)\n",
    "\n",
    "        # Target \n",
    "        target = float(subm['POINT_BALANCE'].mean())\n",
    "        \n",
    "        # Append once per group\n",
    "        X_monthly.append(mat)\n",
    "        X_static.append(s)\n",
    "        mask_valid.append(mv)\n",
    "        mask_w.append(mask_w_template.copy())\n",
    "        mask_a.append(mask_a_template.copy())\n",
    "        y.append(target)\n",
    "        is_winter.append(per == 'winter')\n",
    "        is_annual.append(per == 'annual')\n",
    "        keys.append((g, int(yr), int(mid), per))\n",
    "\n",
    "    def stack(a): return np.stack(a, axis=0) if len(a) else np.empty((0,))\n",
    "\n",
    "    data_dict = dict(\n",
    "        X_monthly = stack(X_monthly),\n",
    "        X_static  = stack(X_static),\n",
    "        mask_valid= stack(mask_valid),\n",
    "        mask_w    = stack(mask_w),\n",
    "        mask_a    = stack(mask_a),\n",
    "        y         = np.asarray(y, dtype=np.float32),\n",
    "        is_winter = np.asarray(is_winter, dtype=bool),\n",
    "        is_annual = np.asarray(is_annual, dtype=bool),\n",
    "        keys      = keys\n",
    "    )\n",
    "\n",
    "    # Uniqueness check\n",
    "    if check_unique:\n",
    "        if len(keys) != len(set(keys)):\n",
    "            dupes = [k for k,c in Counter(keys).items() if c > 1]\n",
    "            raise ValueError(f\"Found {len(dupes)} duplicate keys, e.g. {dupes[:5]}\")\n",
    "        else:\n",
    "            print(f\"All {len(keys)} keys are unique.\")\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN (with targets)\n",
    "df_train = data_train.copy()\n",
    "df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "train_dict = build_sequences(\n",
    "    df_train, MONTHLY_COLS, STATIC_COLS, HYDRO_POS,\n",
    "    show_progress=True, check_unique=True\n",
    ")\n",
    "\n",
    "# TEST (no targets)\n",
    "df_test = data_test.copy()\n",
    "df_test['PERIOD'] = df_test['PERIOD'].str.strip().str.lower()\n",
    "test_dict = build_sequences(\n",
    "    df_test, MONTHLY_COLS, STATIC_COLS, HYDRO_POS,\n",
    "    show_progress=True, check_unique=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBSequenceDataset(Dataset):\n",
    "    def __init__(self, data_dict):\n",
    "        # raw (unscaled) numpy -> tensors (float / bool)\n",
    "        self.Xm = torch.from_numpy(data_dict['X_monthly']).float()   # (B,12,Fm)\n",
    "        self.Xs = torch.from_numpy(data_dict['X_static']).float()    # (B,Fs)\n",
    "        self.mv = torch.from_numpy(data_dict['mask_valid']).float()  # (B,12)\n",
    "        self.mw = torch.from_numpy(data_dict['mask_w']).float()      # (B,12)\n",
    "        self.ma = torch.from_numpy(data_dict['mask_a']).float()      # (B,12)\n",
    "        self.y  = torch.from_numpy(data_dict['y']).float()           # (B,)\n",
    "        self.iw = torch.from_numpy(data_dict['is_winter']).bool()    # (B,)\n",
    "        self.ia = torch.from_numpy(data_dict['is_annual']).bool()    # (B,)\n",
    "        self.keys = data_dict.get('keys', None)\n",
    "\n",
    "        # placeholders for scaling (set later)\n",
    "        self.month_mean = None\n",
    "        self.month_std  = None\n",
    "        self.static_mean = None\n",
    "        self.static_std  = None\n",
    "        self.y_mean = None\n",
    "        self.y_std  = None\n",
    "\n",
    "    def __len__(self): return self.Xm.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"x_m\": self.Xm[idx], \"x_s\": self.Xs[idx],\n",
    "            \"mv\": self.mv[idx], \"mw\": self.mw[idx], \"ma\": self.ma[idx],\n",
    "            \"y\": self.y[idx], \"iw\": self.iw[idx], \"ia\": self.ia[idx],\n",
    "        }\n",
    "\n",
    "    # ---- scaling helpers ----\n",
    "    def fit_scalers(self, idx_train):\n",
    "        # monthly scaler: use only valid months in TRAIN\n",
    "        Xm = self.Xm[idx_train].numpy()    # (N,12,Fm)\n",
    "        Mv = self.mv[idx_train].numpy()    # (N,12)\n",
    "        N, T, Fm = Xm.shape\n",
    "        mask3 = Mv[..., None]              # (N,12,1)\n",
    "        num = (Xm * mask3).sum(axis=(0,1)) # (Fm,)\n",
    "        den = mask3.sum(axis=(0,1))        # (1,)\n",
    "        month_mean = num / np.maximum(den, 1e-8)\n",
    "        # std\n",
    "        var = (( (Xm - month_mean) * mask3 )**2).sum(axis=(0,1)) / np.maximum(den, 1e-8)\n",
    "        month_std = np.sqrt(np.maximum(var, 1e-8))\n",
    "\n",
    "        # static scaler: simple mean/std across TRAIN rows\n",
    "        Xs = self.Xs[idx_train].numpy()\n",
    "        static_mean = Xs.mean(axis=0)\n",
    "        static_std  = np.sqrt(np.maximum(Xs.var(axis=0), 1e-8))\n",
    "\n",
    "        # target scaler\n",
    "        y = self.y[idx_train].numpy()\n",
    "        y_mean = float(np.mean(y))\n",
    "        y_std  = float(np.sqrt(max(np.var(y), 1e-8)))\n",
    "\n",
    "        # store\n",
    "        self.month_mean = torch.from_numpy(month_mean).float()\n",
    "        self.month_std  = torch.from_numpy(month_std).float()\n",
    "        self.static_mean = torch.from_numpy(static_mean).float()\n",
    "        self.static_std  = torch.from_numpy(static_std).float()\n",
    "        self.y_mean = torch.tensor(y_mean, dtype=torch.float32)\n",
    "        self.y_std  = torch.tensor(y_std, dtype=torch.float32)\n",
    "\n",
    "    def transform_inplace(self):\n",
    "        # apply (x - mean)/std; monthly uses broadcasting over (B,12,Fm)\n",
    "        self.Xm = (self.Xm - self.month_mean) / self.month_std\n",
    "        self.Xs = (self.Xs - self.static_mean) / self.static_std\n",
    "        # scale targets\n",
    "        self.y = (self.y - self.y_mean) / self.y_std\n",
    "        \n",
    "    def set_scalers_from(self, other_ds: \"MBSequenceDataset\"):\n",
    "        \"\"\"Copy fitted scalers from another dataset (usually the TRAIN ds).\"\"\"\n",
    "        self.month_mean = other_ds.month_mean.clone()\n",
    "        self.month_std  = other_ds.month_std.clone()\n",
    "        self.static_mean = other_ds.static_mean.clone()\n",
    "        self.static_std  = other_ds.static_std.clone()\n",
    "        self.y_mean = other_ds.y_mean.clone()\n",
    "        self.y_std  = other_ds.y_std.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset from your prepared dict\n",
    "ds_train = MBSequenceDataset(train_dict)\n",
    "\n",
    "# simple random split (for a quick test)\n",
    "def split_indices(n, val_ratio=0.2):\n",
    "    idx = np.arange(n); np.random.shuffle(idx)\n",
    "    cut = max(1, int(n * (1 - val_ratio)))\n",
    "    return idx[:cut], idx[cut:]\n",
    "\n",
    "train_idx, val_idx = split_indices(len(ds_train), val_ratio=0.2)\n",
    "\n",
    "# fit scalers on TRAIN only, then transform whole dataset\n",
    "ds_train.fit_scalers(train_idx)\n",
    "ds_train.transform_inplace()\n",
    "\n",
    "train_ds = Subset(ds_train, train_idx)\n",
    "val_ds   = Subset(ds_train, val_idx)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_dl   = DataLoader(val_ds, batch_size=128, shuffle=False)\n",
    "\n",
    "# 1) Target scale\n",
    "print(\"y mean/std (train):\", float(ds_train.y_mean), float(ds_train.y_std))\n",
    "\n",
    "# 2) Any NaNs?\n",
    "for name, arr in [(\"Xm\", ds_train.Xm), (\"Xs\", ds_train.Xs), (\"y\", ds_train.y)]:\n",
    "    has_nan = torch.isnan(arr).any().item()\n",
    "    print(name, \"has NaN?\", bool(has_nan))\n",
    "\n",
    "# 3) Do winter/annual rows exist in both splits?\n",
    "print(\"Train counts:\", int(ds_train.iw[train_idx].sum()), \"winter |\", int(ds_train.ia[train_idx].sum()), \"annual\")\n",
    "print(\"Val   counts:\", int(ds_train.iw[val_idx].sum()),   \"winter |\", int(ds_train.ia[val_idx].sum()),   \"annual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_MB(nn.Module):\n",
    "    def __init__(self, Fm, Fs, hidden=128, layers=1, bidir=True, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=Fm, hidden_size=hidden, num_layers=layers,\n",
    "            batch_first=True, bidirectional=bidir,\n",
    "            dropout=dropout if layers > 1 else 0.0\n",
    "        )\n",
    "        H = hidden * (2 if bidir else 1)\n",
    "        self.static_mlp = nn.Sequential(\n",
    "            nn.Linear(Fs, 64), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(64, 64), nn.ReLU()\n",
    "        )\n",
    "        self.head = nn.Linear(H + 64, 1)  # per-month scalar\n",
    "\n",
    "    def forward(self, x_m, x_s, mv, mw, ma):\n",
    "        out, _ = self.lstm(x_m)              # (B,12,H or 2H)\n",
    "        s = self.static_mlp(x_s)             # (B,64)\n",
    "        s_rep = s.unsqueeze(1).expand(-1, out.size(1), -1)\n",
    "        z = torch.cat([out, s_rep], dim=-1)  # (B,12,H+64)\n",
    "        y_month = self.head(z).squeeze(-1)   # (B,12)\n",
    "        # zero out invalid months BEFORE summing\n",
    "        y_month = y_month * mv\n",
    "        y_w = (y_month * mw).sum(dim=1)      # (B,)\n",
    "        y_a = (y_month * ma).sum(dim=1)      # (B,)\n",
    "        return y_month, y_w, y_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fm = ds_train.Xm.shape[-1]\n",
    "Fs = ds_train.Xs.shape[-1]\n",
    "model = LSTM_MB(Fm=Fm, Fs=Fs, hidden=128, layers=1, bidir=True, dropout=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_mse(outputs, batch):\n",
    "    _, y_w_pred, y_a_pred = outputs\n",
    "    # predictions are in scaled space because inputs & target are scaled.\n",
    "    # BUT: the seasonal sums are linear, so using scaled y is fine as long as y was scaled.\n",
    "    y_true = batch['y']  # already scaled\n",
    "\n",
    "    iw, ia = batch['iw'], batch['ia']\n",
    "    loss = 0.0; terms = 0\n",
    "\n",
    "    if iw.any():\n",
    "        loss = loss + torch.mean((y_w_pred[iw] - y_true[iw])**2); terms += 1\n",
    "    if ia.any():\n",
    "        loss = loss + torch.mean((y_a_pred[ia] - y_true[ia])**2); terms += 1\n",
    "\n",
    "    if terms == 0:\n",
    "        return torch.tensor(0.0, device=y_true.device)\n",
    "    return loss / terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "clip_val = 1.0\n",
    "epochs = 50\n",
    "\n",
    "def to_device(batch):\n",
    "    return {k: (v.to(device) if torch.is_tensor(v) else v) for k,v in batch.items()}\n",
    "\n",
    "def run_epoch(dl, train=True):\n",
    "    model.train(train)\n",
    "    tot, n = 0.0, 0\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for batch in dl:\n",
    "            batch = to_device(batch)\n",
    "            y_m, y_w, y_a = model(batch['x_m'], batch['x_s'], batch['mv'], batch['mw'], batch['ma'])\n",
    "            loss = seasonal_mse((y_m, y_w, y_a), batch)\n",
    "            if train:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), clip_val)\n",
    "                optimizer.step()\n",
    "            bs = batch['x_m'].shape[0]\n",
    "            tot += loss.item() * bs; n += bs\n",
    "    return tot / max(n,1)\n",
    "\n",
    "best_state, best_val = None, float('inf')\n",
    "for ep in range(1, epochs+1):\n",
    "    tr = run_epoch(train_dl, True)\n",
    "    va = run_epoch(val_dl, False)\n",
    "    if va < best_val:\n",
    "        best_val = va\n",
    "        best_state = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "    if ep % 5 == 0 or ep == 1:\n",
    "        print(f\"Epoch {ep:03d} | train {tr:.4f} | val {va:.4f}\")\n",
    "\n",
    "# load best\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_with_preds(dl, ds):\n",
    "    model.eval()\n",
    "    rows = []\n",
    "\n",
    "    # we assume Dataset has attribute .keys aligned with ds order\n",
    "    all_keys = ds.keys  \n",
    "\n",
    "    i = 0  # running index to map back to keys\n",
    "    for batch in dl:\n",
    "        batch_size = batch['x_m'].shape[0]\n",
    "        batch_keys = all_keys[i:i+batch_size]\n",
    "        i += batch_size\n",
    "\n",
    "        batch = to_device(batch)\n",
    "        _, y_w, y_a = model(batch['x_m'], batch['x_s'],\n",
    "                            batch['mv'], batch['mw'], batch['ma'])\n",
    "\n",
    "        # invert scaling\n",
    "        y_true = batch['y'] * ds.y_std.to(device) + ds.y_mean.to(device)\n",
    "        y_w = y_w * ds.y_std.to(device) + ds.y_mean.to(device)\n",
    "        y_a = y_a * ds.y_std.to(device) + ds.y_mean.to(device)\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            g, yr, mid, per = batch_keys[j]\n",
    "            target = float(y_true[j].cpu())\n",
    "            if per == \"winter\":\n",
    "                pred = float(y_w[j].cpu())\n",
    "            elif per == \"annual\":\n",
    "                pred = float(y_a[j].cpu())\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected PERIOD: {per}\")\n",
    "            rows.append({\n",
    "                \"target\": target,\n",
    "                \"ID\": mid,\n",
    "                \"pred\": pred,\n",
    "                \"PERIOD\": per,\n",
    "                \"GLACIER\": g,\n",
    "                \"YEAR\": yr\n",
    "            })\n",
    "\n",
    "    df_preds = pd.DataFrame(rows)\n",
    "\n",
    "    # Compute RMSEs\n",
    "    def rmse(df, period):\n",
    "        d = df[df[\"PERIOD\"] == period]\n",
    "        if len(d) == 0: return float(\"nan\")\n",
    "        return np.sqrt(((d[\"pred\"] - d[\"target\"])**2).mean())\n",
    "\n",
    "    metrics = {\n",
    "        \"RMSE_winter\": rmse(df_preds, \"winter\"),\n",
    "        \"RMSE_annual\": rmse(df_preds, \"annual\"),\n",
    "    }\n",
    "\n",
    "    return metrics, df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TEST: use the SAME scalers from TRAIN, then transform TEST ---\n",
    "ds_test = MBSequenceDataset(test_dict)       # test_dict was built from your test DF (with targets)\n",
    "ds_test.set_scalers_from(ds_train)           # <-- crucial: copy TRAIN scalers\n",
    "ds_test.transform_inplace()                  # scales inputs & targets using TRAIN stats\n",
    "\n",
    "test_dl = DataLoader(ds_test, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics, test_df_preds = evaluate_with_preds(test_dl, ds_test)\n",
    "print(\"TEST metrics:\", test_metrics)\n",
    "print(test_df_preds.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_annual, scores_winter = compute_seasonal_scores(test_df_preds,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "print(\"Annual scores:\", scores_annual)\n",
    "print(\"Winter scores:\", scores_winter)\n",
    "\n",
    "fig = plot_predictions_summary(grouped_ids=test_df_preds,\n",
    "                               scores_annual=scores_annual,\n",
    "                               scores_winter=scores_winter,\n",
    "                               predVSTruth=predVSTruth,\n",
    "                               plotMeanPred=plotMeanPred,\n",
    "                               ax_xlim=(-8, 6),\n",
    "                               ax_ylim=(-8, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_per_el = data_glamos[data_glamos.PERIOD == 'annual'].groupby(\n",
    "    ['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "\n",
    "test_gl_per_el = gl_per_el[TEST_GLACIERS].sort_values().index\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(20, 15), sharex=True)\n",
    "\n",
    "PlotIndividualGlacierPredVsTruth(test_df_preds,\n",
    "                                 axs=axs,\n",
    "                                 color_annual=color_dark_blue,\n",
    "                                 color_winter=color_pink,\n",
    "                                 custom_order=test_gl_per_el)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
