{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glacier grids from RGI:\n",
    "\n",
    "Creates monthly grid files for the MBM to make PMB predictions over the whole glacier grid. The files come from the RGI grid with OGGM topography. Computing takes a long time because of the conversion to monthly format.\n",
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import massbalancemachine as mbm\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from cmcrameri import cm\n",
    "from oggm import utils, workflow\n",
    "from oggm import cfg as oggmCfg\n",
    "import geopandas as gpd\n",
    "import geopandas as gpd\n",
    "import traceback\n",
    "import salem\n",
    "import oggm\n",
    "import pickle\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from skorch.helper import SliceDataset\n",
    "\n",
    "# scripts\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.geodata import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.nn_helpers import *\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()  # in case no memory\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "# Climate columns\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "# Topographical columns\n",
    "vois_topographical = [\n",
    "    \"aspect\",\n",
    "    \"slope\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "    \"topo\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initalize_oggm = False\n",
    "if initalize_oggm:\n",
    "    gdirs, rgidf = initialize_oggm_glacier_directories(\n",
    "        cfg,\n",
    "        rgi_region=\"11\",\n",
    "        rgi_version=\"6\",\n",
    "        base_url=\n",
    "        \"https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L3-L5_files/2023.1/elev_bands/W5E5_w_data/\",\n",
    "        log_level='WARNING',\n",
    "        task_list=None,\n",
    "    )\n",
    "    # Save OGGM xr for all needed glaciers in RGI region 11.6:\n",
    "    export_oggm_grids(cfg, gdirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGI Ids:\n",
    "# Read glacier ids:\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "rgi_df.loc['rhone']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RGI grids for all glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_RGIs = cfg.dataPath + path_OGGM + 'xr_grids/'\n",
    "glaciers = os.listdir(path_RGIs)\n",
    "\n",
    "print(f\"Found {len(glaciers)} glaciers in RGI region 11.6\")\n",
    "\n",
    "# Open an example\n",
    "# rgi_gl = gdirs[0].rgi_id\n",
    "rgi_gl = 'RGI60-11.01238'\n",
    "\n",
    "ds = xr.open_dataset(path_RGIs + rgi_gl + '.zarr')\n",
    "glacier_mask = np.where(ds['glacier_mask'].values == 0, np.nan,\n",
    "                        ds['glacier_mask'].values)\n",
    "\n",
    "# Create glacier mask\n",
    "ds = ds.assign(masked_slope=glacier_mask * ds['slope'])\n",
    "ds = ds.assign(masked_elev=glacier_mask * ds['topo'])\n",
    "ds = ds.assign(masked_aspect=glacier_mask * ds['aspect'])\n",
    "ds = ds.assign(masked_dis=glacier_mask * ds['dis_from_border'])\n",
    "\n",
    "# Assign other variables only if available\n",
    "if 'hugonnet_dhdt' in ds:\n",
    "    ds = ds.assign(masked_hug=glacier_mask * ds['hugonnet_dhdt'])\n",
    "if 'consensus_ice_thickness' in ds:\n",
    "    ds = ds.assign(masked_cit=glacier_mask * ds['consensus_ice_thickness'])\n",
    "if 'millan_v' in ds:\n",
    "    ds = ds.assign(masked_miv=glacier_mask * ds['millan_v'])\n",
    "\n",
    "glacier_indices = np.where(ds['glacier_mask'].values == 1)\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 8), sharey=True)\n",
    "\n",
    "ds.masked_aspect.plot(ax=axs[0], cmap='twilight_shifted', add_colorbar=False)\n",
    "ds.masked_slope.plot(ax=axs[1], cmap='cividis', add_colorbar=False)\n",
    "ds.masked_elev.plot(ax=axs[2], cmap='terrain', add_colorbar=False)\n",
    "ds.glacier_mask.plot(ax=axs[3], cmap='binary', add_colorbar=False)\n",
    "\n",
    "axs[0].set_title(\"Aspect OGGM\")\n",
    "axs[1].set_title(\"Slope OGGM\")\n",
    "axs[2].set_title(\"DEM OGGM\")\n",
    "axs[3].set_title(\"Glacier mask OGGM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_glacier_grid_RGI(ds: xr.Dataset, years: list,\n",
    "                            glacier_indices: \"tuple[np.array, np.array]\",\n",
    "                            gdir: oggm.GlacierDirectory, rgi_gl: str):\n",
    "\n",
    "    # Assuming the coordinate variables are named 'x' and 'y' in your dataset\n",
    "    x_coords = ds['x'].values\n",
    "    y_coords = ds['y'].values\n",
    "\n",
    "    # Retrieve the x and y values using the glacier indices\n",
    "    glacier_x_vals = x_coords[glacier_indices[1]]\n",
    "    glacier_y_vals = y_coords[glacier_indices[0]]\n",
    "\n",
    "    # Convert glacier coordinates to latitude and longitude\n",
    "    # Transform stake coord to glacier system:\n",
    "    transf = pyproj.Transformer.from_proj(gdir.grid.proj,\n",
    "                                          salem.wgs84,\n",
    "                                          always_xy=True)\n",
    "    lon, lat = transf.transform(glacier_x_vals, glacier_y_vals)\n",
    "\n",
    "    # Glacier mask as boolean array:\n",
    "    gl_mask_bool = ds['glacier_mask'].values.astype(bool)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    data_grid = {\n",
    "        'RGIId': [rgi_gl] * len(ds.masked_elev.values[gl_mask_bool]),\n",
    "        'POINT_LAT': lat,\n",
    "        'POINT_LON': lon,\n",
    "        'aspect': ds.masked_aspect.values[gl_mask_bool],\n",
    "        'slope': ds.masked_slope.values[gl_mask_bool],\n",
    "        'topo': ds.masked_elev.values[gl_mask_bool],\n",
    "    }\n",
    "\n",
    "    # add other variables if available\n",
    "    if 'masked_hug' in ds:\n",
    "        data_grid['hugonnet_dhdt'] = ds.masked_hug.values[gl_mask_bool]\n",
    "    if 'masked_cit' in ds:\n",
    "        data_grid['consensus_ice_thickness'] = ds.masked_cit.values[\n",
    "            gl_mask_bool]\n",
    "    if 'masked_miv' in ds:\n",
    "        data_grid['millan_v'] = ds.masked_miv.values[gl_mask_bool]\n",
    "\n",
    "    df_grid = pd.DataFrame(data_grid)\n",
    "\n",
    "    # Match to WGMS format:\n",
    "    df_grid['POINT_ID'] = np.arange(1, len(df_grid) + 1)\n",
    "    df_grid['N_MONTHS'] = 12\n",
    "    df_grid['POINT_ELEVATION'] = df_grid[\n",
    "        'topo']  # no other elevation available\n",
    "    df_grid['POINT_BALANCE'] = 0  # fake PMB for simplicity (not used)\n",
    "    num_rows_per_year = len(df_grid)\n",
    "    # Repeat the DataFrame num_years times\n",
    "    df_grid = pd.concat([df_grid] * len(years), ignore_index=True)\n",
    "    # Add the 'year' and date columns to the DataFrame\n",
    "    df_grid['YEAR'] = np.repeat(\n",
    "        years, num_rows_per_year\n",
    "    )  # 'year' column that has len(df_grid) instances of year\n",
    "    df_grid['FROM_DATE'] = df_grid['YEAR'].apply(lambda x: str(x) + '1001')\n",
    "    df_grid['TO_DATE'] = df_grid['YEAR'].apply(lambda x: str(x + 1) + '0930')\n",
    "    df_grid[\"PERIOD\"] = \"annual\"\n",
    "\n",
    "    return df_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_masked_glacier(path_RGIs, rgi_gl):\n",
    "    # Load dataset\n",
    "    ds = xr.open_dataset(path_RGIs + rgi_gl + '.zarr')\n",
    "\n",
    "    # Check if 'glacier_mask' exists\n",
    "    if 'glacier_mask' not in ds:\n",
    "        raise ValueError(f\"'glacier_mask' variable not found in dataset {rgi_gl}\")\n",
    "\n",
    "    # Create glacier mask\n",
    "    glacier_mask = np.where(ds['glacier_mask'].values == 0, np.nan,\n",
    "                            ds['glacier_mask'].values)\n",
    "\n",
    "    # Apply mask to core variables\n",
    "    ds = ds.assign(masked_slope=glacier_mask * ds['slope'])\n",
    "    ds = ds.assign(masked_elev=glacier_mask * ds['topo'])\n",
    "    ds = ds.assign(masked_aspect=glacier_mask * ds['aspect'])\n",
    "    ds = ds.assign(masked_dis=glacier_mask * ds['dis_from_border'])\n",
    "\n",
    "    # Apply mask to optional variables if present\n",
    "    if 'hugonnet_dhdt' in ds:\n",
    "        ds = ds.assign(masked_hug=glacier_mask * ds['hugonnet_dhdt'])\n",
    "    if 'consensus_ice_thickness' in ds:\n",
    "        ds = ds.assign(masked_cit=glacier_mask * ds['consensus_ice_thickness'])\n",
    "    if 'millan_v' in ds:\n",
    "        ds = ds.assign(masked_miv=glacier_mask * ds['millan_v'])\n",
    "\n",
    "    # Indices where glacier_mask == 1\n",
    "    glacier_indices = np.where(ds['glacier_mask'].values == 1)\n",
    "\n",
    "    return ds, glacier_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create masked grids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_xr_grids = os.path.join(cfg.dataPath, 'GLAMOS/topo/RGI_v6_11/', 'xr_masked_grids/')\n",
    "\n",
    "RUN = False\n",
    "if RUN:\n",
    "    emptyfolder(path_xr_grids)\n",
    "\n",
    "    for gdir in tqdm(gdirs):\n",
    "        rgi_gl = gdir.rgi_id\n",
    "\n",
    "        try:\n",
    "            # Create masked glacier dataset\n",
    "            ds, glacier_indices = create_masked_glacier(path_RGIs, rgi_gl)\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping {rgi_gl}: {e}\")\n",
    "            continue  # Skip to next glacier\n",
    "\n",
    "        dx_m, dy_m = get_res_from_projected(ds)\n",
    "\n",
    "        # Coarsen to 50 m resolution if needed\n",
    "        if 20 < dx_m < 50:\n",
    "            ds = coarsenDS_mercator(ds, target_res_m=50)\n",
    "            dx_m, dy_m = get_res_from_projected(ds)\n",
    "\n",
    "        # Save xarray dataset\n",
    "        save_path = os.path.join(path_xr_grids, f\"{rgi_gl}.zarr\")\n",
    "        ds.to_zarr(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create monthly dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = False\n",
    "path_rgi_alps = os.path.join(cfg.dataPath, 'GLAMOS/topo/gridded_topo_inputs/RGI_v6_11/')\n",
    "\n",
    "if RUN:\n",
    "    years = range(2000, 2024)\n",
    "    \n",
    "    os.makedirs(path_rgi_alps, exist_ok=True)\n",
    "    emptyfolder(path_rgi_alps)\n",
    "    \n",
    "    valid_rgis = [f.replace('.zarr', '') for f in os.listdir(path_xr_grids) if f.endswith('.zarr')]\n",
    "    \n",
    "    for gdir in tqdm(gdirs, desc=\"Processing glaciers\"):\n",
    "        rgi_gl = gdir.rgi_id\n",
    "    \n",
    "        if rgi_gl not in valid_rgis:\n",
    "            print(f\"Skipping {rgi_gl}: not found in valid RGI glaciers\")\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            file_path = os.path.join(path_xr_grids, f\"{rgi_gl}.zarr\")\n",
    "            if not os.path.exists(file_path):\n",
    "                raise FileNotFoundError(f\"Missing file: {file_path}\")\n",
    "    \n",
    "            ds = xr.open_dataset(file_path)\n",
    "    \n",
    "            # Get glacier_indices safely\n",
    "            if 'glacier_mask' in ds:\n",
    "                glacier_indices = np.where(ds['glacier_mask'].values == 1)\n",
    "            else:\n",
    "                raise ValueError(f\"Missing 'glacier_mask' in dataset for {rgi_gl}\")\n",
    "    \n",
    "            # Create glacier grid\n",
    "            try:\n",
    "                df_grid = create_glacier_grid_RGI(ds, years, glacier_indices, gdir, rgi_gl)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed creating glacier grid for {rgi_gl}: {e}\")\n",
    "                continue\n",
    "    \n",
    "            df_grid.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "            # Add GLWD_ID\n",
    "            df_grid['GLWD_ID'] = df_grid.apply(\n",
    "                lambda x: mbm.data_processing.utils.get_hash(f\"{x.RGIId}_{x.YEAR}\"), axis=1)\n",
    "            df_grid['GLWD_ID'] = df_grid['GLWD_ID'].astype(str)\n",
    "            df_grid['GLACIER'] = df_grid['RGIId']\n",
    "    \n",
    "            # Wrap Dataset creation & climate feature extraction\n",
    "            try:\n",
    "                dataset_grid = mbm.data_processing.Dataset(\n",
    "                    cfg=cfg,\n",
    "                    data=df_grid,\n",
    "                    region_name='CH',\n",
    "                    data_path=os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv)\n",
    "                )\n",
    "    \n",
    "                era5_climate_data = os.path.join(cfg.dataPath, path_ERA5_raw, 'era5_monthly_averaged_data_Alps.nc')\n",
    "                geopotential_data = os.path.join(cfg.dataPath, path_ERA5_raw, 'era5_geopotential_pressure_Alps.nc')\n",
    "    \n",
    "                dataset_grid.get_climate_features(\n",
    "                    climate_data=era5_climate_data,\n",
    "                    geopotential_data=geopotential_data,\n",
    "                    change_units=True,\n",
    "                    smoothing_vois={\n",
    "                        'vois_climate': vois_climate,\n",
    "                        'vois_other': ['ALTITUDE_CLIMATE']\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Failed adding climate features for {rgi_gl}: {e}\")\n",
    "                continue\n",
    "    \n",
    "            # Prepare output folder\n",
    "            folder_path = os.path.join(path_rgi_alps, rgi_gl)\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "            # Process each year\n",
    "            for year in years:\n",
    "                try:\n",
    "                    df_grid_y = dataset_grid.data[dataset_grid.data.YEAR == year].copy()\n",
    "    \n",
    "                    vois_topographical_sub = [voi for voi in vois_topographical if voi in df_grid_y.columns]\n",
    "    \n",
    "                    dataset_grid_yearly = mbm.data_processing.Dataset(\n",
    "                        cfg=cfg,\n",
    "                        data=df_grid_y,\n",
    "                        region_name='CH',\n",
    "                        data_path=os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv)\n",
    "                    )\n",
    "    \n",
    "                    dataset_grid_yearly.convert_to_monthly(\n",
    "                        meta_data_columns=cfg.metaData,\n",
    "                        vois_climate=vois_climate,\n",
    "                        vois_topographical=vois_topographical_sub\n",
    "                    )\n",
    "    \n",
    "                    save_path = os.path.join(folder_path, f\"{rgi_gl}_grid_{year}.parquet\")\n",
    "                    dataset_grid_yearly.data.to_parquet(save_path, engine=\"pyarrow\", compression=\"snappy\")\n",
    "                    #print(f\"Saved: {save_path}\")\n",
    "    \n",
    "                except Exception as e:\n",
    "                    print(f\"Failed processing {rgi_gl} for year {year}: {e}\")\n",
    "                    continue\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error with glacier {rgi_gl}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gdir in gdirs:\n",
    "    if gdir.rgi_id == 'RGI60-11.01238':\n",
    "        gdir_rhone = gdir\n",
    "\n",
    "# Look at one example\n",
    "# load the dataset\n",
    "year = 2000\n",
    "rgi_gl = gdir_rhone.rgi_id\n",
    "\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == 'sep']\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "voi = [\n",
    "    't2m', 'tp', 'ALTITUDE_CLIMATE', 'ELEVATION_DIFFERENCE', 'hugonnet_dhdt',\n",
    "    'consensus_ice_thickness'\n",
    "]\n",
    "axs = axs.flatten()\n",
    "for i, var in enumerate(voi):\n",
    "    sns.scatterplot(df,\n",
    "                    x='POINT_LON',\n",
    "                    y='POINT_LAT',\n",
    "                    hue=var,\n",
    "                    s=5,\n",
    "                    alpha=0.5,\n",
    "                    palette='twilight_shifted',\n",
    "                    ax=axs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train NN model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "# drop taelliboden if in there\n",
    "if 'taelliboden' in data_glamos['GLACIER'].unique():\n",
    "    data_glamos = data_glamos[data_glamos['GLACIER'] != 'taelliboden']\n",
    "\n",
    "print('-------------------')\n",
    "print('Number of glaciers:', len(data_glamos['GLACIER'].unique()))\n",
    "print('Number of winter and annual samples:', len(data_glamos))\n",
    "print('Number of annual samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'winter']))\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_central_alps.csv')\n",
    "data_monthly = dataloader_gl.data\n",
    "\n",
    "data_monthly['GLWD_ID'] = data_monthly.apply(\n",
    "    lambda x: mbm.data_processing.utils.get_hash(f\"{x.GLACIER}_{x.YEAR}\"),\n",
    "    axis=1)\n",
    "data_monthly['GLWD_ID'] = data_monthly['GLWD_ID'].astype(str)\n",
    "\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_glaciers = [\n",
    "    'tortin', 'plattalva', 'sanktanna', 'schwarzberg', 'hohlaub', 'pizol',\n",
    "    'corvatsch', 'tsanfleuron', 'forno'\n",
    "]\n",
    "\n",
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in test_glaciers]\n",
    "\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(test_glaciers)]\n",
    "print('Size of monthly test data:', len(data_test))\n",
    "\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train))\n",
    "\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "# Number of annual versus winter measurements:\n",
    "print('-------------\\nTrain:')\n",
    "print('Number of monthly winter and annual samples:', len(data_train))\n",
    "print('Number of monthly annual samples:',\n",
    "      len(data_train[data_train.PERIOD == 'annual']))\n",
    "print('Number of monthly winter samples:',\n",
    "      len(data_train[data_train.PERIOD == 'winter']))\n",
    "\n",
    "# Same for test\n",
    "data_test_annual = data_test[data_test.PERIOD == 'annual']\n",
    "data_test_winter = data_test[data_test.PERIOD == 'winter']\n",
    "\n",
    "print('Test:')\n",
    "print('Number of monthly winter and annual samples:', len(data_test))\n",
    "print('Number of monthly annual samples:', len(data_test_annual))\n",
    "print('Number of monthly winter samples:', len(data_test_winter))\n",
    "\n",
    "print('Total:')\n",
    "print('Number of monthly rows:', len(dataloader_gl.data))\n",
    "print('Number of annual rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'annual']))\n",
    "print('Number of winter rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'winter']))\n",
    "\n",
    "# same for original data:\n",
    "print('-------------\\nIn annual format:')\n",
    "print('Number of annual train rows:',\n",
    "      len(data_glamos[data_glamos.GLACIER.isin(train_glaciers)]))\n",
    "print('Number of annual test rows:',\n",
    "      len(data_glamos[data_glamos.GLACIER.isin(test_glaciers)]))\n",
    "\n",
    "print('---------------\\n CV splits:')\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "dataloader = mbm.dataloader.DataLoader(cfg, data=data_train)\n",
    "\n",
    "train_itr, val_itr = dataloader.set_train_test_split(test_size=0.2)\n",
    "\n",
    "# Get all indices of the training and valing dataset at once from the iterators. Once called, the iterators are empty.\n",
    "train_indices, val_indices = list(train_itr), list(val_itr)\n",
    "\n",
    "df_X_train = data_train.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Get val set\n",
    "df_X_val = data_train.iloc[val_indices]\n",
    "y_val = df_X_val['POINT_BALANCE'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_topo = [\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "] + list(vois_topographical)\n",
    "\n",
    "feature_columns = features_topo + list(vois_climate)\n",
    "\n",
    "cfg.setFeatures(feature_columns)\n",
    "\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "# Because CH has some extra columns, we need to cut those\n",
    "df_X_train_subset = df_X_train[all_columns]\n",
    "df_X_val_subset = df_X_val[all_columns]\n",
    "df_X_test_subset = test_set['df_X'][all_columns]\n",
    "\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of validation dataset:', df_X_val_subset.shape)\n",
    "print('Shape of testing dataset:', df_X_test_subset.shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "assert all(train_set['df_X'].POINT_BALANCE == train_set['y'])\n",
    "\n",
    "# create the same but for winter only:\n",
    "df_X_train_subset_winter = df_X_train_subset[df_X_train_subset.PERIOD ==\n",
    "                                             'winter']\n",
    "df_X_val_subset_winter = df_X_val_subset[df_X_val_subset.PERIOD == 'winter']\n",
    "y_train_w = df_X_train_subset_winter['POINT_BALANCE'].values\n",
    "y_val_w = df_X_val_subset_winter['POINT_BALANCE'].values\n",
    "print('Shape of training dataset only winter:', df_X_train_subset_winter.shape)\n",
    "print('Shape of validation dataset only winter:', df_X_val_subset_winter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='valid_loss',\n",
    "    patience=20,\n",
    "    threshold=1e-4,  # Optional: stop only when improvement is very small\n",
    ")\n",
    "\n",
    "lr_scheduler_cb = LRScheduler(policy=ReduceLROnPlateau,\n",
    "                              monitor='valid_loss',\n",
    "                              mode='min',\n",
    "                              factor=0.5,\n",
    "                              patience=5,\n",
    "                              threshold=0.01,\n",
    "                              threshold_mode='rel',\n",
    "                              verbose=True)\n",
    "\n",
    "dataset = dataset_val = None  # Initialized hereafter\n",
    "\n",
    "\n",
    "def my_train_split(ds, y=None, **fit_params):\n",
    "    return dataset, dataset_val\n",
    "\n",
    "\n",
    "# param_init = {'device': 'cuda:0'}\n",
    "param_init = {'device': 'cpu'}  # Use CPU for training\n",
    "nInp = len(feature_columns)\n",
    "\n",
    "params = {\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 128,\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optimizer__weight_decay': 1e-05,\n",
    "    'module__hidden_layers': [128, 128, 64, 32],\n",
    "    'module__dropout': 0.2,\n",
    "    'module__use_batchnorm': True,\n",
    "}\n",
    "\n",
    "args = {\n",
    "    'module': FlexibleNetwork,\n",
    "    'nbFeatures': nInp,\n",
    "    'module__input_dim': nInp,\n",
    "    'module__dropout': params['module__dropout'],\n",
    "    'module__hidden_layers': params['module__hidden_layers'],\n",
    "    'train_split': my_train_split,\n",
    "    'batch_size': params['batch_size'],\n",
    "    'verbose': 1,\n",
    "    'iterator_train__shuffle': True,\n",
    "    'lr': params['lr'],\n",
    "    'max_epochs': 200,\n",
    "    'optimizer': params['optimizer'],\n",
    "    'optimizer__weight_decay': params['optimizer__weight_decay'],\n",
    "    'module__use_batchnorm': params['module__use_batchnorm'],\n",
    "    'callbacks': [\n",
    "        ('early_stop', early_stop),\n",
    "        ('lr_scheduler', lr_scheduler_cb),\n",
    "    ]\n",
    "}\n",
    "\n",
    "custom_nn = mbm.models.CustomNeuralNetRegressor(cfg, **args, **param_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, metadata = custom_nn._create_features_metadata(df_X_train_subset)\n",
    "\n",
    "features_val, metadata_val = custom_nn._create_features_metadata(\n",
    "    df_X_val_subset)\n",
    "\n",
    "# Define the dataset for the NN\n",
    "dataset = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                features=features,\n",
    "                                                metadata=metadata,\n",
    "                                                targets=y_train)\n",
    "dataset = mbm.data_processing.SliceDatasetBinding(SliceDataset(dataset, idx=0),\n",
    "                                                  SliceDataset(dataset, idx=1))\n",
    "print(\"train:\", dataset.X.shape, dataset.y.shape)\n",
    "\n",
    "dataset_val = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                    features=features_val,\n",
    "                                                    metadata=metadata_val,\n",
    "                                                    targets=y_val)\n",
    "dataset_val = mbm.data_processing.SliceDatasetBinding(\n",
    "    SliceDataset(dataset_val, idx=0), SliceDataset(dataset_val, idx=1))\n",
    "print(\"validation:\", dataset_val.X.shape, dataset_val.y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train custom NN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "if TRAIN:\n",
    "    custom_nn.seed_all()\n",
    "\n",
    "    print(\"Training the model...\")\n",
    "    print('Model parameters:')\n",
    "    for key, value in args.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    custom_nn.fit(dataset.X, dataset.y)\n",
    "    # The dataset provided in fit is not used as the datasets are overwritten in the provided train_split function\n",
    "\n",
    "    # Generate filename with current date\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    model_filename = f\"nn_model_CA_{current_date}\"\n",
    "\n",
    "    plot_training_history(custom_nn, skip_first_n=5)\n",
    "\n",
    "    # After Training: Best weights are already loaded\n",
    "    # Save the model\n",
    "    custom_nn.save_model(model_filename)\n",
    "\n",
    "    # save params dic\n",
    "    params_filename = f\"nn_params_CA_{current_date}.pkl\"\n",
    "\n",
    "    with open(f\"models/{params_filename}\", \"wb\") as f:\n",
    "        pickle.dump(args, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
