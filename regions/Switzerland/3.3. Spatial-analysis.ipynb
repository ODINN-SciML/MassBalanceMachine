{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import warnings\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import re\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import contextily as cx\n",
    "import matplotlib.colors as mcolors\n",
    "from shapely.geometry import box\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import massbalancemachine as mbm\n",
    "from cmcrameri import cm\n",
    "from collections import defaultdict\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from scripts.geodata import *\n",
    "from scripts.geodata_plots import *\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.config_CH import *\n",
    "#  Suppress warnings issued by Cartopy when downloading data files\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# config file\n",
    "cfg = mbm.SwitzerlandConfig()\n",
    "\n",
    "cfg.dataPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "cmap = cm.devon\n",
    "color_palette_glaciers = sns.color_palette(get_cmap_hex(cmap, 15))\n",
    "\n",
    "colors = get_cmap_hex(cm.batlow, 2)\n",
    "color_xgb = colors[0]\n",
    "color_tim = '#c51b7d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vois_climate = [\n",
    "    't2m_corr', 'tp_corr', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]\n",
    "# Feature columns:\n",
    "feature_columns = [\n",
    "    'ELEVATION_DIFFERENCE'\n",
    "] + list(vois_climate) + list(vois_topographical) + ['pcsr']\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load RGI glacier IDs ===\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids)\n",
    "rgi_df.columns = rgi_df.columns.str.strip()\n",
    "rgi_df = rgi_df.sort_values(by='short_name').set_index('short_name')\n",
    "\n",
    "# === Load SGI region geometries ===\n",
    "SGI_regions = gpd.read_file(\n",
    "    os.path.join(cfg.dataPath, path_SGI_topo, 'inventory_sgi2016_r2020',\n",
    "                 'sgi_regions.geojson'))\n",
    "\n",
    "# Clean object columns\n",
    "SGI_regions[SGI_regions.select_dtypes(include='object').columns] = \\\n",
    "    SGI_regions.select_dtypes(include='object').apply(lambda col: col.str.strip())\n",
    "\n",
    "SGI_regions = SGI_regions.drop_duplicates().dropna()\n",
    "SGI_regions = SGI_regions.set_index('pk_sgi_region')\n",
    "\n",
    "# === Map to Level 0 river basins ===\n",
    "catchment_lv0 = {\n",
    "    'A': 'Rhine',\n",
    "    'B': 'Rhone',\n",
    "    'C': 'Po',\n",
    "    'D': 'Adige',\n",
    "    'E': 'Danube'\n",
    "}\n",
    "rgi_df['rvr_lv0'] = rgi_df['sgi-id'].str[0].map(catchment_lv0)\n",
    "\n",
    "\n",
    "# === Map to Level 1 river basins using SGI regions ===\n",
    "def get_river_basin(sgi_id):\n",
    "    key = sgi_id.split('-')[0]\n",
    "    if key not in SGI_regions.index:\n",
    "        return None\n",
    "    basin = SGI_regions.loc[key, 'river_basin_name']\n",
    "    if isinstance(basin, pd.Series):\n",
    "        return basin.dropna().unique()[0] if not basin.dropna().empty else None\n",
    "    return basin if pd.notna(basin) else None\n",
    "\n",
    "\n",
    "rgi_df['rvr_lv1'] = rgi_df['sgi-id'].apply(get_river_basin)\n",
    "\n",
    "# Final formatting\n",
    "rgi_df = rgi_df.reset_index().rename(columns={\n",
    "    'short_name': 'GLACIER'\n",
    "}).set_index('GLACIER')\n",
    "rgi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PREDICTIONS = cfg.dataPath + path_distributed_MB_glamos + 'MBM/glamos_dems_corr/'\n",
    "# PATH_PREDICTIONS = cfg.dataPath + path_distributed_MB_glamos + 'MBM/glamos_dems_NN/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glaciers covered by GLAMOS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glDirect = np.sort([\n",
    "    re.search(r'xr_direct_(.*?)\\.zarr', f).group(1)\n",
    "    for f in os.listdir(cfg.dataPath + path_pcsr + 'zarr/')\n",
    "])\n",
    "geodetic_mb = get_geodetic_MB(cfg)\n",
    "\n",
    "# filter to glaciers with potential clear sky radiation data\n",
    "geodetic_mb = geodetic_mb[geodetic_mb.glacier_name.isin(glDirect)]\n",
    "\n",
    "# get years per glacier\n",
    "years_start_per_gl = geodetic_mb.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodetic_mb.groupby('glacier_name')['Aend'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "periods_per_glacier, geoMB_per_glacier = build_periods_per_glacier(geodetic_mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the files in both directories\n",
    "glaciers_in_distributed_glamos = os.listdir(cfg.dataPath + path_distributed_MB_glamos +\n",
    "                                            'MBM/glamos_dems/')\n",
    "glaciers_in_geodetic = periods_per_glacier.keys()\n",
    "glaciers_in_glamos = os.listdir(PATH_PREDICTIONS)\n",
    "\n",
    "# Check if all glaciers in the first directory are in the second directory\n",
    "missing_glaciers = [\n",
    "    glacier for glacier in glaciers_in_distributed_glamos\n",
    "    if glacier not in glaciers_in_glamos\n",
    "]\n",
    "missing_glaciers_geod = [\n",
    "    glacier for glacier in glaciers_in_geodetic\n",
    "    if glacier not in glaciers_in_glamos\n",
    "]\n",
    "\n",
    "# Find the intersection of the two lists\n",
    "intersecting_glaciers = list(\n",
    "    set(missing_glaciers_geod) & set(missing_glaciers))\n",
    "\n",
    "print(\"The following glaciers are missing compared to gridded MB:\")\n",
    "print(missing_glaciers)\n",
    "\n",
    "print(\"The following glaciers don't have geodetic MB:\")\n",
    "print(missing_glaciers_geod)\n",
    "\n",
    "print(\"The following glaciers are missing in both lists:\")\n",
    "print(intersecting_glaciers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glaciers with geodetic MB data:\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area(cfg)\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list, gl_area):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "\n",
    "glacier_list = [\n",
    "    f for f in list(periods_per_glacier.keys()) if f in glaciers_in_glamos\n",
    "]\n",
    "glacier_list = sort_by_area(glacier_list, gl_area)\n",
    "# print len and list\n",
    "print('Number of glaciers:', len(glacier_list))\n",
    "print('Glaciers:', glacier_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Geodetic MB:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_name = \"silvretta\"\n",
    "GLAMOS_glwmb = get_GLAMOS_glwmb(glacier_name, cfg)\n",
    "\n",
    "# If GLAMOS data is missing, skip processing\n",
    "if GLAMOS_glwmb is None:\n",
    "    print(f\"Skipping {glacier_name}: No GLAMOS data available.\")\n",
    "else:\n",
    "    # Get all geodetic periods\n",
    "    periods = periods_per_glacier.get(glacier_name, [])\n",
    "    geoMBs = geoMB_per_glacier.get(glacier_name, [])\n",
    "\n",
    "    path_mbm_pred = os.path.join(PATH_PREDICTIONS, glacier_name)\n",
    "\n",
    "    # Storage lists\n",
    "    mbm_mb_mean, glamos_mb_mean, geodetic_mb, target_period = [], [], [], []\n",
    "\n",
    "    for period in periods:\n",
    "        mbm_mb, glamos_mb = [], []\n",
    "\n",
    "        for year in range(period[0], period[1] + 1):\n",
    "            # Construct file path\n",
    "            file_path = os.path.join(cfg.dataPath, path_mbm_pred,\n",
    "                                     f\"{glacier_name}_{year}_annual.zarr\")\n",
    "\n",
    "            # Check if the NetCDF file exists\n",
    "            if not os.path.exists(file_path):\n",
    "                print(\n",
    "                    f\"Warning: Missing MBM file for {glacier_name} ({year}). Skipping...\"\n",
    "                )\n",
    "                mbm_mb.append(np.nan)\n",
    "            else:\n",
    "                ds = xr.open_dataset(file_path)\n",
    "                # Compute mean glacier-wide MB\n",
    "                mbm_mb.append(ds[\"pred_masked\"].mean().values)\n",
    "\n",
    "            # Get GLAMOS Balance for the year, or return NaN if missing\n",
    "            glamos_mb.append(GLAMOS_glwmb[\"GLAMOS Balance\"].get(year, np.nan))\n",
    "\n",
    "        # Store mean values, ignoring NaNs\n",
    "        mbm_mb_mean.append(np.nanmean(mbm_mb))\n",
    "        glamos_mb_mean.append(np.nanmean(glamos_mb))\n",
    "        geodetic_mb.append(geoMBs[periods.index(period)])\n",
    "        target_period.append(period)\n",
    "\n",
    "    # store all in a dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'mbm_mb_mean': mbm_mb_mean,\n",
    "        'glamos_mb_mean': glamos_mb_mean,\n",
    "        'geodetic_mb': geodetic_mb,\n",
    "        'target_period': target_period,\n",
    "        'end_year': [period[1] for period in target_period],\n",
    "        'start_year': [period[0] for period in target_period],\n",
    "    })\n",
    "\n",
    "fig = plot_geodetic_MB(df, glacier_name, color_xgb, color_tim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_glaciers = [\n",
    "    'tortin', 'plattalva', 'sanktanna', 'schwarzberg', 'hohlaub', 'pizol',\n",
    "    'corvatsch', 'tsanfleuron', 'forno'\n",
    "]\n",
    "df_all = process_geodetic_mass_balance_comparison(\n",
    "    glacier_list=glacier_list,\n",
    "    path_SMB_GLAMOS_csv=cfg.dataPath+path_SMB_GLAMOS_csv,\n",
    "    periods_per_glacier=periods_per_glacier,\n",
    "    geoMB_per_glacier=geoMB_per_glacier,\n",
    "    gl_area=gl_area,\n",
    "    test_glaciers=test_glaciers,\n",
    "    path_predictions=PATH_PREDICTIONS,  # or another path if needed\n",
    "    cfg = cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All glaciers\n",
    "scatter_geodetic_MB(df_all, hue='GLACIER', size=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = True\n",
    "\n",
    "# Drop rows where any required columns are NaN\n",
    "df_all = df_all.dropna(subset=['Geodetic MB', 'MBM MB', 'GLAMOS MB'])\n",
    "\n",
    "# Compute RMSE and Pearson correlation\n",
    "rmse_mbm = mean_squared_error(df_all[\"Geodetic MB\"],\n",
    "                              df_all[\"MBM MB\"],\n",
    "                              squared=False)\n",
    "corr_mbm = np.corrcoef(df_all[\"Geodetic MB\"], df_all[\"MBM MB\"])[0, 1]\n",
    "rmse_glamos = mean_squared_error(df_all[\"Geodetic MB\"],\n",
    "                                 df_all[\"GLAMOS MB\"],\n",
    "                                 squared=False)\n",
    "corr_glamos = np.corrcoef(df_all[\"Geodetic MB\"], df_all[\"GLAMOS MB\"])[0, 1]\n",
    "\n",
    "# Define figure and axes\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 8), sharex=True)\n",
    "\n",
    "# Plot MBM MB vs Geodetic MB\n",
    "plot_scatter(df_all, 'GLACIER', size, axs, \"MBM MB\",\n",
    "             rmse_mbm, corr_mbm)\n",
    "\n",
    "axs.set_title('Geodetic target vs Mass Balance Machine', fontsize=24)\n",
    "axs.set_ylabel('Predicted mass balance [m w.e.]', fontsize=18)\n",
    "axs.set_xlabel('Geodetic mass balance [m w.e.]', fontsize=18)\n",
    "\n",
    "# Adjust legend outside of plot\n",
    "handles, labels = axs.get_legend_handles_labels()\n",
    "fig.legend(handles,\n",
    "           labels,\n",
    "           bbox_to_anchor=(1.05, 1),\n",
    "           loc=\"upper left\",\n",
    "           borderaxespad=0.,\n",
    "           ncol=2,\n",
    "           fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glacier wide MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define glacier name\n",
    "glacier_name = \"basodino\"\n",
    "\n",
    "# Load GLAMOS data\n",
    "GLAMOS_glwmb = get_GLAMOS_glwmb(glacier_name, cfg)\n",
    "\n",
    "# Define the path to model predictions\n",
    "path_results = os.path.join(PATH_PREDICTIONS, glacier_name)\n",
    "\n",
    "# Extract available years from NetCDF filenames\n",
    "years = sorted([\n",
    "    int(f.split(\"_\")[1]) for f in os.listdir(path_results)\n",
    "    if f.endswith(\"_annual.zarr\")\n",
    "])\n",
    "\n",
    "# Extract model-predicted mass balance\n",
    "pred_gl = []\n",
    "for year in years:\n",
    "    file_path = os.path.join(path_results,\n",
    "                             f\"{glacier_name}_{year}_annual.zarr\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\n",
    "            f\"Warning: Missing MBM file for {glacier_name} ({year}). Skipping...\"\n",
    "        )\n",
    "        pred_gl.append(np.nan)\n",
    "        continue\n",
    "\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    pred_gl.append(ds.pred_masked.mean().item())\n",
    "\n",
    "# Create DataFrame\n",
    "MBM_glwmb = pd.DataFrame(pred_gl, index=years, columns=[\"MBM Balance\"])\n",
    "\n",
    "# Merge with GLAMOS data\n",
    "MBM_glwmb = MBM_glwmb.join(GLAMOS_glwmb)\n",
    "\n",
    "# Drop NaN values to avoid plotting errors\n",
    "MBM_glwmb = MBM_glwmb.dropna()\n",
    "\n",
    "# Plot the data\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "MBM_glwmb.plot(ax=ax, marker=\"o\", color=[color_xgb, color_tim])\n",
    "ax.set_title(f\"{glacier_name.capitalize()} Glacier\", fontsize=24)\n",
    "ax.set_ylabel(\"Mass Balance [m w.e.]\", fontsize=18)\n",
    "ax.set_xlabel(\"Year\", fontsize=18)\n",
    "ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "# increase font size of legends\n",
    "ax.legend(fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output folder\n",
    "output_folder = \"figures/glacierwide/\"\n",
    "emptyfolder(output_folder)  # Clear existing figures\n",
    "\n",
    "# Process each glacier\n",
    "for glacier_name in tqdm(glacier_list):\n",
    "    # Load GLAMOS data\n",
    "    GLAMOS_glwmb = get_GLAMOS_glwmb(glacier_name, cfg)\n",
    "\n",
    "    # Check if GLAMOS data is available\n",
    "    if GLAMOS_glwmb is None:\n",
    "        print(f\"Skipping {glacier_name}: No GLAMOS data available.\")\n",
    "        continue\n",
    "\n",
    "    # Define the path to model predictions\n",
    "    glacier_path = os.path.join(PATH_PREDICTIONS, glacier_name)\n",
    "\n",
    "    # Check if glacier prediction directory exists\n",
    "    if not os.path.exists(glacier_path):\n",
    "        print(f\"Skipping {glacier_name}: No prediction data available.\")\n",
    "        continue\n",
    "\n",
    "    # Extract available years from NetCDF filenames\n",
    "    try:\n",
    "        years = sorted([\n",
    "            int(f.split(\"_\")[1]) for f in os.listdir(glacier_path)\n",
    "            if f.endswith(\"_annual.zarr\") and \"_\" in f\n",
    "        ])\n",
    "    except ValueError:\n",
    "        print(\n",
    "            f\"Skipping {glacier_name}: Failed to extract years from filenames.\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    if not years:\n",
    "        print(f\"Skipping {glacier_name}: No model prediction files found.\")\n",
    "        continue\n",
    "\n",
    "    # Extract model-predicted mass balance\n",
    "    pred_gl = []\n",
    "    for year in years:\n",
    "        file_path = os.path.join(glacier_path,\n",
    "                                 f\"{glacier_name}_{year}_annual.zarr\")\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            print(\n",
    "                f\"Warning: Missing MBM file for {glacier_name} ({year}). Skipping...\"\n",
    "            )\n",
    "            pred_gl.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        ds = xr.open_dataset(file_path)\n",
    "        pred_gl.append(ds.pred_masked.mean().item())\n",
    "\n",
    "    # Create DataFrame\n",
    "    glwide_MB = pd.DataFrame(pred_gl, index=years, columns=[\"MBM Balance\"])\n",
    "\n",
    "    # Merge with GLAMOS data\n",
    "    glwide_MB = glwide_MB.join(GLAMOS_glwmb)\n",
    "\n",
    "    # Drop NaN values to avoid plotting errors\n",
    "    glwide_MB = glwide_MB.dropna()\n",
    "\n",
    "    # Ensure data exists before plotting\n",
    "    if glwide_MB.empty:\n",
    "        print(f\"Skipping {glacier_name}: No valid data to plot.\")\n",
    "        continue\n",
    "\n",
    "    # Plot the data\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    glwide_MB.plot(ax=ax, marker=\"o\", color=[color_xgb, color_tim])\n",
    "    ax.set_title(f\"{glacier_name.capitalize()} Glacier\", fontsize=24)\n",
    "    ax.set_ylabel(\"Mass Balance [m w.e.]\", fontsize=18)\n",
    "    ax.set_xlabel(\"Year\", fontsize=18)\n",
    "    ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "    # increase font size of legends\n",
    "    ax.legend(fontsize=14)\n",
    "\n",
    "    # Save the figure\n",
    "    output_path = os.path.join(output_folder, f\"{glacier_name}.png\")\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    # Clear the plot to free memory\n",
    "    plt.close(fig)\n",
    "\n",
    "print(\"Processing complete. Figures saved in:\", output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine diff geod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define glacier name\n",
    "glacier_name = \"aletsch\"\n",
    "# Define the path to model predictions\n",
    "path_results = os.path.join(PATH_PREDICTIONS, glacier_name)\n",
    "\n",
    "# Load GLAMOS data\n",
    "GLAMOS_glwmb = get_GLAMOS_glwmb(glacier_name, cfg)\n",
    "\n",
    "# Extract available years from NetCDF filenames\n",
    "years = sorted([\n",
    "    int(f.split(\"_\")[1]) for f in os.listdir(path_results)\n",
    "    if f.endswith(\"_annual.zarr\")\n",
    "])\n",
    "\n",
    "print(years)\n",
    "\n",
    "# Extract model-predicted mass balance\n",
    "pred_gl = []\n",
    "for year in years:\n",
    "    file_path = os.path.join(path_results,\n",
    "                             f\"{glacier_name}_{year}_annual.zarr\")\n",
    "    if not os.path.isdir(file_path):\n",
    "        print(\n",
    "            f\"Warning: Missing MBM file for {glacier_name} ({year}). Skipping...\"\n",
    "        )\n",
    "        pred_gl.append(np.nan)\n",
    "        continue\n",
    "\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    pred_gl.append(ds.pred_masked.mean().item())\n",
    "\n",
    "# Create DataFrame\n",
    "MBM_glwmb = pd.DataFrame(pred_gl, index=years, columns=[\"MBM Balance\"])\n",
    "\n",
    "# Merge with GLAMOS data\n",
    "MBM_glwmb = MBM_glwmb.join(GLAMOS_glwmb)\n",
    "\n",
    "# Drop NaN values to avoid plotting errors\n",
    "MBM_glwmb = MBM_glwmb.dropna()\n",
    "\n",
    "geoMB_target = geoMB_per_glacier.get(glacier_name, [])\n",
    "geoMB_periods = periods_per_glacier.get(glacier_name, [])\n",
    "\n",
    "df = pd.DataFrame(data={'geoMB_periods': geoMB_periods})\n",
    "df['mbm_geod'], df['glamos_geod'], df['target_geod'] = np.nan, np.nan, np.nan\n",
    "\n",
    "for i, period in enumerate(geoMB_periods):\n",
    "    mbm_mb, glamos_mb = [], []\n",
    "    for year in range(period[0], period[1] + 1):\n",
    "        file_path = os.path.join(path_results,\n",
    "                                 f\"{glacier_name}_{year}_annual.zarr\")\n",
    "        ds = xr.open_dataset(file_path)\n",
    "        mbm_mb.append(ds[\"pred_masked\"].mean().values)\n",
    "\n",
    "        file_ann = f\"{year}_ann_fix_lv95.grid\"\n",
    "        grid_path_ann = os.path.join(cfg.dataPath, path_distributed_MB_glamos, 'GLAMOS',\n",
    "                                     glacier_name, file_ann)\n",
    "\n",
    "        metadata_ann, grid_data_ann = load_grid_file(grid_path_ann)\n",
    "        ds_glamos_ann = convert_to_xarray_geodata(grid_data_ann, metadata_ann)\n",
    "        ds_glamos_wgs84_ann = transform_xarray_coords_lv95_to_wgs84(\n",
    "            ds_glamos_ann)\n",
    "        glamos_mb.append(ds_glamos_wgs84_ann.mean().values)\n",
    "\n",
    "    df['mbm_geod'].iloc[i] = np.nanmean(mbm_mb)\n",
    "    df['glamos_geod'].iloc[i] = np.nanmean(glamos_mb)\n",
    "    df['target_geod'].iloc[i] = geoMB_target[geoMB_periods.index(period)]\n",
    "\n",
    "plot_mass_balance_comparison(glacier_name,\n",
    "                             geoMB_periods,\n",
    "                             MBM_glwmb,\n",
    "                             df,\n",
    "                             color_mbm=color_xgb,\n",
    "                             color_model2=color_tim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed GLAMOS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stake data ONCE instead of for every glacier\n",
    "stake_file = os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv, \"CH_wgms_dataset_all.csv\")\n",
    "df_stakes = pd.read_csv(stake_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate against stake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_name = \"aletsch\"\n",
    "year = 2022\n",
    "fig = plot_mass_balance(glacier_name, year, df_stakes,\n",
    "                        os.path.join(cfg.dataPath, path_distributed_MB_glamos, 'GLAMOS'),\n",
    "                        PATH_PREDICTIONS)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define glacier name\n",
    "glacier_name = \"aletsch\"\n",
    "\n",
    "# Get available years from filenames\n",
    "years = sorted(\n",
    "    map(int, [\n",
    "        f.split(\"_\")[1]\n",
    "        for f in os.listdir(os.path.join(PATH_PREDICTIONS, glacier_name))\n",
    "        if \"_\" in f and f.endswith(\"_annual.zarr\")\n",
    "    ]))\n",
    "\n",
    "# if not exist create path:\n",
    "if not os.path.exists(f\"figures/dst_mb/{glacier_name}\"):\n",
    "    os.makedirs(f\"figures/dst_mb/{glacier_name}\")\n",
    "# else empty:\n",
    "else:\n",
    "    emptyfolder(f\"figures/dst_mb/{glacier_name}\")\n",
    "\n",
    "# Iterate through each year\n",
    "for year in tqdm(years):\n",
    "    print(f\"Processing: {glacier_name}, Year: {year}\")\n",
    "\n",
    "    fig = plot_mass_balance(glacier_name, year, df_stakes,\n",
    "                            os.path.join(cfg.dataPath, path_distributed_MB_glamos, 'GLAMOS'),\n",
    "                            PATH_PREDICTIONS)\n",
    "\n",
    "    # save figure\n",
    "    output_path = os.path.join(f\"figures/dst_mb/{glacier_name}\",\n",
    "                               f\"{glacier_name}_{year}.png\")\n",
    "    # if fig not none:\n",
    "    if fig:\n",
    "        fig.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input grids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_name = 'aletsch'\n",
    "year = 2010\n",
    "month = 'jul'\n",
    "\n",
    "# Feature columns:\n",
    "feature_columns = [\n",
    "    'ELEVATION_DIFFERENCE'\n",
    "] + list(vois_climate) + list(vois_topographical) + ['pcsr']\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "glacier_file = os.path.join(cfg.dataPath, path_glacier_grid_glamos, glacier_name,\n",
    "                            f\"{glacier_name}_grid_{year}.parquet\")\n",
    "\n",
    "df_grid_monthly = pd.read_parquet(glacier_file)\n",
    "\n",
    "# Correct climate grids:\n",
    "# Take the biggest grid cell value for each month\n",
    "for voi in [\n",
    "        't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10',\n",
    "        'ALTITUDE_CLIMATE'\n",
    "]:\n",
    "    df_grid_monthly = correct_for_biggest_grid(\n",
    "        df_grid_monthly, group_columns=[\"YEAR\", \"MONTHS\"], value_column=voi)\n",
    "\n",
    "# New elevation difference:\n",
    "df_grid_monthly['ELEVATION_DIFFERENCE'] = df_grid_monthly[\n",
    "    \"POINT_ELEVATION\"] - df_grid_monthly[\"ALTITUDE_CLIMATE\"]\n",
    "\n",
    "# apply T & P correction\n",
    "# Apply temperature gradient correction\n",
    "temp_grad = -6.5 / 1000\n",
    "dpdz = 1.5 / 10000\n",
    "c_prec = 1.434,\n",
    "t_off = 0.617\n",
    "\n",
    "# Apply temperature correction factor\n",
    "df_grid_monthly['t2m_corr'] = df_grid_monthly['t2m'] + (\n",
    "    df_grid_monthly['ELEVATION_DIFFERENCE'] * temp_grad)\n",
    "df_grid_monthly['t2m_corr'] += t_off\n",
    "\n",
    "# Apply elevation correction factor\n",
    "df_grid_monthly['tp_corr'] = df_grid_monthly['tp'] * c_prec\n",
    "df_grid_monthly['tp_corr'] += df_grid_monthly['tp_corr'] * (\n",
    "    df_grid_monthly['ELEVATION_DIFFERENCE'] * dpdz)\n",
    "\n",
    "# Rename aspect and slope to sgi\n",
    "df_grid_monthly.rename(columns={\n",
    "    'aspect': 'aspect_sgi',\n",
    "    'slope': 'slope_sgi'\n",
    "},\n",
    "                       inplace=True)\n",
    "df_grid_monthly['POINT_ELEVATION'] = df_grid_monthly['topo']\n",
    "df_grid_monthly.drop_duplicates(inplace=True)  # remove duplicates\n",
    "df_grid_monthly = df_grid_monthly[all_columns]\n",
    "\n",
    "df_grid_month = df_grid_monthly[df_grid_monthly.MONTHS == month]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features to plot\n",
    "v_plot = [\"tp_corr\", \"ELEVATION_DIFFERENCE\"\n",
    "          ] + vois_topographical + ['slhf', 'sshf']\n",
    "\n",
    "# Ensure the grid layout fits the number of plots dynamically\n",
    "n_rows = 3\n",
    "n_cols = min(3, len(v_plot))  # Ensure no more than 4 columns\n",
    "\n",
    "# Create figure and axes\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(10, 10))\n",
    "\n",
    "# Flatten the axes array for easier indexing\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Plot each feature\n",
    "for i, voi in enumerate(v_plot):\n",
    "    sns.scatterplot(data=df_grid_month,\n",
    "                    x=\"POINT_LON\",\n",
    "                    y=\"POINT_LAT\",\n",
    "                    hue=voi,\n",
    "                    ax=axs[i],\n",
    "                    palette=\"twilight_shifted\",\n",
    "                    s=2)\n",
    "    axs[i].set_title(voi)\n",
    "    axs[i].set_ylabel(\"Lat\")\n",
    "    axs[i].set_xlabel(\"Lon\")\n",
    "    axs[i].legend().remove()\n",
    "    axs[i].grid(False)\n",
    "\n",
    "# Remove unused subplots if any\n",
    "for j in range(i + 1, len(axs)):\n",
    "    fig.delaxes(axs[j])\n",
    "\n",
    "# Adjust layout for clarity\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stake coverage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define year and month for analysis\n",
    "year = 2018\n",
    "month = \"jul\"\n",
    "\n",
    "# Ensure the output folder is empty\n",
    "output_folder = \"figures/stake_coverage/\"\n",
    "emptyfolder(output_folder)\n",
    "\n",
    "# Load stake data ONCE instead of for every glacier\n",
    "stake_file = os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv, \"CH_wgms_dataset_all.csv\")\n",
    "if not os.path.exists(stake_file):\n",
    "    print(\"Error: Stake data file not found. Exiting.\")\n",
    "else:\n",
    "    df_stakes = pd.read_csv(stake_file)\n",
    "\n",
    "# Iterate through each glacier\n",
    "for glacier_name in tqdm(glacier_list, desc=\"Processing glaciers\"):\n",
    "    # Construct glacier grid file path\n",
    "    glacier_file = os.path.join(cfg.dataPath, path_glacier_grid_glamos, glacier_name,\n",
    "                                f\"{glacier_name}_grid_{year}.parquet\")\n",
    "\n",
    "    if not os.path.exists(glacier_file):\n",
    "        print(f\"Skipping {glacier_name}: Grid file not found.\")\n",
    "        print(glacier_file)\n",
    "        continue  # Skip if glacier grid file is missing\n",
    "\n",
    "    # Load and preprocess glacier grid data\n",
    "    df_grid_monthly = pd.read_parquet(glacier_file)\n",
    "    df_grid_monthly = correct_vars_grid(df_grid_monthly)\n",
    "\n",
    "    # Rename aspect and slope to sgi\n",
    "    df_grid_monthly.rename(columns={\n",
    "        \"aspect\": \"aspect_sgi\",\n",
    "        \"slope\": \"slope_sgi\"\n",
    "    },\n",
    "                           inplace=True)\n",
    "    df_grid_monthly[\"POINT_ELEVATION\"] = df_grid_monthly[\"topo\"]\n",
    "    df_grid_monthly.drop_duplicates(inplace=True)  # Remove duplicates\n",
    "\n",
    "    # Filter dataset for selected month\n",
    "    df_grid_month = df_grid_monthly[df_grid_monthly.MONTHS == month]\n",
    "\n",
    "    # Filter stakes for this glacier\n",
    "    df_stakes_glacier = df_stakes[df_stakes[\"GLACIER\"] == glacier_name].copy()\n",
    "\n",
    "    # If no stake data available for this glacier, continue\n",
    "    if df_stakes_glacier.empty:\n",
    "        print(f\"Skipping {glacier_name}: No stake data available.\")\n",
    "        continue\n",
    "\n",
    "    # Convert to GeoDataFrame for geospatial visualization\n",
    "    df_stakes_glacier[\"geometry\"] = gpd.points_from_xy(\n",
    "        df_stakes_glacier.POINT_LON, df_stakes_glacier.POINT_LAT)\n",
    "    gdf_stakes = gpd.GeoDataFrame(df_stakes_glacier, crs=\"EPSG:4326\")\n",
    "\n",
    "    # Load SGI grid\n",
    "    sgi_file = os.path.join(cfg.dataPath, path_SGI_topo,\n",
    "                            f\"xr_masked_grids/{glacier_name}.zarr\")\n",
    "\n",
    "    if not os.path.exists(sgi_file):\n",
    "        print(f\"Skipping {glacier_name}: SGI NetCDF file not found.\")\n",
    "        continue\n",
    "\n",
    "    sgi_grid = xr.open_dataset(sgi_file)\n",
    "\n",
    "    # Create figure and plot\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    # Plot glacier mask\n",
    "    sgi_grid.glacier_mask.plot(cmap=\"binary\", ax=ax, alpha=0.5)\n",
    "\n",
    "    # Scatter plot of the monthly glacier grid with elevation difference\n",
    "    scatter = sns.scatterplot(data=df_grid_month,\n",
    "                              x=\"POINT_LON\",\n",
    "                              y=\"POINT_LAT\",\n",
    "                              hue=\"ELEVATION_DIFFERENCE\",\n",
    "                              ax=ax,\n",
    "                              s=5,\n",
    "                              palette=\"twilight_shifted\",\n",
    "                              legend=True)\n",
    "\n",
    "    # Plot stake locations\n",
    "    gdf_stakes.plot(ax=ax,\n",
    "                    color=color_tim,\n",
    "                    edgecolor=\"black\",\n",
    "                    markersize=20,\n",
    "                    label=\"Stakes\")\n",
    "\n",
    "    # Remove automatic Seaborn legend\n",
    "    ax.legend().remove()\n",
    "\n",
    "    # Set title\n",
    "    ax.set_title(f\"{glacier_name}\")\n",
    "\n",
    "    # Save figure\n",
    "    output_path = os.path.join(output_folder, f\"{glacier_name}.png\")\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    # Clear the plot to free memory\n",
    "    plt.close(fig)\n",
    "\n",
    "print(\"Processing complete. Figures saved to:\", output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2 Sentinel data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get S2 rasters:\n",
    "Get the names of all satellite nc files and classifiy them per month and hydrological year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel-2\n",
    "src_crs = 'EPSG:4326'  # Original CRS (lat/lon) wgs84\n",
    "\n",
    "# Organize the rasters by hydrological year\n",
    "satellite_years = [2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "\n",
    "rasters_S2 = organize_rasters_by_hydro_year(cfg.dataPath+path_S2, satellite_years)\n",
    "\n",
    "# Print the organized rasters\n",
    "for hydro_year, months in rasters_S2.items():\n",
    "    print(f\"-----------------\\nHydrological Year: {hydro_year}\")\n",
    "    for month, files in months.items():\n",
    "        print(f\"  {month}: {files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create gdf for S2 rasters:\n",
    "Create panadas geodataframe for each satellite raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satellite_glaciers = [\n",
    "    'adler', 'aletsch', 'allalin', 'basodino', 'claridenL', 'claridenU',\n",
    "    'findelen', 'gries', 'hohlaub', 'limmern', 'oberaar', 'plattalva', 'rhone',\n",
    "    'sanktanna', 'schwarzbach', 'schwarzberg'\n",
    "]\n",
    "\n",
    "processed_gl = os.listdir(PATH_PREDICTIONS)\n",
    "\n",
    "# list of gl in processed_gl that are also in satellite_glaciers\n",
    "satellite_glaciers = [gl for gl in processed_gl if gl in satellite_glaciers]\n",
    "satellite_glaciers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGI rasters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2 rasters over glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = True\n",
    "if RUN:\n",
    "    # emptyfolder(os.path.join(cfg.dataPath, path_S2, 'perglacier/'))\n",
    "    # For each year and month where there is S2 data\n",
    "    for hydro_year, months in tqdm(rasters_S2.items(),\n",
    "                                   position=0,\n",
    "                                   desc='Hydrological Years'):\n",
    "        for month, files in tqdm(months.items(),\n",
    "                                 position=1,\n",
    "                                 leave=False,\n",
    "                                 desc='Months'):\n",
    "\n",
    "            # More than one if S2 A and B flew in the same month\n",
    "            for raster_name in files:\n",
    "                # Create raster for that year & month:\n",
    "                file_date = datetime.strptime(\n",
    "                    raster_name.split('_')[3][:8],\n",
    "                    \"%Y%m%d\")  # Extract the 8-digit date (YYYYMMDD)\n",
    "\n",
    "                # Path to the raster for that year & month:\n",
    "                raster_path = os.path.join(cfg.dataPath, path_S2, file_date.strftime('%Y'),\n",
    "                                           raster_name)\n",
    "\n",
    "                # Creates a geodataframe from the .tif raster\n",
    "                # This is the longest part of the code\n",
    "                geo_data_S2 = mbm.geodata.GeoData(pd.DataFrame)\n",
    "                gdf_S2 = geo_data_S2.raster_to_gpd(raster_path)\n",
    "                geo_data_S2.set_gdf(gdf_S2)\n",
    "\n",
    "                # For each glacier, clip the S2 raster to the glacier extent\n",
    "                for glacierName in tqdm(satellite_glaciers,\n",
    "                                        position=2,\n",
    "                                        leave=False,\n",
    "                                        desc='Glaciers'):\n",
    "\n",
    "                    # Load MB predictions for that year and month\n",
    "                    path_nc_wgs84 = PATH_PREDICTIONS + f\"{glacierName}/\"\n",
    "                    filename_nc = f\"{glacierName}_{hydro_year}_{cfg.month_abbr_hydr[month]}.nc\"\n",
    "\n",
    "                    # check if file exists:\n",
    "                    if not os.path.exists(\n",
    "                            os.path.join(path_nc_wgs84, filename_nc)):\n",
    "                        continue\n",
    "\n",
    "                    # Open xarray dataset and set to class:\n",
    "                    geoData_gl = mbm.geodata.GeoData(pd.DataFrame)\n",
    "                    geoData_gl.set_ds_latlon(filename_nc, path_nc_wgs84)\n",
    "\n",
    "                    # Clip the S2 dataframe to the glacier extent\n",
    "                    # and resample it to the glacier resolution\n",
    "                    gdf_raster_res = mbm.geodata.GeoData.resample_satellite_to_glacier(\n",
    "                        geoData_gl.gdf, gdf_S2)\n",
    "\n",
    "                    # In case the glacier is outside of the bounds of the raster\n",
    "                    if gdf_raster_res is 0:\n",
    "                        continue\n",
    "                    # In case the raster is empty where the glacier is\n",
    "                    elif gdf_raster_res is 1:\n",
    "                        continue\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                    # Check the percentage of cloud cover\n",
    "                    cloud_cover_glacier = gdf_raster_res.classes[\n",
    "                        gdf_raster_res.classes ==\n",
    "                        5].count() / gdf_raster_res.classes.count()\n",
    "\n",
    "                    # If the cloud cover is too high, skip the glacier\n",
    "                    if cloud_cover_glacier > 0.5:\n",
    "                        continue\n",
    "\n",
    "                    # Save the glacier raster:\n",
    "                    S2_gl_name = '{}_{}.geojson'.format(\n",
    "                        glacierName, file_date.strftime('%Y_%m_%d'))\n",
    "                    S2_gl_path = os.path.join(cfg.dataPath, path_S2, 'perglacier',\n",
    "                                              S2_gl_name)\n",
    "                    gdf_raster_res.to_file(S2_gl_path, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snow cover:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = True\n",
    "if RUN:\n",
    "\n",
    "    # first delete results file\n",
    "    if os.path.exists(f'results/snow_cover.csv'):\n",
    "        os.remove(f'results/snow_cover.csv')\n",
    "\n",
    "    # Add header\n",
    "    with open(f'results/snow_cover.csv', 'w') as f:\n",
    "        f.write(\n",
    "            \"year,month,monthNb,raster_date,glacier_name,snow_cover_S2,snow_cover_glacier\\n\"\n",
    "        )\n",
    "\n",
    "    # All rasters resampled over glaciers:\n",
    "    rasters_resampled = os.listdir(os.path.join(cfg.dataPath, path_S2, 'perglacier'))\n",
    "\n",
    "    for raster_res in tqdm(rasters_resampled):\n",
    "        # Extract glacier name\n",
    "        glacierName = raster_res.split('_')[0]\n",
    "\n",
    "        # Extract date from satellite raster\n",
    "        match = re.search(r\"(\\d{4})_(\\d{2})_(\\d{2})\", raster_res)\n",
    "        year, month, day = match.groups()\n",
    "        date_str = match.group(1) + '-' + match.group(2) + '-' + match.group(3)\n",
    "        raster_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "        # Find closest hydrological year and month\n",
    "        closest_month, hydro_year = get_hydro_year_and_month(raster_date)\n",
    "        monthNb = month_abbr_hydr_full[closest_month]\n",
    "\n",
    "        if hydro_year > 2021:\n",
    "            continue\n",
    "\n",
    "        # Read satellite raster over glacier (previously resampled)\n",
    "        gdf_S2_res = gpd.read_file(\n",
    "            os.path.join(cfg.dataPath, path_S2, 'perglacier', raster_res))\n",
    "\n",
    "        # Calculate percentage of snow cover (class 1)\n",
    "        snow_cover_S2 = IceSnowCover(gdf_S2_res, gdf_S2_res)\n",
    "\n",
    "        # Load MB predictions for that year and month\n",
    "        path_nc_wgs84 = PATH_PREDICTIONS + f\"{glacierName}/\"\n",
    "        filename_nc = f\"{glacierName}_{hydro_year}_{monthNb}.nc\"\n",
    "\n",
    "        geoData_gl = mbm.geodata.GeoData(pd.DataFrame)\n",
    "        geoData_gl.set_ds_latlon(filename_nc, path_nc_wgs84)\n",
    "        geoData_gl.classify_snow_cover(tol=0.1)\n",
    "\n",
    "        snow_cover_glacier = IceSnowCover(geoData_gl.gdf, gdf_S2_res)\n",
    "\n",
    "        # Save the results\n",
    "        with open(f'results/snow_cover.csv', 'a') as f:\n",
    "            f.write(\n",
    "                f\"{hydro_year},{closest_month},{monthNb},{date_str},{glacierName},{snow_cover_S2},{snow_cover_glacier}\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'results/snow_cover.csv').sort_values(by=['year', 'month'],\n",
    "                                                        ascending=True)\n",
    "# remove october and september\n",
    "df = df[~df.month.isin(['oct', 'sep'])]\n",
    "fig, axs = plot_snow_cover_scatter(df, add_corr=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gl = df[(df.glacier_name == 'aletsch') & ~df.month.isin(['oct', 'sep'])]\n",
    "fig, axs = plot_snow_cover_scatter_combined(df_gl)\n",
    "\n",
    "df_gl = df[(df.glacier_name == 'gries') & ~df.month.isin(['oct', 'sep'])]\n",
    "fig, axs = plot_snow_cover_scatter_combined(df_gl)\n",
    "\n",
    "df_gl = df[(df.glacier_name == 'rhone') & ~df.month.isin(['oct', 'sep'])]\n",
    "fig, axs = plot_snow_cover_scatter_combined(df_gl)\n",
    "\n",
    "df_gl = df[(df.glacier_name == 'adler') & ~df.month.isin(['oct', 'sep'])]\n",
    "fig, axs = plot_snow_cover_scatter_combined(df_gl)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the raster files and plot geoplots\n",
    "gl = 'aletsch'\n",
    "yr = 2021\n",
    "rasters_gl = [\n",
    "    f for f in os.listdir(os.path.join(cfg.dataPath, path_S2, 'perglacier'))\n",
    "    if (gl in f) and (str(yr) in f)\n",
    "]\n",
    "rasters_gl.sort()\n",
    "for raster_res in rasters_gl:\n",
    "    plot_snow_cover_geoplots(raster_res, cfg.dataPath+path_S2, month_abbr_hydr_full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snowline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the raster files and plot geoplots\n",
    "gl = 'aletsch'\n",
    "yr = 2021\n",
    "rasters_gl = [\n",
    "    f for f in os.listdir(os.path.join(cfg.dataPath, path_S2, 'perglacier'))\n",
    "    if (gl in f) and (str(yr) in f)\n",
    "]\n",
    "rasters_gl.sort()\n",
    "for raster_res in rasters_gl:\n",
    "    plot_snow_cover_geoplots(raster_res,\n",
    "                             cfg.dataPath+path_S2,\n",
    "                             month_abbr_hydr_full_hydr,\n",
    "                             add_snowline=True,\n",
    "                             band_size=50,\n",
    "                             percentage_threshold=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the raster files and plot geoplots\n",
    "gl = 'rhone'\n",
    "yr = 2021\n",
    "rasters_gl = [\n",
    "    f for f in os.listdir(os.path.join(cfg.dataPath, path_S2, 'perglacier'))\n",
    "    if (gl in f) and (str(yr) in f)\n",
    "]\n",
    "rasters_gl.sort()\n",
    "for raster_res in rasters_gl:\n",
    "    plot_snow_cover_geoplots(raster_res,\n",
    "                             cfg.dataPath+path_S2,\n",
    "                             month_abbr_hydr_full_hydr,\n",
    "                             add_snowline=True,\n",
    "                             band_size=50,\n",
    "                             percentage_threshold=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missed glaciers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1: Glacier is in regions where raster is NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2015\n",
    "glacierName = 'taelliboden'\n",
    "filename_nc = f\"{glacierName}_{year}.nc\"\n",
    "path_nc_wgs84 = f\"results/nc/var_normal/{glacierName}/wgs84/\"\n",
    "path_nc_wgs84_corr = f\"results/nc/var_corr/{glacierName}/wgs84/\"\n",
    "\n",
    "gdf_raster = createRaster(raster_a_2015)\n",
    "\n",
    "# Calculate snow cover for glacier\n",
    "gdf_glacier, snow_cover_glacier, ice_cover_glacier = snowCover(\n",
    "    path_nc_wgs84, filename_nc)\n",
    "# Corrected for temperature & precipitation\n",
    "gdf_glacier, snow_cover_glacier_corr, ice_cover_glacier_corr = snowCover(\n",
    "    path_nc_wgs84_corr, filename_nc)\n",
    "\n",
    "# Clip the raster to the glacier extent and resample it to the glacier resolution\n",
    "# gdf_raster_res = resampleRaster(gdf_glacier, gdf_raster)\n",
    "\n",
    "bounding_box = gdf_glacier.total_bounds  # [minx, miny, maxx, maxy]\n",
    "raster_bounds = gdf_raster.total_bounds  # [minx, miny, maxx, maxy]\n",
    "\n",
    "# Check if glacier bounds are within raster bounds\n",
    "if not (bounding_box[0] >= raster_bounds[0]\n",
    "        and  # minx of glacier >= minx of raster\n",
    "        bounding_box[1] >= raster_bounds[1]\n",
    "        and  # miny of glacier >= miny of raster\n",
    "        bounding_box[2] <= raster_bounds[2]\n",
    "        and  # maxx of glacier <= maxx of raster\n",
    "        bounding_box[3]\n",
    "        <= raster_bounds[3]  # maxy of glacier <= maxy of raster\n",
    "        ):\n",
    "    print(f\"Glacier {glacierName} is out of bounds\")\n",
    "\n",
    "bbox_polygon = box(*bounding_box)\n",
    "\n",
    "# The raster might have no data (NaN values) in the region of the glacier:\n",
    "bounding_box = [7.8, 45.95854232, 8, 46.1]\n",
    "bbox_polygon = box(*bounding_box)\n",
    "gfd_res = gdf_raster[gdf_raster.intersects(bbox_polygon)]\n",
    "ax = gfd_res.plot(color='blue', alpha=0.5)\n",
    "gdf_glacier.plot(ax=ax, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform to tif rasters for QGIS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacierName = 'aletsch'\n",
    "year = 2021\n",
    "\n",
    "filename_nc = f\"{glacierName}_{year}.nc\"\n",
    "path_nc_lv95 = f\"results/nc/{glacierName}/lv95/\"\n",
    "path_nc_wgs84 = f\"results/nc/{glacierName}/wgs84/\"\n",
    "\n",
    "filename_tif = f\"{glacierName}_{year}.tif\"\n",
    "path_tif_wgs84 = f\"results/tif/{glacierName}/wgs84/\"\n",
    "path_tif_lv95 = f\"results/tif/{glacierName}/lv95/\"\n",
    "\n",
    "createPath(path_tif_lv95)\n",
    "createPath(path_tif_wgs84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf, gdf_class, raster_data, extent = TransformToRaster(\n",
    "    filename_nc, filename_tif, path_nc_wgs84, path_tif_wgs84, path_tif_lv95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"000378bd-b0f0-46e2-a46d-f2165b0c6c02\"\n",
    "provider = cx.providers.Stadia.StamenTerrain(api_key=API_KEY)\n",
    "provider[\"url\"] = provider[\"url\"] + f\"?api_key={API_KEY}\"\n",
    "\n",
    "vmin, vmax = gdf.data.min(), gdf.data.max()\n",
    "norm = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "gdf_clean = gdf.dropna(subset=[\"data\"])\n",
    "gdf_clean.plot(\n",
    "    column=\"data\",  # Column to visualize\n",
    "    cmap=\"RdBu\",  # Color map suitable for glacier data\n",
    "    norm=norm,\n",
    "    legend=True,  # Display a legend\n",
    "    ax=axs[0],\n",
    "    markersize=5,  # Adjust size if points are too small or large\n",
    "    missing_kwds={\"color\": \"lightgrey\"}  # Define color for NaN values\n",
    ")\n",
    "cx.add_basemap(axs[0], crs=gdf.crs, source=provider)\n",
    "\n",
    "gdf_clean = gdf_class.dropna(subset=[\"data\"])\n",
    "gdf_clean.plot(\n",
    "    column=\"data\",  # Column to visualize\n",
    "    legend=True,  # Display a legend\n",
    "    ax=axs[1],\n",
    "    markersize=5,  # Adjust size if points are too small or large\n",
    "    missing_kwds={\"color\": \"lightgrey\"}  # Define color for NaN values\n",
    ")\n",
    "cx.add_basemap(axs[1], crs=gdf.crs, source=provider)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin, vmax = gdf.data.min(), gdf.data.max()\n",
    "\n",
    "# Calculate the relative position of 0\n",
    "relative_position = (0 - vmin) / (vmax - vmin) * 100\n",
    "\n",
    "# Print the result\n",
    "print(f\"The relative position of 0 is {relative_position:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "for year in years:\n",
    "    print(year)\n",
    "    for month in month_abbr_hydr_full_hydr:\n",
    "        monthNb = month_abbr_hydr_full_hydr[month]\n",
    "\n",
    "        filename_nc = f\"{glacierName}_{year}_{monthNb}.nc\"\n",
    "        path_nc_lv95 = f\"results/nc/{glacierName}/lv95/\"\n",
    "        path_nc_wgs84 = f\"results/nc/{glacierName}/wgs84/\"\n",
    "\n",
    "        filename_tif = f\"{glacierName}_{year}_{monthNb}.tif\"\n",
    "        path_tif_wgs84 = f\"results/tif/{glacierName}/wgs84/\"\n",
    "        path_tif_lv95 = f\"results/tif/{glacierName}/lv95/\"  # normally EPSG Code: 2056\n",
    "\n",
    "        gdf, gdf_class, raster_data, extent = TransformToRaster(\n",
    "            filename_nc, filename_tif, path_nc_wgs84, path_tif_wgs84,\n",
    "            path_tif_lv95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"000378bd-b0f0-46e2-a46d-f2165b0c6c02\"\n",
    "provider = cx.providers.Stadia.StamenTerrain(api_key=API_KEY)\n",
    "provider[\"url\"] = provider[\"url\"] + f\"?api_key={API_KEY}\"\n",
    "\n",
    "vmin, vmax = gdf.data.min(), gdf.data.max()\n",
    "norm = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(5, 5))\n",
    "gdf_clean = gdf.dropna(subset=[\"data\"])\n",
    "gdf_clean.plot(\n",
    "    column=\"data\",  # Column to visualize\n",
    "    cmap=\"RdBu\",  # Color map suitable for glacier data\n",
    "    norm=norm,\n",
    "    legend=True,  # Display a legend\n",
    "    ax=axs[0],\n",
    "    markersize=5,  # Adjust size if points are too small or large\n",
    "    missing_kwds={\"color\": \"lightgrey\"}  # Define color for NaN values\n",
    ")\n",
    "cx.add_basemap(axs[0], crs=gdf.crs, source=provider)\n",
    "\n",
    "gdf_clean = gdf_class.dropna(subset=[\"data\"])\n",
    "gdf_clean.plot(\n",
    "    column=\"data\",  # Column to visualize\n",
    "    legend=True,  # Display a legend\n",
    "    ax=axs[1],\n",
    "    markersize=5,  # Adjust size if points are too small or large\n",
    "    missing_kwds={\"color\": \"lightgrey\"}  # Define color for NaN values\n",
    ")\n",
    "cx.add_basemap(axs[1], crs=gdf.crs, source=provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by step example of one file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Open the NetCDF file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_xy = xr.open_dataset(path_nc_lv95 + filename_nc)\n",
    "ds_latlon = xr.open_dataset(path_nc_wgs84 + filename_nc)\n",
    "\n",
    "# Smoothing\n",
    "ds_xy_g = GaussianFilter(ds_xy)\n",
    "ds_latlon_g = GaussianFilter(ds_latlon)\n",
    "\n",
    "# Show effet of Smoothing:\n",
    "vmin, vmax = np.min([\n",
    "    ds_xy.pred_masked.min().values,\n",
    "    ds_xy_g.pred_masked.min()\n",
    "]), np.max([ds_xy.pred_masked.max().values,\n",
    "            ds_xy_g.pred_masked.max()])\n",
    "max_abs_value = max(abs(vmin), abs(vmax))\n",
    "norm = mcolors.TwoSlopeNorm(vmin=-max_abs_value, vcenter=0, vmax=max_abs_value)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ds_xy.pred_masked.plot.imshow(cmap='RdBu', norm=norm, ax=axs[0])\n",
    "axs[0].set_title('Original')\n",
    "\n",
    "# Plot or analyze `smoothed_data` as needed\n",
    "ds_xy_g.pred_masked.plot.imshow(cmap='RdBu', norm=norm, ax=axs[1])\n",
    "axs[1].set_title('Gaussian Filter')\n",
    "\n",
    "# print min and max values\n",
    "print(ds_xy.pred_masked.min().values, ds_xy.pred_masked.max().values)\n",
    "print(ds_xy_g.pred_masked.min().values, ds_xy_g.pred_masked.max().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: transform to geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf, lon, lat = toGeoPandas(ds_latlon_g)\n",
    "\n",
    "# Reproject to LV95 (EPSG:2056) swiss coordinates\n",
    "# gdf_lv95 = gdf.to_crs(\"EPSG:2056\")\n",
    "\n",
    "API_KEY = \"000378bd-b0f0-46e2-a46d-f2165b0c6c02\"\n",
    "provider = cx.providers.Stadia.StamenTerrain(api_key=API_KEY)\n",
    "provider[\"url\"] = provider[\"url\"] + f\"?api_key={API_KEY}\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "gdf_clean = gdf.dropna(subset=[\"data\"])\n",
    "gdf_clean.plot(\n",
    "    column=\"data\",  # Column to visualize\n",
    "    cmap=\"Reds\",  # Color map suitable for glacier data\n",
    "    legend=True,  # Display a legend\n",
    "    ax=ax,\n",
    "    markersize=5,  # Adjust size if points are too small or large\n",
    "    missing_kwds={\"color\": \"lightgrey\"}  # Define color for NaN values\n",
    ")\n",
    "cx.add_basemap(ax, crs=gdf.crs, source=provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: transform to raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to raster from geopandas\n",
    "raster_data, extent = toRaster(gdf,\n",
    "                               lon,\n",
    "                               lat,\n",
    "                               file_name=path_tif_wgs84 + filename_tif)\n",
    "\n",
    "# reproject raster to Swiss coordinates (LV95)\n",
    "reproject_raster_to_lv95(path_tif_wgs84 + filename_tif,\n",
    "                         path_tif_lv95 + filename_tif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opt step 4: for clariden\n",
    "Need to merge two rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'clariden' in glacierName:\n",
    "    merge_rasters('results/tif/claridenL_2022_w_lv95.tif',\n",
    "                  'results/tif/claridenU_2022_w_lv95.tif',\n",
    "                  'results/tif/clariden_2022_w_lv95.tif')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
