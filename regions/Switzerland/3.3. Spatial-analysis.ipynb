{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import re\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import contextily as cx\n",
    "import matplotlib.colors as mcolors\n",
    "from shapely.geometry import box\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import massbalancemachine as mbm\n",
    "from cmcrameri import cm\n",
    "\n",
    "from scripts.geodata import *\n",
    "from scripts.geodata_plots import *\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.xgb_helpers import *\n",
    "#  Suppress warnings issued by Cartopy when downloading data files\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "cmap = cm.devon\n",
    "color_palette_glaciers = sns.color_palette(get_cmap_hex(cmap, 15))\n",
    "\n",
    "colors = get_cmap_hex(cm.batlow, 2)\n",
    "color_xgb = colors[0]\n",
    "color_tim = '#c51b7d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vois_climate = [\n",
    "    't2m_corr', 'tp_corr', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]\n",
    "# Feature columns:\n",
    "feature_columns = [\n",
    "    'ELEVATION_DIFFERENCE'\n",
    "] + list(vois_climate) + list(vois_topographical) + ['pcsr']\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "path_rgi = '../../../data/GLAMOS/CH_glacier_ids_long.csv'\n",
    "rgi_df = pd.read_csv(path_rgi, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "rgi_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile\n",
    "shapefile_path = \"../../../data/GLAMOS/topo/SGI2020/SGI_2016_glaciers_copy.shp\"\n",
    "gdf_shapefiles = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Order glaciers by size:\n",
    "gl_area = {}\n",
    "for glacierName in rgi_df.index:\n",
    "    if glacierName == 'clariden':\n",
    "        sgi_id = rgi_df.loc['claridenL']['sgi-id'].strip()\n",
    "        rgi_shp = rgi_df.loc['claridenL']['rgi_id_v6_2016_shp']\n",
    "    else:\n",
    "        sgi_id = rgi_df.loc[glacierName]['sgi-id'].strip()\n",
    "        rgi_shp = rgi_df.loc[glacierName]['rgi_id_v6_2016_shp']\n",
    "\n",
    "    # 2016 shapefile of glacier\n",
    "    gdf_mask_gl = gdf_shapefiles[gdf_shapefiles.RGIId == rgi_shp]\n",
    "    gl_area[glacierName] = gdf_mask_gl.Area.values[0]\n",
    "\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "# sort by area (smallest first)\n",
    "glacier_list = sorted(gl_area, key=lambda x: gl_area[x], reverse=False)\n",
    "glacier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PREDICTIONS = 'results/nc/glamos/OGGM_vars/TP_corr_T2m_corr/'\n",
    "\n",
    "# PATH_PREDICTIONS = 'results/nc/glamos/Simple_vars/TP_corr_T2m_corr/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_glacier_grid_glamos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glaciers covered by GLAMOS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read geodetic MB:\n",
    "geodeticMB = pd.read_csv(path_geodetic_MB_glamos + 'dV_DOI2024_allcomb.csv')\n",
    "\n",
    "# get rgi of those glaciers:\n",
    "rgi_gl = []\n",
    "for gl in os.listdir(PATH_PREDICTIONS):\n",
    "    if gl == 'clariden':\n",
    "        rgi_gl.append(rgi_df.loc['claridenU']['rgi_id_v6_2016_shp'])\n",
    "    else:\n",
    "        rgi_gl.append(rgi_df.loc[gl]['rgi_id_v6_2016_shp'])\n",
    "\n",
    "sgi_gl = [\n",
    "    rgi_df[rgi_df['rgi_id_v6_2016_shp'] == rgi]['sgi-id'].values[0]\n",
    "    for rgi in rgi_gl\n",
    "]\n",
    "geodeticMB = geodeticMB[geodeticMB['SGI-ID'].isin(sgi_gl)]\n",
    "\n",
    "# Add glacierName to geodeticMB\n",
    "# based  on SGI-ID\n",
    "glacierNames = [\n",
    "    rgi_df[rgi_df['sgi-id'] == sgi_id].index[0]\n",
    "    for sgi_id in geodeticMB['SGI-ID'].values\n",
    "]\n",
    "# replace claridenL by clariden\n",
    "glacierNames = [\n",
    "    glacierName.replace('claridenL', 'clariden')\n",
    "    for glacierName in glacierNames\n",
    "]\n",
    "geodeticMB['glacierName'] = glacierNames\n",
    "\n",
    "periods_per_glacier = defaultdict(list)\n",
    "geoMB_per_glacier = defaultdict(list)\n",
    "# Iterate through the DataFrame rows\n",
    "for _, row in geodeticMB.iterrows():\n",
    "    glacierName = row['glacierName']\n",
    "    start_year = row['Astart']\n",
    "    end_year = row['A_end']\n",
    "    geoMB = row['Bgeod']\n",
    "\n",
    "    # Append the (start, end) tuple to the glacier's list\n",
    "    periods_per_glacier[glacierName].append((start_year, end_year))\n",
    "    geoMB_per_glacier[glacierName].append(geoMB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the files in both directories\n",
    "glaciers_in_distributed_glamos = os.listdir(path_distributed_MB_glamos)\n",
    "glaciers_in_geodetic = periods_per_glacier.keys()\n",
    "glaciers_in_glamos_corr = os.listdir(PATH_PREDICTIONS)\n",
    "\n",
    "# Check if all glaciers in the first directory are in the second directory\n",
    "missing_glaciers = [\n",
    "    glacier for glacier in glaciers_in_distributed_glamos\n",
    "    if glacier not in glaciers_in_glamos_corr\n",
    "]\n",
    "missing_glaciers_geod = [\n",
    "    glacier for glacier in glaciers_in_geodetic\n",
    "    if glacier not in glaciers_in_glamos_corr\n",
    "]\n",
    "\n",
    "# Find the intersection of the two lists\n",
    "intersecting_glaciers = list(\n",
    "    set(missing_glaciers_geod) & set(missing_glaciers))\n",
    "\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "\n",
    "missing_glaciers_sorted_gridded = sort_by_area(missing_glaciers)\n",
    "missing_glaciers_geod_sorted = sort_by_area(missing_glaciers_geod)\n",
    "intersecting_glaciers_sorted = sort_by_area(intersecting_glaciers)\n",
    "\n",
    "print(\"The following glaciers are missing compared to gridded MB:\")\n",
    "print(missing_glaciers_sorted_gridded)\n",
    "\n",
    "print(\"The following glaciers are missing compared to the geodetic MB:\")\n",
    "print(missing_glaciers_geod_sorted)\n",
    "\n",
    "print(\"The following glaciers are missing in both lists:\")\n",
    "print(intersecting_glaciers_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersection with geodetic MB:\n",
    "glacier_list_geod = sort_by_area([\n",
    "    glacier for glacier in glaciers_in_geodetic\n",
    "    if glacier in glaciers_in_glamos_corr\n",
    "])\n",
    "glacier_list_geod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Geodetic MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GLAMOS_glwmb(glacier_name):\n",
    "    \"\"\"\n",
    "    Loads and processes GLAMOS glacier-wide mass balance data.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    glacier_name : str\n",
    "        The name of the glacier.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame or None\n",
    "        A DataFrame with columns ['YEAR', 'GLAMOS Balance'] indexed by 'YEAR',\n",
    "        or None if the file is missing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct file path safely\n",
    "    file_path = os.path.join(path_SMB_GLAMOS_csv, \"fix\",\n",
    "                             f\"{glacier_name}_fix.csv\")\n",
    "\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\n",
    "            f\"Warning: GLAMOS data file not found for {glacier_name}. Skipping...\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    # Load CSV and transform dates\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = transformDates(df)\n",
    "\n",
    "    # Remove duplicates based on the date column\n",
    "    df = df.drop_duplicates(subset=[\"date1\"])\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    required_columns = {\"date1\", \"Annual Balance\"}\n",
    "    if not required_columns.issubset(df.columns):\n",
    "        print(\n",
    "            f\"Warning: Missing required columns in {glacier_name} GLAMOS data. Skipping...\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    # Extract year from date and normalize balance\n",
    "    df[\"YEAR\"] = pd.to_datetime(df[\"date1\"]).dt.year.astype(\"int64\")\n",
    "    df[\"GLAMOS Balance\"] = df[\n",
    "        \"Annual Balance\"] / 1000  # Convert to meters water equivalent\n",
    "\n",
    "    # Select relevant columns and set index\n",
    "    return df[[\"YEAR\", \"GLAMOS Balance\"]].set_index(\"YEAR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_name = \"clariden\"\n",
    "GLAMOS_glwmb = get_GLAMOS_glwmb(glacier_name)\n",
    "\n",
    "# If GLAMOS data is missing, skip processing\n",
    "if GLAMOS_glwmb is None:\n",
    "    print(f\"Skipping {glacier_name}: No GLAMOS data available.\")\n",
    "else:\n",
    "    # Get all geodetic periods\n",
    "    periods = periods_per_glacier.get(glacier_name, [])\n",
    "    geoMBs = geoMB_per_glacier.get(glacier_name, [])\n",
    "\n",
    "    path_mbm_pred = os.path.join(PATH_PREDICTIONS, glacier_name)\n",
    "\n",
    "    # Storage lists\n",
    "    mbm_mb_mean, glamos_mb_mean, geodetic_mb, target_period = [], [], [], []\n",
    "\n",
    "    for period in periods:\n",
    "        mbm_mb, glamos_mb = [], []\n",
    "\n",
    "        for year in range(period[0], period[1] + 1):\n",
    "            # Construct file path\n",
    "            file_path = os.path.join(path_mbm_pred,\n",
    "                                     f\"{glacier_name}_{year}_annual.nc\")\n",
    "\n",
    "            # Check if the NetCDF file exists\n",
    "            if not os.path.exists(file_path):\n",
    "                print(\n",
    "                    f\"Warning: Missing MBM file for {glacier_name} ({year}). Skipping...\"\n",
    "                )\n",
    "                mbm_mb.append(np.nan)\n",
    "            else:\n",
    "                ds = xr.open_dataset(file_path)\n",
    "                # Compute mean glacier-wide MB\n",
    "                mbm_mb.append(ds[\"pred_masked\"].mean().values)\n",
    "\n",
    "            # Get GLAMOS Balance for the year, or return NaN if missing\n",
    "            glamos_mb.append(GLAMOS_glwmb[\"GLAMOS Balance\"].get(year, np.nan))\n",
    "\n",
    "        # Store mean values, ignoring NaNs\n",
    "        mbm_mb_mean.append(np.nanmean(mbm_mb))\n",
    "        glamos_mb_mean.append(np.nanmean(glamos_mb))\n",
    "        geodetic_mb.append(geoMBs[periods.index(period)])\n",
    "        target_period.append(period)\n",
    "\n",
    "fig = plotGeodeticMB(geodetic_mb, mbm_mb_mean, glamos_mb_mean, glacier_name,\n",
    "                     color_xgb, color_tim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_years(folder_path, glacier_name, period):\n",
    "    start_year, end_year = period\n",
    "    expected_years = set(range(start_year, end_year + 1))\n",
    "\n",
    "    # Extract years from filenames\n",
    "    available_years = set()\n",
    "    pattern = re.compile(rf'{glacier_name}_(\\d{{4}})_annual\\.nc')\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            year = int(match.group(1))\n",
    "            available_years.add(year)\n",
    "\n",
    "    missing_years = expected_years - available_years\n",
    "    if missing_years:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path for saving figures\n",
    "path_figure = \"figures/geodetic/\"\n",
    "emptyfolder(path_figure)  # Ensure directory is empty before saving\n",
    "\n",
    "for glacier_name in glacier_list_geod:\n",
    "\n",
    "    # Check if GLAMOS file exists\n",
    "    glamos_file = os.path.join(path_SMB_GLAMOS_csv, \"fix\",\n",
    "                               f\"{glacier_name}_fix.csv\")\n",
    "    if not os.path.exists(glamos_file):\n",
    "        print(f\"Skipping {glacier_name}: No GLAMOS data available.\")\n",
    "        continue\n",
    "\n",
    "    # Load GLAMOS data\n",
    "    GLAMOS_glwmb = get_GLAMOS_glwmb(glacier_name)\n",
    "    if GLAMOS_glwmb is None:\n",
    "        print(f\"Skipping {glacier_name}: Failed to load GLAMOS data.\")\n",
    "        continue\n",
    "\n",
    "    # Get all geodetic periods\n",
    "    periods = periods_per_glacier.get(glacier_name, [])\n",
    "    geoMBs = geoMB_per_glacier.get(glacier_name, [])\n",
    "\n",
    "    if not periods or not geoMBs:\n",
    "        print(\n",
    "            f\"Skipping {glacier_name}: No geodetic mass balance data available.\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    # Open glacier-wide MB nc files for all years of the period\n",
    "    folder_path = os.path.join(PATH_PREDICTIONS, glacier_name)\n",
    "\n",
    "    mbm_mb_mean, glamos_mb_mean, geodetic_mb, target_period = [], [], [], []\n",
    "\n",
    "    for period in periods:\n",
    "        # Skip if any required year is missing in the dataset\n",
    "        if check_missing_years(folder_path, glacier_name, period):\n",
    "            print(\n",
    "                f\"Skipping period {period} for {glacier_name} due to missing files.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        mbm_mb, glamos_mb = [], []\n",
    "\n",
    "        for year in range(period[0], period[1] + 1):\n",
    "            file_path = os.path.join(folder_path,\n",
    "                                     f\"{glacier_name}_{year}_annual.nc\")\n",
    "\n",
    "            # Check if the NetCDF file exists before loading\n",
    "            if not os.path.exists(file_path):\n",
    "                print(\n",
    "                    f\"Warning: Missing MBM file for {glacier_name} ({year}).\")\n",
    "                mbm_mb.append(np.nan)\n",
    "            else:\n",
    "                ds = xr.open_dataset(file_path)\n",
    "                mbm_mb.append(ds[\"pred_masked\"].mean().values)\n",
    "\n",
    "            # Get GLAMOS Balance for the year, or return NaN if missing\n",
    "            glamos_mb.append(GLAMOS_glwmb[\"GLAMOS Balance\"].get(year, np.nan))\n",
    "\n",
    "        # Store mean values, ignoring NaNs\n",
    "        mbm_mb_mean.append(np.nanmean(mbm_mb))\n",
    "        glamos_mb_mean.append(np.nanmean(glamos_mb))\n",
    "        geodetic_mb.append(geoMBs[periods.index(period)])\n",
    "        target_period.append(period)\n",
    "\n",
    "    # Only plot if more than one valid measurement exists\n",
    "    valid_measurements = np.isfinite(geodetic_mb) & np.isfinite(\n",
    "        mbm_mb_mean) & np.isfinite(glamos_mb_mean)\n",
    "\n",
    "    if np.sum(valid_measurements) > 1:\n",
    "        fig = plotGeodeticMB(geodetic_mb, mbm_mb_mean, glamos_mb_mean,\n",
    "                             glacier_name, color_xgb, color_tim)\n",
    "\n",
    "        # Ensure layout is adjusted before saving\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(os.path.join(path_figure, f\"{glacier_name}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            f\"Skipping {glacier_name}: Not enough valid measurements for plotting.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_glaciers = [\n",
    "    'tortin', 'plattalva', 'sanktanna', 'schwarzberg', 'hohlaub', 'pizol',\n",
    "    'corvatsch', 'tsanfleuron', 'forno'\n",
    "]\n",
    "\n",
    "# Storage lists for results\n",
    "mbm_mb_mean, glamos_mb_mean, geodetic_mb = [], [], []\n",
    "gl, period_len, gl_type, area = [], [], [], []\n",
    "\n",
    "for glacier_name in tqdm(glacier_list_geod, desc=\"Processing glaciers\"):\n",
    "\n",
    "    # Check if GLAMOS file exists before proceeding\n",
    "    glamos_file = os.path.join(path_SMB_GLAMOS_csv, \"fix\",\n",
    "                               f\"{glacier_name}_fix.csv\")\n",
    "    if not os.path.exists(glamos_file):\n",
    "        continue  # Skip glacier if no GLAMOS data available\n",
    "\n",
    "    # Load GLAMOS data\n",
    "    GLAMOS_glwmb = get_GLAMOS_glwmb(glacier_name)\n",
    "    if GLAMOS_glwmb is None:\n",
    "        print(f\"Skipping {glacier_name}: Failed to load GLAMOS data.\")\n",
    "        continue\n",
    "\n",
    "    # Get glacier-specific periods and geodetic MBs\n",
    "    periods = periods_per_glacier.get(glacier_name, [])\n",
    "    geoMBs = geoMB_per_glacier.get(glacier_name, [])\n",
    "\n",
    "    if not periods or not geoMBs:\n",
    "        print(\n",
    "            f\"Skipping {glacier_name}: No geodetic mass balance data available.\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    # Path to model predictions\n",
    "    folder_path = os.path.join(PATH_PREDICTIONS, glacier_name)\n",
    "\n",
    "    for period in periods:\n",
    "        mbm_mb, glamos_mb = [], []\n",
    "\n",
    "        # Check if any required year is missing in the dataset\n",
    "        if check_missing_years(folder_path, glacier_name, period):\n",
    "            continue\n",
    "\n",
    "        for year in range(period[0], period[1] + 1):\n",
    "            file_path = os.path.join(folder_path,\n",
    "                                     f\"{glacier_name}_{year}_annual.nc\")\n",
    "\n",
    "            # Check if the NetCDF file exists before opening\n",
    "            if not os.path.exists(file_path):\n",
    "                print(\n",
    "                    f\"Warning: Missing MBM file for {glacier_name} ({year}).\")\n",
    "                mbm_mb.append(np.nan)\n",
    "            else:\n",
    "                ds = xr.open_dataset(file_path)\n",
    "                mbm_mb.append(ds[\"pred_masked\"].mean().values)\n",
    "\n",
    "            # Get GLAMOS Balance for the year, handling missing years\n",
    "            glamos_mb.append(GLAMOS_glwmb[\"GLAMOS Balance\"].get(year, np.nan))\n",
    "\n",
    "        # Store mean values, ignoring NaNs\n",
    "        mbm_mb_mean.append(np.nanmean(mbm_mb))\n",
    "        glamos_mb_mean.append(np.nanmean(glamos_mb))\n",
    "        geodetic_mb.append(geoMBs[periods.index(period)])\n",
    "        gl.append(glacier_name)\n",
    "        gl_type.append(glacier_name in test_glaciers)\n",
    "        period_len.append(period[1] - period[0])\n",
    "        area.append(gl_area.get(glacier_name, np.nan))  # Handle missing areas\n",
    "\n",
    "# Create DataFrame\n",
    "df_all = pd.DataFrame({\n",
    "    \"MBM MB\": mbm_mb_mean,\n",
    "    \"GLAMOS MB\": glamos_mb_mean,\n",
    "    \"Geodetic MB\": geodetic_mb,\n",
    "    \"GLACIER\": gl,\n",
    "    \"Period Length\": period_len,\n",
    "    \"Test Glacier\": gl_type,\n",
    "    \"Area\": area  # in kmÂ²\n",
    "})\n",
    "\n",
    "# Sort by area for better visualization\n",
    "df_all.sort_values(by=\"Area\", inplace=True, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All glaciers\n",
    "scatter_geodeticMB(df_all, hue='GLACIER', size=True)\n",
    "\n",
    "# Plot scatterplot but size is  proportional to glacier area\n",
    "scatter_geodeticMB(df_all, hue='Test Glacier', size=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only longest period per glacier\n",
    "df_longest = df_all.loc[df_all.groupby('GLACIER')['Period Length'].idxmax()]\n",
    "df_longest.sort_values(by='Area', inplace=True)\n",
    "scatter_geodeticMB(df_longest, size=True)\n",
    "plt.suptitle('Only longest period per glacier')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glacier wide MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define glacier name\n",
    "glacier_name = \"silvretta\"\n",
    "\n",
    "# Load GLAMOS data\n",
    "GLAMOS_glwmb = get_GLAMOS_glwmb(glacier_name)\n",
    "\n",
    "# Check if GLAMOS data is available\n",
    "if GLAMOS_glwmb is None:\n",
    "    print(f\"Skipping {glacier_name}: No GLAMOS data available.\")\n",
    "else:\n",
    "    # Define the path to model predictions\n",
    "    path_results = os.path.join(PATH_PREDICTIONS, glacier_name)\n",
    "\n",
    "    # Extract available years from NetCDF filenames\n",
    "    years = sorted([\n",
    "        int(f.split(\"_\")[1]) for f in os.listdir(path_results)\n",
    "        if f.endswith(\"_annual.nc\")\n",
    "    ])\n",
    "\n",
    "    if not years:\n",
    "        print(f\"Skipping {glacier_name}: No model prediction files found.\")\n",
    "    else:\n",
    "        # Extract model-predicted mass balance\n",
    "        pred_gl = []\n",
    "        for year in years:\n",
    "            file_path = os.path.join(path_results,\n",
    "                                     f\"{glacier_name}_{year}_annual.nc\")\n",
    "            if not os.path.exists(file_path):\n",
    "                print(\n",
    "                    f\"Warning: Missing MBM file for {glacier_name} ({year}). Skipping...\"\n",
    "                )\n",
    "                pred_gl.append(np.nan)\n",
    "                continue\n",
    "\n",
    "            ds = xr.open_dataset(file_path)\n",
    "            pred_gl.append(ds.pred_masked.mean().item())\n",
    "\n",
    "        # Create DataFrame\n",
    "        MBM_glwmb = pd.DataFrame(pred_gl, index=years, columns=[\"MBM Balance\"])\n",
    "\n",
    "        # Merge with GLAMOS data\n",
    "        MBM_glwmb = MBM_glwmb.join(GLAMOS_glwmb)\n",
    "\n",
    "        # Drop NaN values to avoid plotting errors\n",
    "        MBM_glwmb = MBM_glwmb.dropna()\n",
    "\n",
    "        # Ensure data exists before plotting\n",
    "        if MBM_glwmb.empty:\n",
    "            print(f\"Skipping {glacier_name}: No valid data to plot.\")\n",
    "        else:\n",
    "            # Plot the data\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            MBM_glwmb.plot(ax=ax, marker=\"o\", color=[color_xgb, color_tim])\n",
    "            ax.set_title(f\"{glacier_name.capitalize()} Glacier\", fontsize=24)\n",
    "            ax.set_ylabel(\"Mass Balance [m w.e.]\", fontsize=18)\n",
    "            ax.set_xlabel(\"Year\", fontsize=18)\n",
    "            ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "            \n",
    "            # increase font size of legends\n",
    "            ax.legend(fontsize=14)\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output folder\n",
    "output_folder = \"figures/glacierwide/\"\n",
    "emptyfolder(output_folder)  # Clear existing figures\n",
    "\n",
    "# Process each glacier\n",
    "for glacier_name in tqdm(glacier_list_geod):\n",
    "    # Load GLAMOS data\n",
    "    GLAMOS_glwmb = get_GLAMOS_glwmb(glacier_name)\n",
    "\n",
    "    # Check if GLAMOS data is available\n",
    "    if GLAMOS_glwmb is None:\n",
    "        print(f\"Skipping {glacier_name}: No GLAMOS data available.\")\n",
    "        continue\n",
    "\n",
    "    # Define the path to model predictions\n",
    "    glacier_path = os.path.join(PATH_PREDICTIONS, glacier_name)\n",
    "\n",
    "    # Check if glacier prediction directory exists\n",
    "    if not os.path.exists(glacier_path):\n",
    "        print(f\"Skipping {glacier_name}: No prediction data available.\")\n",
    "        continue\n",
    "\n",
    "    # Extract available years from NetCDF filenames\n",
    "    try:\n",
    "        years = sorted([\n",
    "            int(f.split(\"_\")[1]) for f in os.listdir(glacier_path)\n",
    "            if f.endswith(\"_annual.nc\") and \"_\" in f\n",
    "        ])\n",
    "    except ValueError:\n",
    "        print(\n",
    "            f\"Skipping {glacier_name}: Failed to extract years from filenames.\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    if not years:\n",
    "        print(f\"Skipping {glacier_name}: No model prediction files found.\")\n",
    "        continue\n",
    "\n",
    "    # Extract model-predicted mass balance\n",
    "    pred_gl = []\n",
    "    for year in years:\n",
    "        file_path = os.path.join(glacier_path,\n",
    "                                 f\"{glacier_name}_{year}_annual.nc\")\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            print(\n",
    "                f\"Warning: Missing MBM file for {glacier_name} ({year}). Skipping...\"\n",
    "            )\n",
    "            pred_gl.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        ds = xr.open_dataset(file_path)\n",
    "        pred_gl.append(ds.pred_masked.mean().item())\n",
    "\n",
    "    # Create DataFrame\n",
    "    glwide_MB = pd.DataFrame(pred_gl, index=years, columns=[\"MBM Balance\"])\n",
    "\n",
    "    # Merge with GLAMOS data\n",
    "    glwide_MB = glwide_MB.join(GLAMOS_glwmb)\n",
    "\n",
    "    # Drop NaN values to avoid plotting errors\n",
    "    glwide_MB = glwide_MB.dropna()\n",
    "\n",
    "    # Ensure data exists before plotting\n",
    "    if glwide_MB.empty:\n",
    "        print(f\"Skipping {glacier_name}: No valid data to plot.\")\n",
    "        continue\n",
    "\n",
    "    # Plot the data\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    glwide_MB.plot(ax=ax, marker=\"o\", color=[color_xgb, color_tim])\n",
    "    ax.set_title(f\"{glacier_name.capitalize()} Glacier\", fontsize=24)\n",
    "    ax.set_ylabel(\"Mass Balance [m w.e.]\", fontsize=18)\n",
    "    ax.set_xlabel(\"Year\", fontsize=18)\n",
    "    ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "    \n",
    "    # increase font size of legends\n",
    "    ax.legend(fontsize=14)\n",
    "\n",
    "    # Save the figure\n",
    "    output_path = os.path.join(output_folder, f\"{glacier_name}.png\")\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    # Clear the plot to free memory\n",
    "    plt.close(fig)\n",
    "\n",
    "print(\"Processing complete. Figures saved in:\", output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed GLAMOS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stake data ONCE instead of for every glacier\n",
    "stake_file = os.path.join(path_PMB_GLAMOS_csv, \"CH_wgms_dataset_all.csv\")\n",
    "if not os.path.exists(stake_file):\n",
    "    print(\"Error: Stake data file not found. Exiting.\")\n",
    "else:\n",
    "    df_stakes = pd.read_csv(stake_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gaussian_filter(ds, variable_name='pred_masked', sigma: float = 1):\n",
    "    # Get the DataArray for the specified variable\n",
    "    data_array = ds[variable_name]\n",
    "\n",
    "    # Step 1: Create a mask of valid data (non-NaN values)\n",
    "    mask = ~np.isnan(data_array)\n",
    "\n",
    "    # Step 2: Replace NaNs with zero (or a suitable neutral value)\n",
    "    filled_data = data_array.fillna(0)\n",
    "\n",
    "    # Step 3: Apply Gaussian filter to the filled data\n",
    "    smoothed_data = gaussian_filter(filled_data, sigma=sigma)\n",
    "\n",
    "    # Step 4: Restore NaNs to their original locations\n",
    "    smoothed_data = xr.DataArray(smoothed_data,\n",
    "                                 dims=data_array.dims,\n",
    "                                 coords=data_array.coords,\n",
    "                                 attrs=data_array.attrs).where(\n",
    "                                     mask)  # Apply the mask to restore NaNs\n",
    "    ds[variable_name] = smoothed_data\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define glacier name\n",
    "# glacier_name = \"aletsch\"\n",
    "\n",
    "# # Get available years from filenames\n",
    "# years = sorted(\n",
    "#     map(int, [\n",
    "#         f.split(\"_\")[1]\n",
    "#         for f in os.listdir(os.path.join(PATH_PREDICTIONS, glacier_name))\n",
    "#         if \"_\" in f and f.endswith(\"_annual.nc\")\n",
    "#     ]))\n",
    "\n",
    "# # if not exist create path:\n",
    "# if not os.path.exists(f\"figures/dst_mb/{glacier_name}\"):\n",
    "#     os.makedirs(f\"figures/dst_mb/{glacier_name}\")\n",
    "# # else empty:\n",
    "# else:\n",
    "#     emptyfolder(f\"figures/dst_mb/{glacier_name}\")\n",
    "\n",
    "# # Iterate through each year\n",
    "# for year in tqdm(years):\n",
    "#     print(f\"Processing: {glacier_name}, Year: {year}\")\n",
    "\n",
    "#     # Construct file path for GLAMOS distributed MB\n",
    "#     file_name = f\"{year}_ann_fix_lv95.grid\"\n",
    "#     grid_file_path = os.path.join(path_distributed_MB_glamos, glacier_name,\n",
    "#                                   file_name)\n",
    "\n",
    "#     # if path exists\n",
    "#     if not os.path.exists(grid_file_path):\n",
    "#         print(f\"Skipping {year}: GLAMOS file missing for {glacier_name}.\")\n",
    "#         continue\n",
    "\n",
    "#     # Load GLAMOS data\n",
    "#     metadata, grid_data = load_grid_file(grid_file_path)\n",
    "\n",
    "#     # Convert to xarray and transform to WGS84\n",
    "#     ds_glamos = convert_to_xarray_geodata(grid_data, metadata)\n",
    "#     ds_glamos_wgs84 = transform_xarray_coords_lv95_to_wgs84(ds_glamos)\n",
    "\n",
    "#     # Load MBM predictions\n",
    "#     mbm_file_path = os.path.join(PATH_PREDICTIONS, glacier_name,\n",
    "#                                  f\"{glacier_name}_{year}_annual.nc\")\n",
    "\n",
    "#     if not os.path.exists(mbm_file_path):\n",
    "#         print(f\"Skipping {year}: MBM file missing for {glacier_name}.\")\n",
    "#         continue\n",
    "\n",
    "#     ds_mbm = xr.open_dataset(mbm_file_path)\n",
    "#     ds_mbm = apply_gaussian_filter(ds_mbm)\n",
    "\n",
    "#     # Compute color scale limits\n",
    "#     vmin = min(ds_glamos_wgs84.min().item(), ds_mbm.pred_masked.min().item())\n",
    "#     vmax = max(ds_glamos_wgs84.max().item(), ds_mbm.pred_masked.max().item())\n",
    "#     norm = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)\n",
    "\n",
    "#     print(f\"Color scale range: vmin={vmin:.3f}, vmax={vmax:.3f}\")\n",
    "\n",
    "#     # Plot the data\n",
    "#     fig, axes = plt.subplots(1, 2, figsize=(10, 5), sharex=True, sharey=True)\n",
    "\n",
    "#     # GLAMOS Plot\n",
    "#     ds_glamos_wgs84.plot.imshow(ax=axes[0],\n",
    "#                                 cmap=\"coolwarm_r\",\n",
    "#                                 norm=norm,\n",
    "#                                 cbar_kwargs={\"label\": \"Mass Balance [m w.e.]\"})\n",
    "#     axes[0].set_title(\"GLAMOS\")\n",
    "\n",
    "#     # MBM Predictions Plot\n",
    "#     ds_mbm.pred_masked.plot(ax=axes[1],\n",
    "#                             cmap=\"coolwarm_r\",\n",
    "#                             norm=norm,\n",
    "#                             cbar_kwargs={\"label\": \"Mass Balance [m w.e.]\"})\n",
    "#     axes[1].set_title(\"MBM\")\n",
    "\n",
    "#     # Set figure title\n",
    "#     plt.suptitle(f\"{glacier_name.capitalize()}: Summer {year}\")\n",
    "\n",
    "#     # Remove grid\n",
    "#     for ax in axes:\n",
    "#         ax.grid(False)\n",
    "\n",
    "#     # Adjust layout and show plot\n",
    "#     plt.tight_layout()\n",
    "\n",
    "#     # Save the figure\n",
    "#     output_path = os.path.join(f\"figures/dst_mb/{glacier_name}\",\n",
    "#                                f\"{glacier_name}_{year}.png\")\n",
    "#     plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate against stake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mass_balance(glacier_name, year, df_stakes,\n",
    "                      path_distributed_MB_glamos, PATH_PREDICTIONS):\n",
    "    \"\"\"\n",
    "    Plots the annual and winter mass balance for a given glacier and year,\n",
    "    comparing GLAMOS distributed MB with model predictions.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    glacier_name : str\n",
    "        Name of the glacier.\n",
    "    year : int\n",
    "        Year for which the mass balance should be plotted.\n",
    "    df_stakes : pd.DataFrame\n",
    "        DataFrame containing all stake measurements.\n",
    "    path_distributed_MB_glamos : str\n",
    "        Path to the GLAMOS distributed mass balance files.\n",
    "    PATH_PREDICTIONS : str\n",
    "        Path to the MBM model predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Processing: {glacier_name}, Year: {year}\")\n",
    "\n",
    "    # Extract stake data for this glacier and year\n",
    "    stakes_data = df_stakes[(df_stakes.GLACIER == glacier_name)\n",
    "                            & (df_stakes.YEAR == year)]\n",
    "    stakes_data_ann = stakes_data[stakes_data.PERIOD == \"annual\"].copy()\n",
    "    stakes_data_win = stakes_data[stakes_data.PERIOD == \"winter\"].copy()\n",
    "\n",
    "    # Construct file paths\n",
    "    file_ann = f\"{year}_ann_fix_lv95.grid\"\n",
    "    file_win = f\"{year}_win_fix_lv95.grid\"\n",
    "    grid_path_ann = os.path.join(path_distributed_MB_glamos, glacier_name,\n",
    "                                 file_ann)\n",
    "    grid_path_win = os.path.join(path_distributed_MB_glamos, glacier_name,\n",
    "                                 file_win)\n",
    "\n",
    "    # Load GLAMOS data (Annual)\n",
    "    # if exists\n",
    "    if not os.path.exists(grid_path_ann):\n",
    "        print(f\"Skipping {year}: GLAMOS file missing for {glacier_name}.\")\n",
    "        return None\n",
    "    metadata_ann, grid_data_ann = load_grid_file(grid_path_ann)\n",
    "    ds_glamos_ann = convert_to_xarray_geodata(grid_data_ann, metadata_ann)\n",
    "    ds_glamos_wgs84_ann = transform_xarray_coords_lv95_to_wgs84(ds_glamos_ann)\n",
    "\n",
    "    # Load GLAMOS data (Winter)\n",
    "    try:\n",
    "        metadata_win, grid_data_win = load_grid_file(grid_path_win)\n",
    "        ds_glamos_win = convert_to_xarray_geodata(grid_data_win, metadata_win)\n",
    "        ds_glamos_wgs84_win = transform_xarray_coords_lv95_to_wgs84(\n",
    "            ds_glamos_win)\n",
    "    except FileNotFoundError:\n",
    "        print(\n",
    "            f\"Warning: Winter grid data missing for {glacier_name} ({year}).\")\n",
    "        ds_glamos_wgs84_win = None\n",
    "\n",
    "    # Load MBM predictions (Annual)\n",
    "    mbm_file_ann = os.path.join(PATH_PREDICTIONS, glacier_name,\n",
    "                                f\"{glacier_name}_{year}_annual.nc\")\n",
    "    ds_mbm_ann = xr.open_dataset(mbm_file_ann)\n",
    "    ds_mbm_ann = apply_gaussian_filter(ds_mbm_ann)\n",
    "\n",
    "    # Load MBM predictions (Winter)\n",
    "    mbm_file_win = os.path.join(PATH_PREDICTIONS, glacier_name,\n",
    "                                f\"{glacier_name}_{year}_winter.nc\")\n",
    "    ds_mbm_win = xr.open_dataset(mbm_file_win)\n",
    "    ds_mbm_win = apply_gaussian_filter(ds_mbm_win)\n",
    "\n",
    "    # Ensure correct coordinate names\n",
    "    lon_name = \"lon\" if \"lon\" in ds_mbm_ann.coords else \"longitude\"\n",
    "    lat_name = \"lat\" if \"lat\" in ds_mbm_ann.coords else \"latitude\"\n",
    "\n",
    "    # Function to extract mass balance for each stake\n",
    "    def get_predicted_mb(row, ds):\n",
    "        try:\n",
    "            return ds.sel(\n",
    "                {\n",
    "                    lon_name: row.POINT_LON,\n",
    "                    lat_name: row.POINT_LAT\n",
    "                },\n",
    "                method=\"nearest\").pred_masked.item()  # Convert to scalar\n",
    "        except Exception:\n",
    "            print(\n",
    "                f\"Warning: Stake at ({row.POINT_LON}, {row.POINT_LAT}) is out of bounds.\"\n",
    "            )\n",
    "            return np.nan\n",
    "\n",
    "    def get_predicted_mb_glamos(row, ds):\n",
    "        try:\n",
    "            return ds.sel({\n",
    "                lon_name: row.POINT_LON,\n",
    "                lat_name: row.POINT_LAT\n",
    "            },\n",
    "                          method=\"nearest\").item()  # Convert to scalar\n",
    "        except Exception:\n",
    "            print(\n",
    "                f\"Warning: Stake at ({row.POINT_LON}, {row.POINT_LAT}) is out of bounds.\"\n",
    "            )\n",
    "            return np.nan\n",
    "\n",
    "    # Apply the function correctly using lambda\n",
    "    stakes_data_ann[\"Predicted_MB\"] = stakes_data_ann.apply(\n",
    "        lambda row: get_predicted_mb(row, ds_mbm_ann), axis=1)\n",
    "    stakes_data_ann.dropna(subset=[\"Predicted_MB\"], inplace=True)\n",
    "    stakes_data_ann[\"GLAMOS_MB\"] = stakes_data_ann.apply(\n",
    "        lambda row: get_predicted_mb_glamos(row, ds_glamos_wgs84_ann), axis=1)\n",
    "    stakes_data_ann.dropna(subset=[\"GLAMOS_MB\"], inplace=True)\n",
    "\n",
    "    # Same for winter\n",
    "    stakes_data_win[\"Predicted_MB\"] = stakes_data_win.apply(\n",
    "        lambda row: get_predicted_mb(row, ds_mbm_win), axis=1)\n",
    "    stakes_data_win.dropna(subset=[\"Predicted_MB\"], inplace=True)\n",
    "    stakes_data_win[\"GLAMOS_MB\"] = stakes_data_win.apply(\n",
    "        lambda row: get_predicted_mb_glamos(row, ds_glamos_wgs84_win), axis=1)\n",
    "    stakes_data_win.dropna(subset=[\"GLAMOS_MB\"], inplace=True)\n",
    "\n",
    "    # Compute color scale limits (Annual)\n",
    "    vmin_ann = min(ds_glamos_wgs84_ann.min().item(),\n",
    "                   ds_mbm_ann.pred_masked.min().item())\n",
    "    vmax_ann = max(ds_glamos_wgs84_ann.max().item(),\n",
    "                   ds_mbm_ann.pred_masked.max().item())\n",
    "\n",
    "    # Compute color scale limits (Winter)\n",
    "    vmin_win = min(ds_glamos_wgs84_win.min().item(),\n",
    "                   ds_mbm_win.pred_masked.min().item())\n",
    "    vmax_win = max(ds_glamos_wgs84_win.max().item(),\n",
    "                   ds_mbm_win.pred_masked.max().item())\n",
    "\n",
    "    print(\n",
    "        f\"Color scale range (Annual): vmin={vmin_ann:.3f}, vmax={vmax_ann:.3f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Color scale range (Winter): vmin={vmin_win:.3f}, vmax={vmax_win:.3f}\"\n",
    "    )\n",
    "\n",
    "    if vmin_ann < 0 and vmax_ann > 0:\n",
    "        norm_ann = mcolors.TwoSlopeNorm(vmin=vmin_ann,\n",
    "                                        vcenter=0,\n",
    "                                        vmax=-vmin_ann)\n",
    "        cmap_ann = \"coolwarm_r\"\n",
    "    elif vmin_ann < 0 and vmax_ann < 0:\n",
    "        norm_ann = mcolors.Normalize(vmin=vmin_ann, vmax=vmax_ann)\n",
    "        cmap_ann = \"Reds\"\n",
    "    else:\n",
    "        norm_ann = mcolors.Normalize(vmin=vmin_ann, vmax=vmax_ann)\n",
    "        cmap_ann = \"Blues\"\n",
    "\n",
    "    if vmin_win < 0:\n",
    "        norm_win = mcolors.TwoSlopeNorm(vmin=-vmax_win,\n",
    "                                        vcenter=0,\n",
    "                                        vmax=vmax_win)\n",
    "        cmap_win = \"coolwarm_r\"\n",
    "    else:\n",
    "        norm_win = mcolors.Normalize(vmin=vmin_win, vmax=vmax_win)\n",
    "        cmap_win = \"Blues\"\n",
    "\n",
    "    # Create figure with 2 rows (Annual & Winter)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n",
    "\n",
    "    # Annual GLAMOS Plot\n",
    "    ds_glamos_wgs84_ann.plot.imshow(\n",
    "        ax=axes[0, 0],\n",
    "        cmap=cmap_ann,\n",
    "        norm=norm_ann,\n",
    "        cbar_kwargs={\"label\": \"Mass Balance [m w.e.]\"})\n",
    "    axes[0, 0].set_title(\"GLAMOS (Annual)\")\n",
    "    sns.scatterplot(data=stakes_data_ann,\n",
    "                    x=\"POINT_LON\",\n",
    "                    y=\"POINT_LAT\",\n",
    "                    hue=\"POINT_BALANCE\",\n",
    "                    palette=cmap_ann,\n",
    "                    hue_norm=norm_ann,\n",
    "                    ax=axes[0, 0],\n",
    "                    s=25,\n",
    "                    legend=False)\n",
    "    \n",
    "    \n",
    "    # add rmse if available\n",
    "    if not stakes_data_ann.empty:\n",
    "        rmse = mean_squared_error(stakes_data_ann.POINT_BALANCE,\n",
    "                                  stakes_data_ann.GLAMOS_MB,\n",
    "                                  squared=False)\n",
    "        axes[0, 0].text(0.05,\n",
    "                       0.1,\n",
    "                       f\"RMSE: {rmse:.2f}\",\n",
    "                       transform=axes[0, 0].transAxes,\n",
    "                       ha='left',\n",
    "                       va='top',\n",
    "                       fontsize=18)\n",
    "    \n",
    "    # Annual MBM Predictions Plot\n",
    "    ds_mbm_ann.pred_masked.plot.imshow(\n",
    "        ax=axes[0, 1],\n",
    "        cmap=cmap_ann,\n",
    "        norm=norm_ann,\n",
    "        cbar_kwargs={\"label\": \"Mass Balance [m w.e.]\"})\n",
    "    axes[0, 1].set_title(\"MBM (Annual)\")\n",
    "\n",
    "    # Add Annual Stake Coordinates\n",
    "    sns.scatterplot(data=stakes_data_ann,\n",
    "                    x=\"POINT_LON\",\n",
    "                    y=\"POINT_LAT\",\n",
    "                    hue=\"POINT_BALANCE\",\n",
    "                    palette=cmap_ann,\n",
    "                    hue_norm=norm_ann,\n",
    "                    ax=axes[0, 1],\n",
    "                    s=25,\n",
    "                    legend=False)\n",
    "    \n",
    "    # add rmse\n",
    "    if not stakes_data_ann.empty:\n",
    "        rmse = mean_squared_error(stakes_data_ann.POINT_BALANCE,\n",
    "                              stakes_data_ann.Predicted_MB,\n",
    "                              squared=False)\n",
    "        axes[0, 1].text(0.05,\n",
    "                   0.1,\n",
    "                   f\"RMSE: {rmse:.2f}\",\n",
    "                   transform=axes[0, 1].transAxes,\n",
    "                   ha='left',\n",
    "                   va='top',\n",
    "                   fontsize=18)\n",
    "\n",
    "    # Winter GLAMOS & MBM Plots\n",
    "    ds_glamos_wgs84_win.plot.imshow(\n",
    "        ax=axes[1, 0],\n",
    "        cmap=cmap_win,\n",
    "        norm=norm_win,\n",
    "        cbar_kwargs={\"label\": \"Mass Balance [m w.e.]\"})\n",
    "    axes[1, 0].set_title(\"GLAMOS (Winter)\")\n",
    "    sns.scatterplot(data=stakes_data_win,\n",
    "                    x=\"POINT_LON\",\n",
    "                    y=\"POINT_LAT\",\n",
    "                    hue=\"POINT_BALANCE\",\n",
    "                    palette=cmap_win,\n",
    "                    hue_norm=norm_win,\n",
    "                    ax=axes[1, 0],\n",
    "                    s=25,\n",
    "                    legend=False)\n",
    "    \n",
    "    # add rmse\n",
    "    if not stakes_data_win.empty:\n",
    "        rmse = mean_squared_error(stakes_data_win.POINT_BALANCE,\n",
    "                              stakes_data_win.GLAMOS_MB,\n",
    "                              squared=False)\n",
    "        axes[1, 0].text(0.05,\n",
    "                   0.1,\n",
    "                   f\"RMSE: {rmse:.2f}\",\n",
    "                   transform=axes[1, 0].transAxes,\n",
    "                   ha='left',\n",
    "                   va='top',\n",
    "                   fontsize=18)\n",
    "\n",
    "    # Winter MBM Predictions Plot\n",
    "    ds_mbm_win.pred_masked.plot.imshow(\n",
    "        ax=axes[1, 1],\n",
    "        cmap=cmap_win,\n",
    "        norm=norm_win,\n",
    "        cbar_kwargs={\"label\": \"Mass Balance [m w.e.]\"})\n",
    "    axes[1, 1].set_title(\"MBM (Winter)\")\n",
    "    sns.scatterplot(data=stakes_data_win,\n",
    "                    x=\"POINT_LON\",\n",
    "                    y=\"POINT_LAT\",\n",
    "                    hue=\"POINT_BALANCE\",\n",
    "                    palette=cmap_win,\n",
    "                    hue_norm=norm_win,\n",
    "                    ax=axes[1, 1],\n",
    "                    s=25,\n",
    "                    legend=False)\n",
    "    \n",
    "    # add rmse\n",
    "    if not stakes_data_win.empty:\n",
    "        rmse = mean_squared_error(stakes_data_win.POINT_BALANCE,\n",
    "                              stakes_data_win.Predicted_MB,\n",
    "                              squared=False)\n",
    "        axes[1, 1].text(0.05,\n",
    "                   0.1,\n",
    "                   f\"RMSE: {rmse:.2f}\",\n",
    "                   transform=axes[1, 1].transAxes,\n",
    "                   ha='left',\n",
    "                   va='top',\n",
    "                   fontsize=18)\n",
    "\n",
    "    plt.suptitle(f\"{glacier_name.capitalize()}: Mass Balance {year}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_name = \"rhone\"\n",
    "year = 2022\n",
    "fig = plot_mass_balance(glacier_name, year, df_stakes,\n",
    "                              path_distributed_MB_glamos, PATH_PREDICTIONS)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define glacier name\n",
    "glacier_name = \"tsanfleuron\"\n",
    "\n",
    "# Get available years from filenames\n",
    "years = sorted(\n",
    "    map(int, [\n",
    "        f.split(\"_\")[1]\n",
    "        for f in os.listdir(os.path.join(PATH_PREDICTIONS, glacier_name))\n",
    "        if \"_\" in f and f.endswith(\"_annual.nc\")\n",
    "    ]))\n",
    "\n",
    "# if not exist create path:\n",
    "if not os.path.exists(f\"figures/dst_mb/{glacier_name}\"):\n",
    "    os.makedirs(f\"figures/dst_mb/{glacier_name}\")\n",
    "# else empty:\n",
    "else:\n",
    "    emptyfolder(f\"figures/dst_mb/{glacier_name}\")\n",
    "\n",
    "# Iterate through each year\n",
    "for year in tqdm(years):\n",
    "    print(f\"Processing: {glacier_name}, Year: {year}\")\n",
    "\n",
    "    fig = plot_mass_balance(glacier_name, year, df_stakes,\n",
    "                              path_distributed_MB_glamos, PATH_PREDICTIONS)\n",
    "\n",
    "    # save figure\n",
    "    output_path = os.path.join(f\"figures/dst_mb/{glacier_name}\",\n",
    "                               f\"{glacier_name}_{year}.png\")\n",
    "    # if fig not none:\n",
    "    if fig:\n",
    "        fig.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input grids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_name = 'aletsch'\n",
    "year = 1952\n",
    "month = 'jul'\n",
    "\n",
    "# Feature columns:\n",
    "feature_columns = [\n",
    "    'ELEVATION_DIFFERENCE'\n",
    "] + list(vois_climate) + list(vois_topographical) + ['pcsr']\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "glacier_file = os.path.join(path_glacier_grid_glamos,glacier_name,\n",
    "                            f\"{glacier_name}_grid_{year}.csv\")\n",
    "\n",
    "df_grid_monthly = pd.read_csv(glacier_file)\n",
    "\n",
    "# Correct climate grids:\n",
    "# Take the biggest grid cell value for each month\n",
    "for voi in [\n",
    "        't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10',\n",
    "        'ALTITUDE_CLIMATE'\n",
    "]:\n",
    "    df_grid_monthly = correct_for_biggest_grid(\n",
    "        df_grid_monthly, group_columns=[\"YEAR\", \"MONTHS\"], value_column=voi)\n",
    "\n",
    "# New elevation difference:\n",
    "df_grid_monthly['ELEVATION_DIFFERENCE'] = df_grid_monthly[\n",
    "    \"POINT_ELEVATION\"] - df_grid_monthly[\"ALTITUDE_CLIMATE\"]\n",
    "\n",
    "# apply T & P correction\n",
    "# Apply temperature gradient correction\n",
    "temp_grad = -6.5 / 1000\n",
    "dpdz = 1.5 / 10000\n",
    "c_prec = 1.434,\n",
    "t_off = 0.617\n",
    "\n",
    "# Apply temperature correction factor\n",
    "df_grid_monthly['t2m_corr'] = df_grid_monthly['t2m'] + (\n",
    "    df_grid_monthly['ELEVATION_DIFFERENCE'] * temp_grad)\n",
    "df_grid_monthly['t2m_corr'] += t_off\n",
    "\n",
    "# Apply elevation correction factor\n",
    "df_grid_monthly['tp_corr'] = df_grid_monthly['tp'] * c_prec\n",
    "df_grid_monthly['tp_corr'] += df_grid_monthly['tp_corr'] * (\n",
    "    df_grid_monthly['ELEVATION_DIFFERENCE'] * dpdz)\n",
    "\n",
    "# Rename aspect and slope to sgi\n",
    "df_grid_monthly.rename(columns={\n",
    "    'aspect': 'aspect_sgi',\n",
    "    'slope': 'slope_sgi'\n",
    "},\n",
    "                       inplace=True)\n",
    "df_grid_monthly['POINT_ELEVATION'] = df_grid_monthly['topo']\n",
    "df_grid_monthly.drop_duplicates(inplace=True)  # remove duplicates\n",
    "df_grid_monthly = df_grid_monthly[all_columns]\n",
    "\n",
    "df_grid_month = df_grid_monthly[df_grid_monthly.MONTHS == month]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features to plot\n",
    "v_plot = [\"tp_corr\", \"t2m_corr\"] + vois_topographical + ['slhf', 'sshf']\n",
    "\n",
    "# Ensure the grid layout fits the number of plots dynamically\n",
    "n_rows = 3\n",
    "n_cols = min(3, len(v_plot))  # Ensure no more than 4 columns\n",
    "\n",
    "# Create figure and axes\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(10, 10))\n",
    "\n",
    "# Flatten the axes array for easier indexing\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Plot each feature\n",
    "for i, voi in enumerate(v_plot):\n",
    "    sns.scatterplot(data=df_grid_month,\n",
    "                    x=\"POINT_LON\",\n",
    "                    y=\"POINT_LAT\",\n",
    "                    hue=voi,\n",
    "                    ax=axs[i],\n",
    "                    palette=\"twilight_shifted\",\n",
    "                    s=2)\n",
    "    axs[i].set_title(voi)\n",
    "    axs[i].set_ylabel(\"Lat\")\n",
    "    axs[i].set_xlabel(\"Lon\")\n",
    "    axs[i].legend().remove()\n",
    "    axs[i].grid(False)\n",
    "\n",
    "# Remove unused subplots if any\n",
    "for j in range(i + 1, len(axs)):\n",
    "    fig.delaxes(axs[j])\n",
    "\n",
    "# Adjust layout for clarity\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stake coverage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define year and month for analysis\n",
    "year = 2018\n",
    "month = \"jul\"\n",
    "\n",
    "# Ensure the output folder is empty\n",
    "output_folder = \"figures/stake_coverage/\"\n",
    "emptyfolder(output_folder)\n",
    "\n",
    "# Load stake data ONCE instead of for every glacier\n",
    "stake_file = os.path.join(path_PMB_GLAMOS_csv, \"CH_wgms_dataset_all.csv\")\n",
    "if not os.path.exists(stake_file):\n",
    "    print(\"Error: Stake data file not found. Exiting.\")\n",
    "else:\n",
    "    df_stakes = pd.read_csv(stake_file)\n",
    "\n",
    "# Iterate through each glacier\n",
    "for glacier_name in tqdm(glacier_list_geod, desc=\"Processing glaciers\"):\n",
    "    # Construct glacier grid file path\n",
    "    glacier_file = os.path.join(path_glacier_grid_sgi,\n",
    "                                f\"{glacier_name}_grid_{year}.csv\")\n",
    "\n",
    "    if not os.path.exists(glacier_file):\n",
    "        print(f\"Skipping {glacier_name}: Grid file not found.\")\n",
    "        continue  # Skip if glacier grid file is missing\n",
    "\n",
    "    # Load and preprocess glacier grid data\n",
    "    df_grid_monthly = pd.read_csv(glacier_file)\n",
    "    df_grid_monthly = correct_vars_grid(df_grid_monthly)\n",
    "\n",
    "    # Rename aspect and slope to sgi\n",
    "    df_grid_monthly.rename(columns={\n",
    "        \"aspect\": \"aspect_sgi\",\n",
    "        \"slope\": \"slope_sgi\"\n",
    "    },\n",
    "                           inplace=True)\n",
    "    df_grid_monthly[\"POINT_ELEVATION\"] = df_grid_monthly[\"topo\"]\n",
    "    df_grid_monthly.drop_duplicates(inplace=True)  # Remove duplicates\n",
    "\n",
    "    # Filter dataset for selected month\n",
    "    df_grid_month = df_grid_monthly[df_grid_monthly.MONTHS == month]\n",
    "\n",
    "    # Filter stakes for this glacier\n",
    "    df_stakes_glacier = df_stakes[df_stakes[\"GLACIER\"] == glacier_name].copy()\n",
    "\n",
    "    # If no stake data available for this glacier, continue\n",
    "    if df_stakes_glacier.empty:\n",
    "        print(f\"Skipping {glacier_name}: No stake data available.\")\n",
    "        continue\n",
    "\n",
    "    # Convert to GeoDataFrame for geospatial visualization\n",
    "    df_stakes_glacier[\"geometry\"] = gpd.points_from_xy(\n",
    "        df_stakes_glacier.POINT_LON, df_stakes_glacier.POINT_LAT)\n",
    "    gdf_stakes = gpd.GeoDataFrame(df_stakes_glacier, crs=\"EPSG:4326\")\n",
    "\n",
    "    # Load SGI grid\n",
    "    sgi_file = os.path.join(path_SGI_topo,\n",
    "                            f\"xr_masked_grids/{glacier_name}.nc\")\n",
    "\n",
    "    if not os.path.exists(sgi_file):\n",
    "        print(f\"Skipping {glacier_name}: SGI NetCDF file not found.\")\n",
    "        continue\n",
    "\n",
    "    sgi_grid = xr.open_dataset(sgi_file)\n",
    "\n",
    "    # Create figure and plot\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    # Plot glacier mask\n",
    "    sgi_grid.glacier_mask.plot(cmap=\"binary\", ax=ax, alpha=0.5)\n",
    "\n",
    "    # Scatter plot of the monthly glacier grid with elevation difference\n",
    "    scatter = sns.scatterplot(data=df_grid_month,\n",
    "                              x=\"POINT_LON\",\n",
    "                              y=\"POINT_LAT\",\n",
    "                              hue=\"ELEVATION_DIFFERENCE\",\n",
    "                              ax=ax,\n",
    "                              s=5,\n",
    "                              palette=\"twilight_shifted\",\n",
    "                              legend=True)\n",
    "\n",
    "    # Plot stake locations\n",
    "    gdf_stakes.plot(ax=ax,\n",
    "                    color=color_tim,\n",
    "                    edgecolor=\"black\",\n",
    "                    markersize=20,\n",
    "                    label=\"Stakes\")\n",
    "\n",
    "    # Remove automatic Seaborn legend\n",
    "    ax.legend().remove()\n",
    "\n",
    "    # Set title\n",
    "    ax.set_title(f\"{glacier_name}\")\n",
    "\n",
    "    # Save figure\n",
    "    output_path = os.path.join(output_folder, f\"{glacier_name}.png\")\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    # Clear the plot to free memory\n",
    "    plt.close(fig)\n",
    "\n",
    "print(\"Processing complete. Figures saved to:\", output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2 Sentinel data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get S2 rasters:\n",
    "Get the names of all satellite nc files and classifiy them per month and hydrological year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel-2\n",
    "path_S2 = '../../../data/Sentinel/'\n",
    "src_crs = 'EPSG:4326'  # Original CRS (lat/lon) wgs84\n",
    "\n",
    "# Organize the rasters by hydrological year\n",
    "satellite_years = [2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "\n",
    "rasters_S2 = organize_rasters_by_hydro_year(path_S2, satellite_years)\n",
    "\n",
    "# Print the organized rasters\n",
    "for hydro_year, months in rasters_S2.items():\n",
    "    print(f\"-----------------\\nHydrological Year: {hydro_year}\")\n",
    "    for month, files in months.items():\n",
    "        print(f\"  {month}: {files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create gdf for S2 rasters:\n",
    "Create panadas geodataframe for each satellite raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satellite_glaciers = [\n",
    "    'adler', 'aletsch', 'allalin', 'basodino', 'claridenL', 'claridenU',\n",
    "    'findelen', 'gries', 'hohlaub', 'limmern', 'oberaar', 'plattalva', 'rhone',\n",
    "    'sanktanna', 'schwarzbach', 'schwarzberg'\n",
    "]\n",
    "\n",
    "processed_gl = os.listdir(PATH_PREDICTIONS)\n",
    "\n",
    "# list of gl in processed_gl that are also in satellite_glaciers\n",
    "satellite_glaciers = [gl for gl in processed_gl if gl in satellite_glaciers]\n",
    "satellite_glaciers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGI rasters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2 rasters over glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = False\n",
    "if RUN:\n",
    "    # emptyfolder(os.path.join(path_S2, 'perglacier/'))\n",
    "    # For each year and month where there is S2 data\n",
    "    for hydro_year, months in tqdm(rasters_S2.items(),\n",
    "                                   position=0,\n",
    "                                   desc='Hydrological Years'):\n",
    "        for month, files in tqdm(months.items(),\n",
    "                                 position=1,\n",
    "                                 leave=False,\n",
    "                                 desc='Months'):\n",
    "\n",
    "            # More than one if S2 A and B flew in the same month\n",
    "            for raster_name in files:\n",
    "                # Create raster for that year & month:\n",
    "                file_date = datetime.strptime(\n",
    "                    raster_name.split('_')[3][:8],\n",
    "                    \"%Y%m%d\")  # Extract the 8-digit date (YYYYMMDD)\n",
    "\n",
    "                # Path to the raster for that year & month:\n",
    "                raster_path = os.path.join(path_S2, file_date.strftime('%Y'),\n",
    "                                           raster_name)\n",
    "\n",
    "                # Creates a geodataframe from the .tif raster\n",
    "                # This is the longest part of the code\n",
    "                geo_data_S2 = mbm.GeoData(pd.DataFrame)\n",
    "                gdf_S2 = geo_data_S2.raster_to_gpd(raster_path)\n",
    "                geo_data_S2.set_gdf(gdf_S2)\n",
    "\n",
    "                # For each glacier, clip the S2 raster to the glacier extent\n",
    "                for glacierName in tqdm(satellite_glaciers,\n",
    "                                        position=2,\n",
    "                                        leave=False,\n",
    "                                        desc='Glaciers'):\n",
    "\n",
    "                    # Load MB predictions for that year and month\n",
    "                    path_nc_wgs84 = PATH_PREDICTIONS + f\"{glacierName}/\"\n",
    "                    filename_nc = f\"{glacierName}_{hydro_year}_{month_abbr_hydr_full[month]}.nc\"\n",
    "\n",
    "                    # check if file exists:\n",
    "                    if not os.path.exists(\n",
    "                            os.path.join(path_nc_wgs84, filename_nc)):\n",
    "                        continue\n",
    "\n",
    "                    # Open xarray dataset and set to class:\n",
    "                    geoData_gl = mbm.GeoData(pd.DataFrame)\n",
    "                    geoData_gl.set_ds_latlon(filename_nc, path_nc_wgs84)\n",
    "\n",
    "                    # Clip the S2 dataframe to the glacier extent\n",
    "                    # and resample it to the glacier resolution\n",
    "                    gdf_raster_res = mbm.GeoData.resample_satellite_to_glacier(\n",
    "                        geoData_gl.gdf, gdf_S2)\n",
    "\n",
    "                    # In case the glacier is outside of the bounds of the raster\n",
    "                    if gdf_raster_res is 0:\n",
    "                        continue\n",
    "                    # In case the raster is empty where the glacier is\n",
    "                    elif gdf_raster_res is 1:\n",
    "                        continue\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                    # Check the percentage of cloud cover\n",
    "                    cloud_cover_glacier = gdf_raster_res.classes[\n",
    "                        gdf_raster_res.classes ==\n",
    "                        5].count() / gdf_raster_res.classes.count()\n",
    "\n",
    "                    # If the cloud cover is too high, skip the glacier\n",
    "                    if cloud_cover_glacier > 0.5:\n",
    "                        continue\n",
    "\n",
    "                    # Save the glacier raster:\n",
    "                    S2_gl_name = '{}_{}.geojson'.format(\n",
    "                        glacierName, file_date.strftime('%Y_%m_%d'))\n",
    "                    S2_gl_path = os.path.join(path_S2, 'perglacier',\n",
    "                                              S2_gl_name)\n",
    "                    gdf_raster_res.to_file(S2_gl_path, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snow cover:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = True\n",
    "if RUN:\n",
    "\n",
    "    # first delete results file\n",
    "    if os.path.exists(f'results/snow_cover.csv'):\n",
    "        os.remove(f'results/snow_cover.csv')\n",
    "\n",
    "    # Add header\n",
    "    with open(f'results/snow_cover.csv', 'w') as f:\n",
    "        f.write(\n",
    "            \"year,month,monthNb,raster_date,glacier_name,snow_cover_S2,snow_cover_glacier\\n\"\n",
    "        )\n",
    "\n",
    "    # All rasters resampled over glaciers:\n",
    "    rasters_resampled = os.listdir(os.path.join(path_S2, 'perglacier'))\n",
    "\n",
    "    for raster_res in tqdm(rasters_resampled):\n",
    "        # Extract glacier name\n",
    "        glacierName = raster_res.split('_')[0]\n",
    "\n",
    "        # Extract date from satellite raster\n",
    "        match = re.search(r\"(\\d{4})_(\\d{2})_(\\d{2})\", raster_res)\n",
    "        year, month, day = match.groups()\n",
    "        date_str = match.group(1) + '-' + match.group(2) + '-' + match.group(3)\n",
    "        raster_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "        # Find closest hydrological year and month\n",
    "        closest_month, hydro_year = get_hydro_year_and_month(raster_date)\n",
    "        monthNb = month_abbr_hydr_full[closest_month]\n",
    "\n",
    "        if hydro_year > 2021:\n",
    "            continue\n",
    "\n",
    "        # Read satellite raster over glacier (previously resampled)\n",
    "        gdf_S2_res = gpd.read_file(\n",
    "            os.path.join(path_S2, 'perglacier', raster_res))\n",
    "\n",
    "        # Calculate percentage of snow cover (class 1)\n",
    "        snow_cover_S2 = IceSnowCover(gdf_S2_res, gdf_S2_res)\n",
    "\n",
    "        # Load MB predictions for that year and month\n",
    "        path_nc_wgs84 = PATH_PREDICTIONS + f\"{glacierName}/\"\n",
    "        filename_nc = f\"{glacierName}_{hydro_year}_{monthNb}.nc\"\n",
    "\n",
    "        geoData_gl = mbm.GeoData(pd.DataFrame)\n",
    "        geoData_gl.set_ds_latlon(filename_nc, path_nc_wgs84)\n",
    "        geoData_gl.classify_snow_cover(tol=0.1)\n",
    "\n",
    "        snow_cover_glacier = IceSnowCover(geoData_gl.gdf, gdf_S2_res)\n",
    "\n",
    "        # Save the results\n",
    "        with open(f'results/snow_cover.csv', 'a') as f:\n",
    "            f.write(\n",
    "                f\"{hydro_year},{closest_month},{monthNb},{date_str},{glacierName},{snow_cover_S2},{snow_cover_glacier}\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'results/snow_cover.csv').sort_values(by=['year', 'month'],\n",
    "                                                        ascending=True)\n",
    "# remove october and september\n",
    "df = df[~df.month.isin(['oct', 'sep'])]\n",
    "fig, axs = plot_snow_cover_scatter(df, add_corr=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gl = df[(df.glacier_name == 'aletsch') & ~df.month.isin(['oct', 'sep'])]\n",
    "fig, axs = plot_snow_cover_scatter_combined(df_gl)\n",
    "\n",
    "df_gl = df[(df.glacier_name == 'gries') & ~df.month.isin(['oct', 'sep'])]\n",
    "fig, axs = plot_snow_cover_scatter_combined(df_gl)\n",
    "\n",
    "df_gl = df[(df.glacier_name == 'rhone') & ~df.month.isin(['oct', 'sep'])]\n",
    "fig, axs = plot_snow_cover_scatter_combined(df_gl)\n",
    "\n",
    "df_gl = df[(df.glacier_name == 'adler') & ~df.month.isin(['oct', 'sep'])]\n",
    "fig, axs = plot_snow_cover_scatter_combined(df_gl)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the raster files and plot geoplots\n",
    "gl = 'aletsch'\n",
    "yr = 2021\n",
    "rasters_gl = [\n",
    "    f for f in os.listdir(os.path.join(path_S2, 'perglacier'))\n",
    "    if (gl in f) and (str(yr) in f)\n",
    "]\n",
    "rasters_gl.sort()\n",
    "for raster_res in rasters_gl:\n",
    "    plot_snow_cover_geoplots(raster_res, path_S2, month_abbr_hydr_full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity of tolerance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = False\n",
    "\n",
    "if RUN:\n",
    "    # first delete results file\n",
    "    if os.path.exists(f'results/snow_cover_tol.csv'):\n",
    "        os.remove(f'results/snow_cover_tol.csv')\n",
    "\n",
    "    # Add header\n",
    "    with open(f'results/snow_cover_tol.csv', 'w') as f:\n",
    "        f.write(\n",
    "            \"tol,year,month,monthNb,raster_date,glacier_name,snow_cover_S2,snow_cover_glacier,snow_cover_glacier_corr\\n\"\n",
    "        )\n",
    "\n",
    "    # All rasters resampled over glaciers:\n",
    "    rasters_resampled = [\n",
    "        f for f in os.listdir(os.path.join(path_S2, 'perglacier'))\n",
    "    ]\n",
    "\n",
    "    for tol in tqdm(np.arange(0, 1, 0.1), position=0, desc='Tolerance'):\n",
    "        for raster_res in tqdm(rasters_resampled):\n",
    "            # Extract glacier name\n",
    "            glacierName = raster_res.split('_')[0]\n",
    "\n",
    "            # Extract date from satellite raster\n",
    "            match = re.search(r\"(\\d{4})_(\\d{2})_(\\d{2})\", raster_res)\n",
    "            year, month, day = match.groups()\n",
    "            date_str = match.group(1) + '-' + match.group(\n",
    "                2) + '-' + match.group(3)\n",
    "            raster_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "            # Find closest hydrological year and month\n",
    "            closest_month, hydro_year = get_hydro_year_and_month(raster_date)\n",
    "            monthNb = month_abbr_hydr_full_hydr[closest_month]\n",
    "\n",
    "            if hydro_year > 2021:\n",
    "                continue\n",
    "\n",
    "            # Read satellite raster over glacier (previously resampled)\n",
    "            gdf_S2_res = gpd.read_file(\n",
    "                os.path.join(path_S2, 'perglacier', raster_res))\n",
    "\n",
    "            # Calculate percentage of snow cover (class 1)\n",
    "            snow_cover_S2 = IceSnowCover(gdf_S2_res, gdf_S2_res)\n",
    "\n",
    "            # Load MB predictions for that year and month\n",
    "            path_nc_wgs84 = f\"results/nc/var_normal/{glacierName}/wgs84/\"\n",
    "            path_nc_wgs84_corr = f\"results/nc/var_corr/{glacierName}/wgs84/\"\n",
    "            filename_nc = f\"{glacierName}_{hydro_year}_{monthNb}.nc\"\n",
    "\n",
    "            gdf_glacier = ClassSnowCover(path_nc_wgs84, filename_nc, tol)\n",
    "            snow_cover_glacier = IceSnowCover(gdf_glacier, gdf_S2_res)\n",
    "\n",
    "            # Corrected T and P\n",
    "            gdf_glacier_corr = ClassSnowCover(path_nc_wgs84_corr, filename_nc,\n",
    "                                              tol)\n",
    "            snow_cover_glacier_corr = IceSnowCover(gdf_glacier_corr,\n",
    "                                                   gdf_S2_res)\n",
    "\n",
    "            # Save the results\n",
    "            with open(f'results/snow_cover_tol.csv', 'a') as f:\n",
    "                f.write(\n",
    "                    f\"{tol},{hydro_year},{closest_month},{monthNb},{date_str},{glacierName},{snow_cover_S2},{snow_cover_glacier},{snow_cover_glacier_corr}\\n\"\n",
    "                )\n",
    "\n",
    "df = pd.read_csv(f'results/snow_cover_tol.csv').sort_values(\n",
    "    by=['year', 'month'], ascending=True)\n",
    "df_gl = df[~df.month.isin(['oct', 'sep'])]\n",
    "\n",
    "for tol in df_gl.tol.unique():\n",
    "    df_gl_tol = df_gl[df_gl.tol == tol]\n",
    "    # fig, axs = plot_snow_cover_scatter_combined(df_gl_tol)\n",
    "    fig, axs = plot_snow_cover_scatter(df_gl_tol, add_corr=False)\n",
    "    plt.suptitle(f'Tolerance: {np.round(tol, 4)}', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the results for each tolerance\n",
    "tol_results = {}\n",
    "month_labels = ['April', 'May', 'June', 'July', 'August']\n",
    "# Iterate over each tolerance value\n",
    "for tol in df_gl.tol.unique():\n",
    "    df_gl_tol = df_gl[df_gl.tol == tol]\n",
    "\n",
    "    # Get sorted unique months\n",
    "    months = np.sort(df_gl_tol['monthNb'].unique())\n",
    "\n",
    "    # Lists to store metrics for each month\n",
    "    r2 = []\n",
    "    rmse = []\n",
    "\n",
    "    # Loop over each month\n",
    "    for monthNb in months:\n",
    "        df_month = df_gl_tol[df_gl_tol['monthNb'] == monthNb]\n",
    "\n",
    "        # Calculate R^2\n",
    "        r2.append(\n",
    "            np.corrcoef(df_month['snow_cover_S2'],\n",
    "                        df_month['snow_cover_glacier'])[0, 1]**2)\n",
    "\n",
    "        # Calculate MSE\n",
    "        rmse.append(\n",
    "            mean_squared_error(df_month['snow_cover_glacier'],\n",
    "                               df_month['snow_cover_S2'],\n",
    "                               squared=False))\n",
    "\n",
    "    # Store the results in the dictionary\n",
    "    tol_results[np.round(tol, 4)] = {\n",
    "        'months': month_labels,\n",
    "        'r2': r2,\n",
    "        'rmse': rmse\n",
    "    }\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6), sharex=True)\n",
    "\n",
    "# Create a colormap\n",
    "cmap = plt.cm.Blues\n",
    "norm = plt.Normalize(vmin=min(df_gl.tol.unique()),\n",
    "                     vmax=max(df_gl.tol.unique()))\n",
    "\n",
    "# Plot R^2 values for each tolerance\n",
    "for tol, results in tol_results.items():\n",
    "    color = cmap(norm(tol))\n",
    "    axes[0].plot(results['months'],\n",
    "                 results['r2'],\n",
    "                 marker='o',\n",
    "                 label=f'Tol {tol} m',\n",
    "                 color=color)\n",
    "\n",
    "# Plot MSE values for each tolerance\n",
    "for tol, results in tol_results.items():\n",
    "    color = cmap(norm(tol))\n",
    "    axes[1].plot(results['months'],\n",
    "                 results['rmse'],\n",
    "                 marker='o',\n",
    "                 label=f'Tol {tol} m',\n",
    "                 color=color)\n",
    "\n",
    "# Customize the plots\n",
    "axes[0].set_title('R^2 by Month and Tolerance')\n",
    "axes[0].set_xlabel('Month')\n",
    "axes[0].set_ylabel('R^2')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_title('MSE by Month and Tolerance')\n",
    "axes[1].set_xlabel('Month')\n",
    "axes[1].set_ylabel('MSE')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity of prec & temp correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.read_csv('results/sensitivity_scores.csv')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_c_prec_t_off(filename):\n",
    "    \"\"\"\n",
    "    Extract c_prec and t_off values from a given filename.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): The string containing the values to extract.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple (c_prec, t_off) with the extracted values as floats.\n",
    "    \"\"\"\n",
    "    # Define the regular expression pattern\n",
    "    #pattern = r\"_(\\d+\\.\\d+)_(-?\\d+\\.\\d+)\\.nc$\"\n",
    "    pattern = r\"_(\\d+\\.\\d+)_(-?\\d+)\\.nc$\"\n",
    "\n",
    "    # Search for the pattern in the filename\n",
    "    match = re.search(pattern, filename)\n",
    "\n",
    "    if match:\n",
    "        # Extract the matched groups and convert to floats\n",
    "        c_prec = float(match.group(1))\n",
    "        t_off = float(match.group(2))\n",
    "        return c_prec, t_off\n",
    "    else:\n",
    "        raise ValueError(\"Filename does not match the expected pattern\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "filename = \"hohlaub_2015_1_1.6_0.3333.nc\"\n",
    "c_prec, t_off = extract_c_prec_t_off(filename)\n",
    "print(f\"c_prec: {c_prec}, t_off: {t_off}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_files_by_pattern(file_list,\n",
    "                            glacierName=None,\n",
    "                            hydro_year=None,\n",
    "                            monthNb=None):\n",
    "    \"\"\"\n",
    "    Filter files starting with the pattern \"{glacierName}_{hydro_year}_{monthNb}\".\n",
    "\n",
    "    Parameters:\n",
    "        file_list (list): List of filenames to filter.\n",
    "        glacierName (str, optional): The name of the glacier to match. Defaults to None (any glacier).\n",
    "        hydro_year (int, optional): The hydrological year to match. Defaults to None (any year).\n",
    "        monthNb (int, optional): The month number to match. Defaults to None (any month).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of filenames that match the pattern.\n",
    "    \"\"\"\n",
    "    # Build the regular expression pattern based on the provided attributes\n",
    "    glacier_pattern = glacierName if glacierName else r\"[a-zA-Z0-9]+\"\n",
    "    year_pattern = str(hydro_year) if hydro_year else r\"\\d{4}\"\n",
    "    month_pattern = str(monthNb) if monthNb else r\"\\d{1,2}\"\n",
    "\n",
    "    pattern = rf\"^{glacier_pattern}_{year_pattern}_{month_pattern}.*\"\n",
    "\n",
    "    # Filter the files that match the pattern\n",
    "    matching_files = [\n",
    "        filename for filename in file_list if re.match(pattern, filename)\n",
    "    ]\n",
    "\n",
    "    return matching_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('results/nc/sensitivity/adler/wgs84')\n",
    "matching_files = filter_files_by_pattern(files,\n",
    "                                         glacierName='adler',\n",
    "                                         hydro_year=2015,\n",
    "                                         monthNb=9)\n",
    "matching_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satellite_glaciers = os.listdir('results/nc/sensitivity/')\n",
    "RUN = True\n",
    "if RUN:\n",
    "    # first delete results file\n",
    "    if os.path.exists(f'results/snow_cover_corr.csv'):\n",
    "        os.remove(f'results/snow_cover_corr.csv')\n",
    "\n",
    "    # Add header\n",
    "    with open(f'results/snow_cover_corr.csv', 'w') as f:\n",
    "        f.write(\n",
    "            \"c_prec,t_off,year,month,monthNb,raster_date,glacier_name,snow_cover_S2,snow_cover_glacier\\n\"\n",
    "        )\n",
    "\n",
    "    # All rasters resampled over glaciers:\n",
    "    rasters_resampled = os.listdir(os.path.join(path_S2, 'perglacier'))\n",
    "\n",
    "    for raster_res in tqdm(rasters_resampled):\n",
    "        # Extract glacier name\n",
    "        glacierName = raster_res.split('_')[0]\n",
    "\n",
    "        # Extract date from satellite raster\n",
    "        match = re.search(r\"(\\d{4})_(\\d{2})_(\\d{2})\", raster_res)\n",
    "        year, month, day = match.groups()\n",
    "        date_str = match.group(1) + '-' + match.group(2) + '-' + match.group(3)\n",
    "        raster_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "        # Find closest hydrological year and month\n",
    "        closest_month, hydro_year = get_hydro_year_and_month(raster_date)\n",
    "        monthNb = month_abbr_hydr_full_hydr[closest_month]\n",
    "\n",
    "        if hydro_year > 2021:\n",
    "            continue\n",
    "\n",
    "        # Read satellite raster over glacier (previously resampled)\n",
    "        gdf_S2_res = gpd.read_file(\n",
    "            os.path.join(path_S2, 'perglacier', raster_res))\n",
    "\n",
    "        # Calculate percentage of snow cover (class 1)\n",
    "        snow_cover_S2 = IceSnowCover(gdf_S2_res, gdf_S2_res)\n",
    "\n",
    "        # Load MB predictions for that year and month\n",
    "        path_nc_wgs84 = f\"results/nc/sensitivity/{glacierName}/wgs84/\"\n",
    "\n",
    "        files_list = os.listdir(path_nc_wgs84)\n",
    "        matching_files_list = filter_files_by_pattern(files_list,\n",
    "                                                      glacierName=glacierName,\n",
    "                                                      hydro_year=hydro_year,\n",
    "                                                      monthNb=monthNb)\n",
    "        for filename_nc in matching_files_list:\n",
    "            c_prec, t_off = extract_c_prec_t_off(filename_nc)\n",
    "\n",
    "            gdf_glacier = ClassSnowCover(path_nc_wgs84, filename_nc)\n",
    "            snow_cover_glacier = IceSnowCover(gdf_glacier, gdf_S2_res)\n",
    "\n",
    "            # Save the results\n",
    "            with open(f'results/snow_cover_corr.csv', 'a') as f:\n",
    "                f.write(\n",
    "                    f\"{c_prec},{t_off},{hydro_year},{closest_month},{monthNb},{date_str},{glacierName},{snow_cover_S2},{snow_cover_glacier}\\n\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read results\n",
    "df = pd.read_csv(f'results/snow_cover_corr.csv').sort_values(\n",
    "    by=['year', 'month'], ascending=True)\n",
    "df_gl = df[~df.month.isin(['oct', 'sep'])]\n",
    "\n",
    "factors = df_gl[['c_prec', 't_off']].drop_duplicates().sort_values(by='c_prec')\n",
    "\n",
    "for i, row in factors.iterrows():\n",
    "    c_prec, t_off = row['c_prec'], row['t_off']\n",
    "    df_gl_c_prec = df_gl[(df_gl.c_prec == c_prec) & (df_gl.t_off == t_off)]\n",
    "    fig, axs = plot_snow_cover_scatter(df_gl_c_prec, add_corr=False)\n",
    "    plt.suptitle(f'c_prec: {c_prec}, t_off: {t_off}', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snowline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the raster files and plot geoplots\n",
    "gl = 'aletsch'\n",
    "yr = 2021\n",
    "rasters_gl = [\n",
    "    f for f in os.listdir(os.path.join(path_S2, 'perglacier'))\n",
    "    if (gl in f) and (str(yr) in f)\n",
    "]\n",
    "rasters_gl.sort()\n",
    "for raster_res in rasters_gl:\n",
    "    plot_snow_cover_geoplots(raster_res,\n",
    "                             path_S2,\n",
    "                             month_abbr_hydr_full_hydr,\n",
    "                             add_snowline=True,\n",
    "                             band_size=50,\n",
    "                             percentage_threshold=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the raster files and plot geoplots\n",
    "gl = 'rhone'\n",
    "yr = 2021\n",
    "rasters_gl = [\n",
    "    f for f in os.listdir(os.path.join(path_S2, 'perglacier'))\n",
    "    if (gl in f) and (str(yr) in f)\n",
    "]\n",
    "rasters_gl.sort()\n",
    "for raster_res in rasters_gl:\n",
    "    plot_snow_cover_geoplots(raster_res,\n",
    "                             path_S2,\n",
    "                             month_abbr_hydr_full_hydr,\n",
    "                             add_snowline=True,\n",
    "                             band_size=50,\n",
    "                             percentage_threshold=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missed glaciers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1: Glacier is in regions where raster is NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2015\n",
    "glacierName = 'taelliboden'\n",
    "filename_nc = f\"{glacierName}_{year}.nc\"\n",
    "path_nc_wgs84 = f\"results/nc/var_normal/{glacierName}/wgs84/\"\n",
    "path_nc_wgs84_corr = f\"results/nc/var_corr/{glacierName}/wgs84/\"\n",
    "\n",
    "gdf_raster = createRaster(raster_a_2015)\n",
    "\n",
    "# Calculate snow cover for glacier\n",
    "gdf_glacier, snow_cover_glacier, ice_cover_glacier = snowCover(\n",
    "    path_nc_wgs84, filename_nc)\n",
    "# Corrected for temperature & precipitation\n",
    "gdf_glacier, snow_cover_glacier_corr, ice_cover_glacier_corr = snowCover(\n",
    "    path_nc_wgs84_corr, filename_nc)\n",
    "\n",
    "# Clip the raster to the glacier extent and resample it to the glacier resolution\n",
    "# gdf_raster_res = resampleRaster(gdf_glacier, gdf_raster)\n",
    "\n",
    "bounding_box = gdf_glacier.total_bounds  # [minx, miny, maxx, maxy]\n",
    "raster_bounds = gdf_raster.total_bounds  # [minx, miny, maxx, maxy]\n",
    "\n",
    "# Check if glacier bounds are within raster bounds\n",
    "if not (bounding_box[0] >= raster_bounds[0]\n",
    "        and  # minx of glacier >= minx of raster\n",
    "        bounding_box[1] >= raster_bounds[1]\n",
    "        and  # miny of glacier >= miny of raster\n",
    "        bounding_box[2] <= raster_bounds[2]\n",
    "        and  # maxx of glacier <= maxx of raster\n",
    "        bounding_box[3]\n",
    "        <= raster_bounds[3]  # maxy of glacier <= maxy of raster\n",
    "        ):\n",
    "    print(f\"Glacier {glacierName} is out of bounds\")\n",
    "\n",
    "bbox_polygon = box(*bounding_box)\n",
    "\n",
    "# The raster might have no data (NaN values) in the region of the glacier:\n",
    "bounding_box = [7.8, 45.95854232, 8, 46.1]\n",
    "bbox_polygon = box(*bounding_box)\n",
    "gfd_res = gdf_raster[gdf_raster.intersects(bbox_polygon)]\n",
    "ax = gfd_res.plot(color='blue', alpha=0.5)\n",
    "gdf_glacier.plot(ax=ax, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform to tif rasters for QGIS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacierName = 'aletsch'\n",
    "year = 2021\n",
    "\n",
    "filename_nc = f\"{glacierName}_{year}.nc\"\n",
    "path_nc_lv95 = f\"results/nc/{glacierName}/lv95/\"\n",
    "path_nc_wgs84 = f\"results/nc/{glacierName}/wgs84/\"\n",
    "\n",
    "filename_tif = f\"{glacierName}_{year}.tif\"\n",
    "path_tif_wgs84 = f\"results/tif/{glacierName}/wgs84/\"\n",
    "path_tif_lv95 = f\"results/tif/{glacierName}/lv95/\"\n",
    "\n",
    "createPath(path_tif_lv95)\n",
    "createPath(path_tif_wgs84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf, gdf_class, raster_data, extent = TransformToRaster(\n",
    "    filename_nc, filename_tif, path_nc_wgs84, path_tif_wgs84, path_tif_lv95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"000378bd-b0f0-46e2-a46d-f2165b0c6c02\"\n",
    "provider = cx.providers.Stadia.StamenTerrain(api_key=API_KEY)\n",
    "provider[\"url\"] = provider[\"url\"] + f\"?api_key={API_KEY}\"\n",
    "\n",
    "vmin, vmax = gdf.data.min(), gdf.data.max()\n",
    "norm = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "gdf_clean = gdf.dropna(subset=[\"data\"])\n",
    "gdf_clean.plot(\n",
    "    column=\"data\",  # Column to visualize\n",
    "    cmap=\"RdBu\",  # Color map suitable for glacier data\n",
    "    norm=norm,\n",
    "    legend=True,  # Display a legend\n",
    "    ax=axs[0],\n",
    "    markersize=5,  # Adjust size if points are too small or large\n",
    "    missing_kwds={\"color\": \"lightgrey\"}  # Define color for NaN values\n",
    ")\n",
    "cx.add_basemap(axs[0], crs=gdf.crs, source=provider)\n",
    "\n",
    "gdf_clean = gdf_class.dropna(subset=[\"data\"])\n",
    "gdf_clean.plot(\n",
    "    column=\"data\",  # Column to visualize\n",
    "    legend=True,  # Display a legend\n",
    "    ax=axs[1],\n",
    "    markersize=5,  # Adjust size if points are too small or large\n",
    "    missing_kwds={\"color\": \"lightgrey\"}  # Define color for NaN values\n",
    ")\n",
    "cx.add_basemap(axs[1], crs=gdf.crs, source=provider)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin, vmax = gdf.data.min(), gdf.data.max()\n",
    "\n",
    "# Calculate the relative position of 0\n",
    "relative_position = (0 - vmin) / (vmax - vmin) * 100\n",
    "\n",
    "# Print the result\n",
    "print(f\"The relative position of 0 is {relative_position:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "for year in years:\n",
    "    print(year)\n",
    "    for month in month_abbr_hydr_full_hydr:\n",
    "        monthNb = month_abbr_hydr_full_hydr[month]\n",
    "\n",
    "        filename_nc = f\"{glacierName}_{year}_{monthNb}.nc\"\n",
    "        path_nc_lv95 = f\"results/nc/{glacierName}/lv95/\"\n",
    "        path_nc_wgs84 = f\"results/nc/{glacierName}/wgs84/\"\n",
    "\n",
    "        filename_tif = f\"{glacierName}_{year}_{monthNb}.tif\"\n",
    "        path_tif_wgs84 = f\"results/tif/{glacierName}/wgs84/\"\n",
    "        path_tif_lv95 = f\"results/tif/{glacierName}/lv95/\"  # normally EPSG Code: 2056\n",
    "\n",
    "        gdf, gdf_class, raster_data, extent = TransformToRaster(\n",
    "            filename_nc, filename_tif, path_nc_wgs84, path_tif_wgs84,\n",
    "            path_tif_lv95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"000378bd-b0f0-46e2-a46d-f2165b0c6c02\"\n",
    "provider = cx.providers.Stadia.StamenTerrain(api_key=API_KEY)\n",
    "provider[\"url\"] = provider[\"url\"] + f\"?api_key={API_KEY}\"\n",
    "\n",
    "vmin, vmax = gdf.data.min(), gdf.data.max()\n",
    "norm = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(5, 5))\n",
    "gdf_clean = gdf.dropna(subset=[\"data\"])\n",
    "gdf_clean.plot(\n",
    "    column=\"data\",  # Column to visualize\n",
    "    cmap=\"RdBu\",  # Color map suitable for glacier data\n",
    "    norm=norm,\n",
    "    legend=True,  # Display a legend\n",
    "    ax=axs[0],\n",
    "    markersize=5,  # Adjust size if points are too small or large\n",
    "    missing_kwds={\"color\": \"lightgrey\"}  # Define color for NaN values\n",
    ")\n",
    "cx.add_basemap(axs[0], crs=gdf.crs, source=provider)\n",
    "\n",
    "gdf_clean = gdf_class.dropna(subset=[\"data\"])\n",
    "gdf_clean.plot(\n",
    "    column=\"data\",  # Column to visualize\n",
    "    legend=True,  # Display a legend\n",
    "    ax=axs[1],\n",
    "    markersize=5,  # Adjust size if points are too small or large\n",
    "    missing_kwds={\"color\": \"lightgrey\"}  # Define color for NaN values\n",
    ")\n",
    "cx.add_basemap(axs[1], crs=gdf.crs, source=provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by step example of one file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Open the NetCDF file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_xy = xr.open_dataset(path_nc_lv95 + filename_nc)\n",
    "ds_latlon = xr.open_dataset(path_nc_wgs84 + filename_nc)\n",
    "\n",
    "# Smoothing\n",
    "ds_xy_g = GaussianFilter(ds_xy)\n",
    "ds_latlon_g = GaussianFilter(ds_latlon)\n",
    "\n",
    "# Show effet of Smoothing:\n",
    "vmin, vmax = np.min([\n",
    "    ds_xy.pred_masked.min().values,\n",
    "    ds_xy_g.pred_masked.min()\n",
    "]), np.max([ds_xy.pred_masked.max().values,\n",
    "            ds_xy_g.pred_masked.max()])\n",
    "max_abs_value = max(abs(vmin), abs(vmax))\n",
    "norm = mcolors.TwoSlopeNorm(vmin=-max_abs_value, vcenter=0, vmax=max_abs_value)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ds_xy.pred_masked.plot.imshow(cmap='RdBu', norm=norm, ax=axs[0])\n",
    "axs[0].set_title('Original')\n",
    "\n",
    "# Plot or analyze `smoothed_data` as needed\n",
    "ds_xy_g.pred_masked.plot.imshow(cmap='RdBu', norm=norm, ax=axs[1])\n",
    "axs[1].set_title('Gaussian Filter')\n",
    "\n",
    "# print min and max values\n",
    "print(ds_xy.pred_masked.min().values, ds_xy.pred_masked.max().values)\n",
    "print(ds_xy_g.pred_masked.min().values, ds_xy_g.pred_masked.max().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: transform to geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf, lon, lat = toGeoPandas(ds_latlon_g)\n",
    "\n",
    "# Reproject to LV95 (EPSG:2056) swiss coordinates\n",
    "# gdf_lv95 = gdf.to_crs(\"EPSG:2056\")\n",
    "\n",
    "API_KEY = \"000378bd-b0f0-46e2-a46d-f2165b0c6c02\"\n",
    "provider = cx.providers.Stadia.StamenTerrain(api_key=API_KEY)\n",
    "provider[\"url\"] = provider[\"url\"] + f\"?api_key={API_KEY}\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "gdf_clean = gdf.dropna(subset=[\"data\"])\n",
    "gdf_clean.plot(\n",
    "    column=\"data\",  # Column to visualize\n",
    "    cmap=\"Reds\",  # Color map suitable for glacier data\n",
    "    legend=True,  # Display a legend\n",
    "    ax=ax,\n",
    "    markersize=5,  # Adjust size if points are too small or large\n",
    "    missing_kwds={\"color\": \"lightgrey\"}  # Define color for NaN values\n",
    ")\n",
    "cx.add_basemap(ax, crs=gdf.crs, source=provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: transform to raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to raster from geopandas\n",
    "raster_data, extent = toRaster(gdf,\n",
    "                               lon,\n",
    "                               lat,\n",
    "                               file_name=path_tif_wgs84 + filename_tif)\n",
    "\n",
    "# reproject raster to Swiss coordinates (LV95)\n",
    "reproject_raster_to_lv95(path_tif_wgs84 + filename_tif,\n",
    "                         path_tif_lv95 + filename_tif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opt step 4: for clariden\n",
    "Need to merge two rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'clariden' in glacierName:\n",
    "    merge_rasters('results/tif/claridenL_2022_w_lv95.tif',\n",
    "                  'results/tif/claridenU_2022_w_lv95.tif',\n",
    "                  'results/tif/clariden_2022_w_lv95.tif')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
