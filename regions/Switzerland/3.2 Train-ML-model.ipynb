{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from calendar import month_abbr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.xgb_helpers import *\n",
    "import geopandas as gpd\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()\n",
    "custom_working_dir = '../../../data/OGGM/'\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'\n",
    "color_winter = '#a6cee3'\n",
    "color_annual = '#1f78b4'\n",
    "\n",
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "rgi_df = pd.read_csv(path_glacier_ids, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "\n",
    "# Specify the short names of the climate variables available in the dataset\n",
    "# vois_climate = [\n",
    "#     't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "# ]\n",
    "\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    # \"aspect\",\n",
    "    # \"slope\",\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = pd.read_csv(path_PMB_GLAMOS_csv + 'CH_wgms_dataset_all.csv')\n",
    "\n",
    "print('Number of glaciers:', len(data_glamos['GLACIER'].unique()))\n",
    "print('Number of winter and annual samples:', len(data_glamos))\n",
    "print('Number of annual samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_glamos[data_glamos.PERIOD == 'winter']))\n",
    "\n",
    "# Capitalize glacier names:\n",
    "glacierCap = {}\n",
    "for gl in data_glamos['GLACIER'].unique():\n",
    "    if isinstance(gl, str):  # Ensure the glacier name is a string\n",
    "        if gl.lower() == 'claridenu':\n",
    "            glacierCap[gl] = 'Clariden_U'\n",
    "        elif gl.lower() == 'claridenl':\n",
    "            glacierCap[gl] = 'Clariden_L'\n",
    "        else:\n",
    "            glacierCap[gl] = gl.capitalize()\n",
    "    else:\n",
    "        print(f\"Warning: Non-string glacier name encountered: {gl}\")\n",
    "\n",
    "data_glamos.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of mean mass balance per glacier:\n",
    "# Get the mean mass balance per glacier\n",
    "data_glamos_ = data_glamos.copy()\n",
    "data_glamos_['GLACIER'] = data_glamos.GLACIER.apply(lambda x: glacierCap[x])\n",
    "mean_mb_per_glacier = data_glamos_.groupby(\n",
    "    ['GLACIER', 'YEAR', 'PERIOD'])['POINT_BALANCE'].mean().reset_index()\n",
    "mean_mb_per_glacier = mean_mb_per_glacier[mean_mb_per_glacier['PERIOD'] ==\n",
    "                                          'annual']\n",
    "\n",
    "matrix = mean_mb_per_glacier.pivot(\n",
    "    index='GLACIER', columns='YEAR',\n",
    "    values='POINT_BALANCE').sort_values(by='GLACIER')\n",
    "\n",
    "# get elevation of glaciers:\n",
    "gl_per_el = data_glamos_.groupby(['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "matrix = matrix.loc[gl_per_el.index]\n",
    "\n",
    "# make index categorical\n",
    "matrix.index = pd.Categorical(matrix.index,\n",
    "                              categories=matrix.index,\n",
    "                              ordered=True)\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "sns.heatmap(data=matrix,\n",
    "            center=0,\n",
    "            cmap=cm.vik_r,\n",
    "            cbar_kws={'label': '[m w.e. $a^{-1}$]'},\n",
    "            ax=ax)\n",
    "\n",
    "# Plot elevation:\n",
    "fig = plt.figure(figsize=(10, 2))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "sns.lineplot(gl_per_el.sort_values(ascending=True),\n",
    "             ax=ax,\n",
    "             color='gray',\n",
    "             marker='v')\n",
    "ax.set_xticklabels('', rotation=90)\n",
    "ax.set_ylabel('')\n",
    "\n",
    "fig = plt.figure(figsize=(10, 2))\n",
    "num_gl = data_glamos_.groupby(['GLACIER']).size().sort_values()\n",
    "num_gl.plot(kind='bar', figsize=(15, 5), cmap=cm.devon)\n",
    "plt.title('Number of total measurements per glacier since 1951')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glaciers with pot. radiadation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glaciers with data of potential clear sky radiation\n",
    "# Format to same names as stakes:\n",
    "glDirect = [\n",
    "    re.search(r'xr_direct_(.*?)\\.nc', f).group(1)\n",
    "    for f in os.listdir(path_pcsr + 'csv/')\n",
    "]\n",
    "glDirect.sort()\n",
    "\n",
    "restgl = Diff(list(glDirect), list(data_glamos.GLACIER.unique()))\n",
    "restgl.sort()\n",
    "print('Glaciers with potential clear sky radiation data:\\n', glDirect)\n",
    "print('Number of glaciers:', len(glDirect))\n",
    "print('Glaciers without potential clear sky radiation data:\\n', restgl)\n",
    "\n",
    "# Filter out glaciers without data:\n",
    "data_glamos = data_glamos[data_glamos.GLACIER.isin(glDirect)]\n",
    "\n",
    "# Look at the data of the ERA5 dataset:\n",
    "xr.open_dataset(path_ERA5_raw + 'era5_monthly_averaged_data.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of mean mass balance per glacier:\n",
    "# Get the mean mass balance per glacier\n",
    "data_glamos_ = data_glamos.copy()\n",
    "data_glamos_['GLACIER'] = data_glamos['GLACIER'].apply(lambda x: glacierCap[x])\n",
    "\n",
    "mean_mb_per_glacier = data_glamos_.groupby(\n",
    "    ['GLACIER', 'YEAR', 'PERIOD'])['POINT_BALANCE'].mean().reset_index()\n",
    "mean_mb_per_glacier = mean_mb_per_glacier[mean_mb_per_glacier['PERIOD'] ==\n",
    "                                          'annual']\n",
    "\n",
    "matrix = mean_mb_per_glacier.pivot(\n",
    "    index='GLACIER', columns='YEAR',\n",
    "    values='POINT_BALANCE').sort_values(by='GLACIER')\n",
    "\n",
    "# get elevation of glaciers:\n",
    "gl_per_el = data_glamos_.groupby(['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "\n",
    "# Order matrix:\n",
    "matrix = matrix.loc[gl_per_el.index]\n",
    "matrix.index = pd.Categorical(matrix.index,\n",
    "                              categories=matrix.index,\n",
    "                              ordered=True)\n",
    "\n",
    "# Plot heatmap:\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "heatmap = sns.heatmap(\n",
    "    data=matrix,\n",
    "    center=0,\n",
    "    cmap=cm.vik_r,\n",
    "    cbar_kws={'label': '[m w.e. $a^{-1}$]'},\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# Plot elevation:\n",
    "fig = plt.figure(figsize=(10, 2))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "sns.lineplot(gl_per_el.sort_values(ascending=True),\n",
    "             ax=ax,\n",
    "             color='gray',\n",
    "             marker='v')\n",
    "ax.set_xticklabels('', rotation=90)\n",
    "ax.set_ylabel('')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:\n",
    "### Input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "\n",
    "def process_or_load_data(run_flag, data_glamos, paths, cfg, vois_climate,\n",
    "                         vois_topographical):\n",
    "    \"\"\"\n",
    "    Process or load the data based on the RUN flag.\n",
    "    \"\"\"\n",
    "    if run_flag:\n",
    "        logging.info(\"Number of annual and seasonal samples: %d\",\n",
    "                     len(data_glamos))\n",
    "\n",
    "        # Filter data\n",
    "        logging.info(\"Running on %d glaciers:\\n%s\",\n",
    "                     len(data_glamos.GLACIER.unique()),\n",
    "                     data_glamos.GLACIER.unique())\n",
    "\n",
    "        # Create dataset\n",
    "        dataset_gl = mbm.Dataset(cfg=cfg,\n",
    "                                 data=data_glamos,\n",
    "                                 region_name='CH',\n",
    "                                 data_path=paths['csv_path'])\n",
    "        logging.info(\"Number of winter and annual samples: %d\",\n",
    "                     len(data_glamos))\n",
    "        logging.info(\"Number of annual samples: %d\",\n",
    "                     len(data_glamos[data_glamos.PERIOD == 'annual']))\n",
    "        logging.info(\"Number of winter samples: %d\",\n",
    "                     len(data_glamos[data_glamos.PERIOD == 'winter']))\n",
    "\n",
    "        # Add climate data\n",
    "        logging.info(\"Adding climate features...\")\n",
    "        try:\n",
    "            dataset_gl.get_climate_features(\n",
    "                climate_data=paths['era5_climate_data'],\n",
    "                geopotential_data=paths['geopotential_data'],\n",
    "                change_units=True)\n",
    "        except Exception as e:\n",
    "            logging.error(\"Failed to add climate features: %s\", e)\n",
    "            return None\n",
    "\n",
    "        # Add radiation data\n",
    "        logging.info(\"Adding potential clear sky radiation...\")\n",
    "        logging.info(\"Shape before adding radiation: %s\",\n",
    "                     dataset_gl.data.shape)\n",
    "        dataset_gl.get_potential_rad(paths['radiation_save_path'])\n",
    "        logging.info(\"Shape after adding radiation: %s\", dataset_gl.data.shape)\n",
    "\n",
    "        # Convert to monthly resolution\n",
    "        logging.info(\"Converting to monthly resolution...\")\n",
    "        dataset_gl.convert_to_monthly(meta_data_columns=cfg.metaData,\n",
    "                                      vois_climate=vois_climate + ['pcsr'],\n",
    "                                      vois_topographical=vois_topographical)\n",
    "\n",
    "        # Create DataLoader\n",
    "        dataloader_gl = mbm.DataLoader(cfg,\n",
    "                                       data=dataset_gl.data,\n",
    "                                       random_seed=cfg.seed,\n",
    "                                       meta_data_columns=cfg.metaData)\n",
    "        logging.info(\"Number of monthly rows: %d\", len(dataloader_gl.data))\n",
    "        logging.info(\"Columns in the dataset: %s\", dataloader_gl.data.columns)\n",
    "\n",
    "        # Save processed data\n",
    "        output_file = os.path.join(paths['csv_path'],\n",
    "                                   'CH_wgms_dataset_monthly_full.csv')\n",
    "        dataloader_gl.data.to_csv(output_file, index=False)\n",
    "        logging.info(\"Processed data saved to: %s\", output_file)\n",
    "\n",
    "        return dataloader_gl\n",
    "    else:\n",
    "        # Load preprocessed data\n",
    "        try:\n",
    "            input_file = os.path.join(paths['csv_path'],\n",
    "                                      'CH_wgms_dataset_monthly_full.csv')\n",
    "            data_monthly = pd.read_csv(input_file)\n",
    "            dataloader_gl = mbm.DataLoader(cfg,\n",
    "                                           data=data_monthly,\n",
    "                                           random_seed=cfg.seed,\n",
    "                                           meta_data_columns=cfg.metaData)\n",
    "            logging.info(\"Loaded preprocessed data.\")\n",
    "            logging.info(\"Number of monthly rows: %d\", len(dataloader_gl.data))\n",
    "            logging.info(\n",
    "                \"Number of annual rows: %d\",\n",
    "                len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'annual']))\n",
    "            logging.info(\n",
    "                \"Number of winter rows: %d\",\n",
    "                len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'winter']))\n",
    "\n",
    "            return dataloader_gl\n",
    "        except FileNotFoundError as e:\n",
    "            logging.error(\"Preprocessed data file not found: %s\", e)\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = False\n",
    "\n",
    "paths = {\n",
    "    'csv_path': path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data': path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data': path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': path_pcsr + 'csv/'\n",
    "}\n",
    "\n",
    "dataloader_gl = process_or_load_data(run_flag=RUN,\n",
    "                                     data_glamos=data_glamos,\n",
    "                                     paths=paths,\n",
    "                                     cfg=cfg,\n",
    "                                     vois_climate=vois_climate,\n",
    "                                     vois_topographical=vois_topographical)\n",
    "data_monthly = dataloader_gl.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction for elevation difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess GloGEM factors\n",
    "path_glogem_factors = os.path.join(path_glogem, 'reference_run_GloGEM2024.csv')\n",
    "glogem_factors = pd.read_csv(path_glogem_factors)\n",
    "glogem_factors.rename(columns={'ID': 'RGIId'}, inplace=True)\n",
    "glogem_factors['RGIId'] = glogem_factors['RGIId'].apply(\n",
    "    lambda x: format_rgi_code(x))\n",
    "\n",
    "c_prec_dic, t_off_dic = {}, {}\n",
    "for gl in data_monthly.GLACIER.unique():\n",
    "    rgi_gl = data_monthly[data_monthly.GLACIER == gl].RGIId.unique()[0]\n",
    "    factor_gl = glogem_factors[glogem_factors.RGIId == rgi_gl]\n",
    "    c_prec_dic[gl] = factor_gl['Cprec'].values[0]\n",
    "    t_off_dic[gl] = factor_gl['T_off'].values[0]\n",
    "\n",
    "# Mean of dic values\n",
    "mean_c_prec = np.max(list(c_prec_dic.values()))\n",
    "mean_t_off = np.max(list(t_off_dic.values()))\n",
    "print(f\"Mean Cprec: {mean_c_prec}, Mean T_off: {mean_t_off}\")\n",
    "\n",
    "# Correct t2m and tp for elevation\n",
    "dataloader_gl.correct_for_elevation()\n",
    "\n",
    "# Plot the distribution of corrected and uncorrected precipitation and temperature\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Precipitation\n",
    "sns.histplot(data_monthly['tp'],\n",
    "             bins=50,\n",
    "             kde=True,\n",
    "             label='tp',\n",
    "             alpha=0.5,\n",
    "             ax=axes[0])\n",
    "sns.histplot(data_monthly['tp_corr'],\n",
    "             bins=50,\n",
    "             kde=True,\n",
    "             label='tp_corr',\n",
    "             alpha=0.5,\n",
    "             ax=axes[0])\n",
    "axes[0].set_title('Distribution of Precipitation')\n",
    "axes[0].set_xlabel('Precipitation (tp / tp_corr)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Temperature\n",
    "sns.histplot(data_monthly['t2m'],\n",
    "             bins=50,\n",
    "             kde=True,\n",
    "             label='t2m',\n",
    "             alpha=0.5,\n",
    "             ax=axes[1])\n",
    "sns.histplot(data_monthly['t2m_corr'],\n",
    "             bins=50,\n",
    "             kde=True,\n",
    "             label='t2m_corr',\n",
    "             alpha=0.5,\n",
    "             ax=axes[1])\n",
    "axes[1].set_title('Distribution of Temperature')\n",
    "axes[1].set_xlabel('Temperature (t2m / t2m_corr)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for correlation analysis\n",
    "df = dataloader_gl.data.copy().dropna()\n",
    "\n",
    "# Define the columns to keep\n",
    "columns_to_keep = [\n",
    "    col for col in df.columns if col not in [\n",
    "        'GLACIER', 'PERIOD', 'YEAR', 'POINT_LON', 'POINT_LAT', 'POINT_BALANCE',\n",
    "        'ALTITUDE_CLIMATE', 'POINT_ELEVATION', 'RGIId', 'POINT_ID', 'ID',\n",
    "        'N_MONTHS', 'MONTHS'\n",
    "    ]\n",
    "]\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Rename columns based on long names (if applicable)\n",
    "df.rename(columns=vois_climate_long_name, inplace=True)\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    mask=mask,\n",
    "    cmap='coolwarm',\n",
    "    vmax=1,\n",
    "    vmin=-1,\n",
    "    center=0,\n",
    "    annot=True,  # Add correlation values\n",
    "    fmt=\".2f\",\n",
    "    square=True,\n",
    "    linewidths=.5,\n",
    "    cbar_kws={\"shrink\": 0.8})\n",
    "\n",
    "# Enhance readability\n",
    "plt.title(\"Feature Intercorrelation Heatmap\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Num. meas per year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of measurements per year:\n",
    "dataloader_gl.data.groupby(['YEAR', 'PERIOD']).size().unstack().plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    figsize=(20, 5),\n",
    "    color=[color_dark_blue, color_pink])\n",
    "plt.title('Number of measurements per year for all glaciers')\n",
    "\n",
    "# Plot winter and annual separately:\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 10), sharey=False)\n",
    "dataloader_gl.data[dataloader_gl.data.PERIOD == 'winter'].groupby(\n",
    "    ['YEAR', 'PERIOD']).size().unstack().plot(kind='bar',\n",
    "                                              ax=axs[0],\n",
    "                                              color=[color_pink],\n",
    "                                              legend=False)\n",
    "axs[0].set_title('Number of winter measurements per year for all glaciers',\n",
    "                 fontsize=24)\n",
    "\n",
    "dataloader_gl.data[dataloader_gl.data.PERIOD == 'annual'].groupby(\n",
    "    ['YEAR', 'PERIOD']).size().unstack().plot(kind='bar',\n",
    "                                              stacked=True,\n",
    "                                              ax=axs[1],\n",
    "                                              color=[color_dark_blue],\n",
    "                                              legend=False)\n",
    "axs[1].set_title('Number of annual measurements per year for all glaciers',\n",
    "                 fontsize=24)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check of variables:\n",
    "df = dataloader_gl.data\n",
    "var_to_plot = ['POINT_BALANCE'] + vois_climate + ['pcsr']\n",
    "df = df[(df.GLACIER == 'corvatsch') & (df.YEAR == 2015)].groupby(\n",
    "    ['MONTHS'])[var_to_plot].mean().reset_index()\n",
    "df['month_nb'] = df.MONTHS.apply(\n",
    "    lambda x: list(month_abbr).index(x.capitalize()))\n",
    "df.sort_values(by='month_nb', inplace=True)\n",
    "fig, ax = plt.subplots(3, 4, figsize=(10, 8))\n",
    "\n",
    "for i, var in enumerate(var_to_plot):\n",
    "    df.plot(x='MONTHS', y=var, marker='o', ax=ax.flatten()[i], legend=False)\n",
    "    if var in vois_climate_long_name.keys():\n",
    "        ax.flatten()[i].set_title(vois_climate_long_name[var], fontsize=12)\n",
    "    else:\n",
    "        ax.flatten()[i].set_title(var, fontsize=12)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the topo variables:\n",
    "df = dataloader_gl.data\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15, 6))\n",
    "for i, var in enumerate(vois_topographical + ['ELEVATION_DIFFERENCE']):\n",
    "    ax = axs.flatten()[i]\n",
    "    sns.histplot(df[var], ax=ax, kde=True)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_title(var)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocking on glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_glaciers = [\n",
    "    'tortin', 'plattalva', 'sanktanna', 'schwarzberg', 'hohlaub', 'pizol',\n",
    "    'corvatsch', 'tsanfleuron', 'forno'\n",
    "]\n",
    "\n",
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in test_glaciers]\n",
    "\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(test_glaciers)]\n",
    "print('Size of test data:', len(data_test))\n",
    "\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of train data:', len(data_train))\n",
    "\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "# Number of annual versus winter measurements:\n",
    "print('Train:')\n",
    "print('Number of winter and annual samples:', len(data_train))\n",
    "print('Number of annual samples:',\n",
    "      len(data_train[data_train.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_train[data_train.PERIOD == 'winter']))\n",
    "\n",
    "# Same for test\n",
    "data_test_annual = data_test[data_test.PERIOD == 'annual']\n",
    "data_test_winter = data_test[data_test.PERIOD == 'winter']\n",
    "\n",
    "print('Test:')\n",
    "print('Number of winter and annual samples:', len(data_test))\n",
    "print('Number of annual samples:', len(data_test_annual))\n",
    "print('Number of winter samples:', len(data_test_winter))\n",
    "\n",
    "print('Total:')\n",
    "print('Number of monthly rows:', len(dataloader_gl.data))\n",
    "print('Number of annual rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'annual']))\n",
    "print('Number of winter rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'winter']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap annual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHeatmap(test_glaciers, data_glamos, glacierCap, period='annual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap winter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHeatmap(test_glaciers, data_glamos, glacierCap, period='winter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CV splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits, test_set, train_set = getCVSplits(dataloader_gl,\n",
    "                                          test_split_on='GLACIER',\n",
    "                                          test_splits=test_glaciers,\n",
    "                                          random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))\n",
    "visualiseSplits(test_set['y'], train_set['y'], splits)\n",
    "\n",
    "visualiseInputs(train_set, test_set, vois_climate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distributions of test glaciers:\n",
    "f, ax = plt.subplots(len(test_glaciers),\n",
    "                     len(vois_climate) + 3,\n",
    "                     figsize=(16, 10),\n",
    "                     sharey='row',\n",
    "                     sharex='col')\n",
    "\n",
    "for i, test_gl in enumerate(test_glaciers):\n",
    "    test_df_gl = test_set['df_X'][test_set['df_X'].GLACIER == test_gl]\n",
    "    test_df_gl['POINT_BALANCE'].plot.hist(ax=ax[i, 0],\n",
    "                                          color=color_dark_blue,\n",
    "                                          alpha=0.6,\n",
    "                                          density=False)\n",
    "    ax[i, 0].set_title('PMB')\n",
    "    ax[i, 0].set_ylabel(test_gl)\n",
    "    ax[i, 0].set_xlabel('[m w.e.]')\n",
    "    test_df_gl['ELEVATION_DIFFERENCE'].plot.hist(ax=ax[i, 1],\n",
    "                                                 color=color_dark_blue,\n",
    "                                                 alpha=0.6,\n",
    "                                                 density=False)\n",
    "    ax[i, 1].set_title('ELV_DIFF]')\n",
    "    ax[i, 1].set_xlabel('[m]')\n",
    "\n",
    "    for j, voi_clim in enumerate(vois_climate + ['pcsr']):\n",
    "        ax[i, 2 + j].set_title(voi_clim)\n",
    "        test_df_gl[voi_clim].plot.hist(ax=ax[i, 2 + j],\n",
    "                                       color=color_dark_blue,\n",
    "                                       alpha=0.6,\n",
    "                                       density=False)\n",
    "        ax[i, 2 + j].set_xlabel(vois_units[voi_clim])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,\n",
    "                     len(test_glaciers),\n",
    "                     figsize=(20, 5),\n",
    "                     sharey='row',\n",
    "                     sharex='row')\n",
    "\n",
    "for i, test_gl in enumerate(test_glaciers):\n",
    "    test_df_gl = test_set['df_X'][test_set['df_X'].GLACIER == test_gl]\n",
    "    test_df_gl.POINT_ELEVATION.plot.hist(color=color_dark_blue,\n",
    "                                         alpha=0.5,\n",
    "                                         density=False,\n",
    "                                         ax=ax[i])\n",
    "    # add vertical line for altitude climate\n",
    "    alt_climate = test_df_gl.ALTITUDE_CLIMATE.mean()\n",
    "    ax[i].axvline(x=alt_climate,\n",
    "                  color='red',\n",
    "                  linestyle='--',\n",
    "                  label='Altitude climate')\n",
    "    ax[i].set_xlabel('Elevation [m]')\n",
    "    ax[i].legend()\n",
    "    ax[i].set_title(test_gl)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of measurements per year:\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 10))\n",
    "data_test.groupby(['YEAR', 'PERIOD']).size().unstack().plot(\n",
    "    kind='bar', stacked=True, color=[color_dark_blue, color_pink], ax=ax[0])\n",
    "ax[0].set_title('Number of measurements per year for test glaciers')\n",
    "\n",
    "# Number of measurements per year:\n",
    "data_train.groupby(['YEAR', 'PERIOD']).size().unstack().plot(\n",
    "    kind='bar', stacked=True, color=[color_dark_blue, color_pink], ax=ax[1])\n",
    "ax[1].set_title('Number of measurements per year for train glaciers')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "# For each of the XGBoost parameter, define the grid range\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, 6, 7, 8],\n",
    "    'n_estimators':\n",
    "    [50, 100, 200, 300, 400, 500, 600,\n",
    "     700],  # number of trees (too many = overfitting, too few = underfitting)\n",
    "    'learning_rate': [0.01, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "}\n",
    "\n",
    "param_init = {}\n",
    "param_init['device'] = 'cuda:0'\n",
    "param_init['tree_method'] = 'hist'\n",
    "param_init[\"random_state\"] = cfg.seed\n",
    "param_init[\"n_jobs\"] = cfg.numJobs\n",
    "\n",
    "CORRECTED_VARS = True\n",
    "if CORRECTED_VARS:\n",
    "    vois_climate = [\n",
    "        't2m_corr', 'tp_corr', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10',\n",
    "        'v10'\n",
    "    ]\n",
    "else:\n",
    "    vois_climate = [\n",
    "        't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "    ]\n",
    "# vois_topographical = [\n",
    "#     \"aspect_sgi\",\n",
    "#     \"slope_sgi\",\n",
    "#     \"hugonnet_dhdt\",\n",
    "#     \"consensus_ice_thickness\",\n",
    "#     \"millan_v\",\n",
    "# ]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    # \"consensus_ice_thickness\",\n",
    "    # \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Feature columns:\n",
    "feature_columns = [\n",
    "    'ELEVATION_DIFFERENCE'\n",
    "] + list(vois_climate) + list(vois_topographical) + ['pcsr']\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "df_X_train_subset = train_set['df_X'][all_columns]\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of testing dataset:', test_set['df_X'][all_columns].shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "RUN = False\n",
    "if RUN:\n",
    "    # Create a CustomXGBoostRegressor instance\n",
    "    custom_xgboost = mbm.models.CustomXGBoostRegressor(cfg, **param_init)\n",
    "    custom_xgboost.randomsearch(\n",
    "        parameters=param_grid,\n",
    "        n_iter=45,\n",
    "        splits=splits,\n",
    "        features=df_X_train_subset,\n",
    "        targets=train_set['y'],\n",
    "    )\n",
    "\n",
    "    # custom_xgboost.gridsearch(\n",
    "    #     parameters=param_grid,\n",
    "    #     splits=splits,\n",
    "    #     features=df_X_train_subset,\n",
    "    #     targets=train_set['y'],\n",
    "    # )\n",
    "\n",
    "    # save best model\n",
    "    custom_xgboost.save_model(f'xgb_gl_split_sgi_corr.pkl')\n",
    "else:\n",
    "    # read model\n",
    "    custom_xgboost = mbm.models.CustomXGBoostRegressor(cfg)\n",
    "    custom_xgboost.load_model(\n",
    "        f'xgb_gl_split_sgi_corr.pkl')  # model with SGI aspect&slope\n",
    "\n",
    "# Get best parameters and estimator\n",
    "best_params = custom_xgboost.param_search.best_params_\n",
    "best_estimator = custom_xgboost.param_search.best_estimator_\n",
    "print(\"Best parameters:\\n\", best_params)\n",
    "print(\"Best score:\\n\", custom_xgboost.param_search.best_score_)\n",
    "\n",
    "# Make predictions on test:\n",
    "# Set to CPU for predictions:\n",
    "best_estimator_cpu = best_estimator.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on test\n",
    "features_test, metadata_test = best_estimator_cpu._create_features_metadata(test_set['df_X'][all_columns])\n",
    "y_pred = best_estimator_cpu.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = best_estimator_cpu.aggrPredict(metadata_test, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = best_estimator_cpu.score(test_set['df_X'][all_columns],\n",
    "                                 test_set['y'])  # negative\n",
    "print('Overall score:', np.abs(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualiseValPreds(best_estimator, splits, train_set, feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotGridSearchScore(cv_results_=custom_xgboost.param_search.cv_results_, lossType=cfg.loss)\n",
    "plotGridSearchParams(custom_xgboost.param_search.cv_results_, param_grid, lossType=cfg.loss)\n",
    "plotGridSearchParams(custom_xgboost.param_search.cv_results_, param_grid, lossType=cfg.loss, N=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIPlot(best_estimator, feature_columns, vois_climate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions of best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test:\n",
    "# Set to CPU for predictions:\n",
    "best_estimator_cpu = best_estimator.set_params(device='cpu')\n",
    "\n",
    "features_test, metadata_test = best_estimator_cpu._create_features_metadata(test_set['df_X'][all_columns])\n",
    "y_pred = best_estimator_cpu.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "y_pred_agg = best_estimator_cpu.aggrPredict(metadata_test, features_test)\n",
    "grouped_ids = getDfAggregatePred(test_set, y_pred_agg, all_columns)\n",
    "PlotPredictions(grouped_ids, y_pred, metadata_test, test_set,\n",
    "                best_estimator_cpu)\n",
    "plt.suptitle(f'XGBoost tested on {test_glaciers}', fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions of custom parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_params = {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 800}\n",
    "\n",
    "# Feature columns:\n",
    "feature_columns = [\n",
    "    'ELEVATION_DIFFERENCE'\n",
    "] + list(vois_climate) + list(vois_topographical) + ['pcsr']\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "df_X_train_subset = train_set['df_X'][all_columns]\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of testing dataset:', test_set['df_X'][all_columns].shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "params = {**param_init, **custom_params}\n",
    "print(params)\n",
    "custom_model = mbm.models.CustomXGBoostRegressor(cfg, **params)\n",
    "\n",
    "# Fit on train data:\n",
    "custom_model.fit(train_set['df_X'][all_columns], train_set['y'])\n",
    "\n",
    "# Make predictions on test\n",
    "custom_model = custom_model.set_params(device='cpu')\n",
    "features_test, metadata_test = custom_model._create_features_metadata(test_set['df_X'][all_columns])\n",
    "y_pred = custom_model.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = custom_model.aggrPredict(metadata_test, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = custom_model.score(test_set['df_X'][all_columns],\n",
    "                           test_set['y'])  # negative\n",
    "print('Overall score:', np.abs(score))\n",
    "\n",
    "grouped_ids = getDfAggregatePred(test_set, y_pred_agg, all_columns)\n",
    "PlotPredictions(grouped_ids, y_pred, metadata_test, test_set, custom_model)\n",
    "plt.suptitle(f'MBM tested on {test_glaciers}', fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate predictions to annual or winter:\n",
    "PlotIndividualGlacierPredVsTruth(grouped_ids, figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIPlot(custom_model, feature_columns, vois_climate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrapolate in space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geodetic MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read geodetic MB:\n",
    "geodeticMB = pd.read_csv(path_geodetic_MB_glamos + 'dV_DOI2024_allcomb.csv')\n",
    "\n",
    "# filter to glaciers\n",
    "all_gl = dataloader_gl.data.GLACIER.unique()\n",
    "\n",
    "# get rgi of those glaciers:\n",
    "rgi_gl = data_glamos[data_glamos.GLACIER.isin(all_gl)].RGIId.unique()\n",
    "sgi_gl = [\n",
    "    rgi_df[rgi_df['rgi_id.v6'] == rgi]['sgi-id'].values[0] for rgi in rgi_gl\n",
    "]\n",
    "geodeticMB = geodeticMB[geodeticMB['SGI-ID'].isin(sgi_gl)]\n",
    "\n",
    "# Add glacierName to geodeticMB\n",
    "# based  on SGI-ID\n",
    "glacierNames = [\n",
    "    rgi_df[rgi_df['sgi-id'] == sgi_id].index[0]\n",
    "    for sgi_id in geodeticMB['SGI-ID'].values\n",
    "]\n",
    "geodeticMB['glacierName'] = glacierNames\n",
    "\n",
    "# replace claridenL by clariden\n",
    "geodeticMB['glacierName'] = geodeticMB['glacierName'].apply(\n",
    "    lambda x: 'clariden' if x == 'claridenL' else x)\n",
    "\n",
    "# get years per glacier\n",
    "years_start_per_gl = geodeticMB.groupby(\n",
    "    'glacierName')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodeticMB.groupby('glacierName')['A_end'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "periods_per_glacier = defaultdict(list)\n",
    "geoMB_per_glacier = defaultdict(list)\n",
    "# Iterate through the DataFrame rows\n",
    "for _, row in geodeticMB.iterrows():\n",
    "    glacierName = row['glacierName']\n",
    "    start_year = row['Astart']\n",
    "    end_year = row['A_end']\n",
    "    geoMB = row['Bgeod']\n",
    "\n",
    "    # Append the (start, end) tuple to the glacier's list\n",
    "    periods_per_glacier[glacierName].append((start_year, end_year))\n",
    "    geoMB_per_glacier[glacierName].append(geoMB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glacier wide MB: \n",
    "Compute 2D fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of glaciers to compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glDirect = [\n",
    "    re.search(r'xr_direct_(.*?)\\.nc', f).group(1)\n",
    "    for f in os.listdir(path_pcsr + 'csv/')\n",
    "]\n",
    "data_glamos = data_glamos[data_glamos.GLACIER.isin(glDirect)]\n",
    "glacier_list = list(data_glamos.GLACIER.unique())\n",
    "print('Number of glaciers with pcsr:', len(glacier_list))\n",
    "\n",
    "satellite_glaciers = [\n",
    "    'adler', 'aletsch', 'allalin', 'basodino', 'clariden', 'findelen', 'gries',\n",
    "    'hohlaub', 'limmern', 'oberaar', 'plattalva', 'rhone', 'sanktanna',\n",
    "    'schwarzbach', 'schwarzberg'\n",
    "]\n",
    "geodetic_glaciers = geodeticMB.glacierName.unique()\n",
    "print('Number of glaciers with geodetic MB:', len(geodetic_glaciers))\n",
    "\n",
    "# Intersection of both\n",
    "common_glaciers = list(set(geodetic_glaciers) & set(glacier_list))\n",
    "print('Number of common glaciers:', len(common_glaciers))\n",
    "\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area()\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list, gl_area):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "glacier_list = sort_by_area(common_glaciers, gl_area)\n",
    "glacier_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLAMOS grids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single glacier file\n",
    "def process_glacier_file(xgb_model, glacier_name, file_name, path_save_glw,\n",
    "                         path_xr_grids, all_columns):\n",
    "    try:\n",
    "        year = int(file_name.split('_')[2].split('.')[0])\n",
    "        file_path = os.path.join(path_glacier_grid_glamos, glacier_name,\n",
    "                                 file_name)\n",
    "\n",
    "        # Load and preprocess glacier grid data\n",
    "        df_grid_monthly = pd.read_csv(file_path)\n",
    "        df_grid_monthly = correct_vars_grid(df_grid_monthly)\n",
    "        df_grid_monthly.rename(columns={\n",
    "            'aspect': 'aspect_sgi',\n",
    "            'slope': 'slope_sgi'\n",
    "        },\n",
    "                               inplace=True)\n",
    "        df_grid_monthly['POINT_ELEVATION'] = df_grid_monthly['topo']\n",
    "        df_grid_monthly.drop_duplicates(inplace=True)\n",
    "\n",
    "        # Keep only necessary columns, avoiding missing columns issues\n",
    "        df_grid_monthly = df_grid_monthly[[\n",
    "            col for col in all_columns if col in df_grid_monthly.columns\n",
    "        ]]\n",
    "\n",
    "        # Compute cumulative SMB predictions\n",
    "        df_grid_monthly = xgb_model.cumulative_pred(df_grid_monthly)\n",
    "\n",
    "        # Generate predictions\n",
    "        pred_annual = GlacierWidePred(xgb_model,\n",
    "                                      df_grid_monthly[all_columns],\n",
    "                                      type_pred='annual')\n",
    "        pred_winter = GlacierWidePred(xgb_model,\n",
    "                                      df_grid_monthly[all_columns],\n",
    "                                      type_pred='winter')\n",
    "\n",
    "        # Filter results for the current year\n",
    "        pred_y_annual = pred_annual[pred_annual.YEAR == year].drop(\n",
    "            columns=['YEAR'], errors='ignore')\n",
    "        pred_y_winter = pred_winter[pred_winter.YEAR == year].drop(\n",
    "            columns=['YEAR'], errors='ignore')\n",
    "\n",
    "        # Load glacier DEM\n",
    "        dem_path = os.path.join(path_xr_grids, f\"{glacier_name}_{year}.nc\")\n",
    "        if not os.path.exists(dem_path):\n",
    "            print(\n",
    "                f\"DEM file not found for {glacier_name} ({year}), skipping...\")\n",
    "            return\n",
    "\n",
    "        ds = xr.open_dataset(dem_path)\n",
    "\n",
    "        # Save predictions\n",
    "        save_predictions(pred_y_annual, ds, glacier_name, year, \"annual\",\n",
    "                         path_save_glw)\n",
    "        save_predictions(pred_y_winter, ds, glacier_name, year, \"winter\",\n",
    "                         path_save_glw)\n",
    "\n",
    "        # Save monthly grids\n",
    "        save_monthly_predictions(df_grid_monthly, ds, glacier_name, year,\n",
    "                                 path_save_glw)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {glacier_name} ({year}): {e}\")\n",
    "\n",
    "\n",
    "# Function to save predictions\n",
    "def save_predictions(pred_df, ds, glacier_name, year, season, path_save_glw):\n",
    "    geoData = mbm.GeoData(pred_df)\n",
    "    geoData.pred_to_xr(ds, pred_var='pred', source_type='sgi')\n",
    "    save_path = os.path.join(path_save_glw, glacier_name)\n",
    "    geoData.save_arrays(f\"{glacier_name}_{year}_{season}.nc\",\n",
    "                        path=save_path,\n",
    "                        proj_type='wgs84')\n",
    "\n",
    "\n",
    "# Function to save monthly grids\n",
    "def save_monthly_predictions(df, ds, glacier_name, year, path_save_glw):\n",
    "    for month, month_nb in cfg.month_abbr_hydr.items():\n",
    "        if month in {'sep_', 'oct', 'nov', 'dec', 'jan', 'feb'}:\n",
    "            continue\n",
    "\n",
    "        df_month = df[df['MONTHS'] == month].groupby('ID').agg({\n",
    "            'YEAR':\n",
    "            'mean',\n",
    "            'POINT_LAT':\n",
    "            'mean',\n",
    "            'POINT_LON':\n",
    "            'mean',\n",
    "            'pred':\n",
    "            'mean',\n",
    "            'cum_pred':\n",
    "            'mean'\n",
    "        }).drop(columns=['YEAR'], errors='ignore')\n",
    "\n",
    "        geoData = mbm.GeoData(df_month)\n",
    "        geoData.pred_to_xr(ds, pred_var='cum_pred', source_type='sgi')\n",
    "        save_path = os.path.join(path_save_glw, glacier_name)\n",
    "        geoData.save_arrays(f\"{glacier_name}_{year}_{month_nb}.nc\",\n",
    "                            path=save_path,\n",
    "                            proj_type='wgs84')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With OGGM vars:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Corrected T & P:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "path_glacier_grid_glamos = '../../../data/GLAMOS/topo/gridded_topo_inputs/GLAMOS_grid_with_oggm_vars/'\n",
    "# path_save_glw = 'results/nc/glamos/OGGM_vars/TP_corr_T2m_corr/'\n",
    "path_save_glw = 'results/nc/glamos/OGGM_vars/only_hugonnet/'\n",
    "path_xr_grids = '../../../data/GLAMOS/topo/GLAMOS_DEM/xr_masked_grids/'  # GLAMOS DEMs\n",
    "\n",
    "emptyfolder(path_save_glw)\n",
    "\n",
    "# Feature columns\n",
    "vois_climate = [\n",
    "    't2m_corr', 'tp_corr', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "feature_columns = [\n",
    "    'ELEVATION_DIFFERENCE'\n",
    "] + list(vois_climate) + list(vois_topographical) + ['pcsr']\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "print('Running for feature columns:', all_columns)\n",
    "\n",
    "for glacier_name in glacier_list:\n",
    "    glacier_path = os.path.join(path_glacier_grid_glamos, glacier_name)\n",
    "\n",
    "    if not os.path.exists(glacier_path):\n",
    "        print(f\"Folder not found for {glacier_name}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    glacier_files = sorted(\n",
    "        [f for f in os.listdir(glacier_path) if glacier_name in f])\n",
    "\n",
    "    print(f\"Processing {glacier_name} ({len(glacier_files)} files)\")\n",
    "\n",
    "    for file_name in tqdm(glacier_files,\n",
    "                          desc=f\"Processing {glacier_name}\",\n",
    "                          leave=False):\n",
    "        process_glacier_file(custom_model, glacier_name, file_name,\n",
    "                             path_save_glw, path_xr_grids, all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open xarray\n",
    "xr.open_dataset(path_save_glw +\n",
    "                'aletsch/aletsch_2008_annual.nc').pred_masked.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
