{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from calendar import month_abbr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "from cartopy import crs as ccrs, feature as cfeature\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.enums import Resampling as RResampling\n",
    "import numpy as np\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import geopandas as gpd\n",
    "from matplotlib.patches import Wedge, Patch\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import pickle \n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'\n",
    "\n",
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    # \"aspect\", # OGGM\n",
    "    # \"slope\", # OGGM\n",
    "    \"aspect_sgi\",  # SGI\n",
    "    \"slope_sgi\",  # SGI\n",
    "    \"hugonnet_dhdt\",  # OGGM\n",
    "    \"consensus_ice_thickness\",  # OGGM\n",
    "    \"millan_v\",  # OGGM\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "# Capitalize glacier names:\n",
    "glacierCap = {}\n",
    "for gl in data_glamos['GLACIER'].unique():\n",
    "    if isinstance(gl, str):  # Ensure the glacier name is a string\n",
    "        if gl.lower() == 'claridenu':\n",
    "            glacierCap[gl] = 'Clariden_U'\n",
    "        elif gl.lower() == 'claridenl':\n",
    "            glacierCap[gl] = 'Clariden_L'\n",
    "        else:\n",
    "            glacierCap[gl] = gl.capitalize()\n",
    "    else:\n",
    "        print(f\"Warning: Non-string glacier name encountered: {gl}\")\n",
    "\n",
    "data_glamos.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glacier outlines:\n",
    "glacier_outline_sgi = gpd.read_file(\n",
    "    os.path.join(cfg.dataPath, path_SGI_topo, 'inventory_sgi2016_r2020',\n",
    "                 'SGI_2016_glaciers_copy.shp'))  # Load the shapefile\n",
    "glacier_outline_rgi = gpd.read_file(cfg.dataPath + path_rgi_outlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of measurements per glacier:\n",
    "glacier_info = data_glamos.groupby('GLACIER').size().sort_values(\n",
    "    ascending=False).reset_index()\n",
    "glacier_info.rename(columns={0: 'Nb. measurements'}, inplace=True)\n",
    "glacier_info.set_index('GLACIER', inplace=True)\n",
    "\n",
    "glacier_loc = data_glamos.groupby('GLACIER')[['POINT_LAT', 'POINT_LON']].mean()\n",
    "\n",
    "glacier_info = glacier_loc.merge(glacier_info, on='GLACIER')\n",
    "\n",
    "glacier_period = data_glamos.groupby(['GLACIER', 'PERIOD'\n",
    "                                      ]).size().unstack().fillna(0).astype(int)\n",
    "\n",
    "glacier_info = glacier_info.merge(glacier_period, on='GLACIER')\n",
    "\n",
    "test_glaciers = [\n",
    "    'tortin', 'plattalva', 'sanktanna', 'schwarzberg', 'hohlaub', 'pizol',\n",
    "    'corvatsch', 'tsanfleuron', 'forno'\n",
    "]\n",
    "\n",
    "glacier_info['Train/Test glacier'] = glacier_info.apply(\n",
    "    lambda x: 'Test' if x.name in test_glaciers else 'Train', axis=1)\n",
    "glacier_info.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign glaciers to river basin names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load RGI glacier IDs ===\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids)\n",
    "rgi_df.columns = rgi_df.columns.str.strip()\n",
    "rgi_df = rgi_df.sort_values(by='short_name').set_index('short_name')\n",
    "\n",
    "# === Load SGI region geometries ===\n",
    "SGI_regions = gpd.read_file(\n",
    "    os.path.join(cfg.dataPath, path_SGI_topo, 'inventory_sgi2016_r2020',\n",
    "                 'sgi_regions.geojson'))\n",
    "\n",
    "# Clean object columns\n",
    "SGI_regions[SGI_regions.select_dtypes(include='object').columns] = \\\n",
    "    SGI_regions.select_dtypes(include='object').apply(lambda col: col.str.strip())\n",
    "\n",
    "SGI_regions = SGI_regions.drop_duplicates().dropna()\n",
    "SGI_regions = SGI_regions.set_index('pk_sgi_region')\n",
    "\n",
    "# === Map to Level 0 river basins ===\n",
    "catchment_lv0 = {\n",
    "    'A': 'Rhine',\n",
    "    'B': 'Rhone',\n",
    "    'C': 'Po',\n",
    "    'D': 'Adige',\n",
    "    'E': 'Danube'\n",
    "}\n",
    "rgi_df['rvr_lv0'] = rgi_df['sgi-id'].str[0].map(catchment_lv0)\n",
    "\n",
    "\n",
    "# === Map to Level 1 river basins using SGI regions ===\n",
    "def get_river_basin(sgi_id):\n",
    "    key = sgi_id.split('-')[0]\n",
    "    if key not in SGI_regions.index:\n",
    "        return None\n",
    "    basin = SGI_regions.loc[key, 'river_basin_name']\n",
    "    if isinstance(basin, pd.Series):\n",
    "        return basin.dropna().unique()[0] if not basin.dropna().empty else None\n",
    "    return basin if pd.notna(basin) else None\n",
    "\n",
    "\n",
    "rgi_df['rvr_lv1'] = rgi_df['sgi-id'].apply(get_river_basin)\n",
    "\n",
    "# Final formatting\n",
    "rgi_df = rgi_df.reset_index().rename(columns={\n",
    "    'short_name': 'GLACIER'\n",
    "}).set_index('GLACIER')\n",
    "rgi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_info = glacier_info.merge(rgi_df[['rvr_lv0', 'rvr_lv1']],\n",
    "                                  on='GLACIER',\n",
    "                                  how='left')\n",
    "glacier_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro & methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geoplots:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sqrt scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the original raster\n",
    "tif_name = \"landesforstinventar-vegetationshoehenmodell_relief_sentinel_2024_2056.tif\"\n",
    "tif_path = os.path.join(cfg.dataPath, 'GLAMOS/RGI/', tif_name)\n",
    "\n",
    "# Desired output resolution (in degrees)\n",
    "# Approx. 100 m in degrees: ~0.0009 deg\n",
    "target_res = 0.0009\n",
    "output_crs = \"EPSG:4326\"  # WGS84\n",
    "\n",
    "with rasterio.open(tif_path) as src:\n",
    "    # Calculate transform and shape with coarser resolution\n",
    "    transform, width, height = calculate_default_transform(\n",
    "        src.crs,\n",
    "        output_crs,\n",
    "        src.width,\n",
    "        src.height,\n",
    "        *src.bounds,\n",
    "        resolution=target_res)\n",
    "\n",
    "    # Set up destination array and metadata\n",
    "    kwargs = src.meta.copy()\n",
    "    kwargs.update({\n",
    "        'crs': output_crs,\n",
    "        'transform': transform,\n",
    "        'width': width,\n",
    "        'height': height\n",
    "    })\n",
    "\n",
    "    # Prepare empty destination array\n",
    "    destination = np.empty((height, width), dtype=src.dtypes[0])\n",
    "\n",
    "    # Reproject with coarsening\n",
    "    reproject(\n",
    "        source=rasterio.band(src, 1),\n",
    "        destination=destination,\n",
    "        src_transform=src.transform,\n",
    "        src_crs=src.crs,\n",
    "        dst_transform=transform,\n",
    "        dst_crs=output_crs,\n",
    "        resampling=Resampling.\n",
    "        average  # average to reduce noise when downsampling\n",
    "    )\n",
    "\n",
    "    extent = [\n",
    "        transform[2], transform[2] + transform[0] * width,\n",
    "        transform[5] + transform[4] * height, transform[5]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1. Preprocessing ----\n",
    "# Square-root scaling of number of measurements\n",
    "glacier_info['sqrt_size'] = np.sqrt(glacier_info['Nb. measurements'])\n",
    "\n",
    "# Cache dataset-wide min and max\n",
    "sqrt_min = glacier_info['sqrt_size'].min()\n",
    "sqrt_max = glacier_info['sqrt_size'].max()\n",
    "\n",
    "# Define the desired marker size range in points^2\n",
    "sizes = (30, 1500)  # min and max scatter size\n",
    "\n",
    "\n",
    "# Function to scale individual values consistently\n",
    "def scaled_size(val, min_out=sizes[0], max_out=sizes[1]):\n",
    "    sqrt_val = np.sqrt(val)\n",
    "    if sqrt_max == sqrt_min:\n",
    "        return (min_out + max_out) / 2\n",
    "    return min_out + (max_out - min_out) * ((sqrt_val - sqrt_min) /\n",
    "                                            (sqrt_max - sqrt_min))\n",
    "\n",
    "\n",
    "# Apply scaling to full dataset for the actual plot\n",
    "glacier_info['scaled_size'] = glacier_info['Nb. measurements'].apply(\n",
    "    scaled_size)\n",
    "\n",
    "# ---- 2. Create figure and base map ----\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "\n",
    "#latN, latS = 48, 45.8\n",
    "latN, latS = 47.1, 45.8\n",
    "lonW, lonE = 5.8, 10.5\n",
    "projPC = ccrs.PlateCarree()\n",
    "ax2 = plt.axes(projection=projPC)\n",
    "ax2.set_extent([lonW, lonE, latS, latN], crs=ccrs.Geodetic())\n",
    "\n",
    "ax2.add_feature(cfeature.COASTLINE)\n",
    "ax2.add_feature(cfeature.LAKES)\n",
    "ax2.add_feature(cfeature.RIVERS)\n",
    "# ax2.add_feature(cfeature.BORDERS, linestyle='-', linewidth=1)\n",
    "\n",
    "# Add the image to the cartopy map\n",
    "\n",
    "masked_destination = np.ma.masked_where(destination == 0, destination)\n",
    "cmap = plt.cm.gray\n",
    "cmap.set_bad(color='white')  # Set masked (bad) values to white\n",
    "ax2.imshow(\n",
    "    masked_destination,\n",
    "    origin='upper',\n",
    "    extent=extent,\n",
    "    transform=ccrs.PlateCarree(),  # Assuming raster is in WGS84\n",
    "    cmap=cmap,  # or any other colormap\n",
    "    alpha=0.6,  # transparency\n",
    "    zorder=0)\n",
    "\n",
    "# Glacier outlines\n",
    "glacier_outline_sgi.plot(ax=ax2, transform=projPC, color='black')\n",
    "\n",
    "# ---- 3. Scatterplot ----\n",
    "custom_palette = {'Train': color_dark_blue, 'Test': '#b2182b'}\n",
    "\n",
    "g = sns.scatterplot(\n",
    "    data=glacier_info,\n",
    "    x='POINT_LON',\n",
    "    y='POINT_LAT',\n",
    "    size='scaled_size',\n",
    "    hue='Train/Test glacier',\n",
    "    sizes=sizes,\n",
    "    alpha=0.6,\n",
    "    palette=custom_palette,\n",
    "    transform=projPC,\n",
    "    ax=ax2,\n",
    "    zorder=10,\n",
    "    legend=True  # custom legend added below\n",
    ")\n",
    "\n",
    "# ---- 4. Gridlines ----\n",
    "gl = ax2.gridlines(draw_labels=True,\n",
    "                   linewidth=1,\n",
    "                   color='gray',\n",
    "                   alpha=0.5,\n",
    "                   linestyle='--')\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "gl.xlabel_style = {'size': 16, 'color': 'black'}\n",
    "gl.ylabel_style = {'size': 16, 'color': 'black'}\n",
    "gl.top_labels = gl.right_labels = False\n",
    "\n",
    "# ---- 5. Custom Combined Legend ----\n",
    "\n",
    "# Hue legend handles\n",
    "handles, labels = g.get_legend_handles_labels()\n",
    "expected_labels = list(custom_palette.keys())\n",
    "hue_entries = [(h, l) for h, l in zip(handles, labels) if l in expected_labels]\n",
    "\n",
    "# Size legend values and handles\n",
    "size_values = [30, 100, 1000, 6000]\n",
    "size_handles = [\n",
    "    Line2D(\n",
    "        [],\n",
    "        [],\n",
    "        marker='o',\n",
    "        linestyle='None',\n",
    "        markersize=np.sqrt(scaled_size(val)),  # matplotlib uses radius\n",
    "        markerfacecolor='gray',\n",
    "        alpha=0.6,\n",
    "        label=f'{val}') for val in size_values\n",
    "]\n",
    "\n",
    "# Separator label\n",
    "separator_handle = Patch(facecolor='none',\n",
    "                         edgecolor='none',\n",
    "                         label='Nb. measurements')\n",
    "\n",
    "# Combine all legend entries\n",
    "# combined_handles = [h for h, _ in hue_entries] + [separator_handle] + size_handles\n",
    "# combined_labels = [l for _, l in hue_entries] + ['Nb. measurements'] + [str(v) for v in size_values]\n",
    "\n",
    "# same but without separator\n",
    "combined_handles = [h for h, _ in hue_entries] + size_handles\n",
    "combined_labels = [l for _, l in hue_entries] + [str(v) for v in size_values]\n",
    "\n",
    "# Final legend\n",
    "ax2.legend(combined_handles,\n",
    "           combined_labels,\n",
    "           title='Nb. measurements',\n",
    "           loc='lower right',\n",
    "           frameon=True,\n",
    "           fontsize=18,\n",
    "           title_fontsize=18,\n",
    "           borderpad=1.2,\n",
    "           labelspacing=1.2,\n",
    "           ncol=3)\n",
    "# ax2.set_title('Glacier measurement locations', fontsize = 25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of measurements per year:\n",
    "data_glamos.groupby(['YEAR', 'PERIOD']).count()['POINT_ID'].unstack().plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    figsize=(20, 5),\n",
    "    color=[color_dark_blue, '#abd9e9'])\n",
    "# plt.title('Number of measurements per year for all glaciers', fontsize = 25)\n",
    "# get legend\n",
    "plt.legend(title='Period', fontsize=18, title_fontsize=20, ncol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data(run_flag=RUN,\n",
    "                                     data_glamos=data_glamos,\n",
    "                                     paths=paths,\n",
    "                                     cfg=cfg,\n",
    "                                     vois_climate=vois_climate,\n",
    "                                     vois_topographical=vois_topographical)\n",
    "data_monthly = dataloader_gl.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_glaciers = [\n",
    "    'tortin', 'plattalva', 'sanktanna', 'schwarzberg', 'hohlaub', 'pizol',\n",
    "    'corvatsch', 'tsanfleuron', 'forno'\n",
    "]\n",
    "\n",
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in test_glaciers]\n",
    "\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(test_glaciers)]\n",
    "print('Size of test data:', len(data_test))\n",
    "\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of train data:', len(data_train))\n",
    "\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "# Number of annual versus winter measurements:\n",
    "print('Train:')\n",
    "print('Number of winter and annual samples:', len(data_train))\n",
    "print('Number of annual samples:',\n",
    "      len(data_train[data_train.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_train[data_train.PERIOD == 'winter']))\n",
    "\n",
    "# Same for test\n",
    "data_test_annual = data_test[data_test.PERIOD == 'annual']\n",
    "data_test_winter = data_test[data_test.PERIOD == 'winter']\n",
    "\n",
    "print('Test:')\n",
    "print('Number of winter and annual samples:', len(data_test))\n",
    "print('Number of annual samples:', len(data_test_annual))\n",
    "print('Number of winter samples:', len(data_test_winter))\n",
    "\n",
    "print('Total:')\n",
    "print('Number of monthly rows:', len(dataloader_gl.data))\n",
    "print('Number of annual rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'annual']))\n",
    "print('Number of winter rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'winter']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap annual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHeatmap(test_glaciers, data_glamos, glacierCap, period='annual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get elevation of glaciers:\n",
    "gl_per_el = data_glamos[data_glamos.PERIOD == 'annual'].groupby(\n",
    "    ['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "\n",
    "# Plot elevation:\n",
    "fig = plt.figure(figsize=(10, 2))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "sns.lineplot(gl_per_el.sort_values(ascending=True),\n",
    "             ax=ax,\n",
    "             color='gray',\n",
    "             marker='v')\n",
    "ax.set_xticklabels('', rotation=90)\n",
    "ax.set_ylabel('')\n",
    "ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gl_per_el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_NN.csv')\n",
    "data_monthly = dataloader_gl.data\n",
    "\n",
    "data_monthly['GLWD_ID'] = data_monthly.apply(\n",
    "    lambda x: mbm.data_processing.utils.get_hash(f\"{x.GLACIER}_{x.YEAR}\"),\n",
    "    axis=1)\n",
    "data_monthly['GLWD_ID'] = data_monthly['GLWD_ID'].astype(str)\n",
    "\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))\n",
    "\n",
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "dataloader = mbm.dataloader.DataLoader(cfg, data=data_train)\n",
    "\n",
    "train_itr, val_itr = dataloader.set_train_test_split(test_size=0.2)\n",
    "\n",
    "# Get all indices of the training and valing dataset at once from the iterators. Once called, the iterators are empty.\n",
    "train_indices, val_indices = list(train_itr), list(val_itr)\n",
    "\n",
    "df_X_train = data_train.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Get val set\n",
    "df_X_val = data_train.iloc[val_indices]\n",
    "y_val = df_X_val['POINT_BALANCE'].values\n",
    "\n",
    "features_topo = [\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "    'pcsr',\n",
    "] + list(vois_topographical)\n",
    "\n",
    "feature_columns = features_topo + list(vois_climate)\n",
    "\n",
    "cfg.setFeatures(feature_columns)\n",
    "\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "# Because CH has some extra columns, we need to cut those\n",
    "df_X_train_subset = df_X_train[all_columns]\n",
    "df_X_val_subset = df_X_val[all_columns]\n",
    "df_X_test_subset = test_set['df_X'][all_columns]\n",
    "\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of validation dataset:', df_X_val_subset.shape)\n",
    "print('Shape of testing dataset:', df_X_test_subset.shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "assert all(train_set['df_X'].POINT_BALANCE == train_set['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='valid_loss',\n",
    "    patience=15,\n",
    "    threshold=1e-4,  # Optional: stop only when improvement is very small\n",
    ")\n",
    "\n",
    "lr_scheduler_cb = LRScheduler(policy=ReduceLROnPlateau,\n",
    "                              monitor='valid_loss',\n",
    "                              mode='min',\n",
    "                              factor=0.5,\n",
    "                              patience=5,\n",
    "                              threshold=0.01,\n",
    "                              threshold_mode='rel',\n",
    "                              verbose=True)\n",
    "\n",
    "dataset = dataset_val = None  # Initialized hereafter\n",
    "\n",
    "\n",
    "def my_train_split(ds, y=None, **fit_params):\n",
    "    return dataset, dataset_val\n",
    "\n",
    "\n",
    "# param_init = {'device': 'cuda:0'}\n",
    "param_init = {'device': 'cpu'}  # Use CPU for training\n",
    "nInp = len(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and set to CPU\n",
    "model_filename = \"nn_model_2025-07-08.pt\"  # Replace with actual date if needed\n",
    "\n",
    "# read pickle with params\n",
    "params_filename = \"nn_params_2025-07-08.pkl\"  # Replace with actual date if needed\n",
    "with open(f\"models/{params_filename}\", \"rb\") as f:\n",
    "    custom_params = pickle.load(f)\n",
    "\n",
    "params = custom_params\n",
    "\n",
    "args = {\n",
    "    'module': FlexibleNetwork,\n",
    "    'nbFeatures': nInp,\n",
    "    'module__input_dim': nInp,\n",
    "    'module__dropout': params['module__dropout'],\n",
    "    'module__hidden_layers': params['module__hidden_layers'],\n",
    "    'train_split': my_train_split,\n",
    "    'batch_size': params['batch_size'],\n",
    "    'verbose': 1,\n",
    "    'iterator_train__shuffle': True,\n",
    "    'lr': params['lr'],\n",
    "    'max_epochs': 300,\n",
    "    'optimizer': params['optimizer'],\n",
    "    'optimizer__weight_decay': params['optimizer__weight_decay'],\n",
    "    'module__use_batchnorm': params['module__use_batchnorm'],\n",
    "    'callbacks': [\n",
    "        ('early_stop', early_stop),\n",
    "        ('lr_scheduler', lr_scheduler_cb),\n",
    "    ]\n",
    "}\n",
    "\n",
    "custom_NN_model = mbm.models.CustomNeuralNetRegressor.load_model(\n",
    "    cfg,\n",
    "    model_filename,\n",
    "    **{\n",
    "        **args,\n",
    "        **param_init\n",
    "    },\n",
    ")\n",
    "custom_NN_model = custom_NN_model.set_params(device='cpu')\n",
    "custom_NN_model = custom_NN_model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plot test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_ids_NN, scores_NN, ids_NN, y_pred_NN = evaluate_model_and_group_predictions(\n",
    "    custom_NN_model, df_X_test_subset, test_set['y'], cfg, mbm)\n",
    "\n",
    "months_per_id = test_set['df_X'][all_columns].groupby('ID')['MONTHS'].unique()\n",
    "grouped_ids_NN = grouped_ids_NN.merge(months_per_id, on='ID')\n",
    "\n",
    "scores_annual_NN, scores_winter_NN = compute_seasonal_scores(\n",
    "    grouped_ids_NN, target_col='target', pred_col='pred')\n",
    "fig = plot_predictions_summary(grouped_ids=grouped_ids_NN,\n",
    "                               scores_annual=scores_annual_NN,\n",
    "                               scores_winter=scores_winter_NN,\n",
    "                               predVSTruth=predVSTruth,\n",
    "                               plotMeanPred=plotMeanPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_per_el = data_glamos[data_glamos.PERIOD == 'annual'].groupby(\n",
    "    ['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "\n",
    "test_gl_per_el = gl_per_el[test_glaciers].sort_values().index\n",
    "elvs = gl_per_el[test_glaciers].sort_values()\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(20, 15))\n",
    "\n",
    "PlotIndividualGlacierPredVsTruth(grouped_ids_NN,\n",
    "                                 color_annual=color_dark_blue,\n",
    "                                 color_winter=color_pink,\n",
    "                                 axs=axs,\n",
    "                                 custom_order=test_gl_per_el)\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    test_gl = test_gl_per_el[i]\n",
    "    el = elvs[test_gl]\n",
    "    ax.set_title(f'{test_gl.capitalize()}: {round(el, 2)} m', fontsize=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geodetic mass balance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PREDICTIONS_NN = cfg.dataPath + path_distributed_MB_glamos + 'MBM/glamos_dems_NN_full/'\n",
    "\n",
    "glaciers_in_glamos = os.listdir(PATH_PREDICTIONS_NN)\n",
    "\n",
    "geodetic_mb = get_geodetic_MB(cfg)\n",
    "\n",
    "# get years per glacier\n",
    "years_start_per_gl = geodetic_mb.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodetic_mb.groupby('glacier_name')['Aend'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "periods_per_glacier, geoMB_per_glacier = build_periods_per_glacier(geodetic_mb)\n",
    "\n",
    "# Glaciers with geodetic MB data:\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area(cfg)\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list, gl_area):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "\n",
    "glacier_list = [\n",
    "    f for f in list(periods_per_glacier.keys()) if f in glaciers_in_glamos\n",
    "]\n",
    "glacier_list = sort_by_area(glacier_list, gl_area)\n",
    "print('Number of glaciers:', len(glacier_list))\n",
    "print('Glaciers:', glacier_list)\n",
    "\n",
    "df_all_nn = process_geodetic_mass_balance_comparison(\n",
    "    glacier_list=glacier_list,\n",
    "    path_SMB_GLAMOS_csv=cfg.dataPath + path_SMB_GLAMOS_csv,\n",
    "    periods_per_glacier=periods_per_glacier,\n",
    "    geoMB_per_glacier=geoMB_per_glacier,\n",
    "    gl_area=gl_area,\n",
    "    test_glaciers=test_glaciers,\n",
    "    path_predictions=PATH_PREDICTIONS_NN,  # or another path if needed\n",
    "    cfg=cfg)\n",
    "\n",
    "# Drop rows where any required columns are NaN\n",
    "df_all_nn = df_all_nn.dropna(subset=['Geodetic MB', 'MBM MB'])\n",
    "df_all_nn = df_all_nn.sort_values(by='Area')\n",
    "df_all_nn['GLACIER'] = df_all_nn['GLACIER'].apply(lambda x: x.capitalize())\n",
    "\n",
    "# Compute RMSE and Pearson correlation\n",
    "rmse_nn = mean_squared_error(df_all_nn[\"Geodetic MB\"],\n",
    "                             df_all_nn[\"MBM MB\"],\n",
    "                             squared=False)\n",
    "corr_nn = np.corrcoef(df_all_nn[\"Geodetic MB\"], df_all_nn[\"MBM MB\"])[0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 10))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=df_all_nn,\n",
    "    x=\"Geodetic MB\",\n",
    "    y='MBM MB',\n",
    "    hue='GLACIER',\n",
    "    # size=\"Area\",\n",
    "    # sizes=(10, 1000),\n",
    "    alpha=0.7,\n",
    "    ax=ax,\n",
    "    palette=sns.color_palette(\"hls\", 17))\n",
    "\n",
    "# Identity line (diagonal y=x)\n",
    "# diagonal line\n",
    "pt = (0, 0)\n",
    "ax.axline(pt, slope=1, color=\"grey\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "# Grid and axis labels\n",
    "ax.axvline(0, color=\"grey\", linestyle=\"--\", linewidth=1)\n",
    "ax.axhline(0, color=\"grey\", linestyle=\"--\", linewidth=1)\n",
    "ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "ax.set_xlabel(\"Geodetic MB [m w.e.]\")\n",
    "\n",
    "# RMSE and correlation annotation\n",
    "legend_text = \"\\n\".join(\n",
    "    (r\"$\\mathrm{RMSE}=%.3f$\" % rmse_nn, r\"$\\mathrm{\\rho}=%.3f$\" % corr_nn))\n",
    "props = dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.5)\n",
    "ax.text(0.03,\n",
    "        0.94,\n",
    "        legend_text,\n",
    "        transform=ax.transAxes,\n",
    "        verticalalignment=\"top\",\n",
    "        fontsize=18,\n",
    "        bbox=props)\n",
    "ax.legend([], [], frameon=False)\n",
    "\n",
    "# Adjust legend outside of plot\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles,\n",
    "          labels,\n",
    "          bbox_to_anchor=(1.05, 1),\n",
    "          loc=\"upper left\",\n",
    "          borderaxespad=0.,\n",
    "          ncol=2,\n",
    "          fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example bins in km²\n",
    "bins = [0, 5, 10, 100, np.inf]\n",
    "labels = ['<5', '5–10', '10–100', '>100']\n",
    "\n",
    "df_all_nn['Area_bin'] = pd.cut(df_all_nn['Area'],\n",
    "                               bins=bins,\n",
    "                               labels=labels,\n",
    "                               right=False)\n",
    "\n",
    "# Plot setup\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 6), sharex=True, sharey=True)\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Unique bin labels (ensure consistent order and drop NaN)\n",
    "unique_bins = df_all_nn['Area_bin'].dropna().unique().tolist()\n",
    "\n",
    "for i, area_bin in enumerate(unique_bins[:3]):  # plot only up to 3 bins\n",
    "    ax = axs[i]\n",
    "    df_all_nn_bin = df_all_nn[df_all_nn['Area_bin'] == area_bin]\n",
    "    \n",
    "    sns.scatterplot(\n",
    "        data=df_all_nn_bin,\n",
    "        x=\"Geodetic MB\",\n",
    "        y=\"MBM MB\",\n",
    "        hue=\"GLACIER\",\n",
    "        alpha=0.7,\n",
    "        ax=ax,\n",
    "        palette = sns.color_palette(\"Paired\", len(df_all_nn_bin.GLACIER.unique())),\n",
    "    )\n",
    "\n",
    "    # Grid and axis labels\n",
    "    ax.axvline(0, color=\"grey\", linestyle=\"--\", linewidth=1)\n",
    "    ax.axhline(0, color=\"grey\", linestyle=\"--\", linewidth=1)\n",
    "    ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "    ax.set_xlabel(\"Geodetic MB [m w.e.]\")\n",
    "    ax.set_ylabel(\"MBM MB [m w.e.]\")\n",
    "    ax.set_title(f\"Area: {area_bin} km²\")\n",
    "\n",
    "    # ax.legend(\n",
    "    #     loc=\"lower center\",\n",
    "    #     borderaxespad=0.5,\n",
    "    #     fontsize=12,\n",
    "    #     bbox_to_anchor=(0.5, -0.3),\n",
    "    #     ncol=2\n",
    "    # )\n",
    "    ax.legend([], [], frameon=False)  # Hide legend for individual plots\n",
    "\n",
    "# After plotting, before plt.tight_layout()\n",
    "for ax in axs:\n",
    "    # Get combined x and y limits across both axes\n",
    "    xmin, xmax = ax.get_xlim()\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    min_limit = min(xmin, ymin)\n",
    "    max_limit = max(xmax, ymax)\n",
    "\n",
    "    # Set symmetric limits\n",
    "    ax.set_xlim(min_limit, max_limit)\n",
    "    ax.set_ylim(min_limit, max_limit)\n",
    "\n",
    "    # Now add the identity line\n",
    "    ax.axline((0, 0), slope=1, color=\"grey\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
