{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import massbalancemachine as mbm\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, LineString, Point\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import GroupKFold, KFold, train_test_split, GroupShuffleSplit\n",
    "\n",
    "import cupy as cp\n",
    "import pyproj\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(SEED)\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "path_rgi = '../../../data/GLAMOS/CH_glacier_ids_long.csv'\n",
    "rgi_df = pd.read_csv(path_rgi, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to oggm datapulling notebook:\n",
    "df_pmb_topo = pd.read_csv(path_PMB_GLAMOS_csv + 'CH_wgms_dataset.csv')\n",
    "print('Number of winter and annual samples:', len(df_pmb_topo))\n",
    "print('Number of annual samples:',\n",
    "      len(df_pmb_topo[df_pmb_topo.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_pmb_topo[df_pmb_topo.PERIOD == 'winter']))\n",
    "df_pmb_topo.head(2)\n",
    "\n",
    "# Specify the short names of the climate variables available in the dataset\n",
    "vois_climate = ['t2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str']\n",
    "# voi_topographical = ['aspect', 'slope', 'dis_from_border', 'topo']\n",
    "voi_topographical = ['aspect', 'slope']\n",
    "\n",
    "# take only annual and not winter MB\n",
    "data = pd.read_csv(path_PMB_GLAMOS_csv + 'CH_wgms_dataset.csv')\n",
    "data = data[data.PERIOD == 'annual']\n",
    "\n",
    "# change mm w.e. to m w.e.\n",
    "data['POINT_BALANCE'] = data['POINT_BALANCE'] / 1000\n",
    "\n",
    "# Provide the column name for the column that has the RGI IDs for each of the stakes\n",
    "dataset = mbm.Dataset(data=data,\n",
    "                      region_name='CH',\n",
    "                      data_path=path_PMB_GLAMOS_csv)\n",
    "\n",
    "# Add climate data:\n",
    "# Specify the files of the climate data, that will be matched with the coordinates of the stake data\n",
    "era5_climate_data = path_ERA5_raw + 'era5_monthly_averaged_data.nc'\n",
    "geopotential_data = path_ERA5_raw + 'era5_geopotential_pressure.nc'\n",
    "\n",
    "# Match the climate features, from the ERA5Land netCDF file, for each of the stake measurement dataset\n",
    "dataset.get_climate_features(climate_data=era5_climate_data,\n",
    "                             geopotential_data=geopotential_data,\n",
    "                             change_units=True)\n",
    "dataset.data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot temperature and precipitation for all stakes:\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "df = dataset.data\n",
    "year = 2006\n",
    "\n",
    "for i, var in enumerate(vois_climate):\n",
    "    temp = df[df.YEAR == year][[col for col in df.columns if var in col]]\n",
    "    mean = temp.mean().values\n",
    "    std = temp.std().values\n",
    "    ax = plt.subplot(4, 2, i + 1)\n",
    "    ax.fill_between(\n",
    "        temp.columns,\n",
    "        mean - std,\n",
    "        mean + std,\n",
    "        color=\"orange\",\n",
    "        alpha=0.3,\n",
    "    )\n",
    "    # put x axis at 45 degrees\n",
    "    ax.set_xticklabels(temp.columns, rotation=90)\n",
    "    ax.plot(temp.columns, mean, label='Mean Annual Point SMB', linestyle=\"--\")\n",
    "    ax.set_title(vois_climate_long_name[var])\n",
    "    ax.set_ylabel(vois_units[var])\n",
    "\n",
    "plt.suptitle('Over all stakes for year ' + str(year))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Silvretta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_Silvretta = rgi_df.loc['silvretta']['rgi_id.v6']\n",
    "\n",
    "data = pd.read_csv(path_PMB_GLAMOS_csv + 'CH_wgms_dataset.csv')\n",
    "# data = data[(data.PERIOD == 'annual') & (data.RGIId == rgi_Silvretta)]\n",
    "data = data[data.RGIId == rgi_Silvretta]\n",
    "\n",
    "print('Number of winter and annual samples:', len(data))\n",
    "print('Number of annual samples:', len(data[data.PERIOD == 'annual']))\n",
    "print('Number of winter samples:', len(data[data.PERIOD == 'winter']))\n",
    "\n",
    "# change mm w.e. to m w.e.\n",
    "data['POINT_BALANCE'] = data['POINT_BALANCE'] / 1000\n",
    "\n",
    "# Plot number of measurements per year\n",
    "# Number of measurements per glacier per year:\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
    "num_gl_yr = data.groupby(['YEAR', 'PERIOD']).size().unstack().reset_index()\n",
    "num_gl_yr.plot(x='YEAR', kind='bar', stacked=True, ax=ax, title='Silvretta')\n",
    "ax.set_ylabel('Number of measurements')\n",
    "ax.set_title('Number of measurements per year: Silvretta', fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ERA5_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the short names of the climate variables available in the dataset\n",
    "vois_climate = ['t2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str']\n",
    "# voi_topographical = ['aspect', 'slope', 'dis_from_border', 'topo']\n",
    "voi_topographical = ['aspect', 'slope']\n",
    "meta_data_columns = [\"RGIId\", \"POINT_ID\", \"ID\", \"N_MONTHS\", \"MONTHS\", \"PERIOD\"]\n",
    "\n",
    "# Provide the column name for the column that has the RGI IDs for each of the stakes\n",
    "dataset = mbm.Dataset(data=data,\n",
    "                      region_name='CH',\n",
    "                      data_path=path_PMB_GLAMOS_csv)\n",
    "\n",
    "# Add climate data:\n",
    "# Specify the files of the climate data, that will be matched with the coordinates of the stake data\n",
    "era5_climate_data = path_ERA5_raw + 'era5_monthly_averaged_data.nc'\n",
    "geopotential_data = path_ERA5_raw + 'era5_geopotential_pressure.nc'\n",
    "\n",
    "# Match the climate features, from the ERA5Land netCDF file, for each of the stake measurement dataset\n",
    "dataset.get_climate_features(climate_data=era5_climate_data,\n",
    "                             geopotential_data=geopotential_data,\n",
    "                             change_units=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot temperature and precipitation for all stakes:\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "df = dataset.data\n",
    "year = 2006\n",
    "\n",
    "for i, var in enumerate(vois_climate):\n",
    "    temp = df[df.YEAR == year][[col for col in df.columns if var in col]]\n",
    "    mean = temp.mean().values\n",
    "    std = temp.std().values\n",
    "    ax = plt.subplot(4, 2, i + 1)\n",
    "    ax.fill_between(\n",
    "        temp.columns,\n",
    "        mean - std,\n",
    "        mean + std,\n",
    "        color=\"orange\",\n",
    "        alpha=0.3,\n",
    "    )\n",
    "    # put x axis at 45 degrees\n",
    "    ax.set_xticklabels(temp.columns, rotation=90)\n",
    "    ax.plot(temp.columns, mean, label='Mean Annual Point SMB', linestyle=\"--\")\n",
    "    ax.set_title(vois_climate_long_name[var])\n",
    "    ax.set_ylabel(vois_units[var])\n",
    "\n",
    "plt.suptitle('Silvretta for year ' + str(year))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each record, convert to a monthly time resolution\n",
    "dataset.convert_to_monthly(meta_data_columns=meta_data_columns,\n",
    "                           vois_climate=vois_climate,\n",
    "                           vois_topographical=voi_topographical)\n",
    "\n",
    "# Create a new DataLoader object with the monthly stake data measurements.\n",
    "dataloader = mbm.DataLoader(data=dataset.data,\n",
    "                            random_seed=SEED,\n",
    "                            meta_data_columns=meta_data_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE_SPLIT = 'year'\n",
    "\n",
    "if TYPE_SPLIT == 'year':\n",
    "    # Split into training and test years with train_test_split\n",
    "    train_years, test_years = train_test_split(dataset.data.YEAR.unique(),\n",
    "                                               test_size=0.2,\n",
    "                                               random_state=SEED)\n",
    "\n",
    "    train_indices = dataset.data[dataset.data.YEAR.isin(train_years)].index\n",
    "    test_indices = dataset.data[dataset.data.YEAR.isin(test_years)].index\n",
    "\n",
    "    dataloader.set_custom_train_test_indices(train_indices, test_indices)\n",
    "\n",
    "else:\n",
    "    # Randomly (though does not separate meas ID)\n",
    "    train_itr, test_itr = dataloader.set_train_test_split(test_size=0.2,\n",
    "                                                          shuffle=True)\n",
    "\n",
    "    train_indices, test_indices = list(train_itr), list(test_itr)\n",
    "\n",
    "# Get the features and targets of the training data for the indices as defined above, that will be used during the cross validation.\n",
    "df_X_train = dataset.data.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Get test set\n",
    "df_X_test = dataset.data.iloc[test_indices]\n",
    "y_test = df_X_test['POINT_BALANCE'].values\n",
    "\n",
    "# Create the CV splits based on the training dataset. The default value for the number of splits is 5.\n",
    "splits = dataloader.get_cv_split(n_splits=5, type_fold='group-meas-id')\n",
    "test_meas_id = df_X_test['ID'].unique()\n",
    "\n",
    "# Years in training and test set\n",
    "train_years = df_X_train.YEAR.unique()\n",
    "test_years = df_X_test.YEAR.unique()\n",
    "print('Train years:', train_years)\n",
    "print('Test years:', test_years)\n",
    "\n",
    "# Plot splits\n",
    "visualiseSplits(y_test, y_train, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Grid search\n",
    "# For each of the XGBoost parameter, define the grid range\n",
    "parameters = {\n",
    "    'max_depth': [\n",
    "        3,\n",
    "        4,\n",
    "        5,\n",
    "        6,\n",
    "    ],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'gamma': [0, 1]\n",
    "}\n",
    "\n",
    "param_init = {}\n",
    "param_init['device'] = 'cuda:0'\n",
    "param_init['tree_method'] = 'hist'\n",
    "param_init[\"random_state\"] = SEED\n",
    "\n",
    "# Create a CustomXGBoostRegressor instance\n",
    "custom_xgboost = mbm.models.CustomXGBoostRegressor(\n",
    "    meta_data_columns=meta_data_columns, **param_init)\n",
    "custom_xgboost.randomsearch(\n",
    "    parameters=parameters,\n",
    "    n_iter=20,\n",
    "    splits=splits,\n",
    "    features=df_X_train,\n",
    "    targets=y_train,\n",
    "    num_jobs=-1,\n",
    "    random_seed=SEED,\n",
    ")\n",
    "\n",
    "best_params = params = custom_xgboost.param_search.best_params_\n",
    "best_estimator = custom_xgboost.param_search.best_estimator_\n",
    "print(\"Best parameters:\\n\", best_params)\n",
    "print(\"Best score:\\n\", custom_xgboost.param_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to CPU for predictions:\n",
    "xgb = best_estimator.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on test\n",
    "features_test, metadata_test = xgb._create_features_metadata(\n",
    "    df_X_test, meta_data_columns)\n",
    "y_pred = xgb.predict(features_test)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = xgb.aggrPredict(metadata_test, meta_data_columns, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = xgb.score(df_X_test, y_test)  # negative\n",
    "mse, rmse, mae = xgb.evalMetrics(metadata_test, y_pred, y_test)\n",
    "\n",
    "# Aggregate predictions to annual or winter:\n",
    "df_pred = df_X_test.copy()\n",
    "df_pred['target'] = y_test\n",
    "grouped_ids = df_pred.groupby('ID').agg({'target': 'mean'})\n",
    "grouped_ids['pred'] = y_pred_agg\n",
    "grouped_ids['PERIOD'] = df_X_test.groupby('ID')['PERIOD'].first()\n",
    "\n",
    "predVSTruth(grouped_ids, mae, rmse, title='XGBoost on Silvretta (split years)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Gries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_Gries = rgi_df.loc['gries']['rgi_id.v6']\n",
    "\n",
    "data = pd.read_csv(path_PMB_GLAMOS_csv + 'CH_wgms_dataset.csv')\n",
    "# data = data[(data.PERIOD == 'annual') & (data.RGIId == rgi_Gries)]\n",
    "data = data[data.RGIId == rgi_Gries]\n",
    "\n",
    "print('Number of winter and annual samples:', len(data))\n",
    "print('Number of annual samples:', len(data[data.PERIOD == 'annual']))\n",
    "print('Number of winter samples:', len(data[data.PERIOD == 'winter']))\n",
    "\n",
    "# change mm w.e. to m w.e.\n",
    "data['POINT_BALANCE'] = data['POINT_BALANCE'] / 1000\n",
    "\n",
    "# Plot number of measurements per year\n",
    "# Number of measurements per glacier per year:\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
    "num_gl_yr = data.groupby(['YEAR', 'PERIOD']).size().unstack().reset_index()\n",
    "num_gl_yr.plot(x='YEAR', kind='bar', stacked=True, ax=ax, title='Gries')\n",
    "ax.set_ylabel('Number of measurements')\n",
    "ax.set_title('Number of measurements per year: Gries', fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the short names of the climate variables available in the dataset\n",
    "vois_climate = ['t2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str']\n",
    "# voi_topographical = ['aspect', 'slope', 'dis_from_border', 'topo']\n",
    "voi_topographical = ['aspect', 'slope']\n",
    "meta_data_columns = [\"RGIId\", \"POINT_ID\", \"ID\", \"N_MONTHS\", \"MONTHS\", \"PERIOD\"]\n",
    "\n",
    "# Provide the column name for the column that has the RGI IDs for each of the stakes\n",
    "dataset = mbm.Dataset(data=data,\n",
    "                      region_name='CH',\n",
    "                      data_path=path_PMB_GLAMOS_csv)\n",
    "\n",
    "# Add climate data:\n",
    "# Specify the files of the climate data, that will be matched with the coordinates of the stake data\n",
    "era5_climate_data = path_ERA5_raw + 'era5_monthly_averaged_data.nc'\n",
    "geopotential_data = path_ERA5_raw + 'era5_geopotential_pressure.nc'\n",
    "\n",
    "# Match the climate features, from the ERA5Land netCDF file, for each of the stake measurement dataset\n",
    "dataset.get_climate_features(climate_data=era5_climate_data,\n",
    "                             geopotential_data=geopotential_data,\n",
    "                             change_units=True)\n",
    "\n",
    "# For each record, convert to a monthly time resolution\n",
    "dataset.convert_to_monthly(meta_data_columns=meta_data_columns,\n",
    "                           vois_climate=vois_climate,\n",
    "                           vois_topographical=voi_topographical)\n",
    "\n",
    "# Create a new DataLoader object with the monthly stake data measurements.\n",
    "dataloader = mbm.DataLoader(data=dataset.data,\n",
    "                            random_seed=SEED,\n",
    "                            meta_data_columns=meta_data_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE_SPLIT = 'year'\n",
    "\n",
    "if TYPE_SPLIT == 'year':\n",
    "    # Split into training and test years with train_test_split\n",
    "    train_years, test_years = train_test_split(dataset.data.YEAR.unique(),\n",
    "                                               test_size=0.2,\n",
    "                                               random_state=SEED)\n",
    "\n",
    "    train_indices = dataset.data[dataset.data.YEAR.isin(train_years)].index\n",
    "    test_indices = dataset.data[dataset.data.YEAR.isin(test_years)].index\n",
    "\n",
    "    dataloader.set_custom_train_test_indices(train_indices, test_indices)\n",
    "\n",
    "else:\n",
    "    # Randomly (though does not separate meas ID)\n",
    "    train_itr, test_itr = dataloader.set_train_test_split(test_size=0.2,\n",
    "                                                          shuffle=True)\n",
    "\n",
    "    train_indices, test_indices = list(train_itr), list(test_itr)\n",
    "\n",
    "# Get the features and targets of the training data for the indices as defined above, that will be used during the cross validation.\n",
    "df_X_train = dataset.data.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Get test set\n",
    "df_X_test = dataset.data.iloc[test_indices]\n",
    "y_test = df_X_test['POINT_BALANCE'].values\n",
    "\n",
    "# Create the CV splits based on the training dataset. The default value for the number of splits is 5.\n",
    "splits = dataloader.get_cv_split(n_splits=5, type_fold='group-meas-id')\n",
    "test_meas_id = df_X_test['ID'].unique()\n",
    "\n",
    "# Years in training and test set\n",
    "train_years = df_X_train.YEAR.unique()\n",
    "test_years = df_X_test.YEAR.unique()\n",
    "print('Train years:', train_years)\n",
    "print('Test years:', test_years)\n",
    "\n",
    "# Plot splits\n",
    "visualiseSplits(y_test, y_train, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Grid search\n",
    "# For each of the XGBoost parameter, define the grid range\n",
    "parameters = {\n",
    "    'max_depth': [\n",
    "        3,\n",
    "        4,\n",
    "        5,\n",
    "        6,\n",
    "    ],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'gamma': [0, 1]\n",
    "}\n",
    "\n",
    "feature_columns = [\n",
    "    'ALTITUDE_CLIMATE', 'ELEVATION_DIFFERENCE', 'aspect', 'fal', 'slhf',\n",
    "    'slope', 'sshf', 'ssrd', 'str', 't2m', 'tp'\n",
    "]\n",
    "\n",
    "param_init = {}\n",
    "param_init['device'] = 'cuda:0'\n",
    "param_init['tree_method'] = 'hist'\n",
    "param_init[\"random_state\"] = SEED\n",
    "\n",
    "# Create a CustomXGBoostRegressor instance\n",
    "custom_xgboost = mbm.models.CustomXGBoostRegressor(\n",
    "    meta_data_columns=meta_data_columns, **param_init)\n",
    "custom_xgboost.randomsearch(\n",
    "    parameters=parameters,\n",
    "    n_iter=20,\n",
    "    splits=splits,\n",
    "    features=df_X_train,\n",
    "    targets=y_train,\n",
    "    num_jobs=-1,\n",
    "    random_seed=SEED,\n",
    ")\n",
    "\n",
    "# save best model\n",
    "custom_xgboost.save_model('xgb_gries.pkl')\n",
    "\n",
    "best_params = params = custom_xgboost.param_search.best_params_\n",
    "best_estimator = custom_xgboost.param_search.best_estimator_\n",
    "print(\"Best parameters:\\n\", best_params)\n",
    "print(\"Best score:\\n\", custom_xgboost.param_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to CPU for predictions:\n",
    "xgb = best_estimator.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on test\n",
    "features_test, metadata_test = xgb._create_features_metadata(\n",
    "    df_X_test, meta_data_columns)\n",
    "y_pred = xgb.predict(features_test)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = xgb.aggrPredict(metadata_test, meta_data_columns, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = xgb.score(df_X_test, y_test)  # negative\n",
    "mse, rmse, mae = xgb.evalMetrics(metadata_test, y_pred, y_test)\n",
    "\n",
    "# Aggregate predictions to annual or winter:\n",
    "df_pred = df_X_test.copy()\n",
    "df_pred['target'] = y_test\n",
    "grouped_ids = df_pred.groupby('ID').agg({'target': 'mean'})\n",
    "grouped_ids['pred'] = y_pred_agg\n",
    "grouped_ids['PERIOD'] = df_X_test.groupby('ID')['PERIOD'].first()\n",
    "\n",
    "predVSTruth(grouped_ids, mae, rmse, title='XGBoost on Gries (split years)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of input variables:\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(feature_columns):\n",
    "    ax = plt.subplot(3, 5, i + 1)\n",
    "    sns.histplot(df_X_test[feature], ax=ax)\n",
    "    ax.set_title(feature)\n",
    "    ax.set_xlabel('')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
