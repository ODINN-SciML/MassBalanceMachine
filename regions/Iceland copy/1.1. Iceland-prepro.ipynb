{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import massbalancemachine as mbm\n",
    "import pyproj\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import logging\n",
    "import glob\n",
    "from cmcrameri import cm\n",
    "from oggm import utils\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.iceland_preprocess import *\n",
    "from scripts.config_ICE import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.IcelandConfig(dataPath='/home/mburlet/scratch/data/DATA_MB/WGMS/Iceland/')\n",
    "\n",
    "# Module logger\n",
    "log = logging.getLogger('.'.join(__name__.split('.')[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "cmap = cm.devon\n",
    "\n",
    "# For bars and lines:\n",
    "color_diff_xgb = '#4d4d4d'\n",
    "\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_1 = colors[0]\n",
    "color_2 = '#c51b7d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all stake csv files into 1 df\n",
    "\n",
    "###### The data used in this code comes from the data scraping done in the 1.0 Iceland-data-acquisition notebook in June 2025, only winter and annual measurements are used. Code might have to be adjusted if new data is added to https://joklavefsja.vedur.is/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob(os.path.join(cfg.dataPath + path_PMB_WGMS_raw, \"*.csv\"))\n",
    "\n",
    "# Initialize empty list to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Read each CSV file into a dataframe and append to list\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Print info\n",
    "print(f\"Combined {len(all_files)} CSV files into one dataframe with {len(combined_df)} rows\")\n",
    "\n",
    "# Add data modification column to keep track of mannual changes\n",
    "combined_df['DATA_MODIFICATION'] = ''\n",
    "\n",
    "\n",
    "display(combined_df.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Split into annual and winter rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stakes_split = split_stake_measurements(combined_df)\n",
    "\n",
    "# Convert date columns to string in 'YYYYMMDD' format\n",
    "df_stakes_split['TO_DATE'] = pd.to_datetime(df_stakes_split['TO_DATE']).dt.strftime('%Y%m%d')\n",
    "df_stakes_split['FROM_DATE'] = pd.to_datetime(df_stakes_split['FROM_DATE']).dt.strftime('%Y%m%d')\n",
    "\n",
    "display(df_stakes_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date Fixes\n",
    "\n",
    "###### Fix NaN dates by adding hydrological year dates. (It would be nicer if this code also checked if there was a previous year of the same stake with a date and then takes that date instead of hydr. year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_stakes_split[df_stakes_split['FROM_DATE'].isna()])\n",
    "display(df_stakes_split[df_stakes_split['TO_DATE'].isna()])\n",
    "display(df_stakes_split[df_stakes_split['YEAR'].isna()])\n",
    "\n",
    "# Change NaN year values to the year of the TO_DATE\n",
    "df_stakes_split.loc[df_stakes_split['YEAR'].isna(), 'YEAR'] = df_stakes_split.loc[df_stakes_split['YEAR'].isna(), 'TO_DATE'].astype(str).str[:4].astype(float)\n",
    "\n",
    "# Data modification column update\n",
    "date_nan_mask = df_stakes_split['FROM_DATE'].isna() | df_stakes_split['TO_DATE'].isna()\n",
    "df_stakes_split.loc[date_nan_mask, 'DATA_MODIFICATION'] = 'Dates filled in according to hydrological year'\n",
    "# Set FROM_DATE from NaN to 01 Oct of previous year\n",
    "df_stakes_split.loc[df_stakes_split['FROM_DATE'].isna(), 'FROM_DATE'] = (\n",
    "    (df_stakes_split.loc[df_stakes_split['FROM_DATE'].isna(), 'YEAR'].astype(int) - 1).astype(str) + '1001'\n",
    ")\n",
    "# Set TO_DATE from NaN to 30 Sept of the year (as only annual rows have NaN, no need for period distinction)\n",
    "df_stakes_split.loc[df_stakes_split['TO_DATE'].isna(), 'TO_DATE'] = (\n",
    "    df_stakes_split.loc[df_stakes_split['TO_DATE'].isna(), 'YEAR'].astype(int).astype(str) + '0930'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Check for problematic date ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_inconsistent, winter_inconsistent = check_period_consistency(df_stakes_split)\n",
    "\n",
    "\n",
    "# Display the inconsistent records\n",
    "if len(annual_inconsistent) > 0:\n",
    "    print(\"\\nInconsistent annual periods:\")\n",
    "    display(annual_inconsistent)\n",
    "\n",
    "if len(winter_inconsistent) > 0:\n",
    "    print(\"\\nInconsistent winter periods:\")\n",
    "    display(winter_inconsistent)\n",
    "\n",
    "# Only index 5084 is unreasonabl (-2), probably wrong FROM_DATE year, change to year - 1\n",
    "df_stakes_split.loc[df_stakes_split['stake']=='GL10a', 'FROM_DATE'] = '19960825'\n",
    "df_stakes_split.loc[df_stakes_split['stake']=='GL10a', 'DATA_MODIFICATION'] = 'FROM_DATE year corrected from 1997 to 1996'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Rename Columns and general data cleaning, we can skip the close stake removal, as seen form the leaflet map online, the stakes are spaced out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stakes_renamed = df_stakes_split.rename(columns={\n",
    "    'lat': 'POINT_LAT', \n",
    "    'lon': 'POINT_LON',\n",
    "    'elevation': 'POINT_ELEVATION',\n",
    "    'stake': 'ID',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN check\n",
    "display(df_stakes_renamed[df_stakes_renamed.isna().any(axis=1)])\n",
    "\n",
    "# Remove all rows with any NaN values\n",
    "df_stakes_renamed = df_stakes_renamed.dropna()\n",
    "\n",
    "# Confirm removal - this should show 0 rows if all NaNs were removed\n",
    "print(f\"Rows with NaN values after removal: {len(df_stakes_renamed[df_stakes_renamed.isna().any(axis=1)])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find RGIId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load glacier outlines\n",
    "rgi_file = utils.get_rgi_region_file(region=\"06\", version=\"6\")\n",
    "glacier_outline = gpd.read_file(rgi_file)\n",
    "\n",
    "# Add RGI IDs through intersection\n",
    "df_stakes_renamed_rgiid = mbm.data_processing.utils.get_rgi(data=df_stakes_renamed,\n",
    "                                           glacier_outlines=glacier_outline)\n",
    "\n",
    "display(df_stakes_renamed_rgiid[df_stakes_renamed_rgiid['RGIId'].isna()])\n",
    "# Remove (nine) stakes without RGIId, as they wont have OGGM data anyways\n",
    "df_stakes_renamed_rgiid = df_stakes_renamed_rgiid.dropna(subset=['RGIId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add OGGM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize OGGM glacier directories\n",
    "gdirs, rgidf = initialize_oggm_glacier_directories(\n",
    "    working_dir= cfg.dataPath + path_OGGM,\n",
    "    rgi_region=\"06\", #06 iceland\n",
    "    rgi_version=\"6\",\n",
    "    base_url=\n",
    "    \"https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L3-L5_files/2023.1/elev_bands/W5E5_w_data/\",\n",
    "    log_level='WARNING',\n",
    "    task_list=None,\n",
    ")\n",
    "\n",
    "\n",
    "unique_rgis = df_stakes_renamed_rgiid['RGIId'].unique()\n",
    "\n",
    "run = True\n",
    "if run:\n",
    "    export_oggm_grids(gdirs, subset_rgis=unique_rgis, output_path=cfg.dataPath + path_OGGM_xrgrids)\n",
    "\n",
    "## Around 10% of all the measurements have no hugonnet_dhdt data, so I removed the entire variable from merge_pmb_with_oggm_data()\n",
    "df_stakes_topo = merge_pmb_with_oggm_data(df_pmb=df_stakes_renamed_rgiid,\n",
    "                                       gdirs=gdirs,\n",
    "                                       rgi_region=\"06\", #06 iceland\n",
    "                                       rgi_version=\"6\")\n",
    "                                       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get Glacier names from RGIId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping from RGIId to glacier name\n",
    "rgi_to_name_dict = dict(zip(rgidf.RGIId, rgidf.Name))\n",
    "df_stakes_topo['GLACIER'] = df_stakes_topo['RGIId'].map(rgi_to_name_dict)\n",
    "\n",
    "\n",
    "display(df_stakes_topo[df_stakes_topo['GLACIER'].isna()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Multiple RGIIds have no associated glacier name, assign the 'RGIId' as the 'GLACIER' name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rgi_ids = df_stakes_topo.loc[df_stakes_topo['GLACIER'].isna(), 'RGIId'].unique()\n",
    "print(f\"Number of unique RGI IDs without names: {len(missing_rgi_ids)}\")\n",
    "print(\"RGI IDs without names:\", missing_rgi_ids)\n",
    "\n",
    "# Just assign RGIId to 'GLACIER' as name for the ones that are missing\n",
    "df_stakes_topo.loc[df_stakes_topo['GLACIER'].isna(), 'GLACIER'] = df_stakes_topo.loc[df_stakes_topo['GLACIER'].isna(), 'RGIId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "glacierName = 'Thjorsarjoekull (Hofsjoekull E)'\n",
    "# stakes\n",
    "df_stakes_topo_1 = df_stakes_topo.copy()\n",
    "df_stakes_topo_1 = df_stakes_topo_1[(df_stakes_topo_1['GLACIER'] == glacierName)]\n",
    "RGIId = df_stakes_topo_1['RGIId'].unique()[0]\n",
    "print(RGIId)\n",
    "# open OGGM xr for glacier\n",
    "# Get oggm data for that RGI grid\n",
    "ds_oggm = xr.open_dataset(f'{cfg.dataPath + path_OGGM_xrgrids}/{RGIId}.zarr')\n",
    "\n",
    "# Define the coordinate transformation\n",
    "transf = pyproj.Transformer.from_proj(\n",
    "    pyproj.CRS.from_user_input(\"EPSG:4326\"),  # Input CRS (WGS84)\n",
    "    pyproj.CRS.from_user_input(ds_oggm.pyproj_srs),  # Output CRS from dataset\n",
    "    always_xy=True)\n",
    "\n",
    "# Transform all coordinates in the group\n",
    "lon, lat = df_stakes_topo_1[\"POINT_LON\"].values, df_stakes_topo_1[\"POINT_LAT\"].values\n",
    "x_stake, y_stake = transf.transform(lon, lat)\n",
    "df_stakes_topo_1['x'] = x_stake\n",
    "df_stakes_topo_1['y'] = y_stake\n",
    "\n",
    "# plot stakes\n",
    "plt.figure(figsize=(8, 6))\n",
    "ds_oggm.glacier_mask.plot(cmap='binary')\n",
    "sns.scatterplot(df_stakes_topo_1,\n",
    "                x='x',\n",
    "                y='y',\n",
    "                hue='within_glacier_shape',\n",
    "                palette=['r', 'b'])\n",
    "plt.title(f'Stakes on {glacierName} (OGGM)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict to within glacier shape\n",
    "df_stakes_topo = df_stakes_topo[df_stakes_topo['within_glacier_shape'] == True]\n",
    "df_stakes_topo = df_stakes_topo.drop(columns=['within_glacier_shape'])\n",
    "\n",
    "# Display rows that have any NaN values\n",
    "display(df_stakes_topo[df_stakes_topo.isna().any(axis=1)])\n",
    "\n",
    "# Drop 3 rows where consensus_ice_thickness is NaN\n",
    "df_stakes_topo_dropped = df_stakes_topo.dropna(subset=['consensus_ice_thickness'])\n",
    "\n",
    "display(len(df_stakes_topo_dropped[df_stakes_topo_dropped['consensus_ice_thickness'].isna()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new POINT_ID column\n",
    "df_stakes_topo_dropped['POINT_ID'] = (\n",
    "    df_stakes_topo_dropped['GLACIER'] + '_' + \n",
    "    df_stakes_topo_dropped['YEAR'].astype(str) + '_' + \n",
    "    df_stakes_topo_dropped['PERIOD'].astype(str) + '_' +\n",
    "    df_stakes_topo_dropped['ID'].astype(str)\n",
    ")\n",
    "\n",
    "df_stakes_topo_dropped = df_stakes_topo_dropped.drop(columns=['ID'])\n",
    "\n",
    "display(df_stakes_topo_dropped.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN\n",
    "display(df_stakes_topo_dropped[df_stakes_topo_dropped.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = (cfg.dataPath + path_PMB_WGMS_csv + 'ICE_dataset_all_oggm.csv')\n",
    "df_stakes_topo_dropped.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
