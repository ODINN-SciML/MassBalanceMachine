{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing of GLACIOCLIM MB data:\n",
    "\n",
    "Does the pre-processing of the point MB measurements from GLACIOCLIM (French Alps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Mass Balance data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import massbalancemachine as mbm\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from cmcrameri import cm\n",
    "from oggm import utils\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glacioclim_preprocess import *\n",
    "from scripts.config_FR import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "cfg = mbm.FranceConfig(dataPath='/home/mburlet/scratch/data/DATA_MB/GLACIOCLIM/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "cmap = cm.devon\n",
    "\n",
    "# For bars and lines:\n",
    "color_diff_xgb = '#4d4d4d'\n",
    "\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_1 = colors[0]\n",
    "color_2 = '#c51b7d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data into dictionary of dataframes\n",
    "\n",
    "###### unzip GLACIOCLIM Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_in = False\n",
    "\n",
    "if read_in == True:\n",
    "    extract_glacioclim_files(cfg.dataPath + path_PMB_GLACIOCLIM_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Read all csv from \"unzipped\" into single dictionary of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stakes_csv_all = {}\n",
    "\n",
    "for path, _, files in os.walk(cfg.dataPath + path_PMB_GLACIOCLIM_raw):\n",
    "    # Find all .csv files in the current directory\n",
    "    csv_files = glob.glob(os.path.join(path, '*.csv'))\n",
    "    \n",
    "    # Read each CSV file and add to dictionary\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            # Use the filename as the key\n",
    "            key = os.path.splitext(os.path.basename(file))[0]\n",
    "            # Read the CSV file\n",
    "            stakes_csv_all[key] = pd.read_csv(file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nTotal number of files processed: {len(stakes_csv_all)}\")\n",
    "display(stakes_csv_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special case Sarennes. \n",
    "###### The File is not in a typical csv structure, so cant just be read in. For this code to work, in each sheet in the B_SARENNES_94_20.xls file, make a table with start_date, spring_date, end_date, X, Y, Z, winter mb, summer mb, annual mb (requires xlrd package to read in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarennes_path = cfg.dataPath + 'Glacier de SARENNES_discontinued/B_SARENNES_94-20.xls'\n",
    "\n",
    "# Read all sheets into a dictionary of dfs\n",
    "all_sheets = pd.read_excel(sarennes_path, sheet_name=None)\n",
    "sarennes_dfs = extract_sarennes_data(all_sheets)\n",
    "display(sarennes_dfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Convert from Lambert3 / Lambert2 cooridnates to WGS84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For some reason there is a 2 in front of the y_lambert2e coordinates in certain years, hardcode remove them:\n",
    "stakes_csv_all['mdg_Tacul_winter_smb_abl_2007']['y_lambert2e'] = stakes_csv_all['mdg_Tacul_winter_smb_abl_2007']['y_lambert2e'].apply(lambda x: x - 2000000 if x > 2000000 else x)\n",
    "stakes_csv_all['mdg_Talefre_annual_smb_abl_2006']['y_lambert2e'] = stakes_csv_all['mdg_Talefre_annual_smb_abl_2006']['y_lambert2e'].apply(lambda x: x - 2000000 if x > 2000000 else x)\n",
    "\n",
    "stakes_csv_all = lambert_transform(stakes_csv_all)\n",
    "sarennes_dfs = lambert_transform(sarennes_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot stakes over map for visual conformation of coordinate transformation\n",
    "\n",
    "###### This needs additional folium package to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "glacier_dfs = {\n",
    "    'GEBROULAZ': (stakes_csv_all['geb_annual_smb_abl_1979'], 'red'),\n",
    "    'ARGENTIERE': (stakes_csv_all['Argentiere_annual_smb_accu_1995'], 'blue'),\n",
    "    'SAINT_SORLIN': (stakes_csv_all['stso_annual_smb_abl_1957'], 'green'),\n",
    "    'MER_DE_GLACE': (stakes_csv_all['mdg_Leschaux_winter_smb_abl_2020'], 'purple'),\n",
    "    'SARENNES': (sarennes_dfs['sarennes_complete_winter_2006'], 'orange')\n",
    "}\n",
    "\n",
    "center_lat, center_lon = 45.8736, 6.8770\n",
    "\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=10)\n",
    "\n",
    "# Add markers for each glacier\n",
    "for glacier_name, (df, color) in glacier_dfs.items():\n",
    "    fg = folium.FeatureGroup(name=glacier_name)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        if glacier_name == 'SARENNES':\n",
    "            stake_id = row['POINT_ID'].split('_')[-1]\n",
    "            altitude = row['POINT_ELEVATION']\n",
    "        else:\n",
    "            stake_id = row['stake_number']\n",
    "            altitude = row['altitude']\n",
    "            \n",
    "        # Add circle marker with popup\n",
    "        folium.CircleMarker(\n",
    "            location=[row['lat'], row['lon']],\n",
    "            radius=5,\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fill_color=color,\n",
    "            popup=f\"{glacier_name} - Stake {stake_id}: {altitude}m\"\n",
    "        ).add_to(fg)\n",
    "    \n",
    "    fg.add_to(m)\n",
    "\n",
    "# Add a legend\n",
    "legend_html = '''\n",
    "<div style=\"position: fixed; bottom: 50px; left: 50px; z-index: 1000; background-color: white; padding: 10px; border-radius: 5px;\">\n",
    "    <p><strong>Glaciers</strong></p>\n",
    "    <p><span style=\"color: red;\">●</span> GEBROULAZ</p>\n",
    "    <p><span style=\"color: blue;\">●</span> ARGENTIERE</p>\n",
    "    <p><span style=\"color: green;\">●</span> SAINT SORLIN</p>\n",
    "    <p><span style=\"color: purple;\">●</span> MER DE GLACE</p>\n",
    "    <p><span style=\"color: orange;\">●</span> SARENNES</p>\n",
    "</div>\n",
    "'''\n",
    "m.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. WGMS Mapping, appending SARENNES and conversion into single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stakes_csv_WGMS = {}\n",
    "\n",
    "# Process each DataFrame in the original dictionary to WGMS format\n",
    "for key, df in stakes_csv_all.items():\n",
    "    try:\n",
    "        required_cols = {'lat', 'lon', 'altitude', 'stake_number', \n",
    "                        'day_start', 'month_start', 'year_start',\n",
    "                        'day_end', 'month_end', 'year_end'}\n",
    "        if all(col in df.columns for col in required_cols):\n",
    "            stakes_csv_WGMS[key] = transform_WGMS_df(df, key)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {key}: {str(e)}\")\n",
    "\n",
    "# Append SARENNES to the dictionary\n",
    "columns = ['POINT_ID', 'POINT_LAT', 'POINT_LON', 'POINT_ELEVATION', 'FROM_DATE', \n",
    "           'TO_DATE', 'POINT_BALANCE', 'GLACIER', 'PERIOD', 'GLACIER_ZONE']\n",
    "for key, df in sarennes_dfs.items():\n",
    "    stakes_csv_WGMS[key] = df.rename(columns={'lat': 'POINT_LAT', 'lon': 'POINT_LON'})[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Combine dictionary of dfs into 1 df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary to single DataFrame\n",
    "stakes_csv_WGMS_combined = pd.concat(stakes_csv_WGMS.values(), ignore_index=True)\n",
    "# Sort by date and glacier\n",
    "stakes_csv_WGMS_combined = stakes_csv_WGMS_combined.sort_values(['GLACIER', 'GLACIER_ZONE', 'PERIOD', 'FROM_DATE'])\n",
    "# Add YEAR column to df\n",
    "stakes_csv_WGMS_combined['YEAR'] = stakes_csv_WGMS_combined['TO_DATE'].astype(str).str[:4].astype(int)\n",
    "# Add data modification column to keep track of mannual changes\n",
    "stakes_csv_WGMS_combined['DATA_MODIFICATION'] = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Merge stakes that are close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stakes_csv_WGMS_combined_dropped = remove_close_points(stakes_csv_WGMS_combined)\n",
    "\n",
    "display(find_close_stakes(stakes_csv_WGMS_combined_dropped).sort_values('DISTANCE_M'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. General data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rows with NaN values:\")\n",
    "display(stakes_csv_WGMS_combined_dropped[stakes_csv_WGMS_combined_dropped.isna().any(axis=1)])\n",
    "\n",
    "print(\"Rows with zero values:\")\n",
    "display(stakes_csv_WGMS_combined_dropped[stakes_csv_WGMS_combined_dropped.eq(0).any(axis=1)])\n",
    "\n",
    "print(\"Rows with extreme POINT_BALANCE values (>5 or <-15):\")\n",
    "display(stakes_csv_WGMS_combined_dropped[\n",
    "    (stakes_csv_WGMS_combined_dropped['POINT_BALANCE'] > 5) | \n",
    "    (stakes_csv_WGMS_combined_dropped['POINT_BALANCE'] < -15)\n",
    "])\n",
    "\n",
    "# Saint-Sorlin has POINT_ELEVATION 0.0 on 8 stakes, also about 20 stakes have point balance 0.00, remove them all\n",
    "mask_zeros = stakes_csv_WGMS_combined_dropped.eq(0).any(axis=1)\n",
    "stakes_csv_WGMS_combined_dropped = stakes_csv_WGMS_combined_dropped[~mask_zeros]\n",
    "\n",
    "# stso_summer_smb_abl_2017_setup2015_14 -88 pmb, nonsensical value, remove it\n",
    "mask = stakes_csv_WGMS_combined_dropped['POINT_ID'] != 'stso_summer_smb_abl_2017_setup2015_14'\n",
    "stakes_csv_WGMS_combined_dropped = stakes_csv_WGMS_combined_dropped[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_inconsistent, winter_inconsistent, summer_inconsistent = check_period_consistency(stakes_csv_WGMS_combined_dropped)\n",
    "\n",
    "display(annual_inconsistent)\n",
    "display(summer_inconsistent)\n",
    "\n",
    "# 7 short summers but nothing majorly inconsistent, leaving them in\n",
    "# Annuual, mdg_langue_annual_smb_abl_2008_setup2008_9 is a faulty measurement, goes from 2009 to 2008, pop it\n",
    "mask = stakes_csv_WGMS_combined_dropped['POINT_ID'] != 'mdg_langue_annual_smb_abl_2008_Langue_setup2008_9'\n",
    "stakes_csv_WGMS_combined_dropped = stakes_csv_WGMS_combined_dropped[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add RGIId and OGGM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize OGGM glacier directories\n",
    "gdirs, rgidf = initialize_oggm_glacier_directories(\n",
    "    working_dir = cfg.dataPath + path_OGGM,\n",
    "    rgi_region=\"11\",\n",
    "    rgi_version=\"6\",\n",
    "    base_url=\n",
    "    \"https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L3-L5_files/2023.1/elev_bands/W5E5_w_data/\",\n",
    "    log_level='WARNING',\n",
    "    task_list=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load glacier outlines\n",
    "rgi_file = utils.get_rgi_region_file(region=\"11\", version=\"6\")\n",
    "glacier_outline = gpd.read_file(rgi_file)\n",
    "\n",
    "# Add RGI IDs through intersection\n",
    "stakes_csv_WGMS_RGIID = mbm.data_processing.utils.get_rgi(data=stakes_csv_WGMS_combined_dropped,\n",
    "                                           glacier_outlines=glacier_outline)\n",
    "\n",
    "display(len(stakes_csv_WGMS_RGIID[stakes_csv_WGMS_RGIID['RGIId'].isna()]))\n",
    "\n",
    "# Remove stakes without RGIId, as they wont have OGGM data anyways\n",
    "stakes_csv_WGMS_RGIID = stakes_csv_WGMS_RGIID.dropna(subset=['RGIId'])\n",
    "\n",
    "\n",
    "# Create a dictionary mapping from RGIId to glacier name\n",
    "rgi_to_name_dict = dict(zip(rgidf.RGIId, rgidf.Name))\n",
    "stakes_csv_WGMS_RGIID['GLACIER'] = stakes_csv_WGMS_RGIID['RGIId'].map(rgi_to_name_dict)\n",
    "\n",
    "display(stakes_csv_WGMS_RGIID['GLACIER'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_rgis = stakes_csv_WGMS_RGIID['RGIId'].unique()\n",
    "\n",
    "run = True\n",
    "if run:\n",
    "    export_oggm_grids(gdirs, subset_rgis=unique_rgis, output_path= cfg.dataPath + path_OGGM_xrgrids)\n",
    "\n",
    "stakes_csv_WGMS_RGIID_oggm = merge_pmb_with_oggm_data(df_pmb=stakes_csv_WGMS_RGIID,\n",
    "                                       gdirs=gdirs,\n",
    "                                       rgi_region=\"11\",\n",
    "                                       rgi_version=\"6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict to within glacier shape and drop the column\n",
    "stakes_csv_WGMS_RGIID_oggm = stakes_csv_WGMS_RGIID_oggm[stakes_csv_WGMS_RGIID_oggm['within_glacier_shape'] == True]\n",
    "stakes_csv_WGMS_RGIID_oggm = stakes_csv_WGMS_RGIID_oggm.drop(columns=['within_glacier_shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN\n",
    "\n",
    "#pd.set_option('display.max_rows', None)\n",
    "display(stakes_csv_WGMS_RGIID_oggm[stakes_csv_WGMS_RGIID_oggm.isna().any(axis=1)])\n",
    "#pd.reset_option('display.max_rows')\n",
    "\n",
    "# ~90 hugonnet_dhdt and 1 consensus_ice_thickness are NaN, drop them\n",
    "stakes_csv_WGMS_RGIID_oggm = stakes_csv_WGMS_RGIID_oggm.dropna()\n",
    "\n",
    "display(stakes_csv_WGMS_RGIID_oggm[stakes_csv_WGMS_RGIID_oggm.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined DataFrame to a CSV file\n",
    "stakes_csv_WGMS_RGIID_oggm.to_csv(cfg.dataPath + path_PMB_GLACIOCLIM_csv + 'FR_wgms_dataset_all_oggm.csv', index=False)\n",
    "display(stakes_csv_WGMS_RGIID_oggm.head(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
