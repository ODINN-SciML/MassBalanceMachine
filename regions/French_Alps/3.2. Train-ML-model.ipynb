{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost ML-Model. \n",
    "\n",
    "###### A. part of Notebook is regional learning with the French/GLACIOCLIM dataset. B. part is transfer learning with Swiss/GLAMOS train set and French/GLACIOCLIM test set\n",
    "\n",
    "### Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from calendar import month_abbr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glacioclim_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_FR import *\n",
    "from scripts.xgb_helpers import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "# Set dataPath with home directory of data\n",
    "cfg = mbm.FranceConfig(dataPath='/home/mburlet/scratch/data/DATA_MB/GLACIOCLIM/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'\n",
    "color_orange = '#FFA500'\n",
    "\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect\",\n",
    "    \"slope\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.1. Read in stake data from preprocess, transform to monthly and add ERA5Land data\n",
    "\n",
    "###### Load csv into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glacioclim = pd.read_csv(cfg.dataPath + path_PMB_GLACIOCLIM_csv + 'FR_wgms_dataset_all_oggm.csv')\n",
    "\n",
    "print('Number of glaciers:', len(data_glacioclim['GLACIER'].unique()))\n",
    "print('Number of winter, summer and annual samples:', len(data_glacioclim[data_glacioclim.PERIOD == 'annual']) + len(data_glacioclim[data_glacioclim.PERIOD == 'winter']) + len(data_glacioclim[data_glacioclim.PERIOD == 'summer']))\n",
    "print('Number of annual samples:',\n",
    "      len(data_glacioclim[data_glacioclim.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_glacioclim[data_glacioclim.PERIOD == 'winter']))\n",
    "print('Number of summer samples:',\n",
    "      len(data_glacioclim[data_glacioclim.PERIOD == 'summer']))\n",
    "\n",
    "data_glacioclim.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Transform into monthly and add ERA5Land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glacioclim_test = data_glacioclim.copy()\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLACIOCLIM_csv,\n",
    "    'era5_climate_data': cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data_Alps.nc',\n",
    "    'geopotential_data': cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure_Alps.nc'\n",
    "}\n",
    "\n",
    "RUN = True\n",
    "dataloader_gl = process_or_load_data_glacioclim(run_flag=RUN,\n",
    "                                     df=data_glacioclim_test,\n",
    "                                     paths=paths,\n",
    "                                     cfg=cfg,\n",
    "                                     vois_climate=vois_climate,\n",
    "                                     vois_topographical=vois_topographical,\n",
    "                                     output_file = 'FR_wgms_dataset_monthly_full.csv')\n",
    "data_monthly = dataloader_gl.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.2. Dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHeatmap(data_monthly, period='annual', plot_elevation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_monthly.groupby(['YEAR', 'PERIOD']).size().unstack().plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    figsize=(20, 5),\n",
    "    color=[color_dark_blue, color_pink,  color_orange])\n",
    "plt.title('Number of measurements per year for all glaciers')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature intercorrelation\n",
    "plot_feature_correlation(dataloader_gl.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check of variables:\n",
    "df = dataloader_gl.data\n",
    "var_to_plot = ['POINT_BALANCE'] + vois_climate\n",
    "df = df[(df.GLACIER == 'FR4N01235A08 dArgentiere') & (df.YEAR == 2015)].groupby(\n",
    "    ['MONTHS'])[var_to_plot].mean().reset_index()\n",
    "df['month_nb'] = df.MONTHS.apply(\n",
    "    lambda x: list(month_abbr).index(x.capitalize()))\n",
    "df.sort_values(by='month_nb', inplace=True)\n",
    "fig, ax = plt.subplots(3, 4, figsize=(10, 8))\n",
    "\n",
    "for i, var in enumerate(var_to_plot):\n",
    "    df.plot(x='MONTHS', y=var, marker='o', ax=ax.flatten()[i], legend=False)\n",
    "    if var in vois_climate_long_name.keys():\n",
    "        ax.flatten()[i].set_title(vois_climate_long_name[var], fontsize=12)\n",
    "    else:\n",
    "        ax.flatten()[i].set_title(var, fontsize=12)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the topo variables:\n",
    "df = dataloader_gl.data\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15, 6))\n",
    "for i, var in enumerate(vois_topographical + ['ELEVATION_DIFFERENCE']):\n",
    "    ax = axs.flatten()[i]\n",
    "    sns.histplot(df[var], ax=ax, kde=True)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_title(var)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3. Train-test set split:\n",
    "\n",
    "###### Either run A.3.1. or A.3.2.\n",
    "\n",
    "##### A.3.1. Spatial Cross-Validation\n",
    "\n",
    "###### Uses specific glaciers as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_glaciers = ['FR4N01146D09+E06 Gebroulaz']\n",
    "\n",
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers\n",
    "train_glaciers = [i for i in existing_glaciers if i not in test_glaciers]\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(test_glaciers)]\n",
    "\n",
    "# Statistics prints\n",
    "print('Size of test data:', len(data_test))\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of train data:', len(data_train))\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "## CV Splits\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "    \n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                      train_set['splits_vals']))\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A.3.2. Temporal Cross-Validation\n",
    "\n",
    "###### Uses the last X years as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_years_count = 10\n",
    "\n",
    "all_years = sorted(dataloader_gl.data['YEAR'].unique())\n",
    "\n",
    "# Use the most recent years as test data\n",
    "train_years = all_years[:-test_years_count]\n",
    "test_years = all_years[-test_years_count:]\n",
    "\n",
    "# Statistics prints\n",
    "print('Size of test data:', len(test_years))\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_years)]\n",
    "print('Size of train data:', len(train_years))\n",
    "if len(train_years) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(test_years) / len(train_years)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "## CV Splits\n",
    "splits, test_set, train_set = get_CV_splits(\n",
    "                                dataloader_gl,\n",
    "                                test_split_on='YEAR',\n",
    "                                test_splits=test_years,\n",
    "                                random_state=cfg.seed)\n",
    "\n",
    "\n",
    "\n",
    "print('Train year: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                      train_set['splits_vals']))\n",
    "print('Test years: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "\n",
    "region_name='Prediction most recent 10 years'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A.3.3. Train-Test set plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHeatmap(dataloader_gl.data, test_glaciers, period='annual')\n",
    "#plotHeatmap(dataloader_gl.data, test_glaciers, period='winter')\n",
    "#plotHeatmap(dataloader_gl.data, test_glaciers, period='summer')\n",
    "visualiseSplits(test_set['y'], train_set['y'], splits)\n",
    "visualiseInputs(train_set, test_set, vois_climate)\n",
    "\n",
    "# Number of measurements per year:\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 10))\n",
    "test_set['df_X'].groupby(['YEAR', 'PERIOD']).size().unstack().plot(\n",
    "    kind='bar', stacked=True, color=[color_dark_blue, color_pink, color_orange], ax=ax[0])\n",
    "ax[0].set_title('Number of measurements per year for test glaciers')\n",
    "\n",
    "# Number of measurements per year:\n",
    "train_set['df_X'].groupby(['YEAR', 'PERIOD']).size().unstack().plot(\n",
    "    kind='bar', stacked=True, color=[color_dark_blue, color_pink, color_orange], ax=ax[1])\n",
    "ax[1].set_title('Number of measurements per year for train glaciers')\n",
    "plt.tight_layout()\n",
    "\n",
    "plot_climate_glacier_elevations(test_glaciers, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.4. XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "# For each of the XGBoost parameter, define the grid range\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, 6, 7, 8],\n",
    "    'n_estimators':\n",
    "    [50, 100, 200, 300, 400, 500, 600,\n",
    "     700],  # number of trees (too many = overfitting, too few = underfitting)\n",
    "    'learning_rate': [0.01, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "}\n",
    "\n",
    "param_init = {}\n",
    "param_init['device'] = 'cuda:0'\n",
    "param_init['tree_method'] = 'hist'\n",
    "param_init[\"random_state\"] = cfg.seed\n",
    "param_init[\"n_jobs\"] = cfg.numJobs\n",
    "\n",
    "\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect\",\n",
    "    \"slope\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Feature columns:\n",
    "feature_columns = [\n",
    "    'ELEVATION_DIFFERENCE'\n",
    "] + list(vois_climate) + list(vois_topographical)\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "df_X_train_subset = train_set['df_X'][all_columns]\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of testing dataset:', test_set['df_X'][all_columns].shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "RUN = False\n",
    "if RUN:\n",
    "    # Create a CustomXGBoostRegressor instance\n",
    "    custom_xgboost = mbm.models.CustomXGBoostRegressor(cfg, **param_init)\n",
    "    custom_xgboost.randomsearch(\n",
    "        parameters=param_grid,\n",
    "        n_iter=45,\n",
    "        splits=splits,\n",
    "        features=df_X_train_subset,\n",
    "        targets=train_set['y'],\n",
    "    )\n",
    "\n",
    "    # save best model\n",
    "    custom_xgboost.save_model(f'xgb_Gebroulaz_reworked_11_06.pkl')\n",
    "else:\n",
    "    # read model\n",
    "    custom_xgboost = mbm.models.CustomXGBoostRegressor(cfg)\n",
    "    custom_xgboost.load_model(\n",
    "        f'xgb_Gebroulaz_reworked_11_06.pkl')\n",
    "\n",
    "# Get best parameters and estimator\n",
    "best_params = custom_xgboost.param_search.best_params_\n",
    "best_estimator = custom_xgboost.param_search.best_estimator_\n",
    "print(\"Best parameters:\\n\", best_params)\n",
    "print(\"Best score:\\n\", custom_xgboost.param_search.best_score_)\n",
    "\n",
    "# Make predictions on test:\n",
    "# Set to CPU for predictions:\n",
    "best_estimator_cpu = best_estimator.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on test\n",
    "features_test, metadata_test = best_estimator_cpu._create_features_metadata(\n",
    "    test_set['df_X'][all_columns])\n",
    "y_pred = best_estimator_cpu.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = best_estimator_cpu.aggrPredict(metadata_test, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = best_estimator_cpu.score(test_set['df_X'][all_columns],\n",
    "                                 test_set['y'])  # negative\n",
    "print('Overall score:', np.abs(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotGridSearchScore(cv_results_=custom_xgboost.param_search.cv_results_,\n",
    "                    lossType=cfg.loss)\n",
    "plotGridSearchParams(custom_xgboost.param_search.cv_results_,\n",
    "                     param_grid,\n",
    "                     lossType=cfg.loss)\n",
    "plotGridSearchParams(custom_xgboost.param_search.cv_results_,\n",
    "                     param_grid,\n",
    "                     lossType=cfg.loss,\n",
    "                     N=10)\n",
    "\n",
    "print_top_n_models(custom_xgboost.param_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIPlot(best_estimator, feature_columns, vois_climate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Predictions of best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test:\n",
    "# Set to CPU for predictions:\n",
    "best_estimator_cpu = best_estimator.set_params(device='cpu')\n",
    "\n",
    "features_test, metadata_test = best_estimator_cpu._create_features_metadata(\n",
    "    test_set['df_X'][all_columns])\n",
    "y_pred = best_estimator_cpu.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "y_pred_agg = best_estimator_cpu.aggrPredict(metadata_test, features_test)\n",
    "grouped_ids = getDfAggregatePred(test_set, y_pred_agg, all_columns)\n",
    "PlotPredictions(grouped_ids, y_pred, metadata_test, test_set,\n",
    "                best_estimator_cpu, include_summer=True)\n",
    "plt.suptitle(f'XGBoost tested on {test_set[\"splits_vals\"]}', fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotIndividualGlacierPredVsTruth(grouped_ids, base_figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPredictionsCombined(grouped_ids, y_pred, metadata_test, test_set,\n",
    "                best_estimator_cpu, region_name=test_set[\"splits_vals\"], include_summer = True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for points with large prediction errors\n",
    "grouped_ids_test = grouped_ids.copy()\n",
    "grouped_ids_test['pmb_diff'] = grouped_ids_test['target'] - grouped_ids_test['pred']\n",
    "pd.set_option('display.max_colwidth', None) \n",
    "display(grouped_ids_test[abs(grouped_ids_test['pmb_diff'] > 3)\n",
    "])\n",
    "pd.reset_option('display.max_colwidth')\n",
    "\n",
    "# Plot climate variables for specific points\n",
    "point_ids = [ 'geb_summer_smb_abl_2016_ablation_setup2013_13',\n",
    "             'geb_summer_smb_abl_2017_ablation_setup2016_10'\n",
    "]\n",
    "plot_point_climate_variables(\n",
    "    point_ids=point_ids,\n",
    "    data_monthly=data_monthly,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_units=vois_units\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.1. Train CH Test FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in\n",
    "data_FR = pd.read_csv(cfg.dataPath + path_PMB_GLACIOCLIM_csv + 'FR_wgms_dataset_all_oggm.csv')\n",
    "data_CH = pd.read_csv('/home/mburlet/scratch/data/DATA_MB/CH_wgms_dataset_all_04_06_oggm.csv')\n",
    "# Adjust dfs to match\n",
    "data_CH['GLACIER_ZONE'] = 'Placeholder'\n",
    "data_CH['DATA_MODIFICATION'] = ''\n",
    "data_CH = data_CH.drop(columns=['aspect_sgi', 'slope_sgi', 'topo_sgi'], errors='ignore')\n",
    "\n",
    "data_FR = data_FR[data_FR['PERIOD'] != 'summer']\n",
    "\n",
    "# Merge FR with CH\n",
    "data_FR_CH = pd.concat([data_FR, data_CH], axis=0).reset_index(drop=True)\n",
    "\n",
    "display(len(data_FR_CH['GLACIER'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLACIOCLIM_csv,\n",
    "    'era5_climate_data': cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data_Alps.nc',\n",
    "    'geopotential_data': cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure_Alps.nc'\n",
    "}\n",
    "\n",
    "RUN = True\n",
    "dataloader_gl = process_or_load_data_glacioclim(run_flag=RUN,\n",
    "                                     df=data_FR_CH,\n",
    "                                     paths=paths,\n",
    "                                     cfg=cfg,\n",
    "                                     vois_climate=vois_climate,\n",
    "                                     vois_topographical=vois_topographical,\n",
    "                                     output_file= 'FR_CH_wgms_dataset_monthly_full.csv')\n",
    "data_monthly_FR_CH = dataloader_gl.data\n",
    "\n",
    "display(data_monthly_FR_CH.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_glaciers = list(data_FR['GLACIER'].unique())\n",
    "\n",
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers\n",
    "train_glaciers = list(data_CH['GLACIER'].unique())\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(test_glaciers)]\n",
    "\n",
    "# Statistics prints\n",
    "print('Size of test data:', len(data_test))\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of train data:', len(data_train))\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "## CV Splits\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "    \n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                      train_set['splits_vals']))\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHeatmap(dataloader_gl.data, test_glaciers, period='annual')\n",
    "#plotHeatmap(dataloader_gl.data, test_glaciers, period='winter')\n",
    "#plotHeatmap(dataloader_gl.data, test_glaciers, period='summer')\n",
    "visualiseSplits(test_set['y'], train_set['y'], splits)\n",
    "visualiseInputs(train_set, test_set, vois_climate)\n",
    "\n",
    "# Number of measurements per year:\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 10))\n",
    "test_set['df_X'].groupby(['YEAR', 'PERIOD']).size().unstack().plot(\n",
    "    kind='bar', stacked=True, color=[color_dark_blue, color_pink], ax=ax[0])\n",
    "ax[0].set_title('Number of measurements per year for test glaciers')\n",
    "\n",
    "# Number of measurements per year:\n",
    "train_set['df_X'].groupby(['YEAR', 'PERIOD']).size().unstack().plot(\n",
    "    kind='bar', stacked=True, color=[color_dark_blue, color_pink], ax=ax[1])\n",
    "ax[1].set_title('Number of measurements per year for train glaciers')\n",
    "plt.tight_layout()\n",
    "\n",
    "plot_climate_glacier_elevations(test_glaciers, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.2.2. XGBoost Transfer Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "# For each of the XGBoost parameter, define the grid range\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, 6, 7, 8],\n",
    "    'n_estimators':\n",
    "    [50, 100, 200, 300, 400, 500, 600,\n",
    "     700],  # number of trees (too many = overfitting, too few = underfitting)\n",
    "    'learning_rate': [0.01, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "}\n",
    "\n",
    "param_init = {}\n",
    "param_init['device'] = 'cuda:0'\n",
    "param_init['tree_method'] = 'hist'\n",
    "param_init[\"random_state\"] = cfg.seed\n",
    "param_init[\"n_jobs\"] = cfg.numJobs\n",
    "\n",
    "\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect\",\n",
    "    \"slope\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Feature columns:\n",
    "feature_columns = [\n",
    "    'ELEVATION_DIFFERENCE'\n",
    "] + list(vois_climate) + list(vois_topographical)\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "df_X_train_subset = train_set['df_X'][all_columns]\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of testing dataset:', test_set['df_X'][all_columns].shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "RUN = False\n",
    "if RUN:\n",
    "    # Create a CustomXGBoostRegressor instance\n",
    "    custom_xgboost = mbm.models.CustomXGBoostRegressor(cfg, **param_init)\n",
    "    custom_xgboost.randomsearch(\n",
    "        parameters=param_grid,\n",
    "        n_iter=45,\n",
    "        splits=splits,\n",
    "        features=df_X_train_subset,\n",
    "        targets=train_set['y'],\n",
    "    )\n",
    "\n",
    "    # save best model\n",
    "    custom_xgboost.save_model(f'xgb_CH_FR_11_06.pkl')\n",
    "else:\n",
    "    # read model\n",
    "    custom_xgboost = mbm.models.CustomXGBoostRegressor(cfg)\n",
    "    custom_xgboost.load_model(\n",
    "        f'xgb_CH_FR_11_06.pkl')\n",
    "\n",
    "# Get best parameters and estimator\n",
    "best_params = custom_xgboost.param_search.best_params_\n",
    "best_estimator = custom_xgboost.param_search.best_estimator_\n",
    "print(\"Best parameters:\\n\", best_params)\n",
    "print(\"Best score:\\n\", custom_xgboost.param_search.best_score_)\n",
    "\n",
    "# Make predictions on test:\n",
    "# Set to CPU for predictions:\n",
    "best_estimator_cpu = best_estimator.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on test\n",
    "features_test, metadata_test = best_estimator_cpu._create_features_metadata(\n",
    "    test_set['df_X'][all_columns])\n",
    "y_pred = best_estimator_cpu.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = best_estimator_cpu.aggrPredict(metadata_test, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = best_estimator_cpu.score(test_set['df_X'][all_columns],\n",
    "                                 test_set['y'])  # negative\n",
    "print('Overall score:', np.abs(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotGridSearchScore(cv_results_=custom_xgboost.param_search.cv_results_,\n",
    "                    lossType=cfg.loss)\n",
    "plotGridSearchParams(custom_xgboost.param_search.cv_results_,\n",
    "                     param_grid,\n",
    "                     lossType=cfg.loss)\n",
    "plotGridSearchParams(custom_xgboost.param_search.cv_results_,\n",
    "                     param_grid,\n",
    "                     lossType=cfg.loss,\n",
    "                     N=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIPlot(best_estimator, feature_columns, vois_climate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test:\n",
    "# Set to CPU for predictions:\n",
    "best_estimator_cpu = best_estimator.set_params(device='cpu')\n",
    "\n",
    "features_test, metadata_test = best_estimator_cpu._create_features_metadata(\n",
    "    test_set['df_X'][all_columns])\n",
    "y_pred = best_estimator_cpu.predict(features_test)\n",
    "\n",
    "y_pred_agg = best_estimator_cpu.aggrPredict(metadata_test, features_test)\n",
    "grouped_ids = getDfAggregatePred(test_set, y_pred_agg, all_columns)\n",
    "\n",
    "\n",
    "PlotPredictions(grouped_ids, y_pred, metadata_test, test_set,\n",
    "                best_estimator_cpu)\n",
    "plt.suptitle(f'XGBoost tested on {test_set[\"splits_vals\"]}', fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotIndividualGlacierPredVsTruth(grouped_ids, base_figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPredictionsCombined(grouped_ids, y_pred, metadata_test, test_set,\n",
    "                best_estimator_cpu, region_name='CH Train FR Test', include_summer = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for points with large prediction errors\n",
    "grouped_ids_test = grouped_ids.copy()\n",
    "grouped_ids_test['pmb_diff'] = grouped_ids_test['target'] - grouped_ids_test['pred']\n",
    "pd.set_option('display.max_colwidth', None) \n",
    "display(grouped_ids_test[abs(grouped_ids_test['pmb_diff'] > 5)\n",
    "])\n",
    "pd.reset_option('display.max_colwidth')\n",
    "\n",
    "# Plot climate variables for specific points\n",
    "point_ids = ['mdg_langue_winter_smb_abl_2011_Langue_setup2009_4',\n",
    "             'sarennes_complete_annual_2010_1',\n",
    "             'sarennes_complete_annual_1997_2'\n",
    "]\n",
    "plot_point_climate_variables(\n",
    "    point_ids=point_ids,\n",
    "    data_monthly=data_monthly_FR_CH,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_units=vois_units\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions of custom parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_params = {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 800}\n",
    "\n",
    "# Feature columns:\n",
    "feature_columns = [\n",
    "    'ELEVATION_DIFFERENCE'\n",
    "] + list(vois_climate) + list(vois_topographical)\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "df_X_train_subset = train_set['df_X'][all_columns]\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of testing dataset:', test_set['df_X'][all_columns].shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "params = {**param_init, **custom_params}\n",
    "print(params)\n",
    "custom_model = mbm.models.CustomXGBoostRegressor(cfg, **params)\n",
    "\n",
    "# Fit on train data:\n",
    "custom_model.fit(train_set['df_X'][all_columns], train_set['y'])\n",
    "\n",
    "# Make predictions on test\n",
    "custom_model = custom_model.set_params(device='cpu')\n",
    "features_test, metadata_test = custom_model._create_features_metadata(\n",
    "    test_set['df_X'][all_columns])\n",
    "y_pred = custom_model.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = custom_model.aggrPredict(metadata_test, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = custom_model.score(test_set['df_X'][all_columns],\n",
    "                           test_set['y'])  # negative\n",
    "print('Overall score:', np.abs(score))\n",
    "\n",
    "grouped_ids = getDfAggregatePred(test_set, y_pred_agg, all_columns)\n",
    "PlotPredictions(grouped_ids, y_pred, metadata_test, test_set, custom_model)\n",
    "plt.suptitle(f'MBM tested on {test_set[\"splits_vals\"]}', fontsize=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Aggregate predictions to annual or winter:\n",
    "PlotIndividualGlacierPredVsTruth(grouped_ids, base_figsize=(20, 15))\n",
    "\n",
    "FIPlot(custom_model, feature_columns, vois_climate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
