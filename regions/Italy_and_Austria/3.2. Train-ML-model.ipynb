{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost ML-Model. \n",
    "\n",
    "###### A. part of Notebook is regional learning with the IT_AT/WGMS dataset. B. part is transfer learning with Swiss/GLAMOS train set and IT_AT/WGMS test set\n",
    "\n",
    "### Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from calendar import month_abbr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.italy_austria_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_IT_AT import *\n",
    "from scripts.xgb_helpers import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "cfg = mbm.ItalyAustriaConfig(dataPath='/home/mburlet/scratch/data/DATA_MB/WGMS/IT_AT/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'\n",
    "color_orange = '#FFA500'\n",
    "\n",
    "\n",
    "\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect\", # OGGM\n",
    "    \"slope\", # OGGM\n",
    "    \"hugonnet_dhdt\",  # OGGM\n",
    "    \"consensus_ice_thickness\",  # OGGM\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.1. Read in stake data from preprocess, transform to monthly and add ERA5Land data\n",
    "\n",
    "###### Load csv into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wgms = pd.read_csv(cfg.dataPath + path_PMB_WGMS_csv + 'IT_AT_wgms_dataset_all_oggm.csv')\n",
    "\n",
    "print('Number of glaciers:', len(data_wgms['GLACIER'].unique()))\n",
    "print('Number of winter, summer and annual samples:', len(data_wgms[data_wgms.PERIOD == 'annual']) + len(data_wgms[data_wgms.PERIOD == 'winter']) + len(data_wgms[data_wgms.PERIOD == 'summer']))\n",
    "print('Number of annual samples:',\n",
    "      len(data_wgms[data_wgms.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_wgms[data_wgms.PERIOD == 'winter']))\n",
    "print('Number of summer samples:',\n",
    "      len(data_wgms[data_wgms.PERIOD == 'summer']))\n",
    "\n",
    "data_wgms.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Transform into monthly and add ERA5Land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wgms_test = data_wgms.copy()\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_WGMS_csv,\n",
    "    'era5_climate_data': '/home/mburlet/scratch/data/DATA_MB/GLACIOCLIM/ERA5Land/raw/era5_monthly_averaged_data_Alps.nc', #cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data_Alps.nc'\n",
    "    'geopotential_data': '/home/mburlet/scratch/data/DATA_MB/GLACIOCLIM/ERA5Land/raw/era5_geopotential_pressure_Alps.nc'  #cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure_Alps.nc'\n",
    "}\n",
    "\n",
    "RUN = True\n",
    "dataloader_gl = process_or_load_data(run_flag=RUN,\n",
    "                                     df=data_wgms_test,\n",
    "                                     paths=paths,\n",
    "                                     cfg=cfg,\n",
    "                                     vois_climate=vois_climate,\n",
    "                                     vois_topographical=vois_topographical,\n",
    "                                     output_file= 'IT_AT_wgms_dataset_monthly_full.csv')\n",
    "data_monthly = dataloader_gl.data\n",
    "\n",
    "display(data_monthly.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.2. Dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHeatmap(data_monthly, period='annual', plot_elevation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_monthly.groupby(['YEAR', 'PERIOD']).size().unstack().plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    figsize=(20, 5),\n",
    "    color=[color_dark_blue, color_orange, color_pink])\n",
    "plt.title('Number of measurements per year for all glaciers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature intercorrelation\n",
    "plot_feature_correlation(dataloader_gl.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check of variables:\n",
    "df = dataloader_gl.data\n",
    "var_to_plot = ['POINT_BALANCE'] + vois_climate\n",
    "df = df[(df.GLACIER == 'OE. WURTEN K.') & (df.YEAR == 2015)].groupby(\n",
    "    ['MONTHS'])[var_to_plot].mean().reset_index()\n",
    "df['month_nb'] = df.MONTHS.apply(\n",
    "    lambda x: list(month_abbr).index(x.capitalize()))\n",
    "df.sort_values(by='month_nb', inplace=True)\n",
    "fig, ax = plt.subplots(3, 4, figsize=(10, 8))\n",
    "\n",
    "for i, var in enumerate(var_to_plot):\n",
    "    df.plot(x='MONTHS', y=var, marker='o', ax=ax.flatten()[i], legend=False)\n",
    "    if var in vois_climate_long_name.keys():\n",
    "        ax.flatten()[i].set_title(vois_climate_long_name[var], fontsize=12)\n",
    "    else:\n",
    "        ax.flatten()[i].set_title(var, fontsize=12)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the topo variables:\n",
    "df = dataloader_gl.data\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15, 6))\n",
    "for i, var in enumerate(vois_topographical + ['ELEVATION_DIFFERENCE']):\n",
    "    ax = axs.flatten()[i]\n",
    "    sns.histplot(df[var], ax=ax, kde=True)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_title(var)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3. Train-test set split:\n",
    "\n",
    "###### Either run A.3.1. or A.3.2.\n",
    "\n",
    "##### A.3.1. Spatial Cross-Validation\n",
    "\n",
    "###### Uses specific glaciers as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_glaciers = [\n",
    "    'GOLDBERG K.', 'HALLSTAETTER G.', 'HINTEREIS F.', 'JAMTAL F.',\n",
    "    'KESSELWAND F.', 'KLEINFLEISS K.', 'OE. WURTEN K.', 'VENEDIGER K.',\n",
    "    'VERNAGT F.', 'ZETTALUNITZ/MULLWITZ K.'\n",
    "]\n",
    "\n",
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers\n",
    "train_glaciers = [i for i in existing_glaciers if i not in test_glaciers]\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(test_glaciers)]\n",
    "\n",
    "# Statistics prints\n",
    "print('Size of test data:', len(data_test))\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of train data:', len(data_train))\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "## CV Splits\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "    \n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                      train_set['splits_vals']))\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A.3.2. Temporal Cross-Validation\n",
    "\n",
    "###### Uses the last X years as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_years_count = 10\n",
    "\n",
    "all_years = sorted(dataloader_gl.data['YEAR'].unique())\n",
    "\n",
    "# Use the most recent years as test data\n",
    "train_years = all_years[:-test_years_count]\n",
    "test_years = all_years[-test_years_count:]\n",
    "\n",
    "# Statistics prints\n",
    "print('Size of test data:', len(test_years))\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_years)]\n",
    "print('Size of train data:', len(train_years))\n",
    "if len(train_years) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(test_years) / len(train_years)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "## CV Splits\n",
    "splits, test_set, train_set = get_CV_splits(\n",
    "                                dataloader_gl,\n",
    "                                test_split_on='YEAR',\n",
    "                                test_splits=test_years,\n",
    "                                random_state=cfg.seed)\n",
    "\n",
    "\n",
    "\n",
    "print('Train year: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                      train_set['splits_vals']))\n",
    "print('Test years: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A.3.3. Train-Test set plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHeatmap(dataloader_gl.data, test_glaciers, period='annual')\n",
    "#plotHeatmap(dataloader_gl.data, test_glaciers, period='winter')\n",
    "#plotHeatmap(dataloader_gl.data, test_glaciers, period='summer')\n",
    "visualiseSplits(test_set['y'], train_set['y'], splits)\n",
    "visualiseInputs(train_set, test_set, vois_climate)\n",
    "\n",
    "# Number of measurements per year:\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 10))\n",
    "test_set['df_X'].groupby(['YEAR', 'PERIOD']).size().unstack().plot(\n",
    "    kind='bar', stacked=True, color=[color_dark_blue, color_pink, color_orange], ax=ax[0])\n",
    "ax[0].set_title('Number of measurements per year for test glaciers')\n",
    "\n",
    "# Number of measurements per year:\n",
    "train_set['df_X'].groupby(['YEAR', 'PERIOD']).size().unstack().plot(\n",
    "    kind='bar', stacked=True, color=[color_dark_blue, color_pink, color_orange], ax=ax[1])\n",
    "ax[1].set_title('Number of measurements per year for train glaciers')\n",
    "plt.tight_layout()\n",
    "\n",
    "plot_climate_glacier_elevations(test_glaciers, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distributions of test glaciers:\n",
    "f, ax = plt.subplots(len(test_glaciers),\n",
    "                     len(vois_climate) + 3,\n",
    "                     figsize=(16, 10),\n",
    "                     sharey='row',\n",
    "                     sharex='col')\n",
    "\n",
    "for i, test_gl in enumerate(test_glaciers):\n",
    "    test_df_gl = test_set['df_X'][test_set['df_X'].GLACIER == test_gl]\n",
    "    test_df_gl['POINT_BALANCE'].plot.hist(ax=ax[i, 0],\n",
    "                                          color=color_dark_blue,\n",
    "                                          alpha=0.6,\n",
    "                                          density=False)\n",
    "    ax[i, 0].set_title('PMB')\n",
    "    ax[i, 0].set_ylabel(test_gl)\n",
    "    ax[i, 0].set_xlabel('[m w.e.]')\n",
    "    test_df_gl['ELEVATION_DIFFERENCE'].plot.hist(ax=ax[i, 1],\n",
    "                                                 color=color_dark_blue,\n",
    "                                                 alpha=0.6,\n",
    "                                                 density=False)\n",
    "    ax[i, 1].set_title('ELV_DIFF]')\n",
    "    ax[i, 1].set_xlabel('[m]')\n",
    "\n",
    "    for j, voi_clim in enumerate(vois_climate):\n",
    "        ax[i, 2 + j].set_title(voi_clim)\n",
    "        test_df_gl[voi_clim].plot.hist(ax=ax[i, 2 + j],\n",
    "                                       color=color_dark_blue,\n",
    "                                       alpha=0.6,\n",
    "                                       density=False)\n",
    "        ax[i, 2 + j].set_xlabel(vois_units[voi_clim])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.4. XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "# For each of the XGBoost parameter, define the grid range\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, 6, 7, 8],\n",
    "    'n_estimators':\n",
    "    [50, 100, 200, 300, 400, 500, 600,\n",
    "     700],  # number of trees (too many = overfitting, too few = underfitting)\n",
    "    'learning_rate': [0.01, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "}\n",
    "\n",
    "param_init = {}\n",
    "param_init['device'] = 'cuda:0'\n",
    "param_init['tree_method'] = 'hist'\n",
    "param_init[\"random_state\"] = cfg.seed\n",
    "param_init[\"n_jobs\"] = cfg.numJobs\n",
    "\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect\",\n",
    "    \"slope\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Feature columns:\n",
    "feature_columns = [\n",
    "    'ELEVATION_DIFFERENCE'\n",
    "] + list(vois_climate) + list(vois_topographical)\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "df_X_train_subset = train_set['df_X'][all_columns]\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of testing dataset:', test_set['df_X'][all_columns].shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "RUN = False\n",
    "if RUN:\n",
    "    # Create a CustomXGBoostRegressor instance\n",
    "    custom_xgboost = mbm.models.CustomXGBoostRegressor(cfg, **param_init)\n",
    "    custom_xgboost.randomsearch(\n",
    "        parameters=param_grid,\n",
    "        n_iter=45,\n",
    "        splits=splits,\n",
    "        features=df_X_train_subset,\n",
    "        targets=train_set['y'],\n",
    "    )\n",
    "\n",
    "    # save best model\n",
    "    custom_xgboost.save_model(f'IT_train_AT_test_reworked_11_06.pkl')\n",
    "else:\n",
    "    # read model\n",
    "    custom_xgboost = mbm.models.CustomXGBoostRegressor(cfg)\n",
    "    custom_xgboost.load_model(\n",
    "        f'IT_train_AT_test_reworked_11_06.pkl')\n",
    "\n",
    "# Get best parameters and estimator\n",
    "best_params = custom_xgboost.param_search.best_params_\n",
    "best_estimator = custom_xgboost.param_search.best_estimator_\n",
    "print(\"Best parameters:\\n\", best_params)\n",
    "print(\"Best score:\\n\", custom_xgboost.param_search.best_score_)\n",
    "\n",
    "# Make predictions on test:\n",
    "# Set to CPU for predictions:\n",
    "best_estimator_cpu = best_estimator.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on test\n",
    "features_test, metadata_test = best_estimator_cpu._create_features_metadata(\n",
    "    test_set['df_X'][all_columns])\n",
    "y_pred = best_estimator_cpu.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = best_estimator_cpu.aggrPredict(metadata_test, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = best_estimator_cpu.score(test_set['df_X'][all_columns],\n",
    "                                 test_set['y'])  # negative\n",
    "print('Overall score:', np.abs(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotGridSearchScore(cv_results_=custom_xgboost.param_search.cv_results_,\n",
    "                    lossType=cfg.loss)\n",
    "plotGridSearchParams(custom_xgboost.param_search.cv_results_,\n",
    "                     param_grid,\n",
    "                     lossType=cfg.loss)\n",
    "plotGridSearchParams(custom_xgboost.param_search.cv_results_,\n",
    "                     param_grid,\n",
    "                     lossType=cfg.loss,\n",
    "                     N=10)\n",
    "\n",
    "print_top_n_models(custom_xgboost.param_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIPlot(best_estimator, feature_columns, vois_climate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Predictions of best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test:\n",
    "# Set to CPU for predictions:\n",
    "best_estimator_cpu = best_estimator.set_params(device='cpu')\n",
    "\n",
    "features_test, metadata_test = best_estimator_cpu._create_features_metadata(\n",
    "    test_set['df_X'][all_columns])\n",
    "y_pred = best_estimator_cpu.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "y_pred_agg = best_estimator_cpu.aggrPredict(metadata_test, features_test)\n",
    "grouped_ids = getDfAggregatePred(test_set, y_pred_agg, all_columns)\n",
    "PlotPredictions(grouped_ids, y_pred, metadata_test, test_set,\n",
    "                best_estimator_cpu, include_summer=True)\n",
    "plt.suptitle(f'XGBoost tested on {test_set[\"splits_vals\"]}', fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotIndividualGlacierPredVsTruth(grouped_ids, base_figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPredictionsCombined(grouped_ids, y_pred, metadata_test, test_set,\n",
    "                best_estimator_cpu, region_name='Prediction most recent 10 years', include_summer = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for points with large prediction errors\n",
    "grouped_ids_test = grouped_ids.copy()\n",
    "grouped_ids_test['pmb_diff'] = grouped_ids_test['target'] - grouped_ids_test['pred']\n",
    "pd.set_option('display.max_colwidth', None) \n",
    "display(grouped_ids_test[abs(grouped_ids_test['pmb_diff'] > 1)\n",
    "])\n",
    "pd.reset_option('display.max_colwidth')\n",
    "\n",
    "# Plot climate variables for specific points\n",
    "point_ids = [ 'GOLDBERG K._2020_43366_AT'\n",
    "]\n",
    "plot_point_climate_variables(\n",
    "    point_ids=point_ids,\n",
    "    data_monthly=data_monthly,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_units=vois_units\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.1. Train CH Test FR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load CH galciers and merge with IT_AT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_CH = pd.read_csv('/home/mburlet/scratch/data/DATA_MB/CH_wgms_dataset_all_04_06_oggm.csv')\n",
    "data_IT_AT = pd.read_csv(cfg.dataPath + path_PMB_WGMS_csv + 'IT_AT_wgms_dataset_all_oggm.csv')\n",
    "data_IT_AT = data_IT_AT[data_IT_AT['PERIOD'] != 'summer']\n",
    "\n",
    "display(data_CH.columns)\n",
    "\n",
    "display(data_IT_AT.columns)\n",
    "\n",
    "data_CH = data_CH.drop(['millan_v', 'aspect_sgi', 'slope_sgi', 'topo_sgi'], axis=1)\n",
    "\n",
    "display(data_CH.columns)\n",
    "\n",
    "# Merge CH with IT_AT\n",
    "data_IT_AT_CH = pd.concat([data_IT_AT, data_CH], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_IT_AT_CH_test = data_IT_AT_CH.copy()\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_WGMS_csv,\n",
    "    'era5_climate_data': '/home/mburlet/scratch/data/DATA_MB/GLACIOCLIM/ERA5Land/raw/era5_monthly_averaged_data_Alps.nc', # cfg.dataPath + path_ERA5_raw 'era5_monthly_averaged_data_Alps.nc'\n",
    "    'geopotential_data': '/home/mburlet/scratch/data/DATA_MB/GLACIOCLIM/ERA5Land/raw/era5_geopotential_pressure_Alps.nc'  # cfg.dataPath + path_ERA5_raw 'era5_geopotential_pressure_Alps.nc'\n",
    "}\n",
    "\n",
    "RUN = True\n",
    "dataloader_gl = process_or_load_data(run_flag=RUN,\n",
    "                                     df=data_IT_AT_CH_test,\n",
    "                                     paths=paths,\n",
    "                                     cfg=cfg,\n",
    "                                     vois_climate=vois_climate,\n",
    "                                     vois_topographical=vois_topographical,\n",
    "                                     output_file= 'CH_IT_AT_wgms_dataset_monthly_full.csv')\n",
    "data_monthly_CH_IT_AT = dataloader_gl.data\n",
    "\n",
    "display(data_monthly_CH_IT_AT.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_glaciers = list(data_IT_AT['GLACIER'].unique())\n",
    "\n",
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers\n",
    "train_glaciers = list(data_CH['GLACIER'].unique())\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(test_glaciers)]\n",
    "\n",
    "# Statistics prints\n",
    "print('Size of test data:', len(data_test))\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of train data:', len(data_train))\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "## CV Splits\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "    \n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                      train_set['splits_vals']))\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHeatmap(dataloader_gl.data, test_glaciers, period='annual')\n",
    "#plotHeatmap(dataloader_gl.data, test_glaciers, period='winter')\n",
    "visualiseSplits(test_set['y'], train_set['y'], splits)\n",
    "visualiseInputs(train_set, test_set, vois_climate)\n",
    "\n",
    "# Number of measurements per year:\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 10))\n",
    "test_set['df_X'].groupby(['YEAR', 'PERIOD']).size().unstack().plot(\n",
    "    kind='bar', stacked=True, color=[color_dark_blue, color_pink], ax=ax[0])\n",
    "ax[0].set_title('Number of measurements per year for test glaciers')\n",
    "\n",
    "# Number of measurements per year:\n",
    "train_set['df_X'].groupby(['YEAR', 'PERIOD']).size().unstack().plot(\n",
    "    kind='bar', stacked=True, color=[color_dark_blue, color_pink], ax=ax[1])\n",
    "ax[1].set_title('Number of measurements per year for train glaciers')\n",
    "plt.tight_layout()\n",
    "\n",
    "plot_climate_glacier_elevations(test_glaciers, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.2.2. XGBoost Transfer Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "# For each of the XGBoost parameter, define the grid range\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, 6, 7, 8],\n",
    "    'n_estimators':\n",
    "    [50, 100, 200, 300, 400, 500, 600,\n",
    "     700],  # number of trees (too many = overfitting, too few = underfitting)\n",
    "    'learning_rate': [0.01, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "}\n",
    "\n",
    "param_init = {}\n",
    "param_init['device'] = 'cuda:0'\n",
    "param_init['tree_method'] = 'hist'\n",
    "param_init[\"random_state\"] = cfg.seed\n",
    "param_init[\"n_jobs\"] = cfg.numJobs\n",
    "\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect\",\n",
    "    \"slope\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Feature columns:\n",
    "feature_columns = [\n",
    "    'ELEVATION_DIFFERENCE'\n",
    "] + list(vois_climate) + list(vois_topographical)\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "df_X_train_subset = train_set['df_X'][all_columns]\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of testing dataset:', test_set['df_X'][all_columns].shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "RUN = False\n",
    "if RUN:\n",
    "    # Create a CustomXGBoostRegressor instance\n",
    "    custom_xgboost = mbm.models.CustomXGBoostRegressor(cfg, **param_init)\n",
    "    custom_xgboost.randomsearch(\n",
    "        parameters=param_grid,\n",
    "        n_iter=45,\n",
    "        splits=splits,\n",
    "        features=df_X_train_subset,\n",
    "        targets=train_set['y'],\n",
    "    )\n",
    "\n",
    "    # save best model\n",
    "    custom_xgboost.save_model(f'xgb_CH_11_06_IT_AT.pkl')\n",
    "else:\n",
    "    # read model\n",
    "    custom_xgboost = mbm.models.CustomXGBoostRegressor(cfg)\n",
    "    custom_xgboost.load_model(\n",
    "        f'xgb_CH_11_06_IT_AT.pkl')\n",
    "\n",
    "# Get best parameters and estimator\n",
    "best_params = custom_xgboost.param_search.best_params_\n",
    "best_estimator = custom_xgboost.param_search.best_estimator_\n",
    "print(\"Best parameters:\\n\", best_params)\n",
    "print(\"Best score:\\n\", custom_xgboost.param_search.best_score_)\n",
    "\n",
    "# Make predictions on test:\n",
    "# Set to CPU for predictions:\n",
    "best_estimator_cpu = best_estimator.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on test\n",
    "features_test, metadata_test = best_estimator_cpu._create_features_metadata(\n",
    "    test_set['df_X'][all_columns])\n",
    "y_pred = best_estimator_cpu.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = best_estimator_cpu.aggrPredict(metadata_test, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = best_estimator_cpu.score(test_set['df_X'][all_columns],\n",
    "                                 test_set['y'])  # negative\n",
    "print('Overall score:', np.abs(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotGridSearchScore(cv_results_=custom_xgboost.param_search.cv_results_,\n",
    "                    lossType=cfg.loss)\n",
    "plotGridSearchParams(custom_xgboost.param_search.cv_results_,\n",
    "                     param_grid,\n",
    "                     lossType=cfg.loss)\n",
    "plotGridSearchParams(custom_xgboost.param_search.cv_results_,\n",
    "                     param_grid,\n",
    "                     lossType=cfg.loss,\n",
    "                     N=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIPlot(best_estimator, feature_columns, vois_climate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test:\n",
    "# Set to CPU for predictions:\n",
    "best_estimator_cpu = best_estimator.set_params(device='cpu')\n",
    "\n",
    "features_test, metadata_test = best_estimator_cpu._create_features_metadata(\n",
    "    test_set['df_X'][all_columns])\n",
    "y_pred = best_estimator_cpu.predict(features_test)\n",
    "\n",
    "y_pred_agg = best_estimator_cpu.aggrPredict(metadata_test, features_test)\n",
    "grouped_ids = getDfAggregatePred(test_set, y_pred_agg, all_columns)\n",
    "\n",
    "\n",
    "PlotPredictions(grouped_ids, y_pred, metadata_test, test_set,\n",
    "                best_estimator_cpu)\n",
    "plt.suptitle(f'XGBoost tested on IT_AT trained on CH', fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotIndividualGlacierPredVsTruth(grouped_ids, base_figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPredictionsCombined(grouped_ids, y_pred, metadata_test, test_set,\n",
    "                best_estimator_cpu, region_name='CH Train IT_AT Test', include_summer = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for points with large prediction errors\n",
    "grouped_ids_test = grouped_ids.copy()\n",
    "grouped_ids_test['pmb_diff'] = grouped_ids_test['target'] - grouped_ids_test['pred']\n",
    "pd.set_option('display.max_colwidth', None) \n",
    "display(grouped_ids_test[abs(grouped_ids_test['pmb_diff'] > 8)\n",
    "])\n",
    "pd.reset_option('display.max_colwidth')\n",
    "\n",
    "# Plot climate variables for specific points\n",
    "point_ids = [ 'MALAVALLE (VEDR. DI) / UEBELTALF._2017_18059_IT',\n",
    "            'MALAVALLE (VEDR. DI) / UEBELTALF._2018_18115_IT'\n",
    "]\n",
    "plot_point_climate_variables(\n",
    "    point_ids=point_ids,\n",
    "    data_monthly=data_monthly_CH_IT_AT,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_units=vois_units\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions of custom parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_params = {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 800}\n",
    "\n",
    "# Feature columns:\n",
    "feature_columns = [\n",
    "    'ELEVATION_DIFFERENCE'\n",
    "] + list(vois_climate) + list(vois_topographical)\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "df_X_train_subset = train_set['df_X'][all_columns]\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of testing dataset:', test_set['df_X'][all_columns].shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "params = {**param_init, **custom_params}\n",
    "print(params)\n",
    "custom_model = mbm.models.CustomXGBoostRegressor(cfg, **params)\n",
    "\n",
    "# Fit on train data:\n",
    "custom_model.fit(train_set['df_X'][all_columns], train_set['y'])\n",
    "\n",
    "# Make predictions on test\n",
    "custom_model = custom_model.set_params(device='cpu')\n",
    "features_test, metadata_test = custom_model._create_features_metadata(\n",
    "    test_set['df_X'][all_columns])\n",
    "y_pred = custom_model.predict(features_test)\n",
    "print('Shape of the test:', features_test.shape)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = custom_model.aggrPredict(metadata_test, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = custom_model.score(test_set['df_X'][all_columns],\n",
    "                           test_set['y'])  # negative\n",
    "print('Overall score:', np.abs(score))\n",
    "\n",
    "grouped_ids = getDfAggregatePred(test_set, y_pred_agg, all_columns)\n",
    "PlotPredictions(grouped_ids, y_pred, metadata_test, test_set, custom_model)\n",
    "plt.suptitle(f'MBM tested on {test_set[\"splits_vals\"]}', fontsize=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Aggregate predictions to annual or winter:\n",
    "PlotIndividualGlacierPredVsTruth(grouped_ids, base_figsize=(20, 15))\n",
    "\n",
    "FIPlot(custom_model, feature_columns, vois_climate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
