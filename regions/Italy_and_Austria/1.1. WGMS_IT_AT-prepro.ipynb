{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import massbalancemachine as mbm\n",
    "import pyproj\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from oggm import utils\n",
    "from tqdm import tqdm\n",
    "from cmcrameri import cm\n",
    "from scripts.helpers import *\n",
    "from scripts.italy_austria_preprocess import *\n",
    "from scripts.config_IT_AT import *\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.ItalyAustriaConfig(dataPath='/home/mburlet/scratch/data/DATA_MB/WGMS/IT_AT/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "cmap = cm.devon\n",
    "\n",
    "# For bars and lines:\n",
    "color_diff_xgb = '#4d4d4d'\n",
    "\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_1 = colors[0]\n",
    "color_2 = '#c51b7d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load stakes into 1 df\n",
    "\n",
    "###### The data has been acquired directly from WGMS's Fluctuations of Glaciers (FoG) Database. Version  10.5904/wgms-fog-2025-02b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stakes = pd.read_csv(cfg.dataPath + path_PMB_WGMS_raw + 'mass_balance_point.csv')\n",
    "df_it_at_RGIId = pd.read_csv(cfg.dataPath + path_PMB_WGMS_raw + 'glacier.csv')\n",
    "\n",
    "# Filter df_stakes to include only rows where country is AT or IT\n",
    "df_it_at_stakes = df_stakes[df_stakes['country'].isin(['AT', 'IT'])].reset_index(drop=True)\n",
    "\n",
    "# Create a mapping dictionary from id to rgi60_ids\n",
    "id_to_rgi_map = dict(zip(df_it_at_RGIId['id'], df_it_at_RGIId['rgi60_ids']))\n",
    "\n",
    "# Add the RGIId column to the filtered DataFrame using glacier_id instead of id\n",
    "df_it_at_stakes['RGIId'] = df_it_at_stakes['glacier_id'].map(id_to_rgi_map)\n",
    "\n",
    "# Display glacier names with NaN RGIId\n",
    "display(f\"Number of rows with NaN RGIId: {df_it_at_stakes['RGIId'].isna().sum()}\")\n",
    "display(df_it_at_stakes[df_it_at_stakes['RGIId'].isna()]['glacier_name'].unique())\n",
    "\n",
    "\n",
    "# Only Careser glacier has NaN RGIIds as only RGIId_50 are listed in the csv file.\n",
    "\n",
    "## find RGIId_60 for Careser glaciers\n",
    "rgi_file = utils.get_rgi_region_file(region=\"11\", version=\"6\")\n",
    "glacier_outline = gpd.read_file(rgi_file)\n",
    "\n",
    "# Search by name\n",
    "careser_glacier = glacier_outline[glacier_outline['Name'].notna() & glacier_outline['Name'].str.contains('CARESER', case=False)]\n",
    "display(careser_glacier[['RGIId', 'Name']])\n",
    "\n",
    "# RGIId_60 and 50 are the same: RGI50-11.01834 and RGI60-11.01834, add to df\n",
    "for glacier_id in df_it_at_stakes[df_it_at_stakes['RGIId'].isna()]['glacier_id'].unique():\n",
    "    df_it_at_stakes.loc[df_it_at_stakes['glacier_id'] == glacier_id, 'RGIId'] = 'RGI60-11.01834'\n",
    "\n",
    "display(f\"Number of rows with NaN RGIId: {df_it_at_stakes['RGIId'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_it_at_stakes['remarks'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and rename columns\n",
    "df_it_at_stakes_renamed = df_it_at_stakes.rename(columns={\n",
    "    'point_id': 'POINT_ID',\n",
    "    'latitude': 'POINT_LAT', \n",
    "    'longitude': 'POINT_LON',\n",
    "    'elevation': 'POINT_ELEVATION',\n",
    "    'begin_date': 'FROM_DATE',\n",
    "    'end_date': 'TO_DATE',\n",
    "    'balance': 'POINT_BALANCE',\n",
    "    'glacier_name': 'GLACIER',\n",
    "    'year': 'YEAR',\n",
    "    'country': 'COUNTRY',\n",
    "    'balance_code': 'PERIOD'\n",
    "})\n",
    "\n",
    "# Create new POINT_ID column\n",
    "df_it_at_stakes_renamed['POINT_ID'] = (\n",
    "    df_it_at_stakes_renamed['GLACIER'] + '_' + \n",
    "    df_it_at_stakes_renamed['YEAR'].astype(str) + '_' + \n",
    "    df_it_at_stakes['id'].astype(str) + '_' + \n",
    "    df_it_at_stakes_renamed['COUNTRY']\n",
    ")\n",
    "# Only keep relevant columns in df\n",
    "df_it_at_stakes_renamed = df_it_at_stakes_renamed[['POINT_ID', \n",
    "                                                 'POINT_LAT', \n",
    "                                                 'POINT_LON', \n",
    "                                                 'POINT_ELEVATION', \n",
    "                                                 'FROM_DATE', \n",
    "                                                 'TO_DATE', \n",
    "                                                 'POINT_BALANCE', \n",
    "                                                 'GLACIER', \n",
    "                                                 'PERIOD', \n",
    "                                                 'RGIId', \n",
    "                                                 'YEAR',\n",
    "                                                 'begin_date_unc',\n",
    "                                                 'end_date_unc']]\n",
    "\n",
    "# Remove rows with NaN values in POINT_LAT, POINT_LON, and POINT_ELEVATION\n",
    "df_it_at_stakes_renamed = df_it_at_stakes_renamed.dropna(subset=['POINT_LAT', 'POINT_LON', 'POINT_ELEVATION'])\n",
    "\n",
    "# change date format to YYYYMMDD\n",
    "df_it_at_stakes_renamed['FROM_DATE'] = df_it_at_stakes_renamed['FROM_DATE'].astype(str).str.replace('-', '')\n",
    "df_it_at_stakes_renamed['TO_DATE'] = df_it_at_stakes_renamed['TO_DATE'].astype(str).str.replace('-', '')\n",
    "\n",
    "# Add data modification column to keep track of mannual changes\n",
    "df_it_at_stakes_renamed['DATA_MODIFICATION'] = ''\n",
    "\n",
    "display(df_it_at_stakes_renamed.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### General Fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any entry anywhere is NaN\n",
    "display(df_it_at_stakes_renamed[df_it_at_stakes_renamed.isna().any(axis=1)])\n",
    "\n",
    "# One stake has a wrong elevation of 296 instead of 2960\n",
    "display(df_it_at_stakes_renamed[df_it_at_stakes_renamed['POINT_ID'] == 'VERNAGT F._2013_15124_AT'])\n",
    "df_it_at_stakes_renamed.loc[df_it_at_stakes_renamed['POINT_ID'] == 'VERNAGT F._2013_15124_AT', 'POINT_ELEVATION'] = 2960\n",
    "df_it_at_stakes_renamed.loc[df_it_at_stakes_renamed['POINT_ID'] == 'VERNAGT F._2013_15124_AT', 'DATA_MODIFICATION'] = 'Elevation corrected from 296 to 2960 m'\n",
    "display(df_it_at_stakes_renamed[df_it_at_stakes_renamed['POINT_ID'] == 'VERNAGT F._2013_15124_AT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Date Fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some stakes have the year 1012 instead of 2012 etc. find all these stakes\n",
    "problematic_dates = []\n",
    "for i, date in enumerate(df_it_at_stakes_renamed['FROM_DATE']):\n",
    "    try:\n",
    "        pd.to_datetime(str(date), format=\"%Y%m%d\")\n",
    "    except:\n",
    "        problematic_dates.append((i, date, 'FROM_DATE'))\n",
    "\n",
    "for i, date in enumerate(df_it_at_stakes_renamed['TO_DATE']):\n",
    "    try:\n",
    "        pd.to_datetime(str(date), format=\"%Y%m%d\")\n",
    "    except:\n",
    "        problematic_dates.append((i, date, 'TO_DATE'))\n",
    "\n",
    "print(f\"Found {len(problematic_dates)} problematic date entries\")\n",
    "if problematic_dates:\n",
    "    print(problematic_dates)\n",
    "\n",
    "# All stakes from same glacier MALAVALLE and date 10120508. Correct the date\n",
    "df_it_at_stakes_renamed.loc[df_it_at_stakes_renamed['FROM_DATE'] == '10120508', 'DATA_MODIFICATION'] = 'Date corrected from 10120508 to 20120508'\n",
    "df_it_at_stakes_renamed['FROM_DATE'] = df_it_at_stakes_renamed['FROM_DATE'].replace('10120508', '20120508')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In the original dataset Glaciers \"OE. WURTEN K.\", \"VERNAGT F.\" and \"GRAND ETRET\" have multiple measurements with date_unc 182 or 182.5\n",
    "## These dates are always entered as start of july, correct them to 30.04 and 01.10\n",
    "\n",
    "display(df_it_at_stakes_renamed[(df_it_at_stakes_renamed['begin_date_unc'] >= 182)|(df_it_at_stakes_renamed['end_date_unc'] >= 182)])\n",
    "\n",
    "# Update the DATA_MODIFICATION column for these rows\n",
    "uncertain_date_mask = (df_it_at_stakes_renamed['begin_date_unc'] >= 182) | (df_it_at_stakes_renamed['end_date_unc'] >= 182)\n",
    "df_it_at_stakes_renamed.loc[uncertain_date_mask, 'DATA_MODIFICATION'] = \"Dates corrected due to high uncertainty (~= 182 days)\"\n",
    "\n",
    "# Update dates\n",
    "df_it_at_stakes_renamed = fix_uncertain_dates(df_it_at_stakes_renamed)\n",
    "\n",
    "display(df_it_at_stakes_renamed[(df_it_at_stakes_renamed['begin_date_unc'] >= 182)|(df_it_at_stakes_renamed['end_date_unc'] >= 182)])\n",
    "\n",
    "\n",
    "# Remove _unc columns, were only needed for fixing uncertain dates\n",
    "df_it_at_stakes_renamed = df_it_at_stakes_renamed[['POINT_ID', \n",
    "                                                 'POINT_LAT', \n",
    "                                                 'POINT_LON', \n",
    "                                                 'POINT_ELEVATION', \n",
    "                                                 'FROM_DATE', \n",
    "                                                 'TO_DATE', \n",
    "                                                 'POINT_BALANCE', \n",
    "                                                 'GLACIER', \n",
    "                                                 'PERIOD', \n",
    "                                                 'RGIId', \n",
    "                                                 'YEAR',\n",
    "                                                 'DATA_MODIFICATION']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_inconsistent, winter_inconsistent = check_period_consistency(df_it_at_stakes_renamed)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "display(annual_inconsistent)\n",
    "display(winter_inconsistent)\n",
    "pd.reset_option('display.max_rows')\n",
    "\n",
    "## 2 Cases of inconsistent periods:\n",
    "# 1. HALLSTAETTER G._2024_63282_AT has MONTH_DIFF of 1, unclear whether this is a date error or an actual measurement (since pmb is also lower than other stakes in that year, just remove it)\n",
    "df_it_at_stakes_renamed = df_it_at_stakes_renamed.loc[df_it_at_stakes_renamed['POINT_ID'] != 'HALLSTAETTER G._2024_63282_AT']\n",
    "\n",
    "# 2. GRAND ETRET in Year 2008 goes from 1999 to 2008, assuming this is a date error and changing year to 2007\n",
    "mask = (df_it_at_stakes_renamed['GLACIER'] == 'GRAND ETRET') & (df_it_at_stakes_renamed['YEAR'] == 2008)\n",
    "df_it_at_stakes_renamed.loc[mask, 'DATA_MODIFICATION'] = 'FROM_DATE year corrected from 1999 to 2007'\n",
    "df_it_at_stakes_renamed.loc[mask, 'FROM_DATE'] = df_it_at_stakes_renamed.loc[mask, 'FROM_DATE'].str.replace('1999', '2007')\n",
    "\n",
    "annual_inconsistent, winter_inconsistent = check_period_consistency(df_it_at_stakes_renamed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge stakes that are close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_it_at_stakes_dropped_stakes = remove_close_points(df_it_at_stakes_renamed)\n",
    "\n",
    "display(find_close_stakes(df_it_at_stakes_dropped_stakes).sort_values('DISTANCE_M'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add OGGM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize OGGM glacier directories\n",
    "gdirs, rgidf = initialize_oggm_glacier_directories(\n",
    "    working_dir='/home/mburlet/scratch/data/DATA_MB/WGMS/OGGM/',\n",
    "    rgi_region=\"11\",\n",
    "    rgi_version=\"6\",\n",
    "    base_url=\n",
    "    \"https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L3-L5_files/2023.1/elev_bands/W5E5_w_data/\",\n",
    "    log_level='WARNING',\n",
    "    task_list=None,\n",
    ")\n",
    "\n",
    "\n",
    "unique_rgis = df_it_at_stakes_dropped_stakes['RGIId'].unique()\n",
    "\n",
    "run = True\n",
    "if run:\n",
    "    export_oggm_grids(gdirs, subset_rgis=unique_rgis, output_path=cfg.dataPath + path_OGGM_xrgrids)\n",
    "\n",
    "df_it_at_stakes_dropped_stakes_topo = merge_pmb_with_oggm_data(df_pmb=df_it_at_stakes_dropped_stakes,\n",
    "                                       gdirs=gdirs,\n",
    "                                       rgi_region=\"11\",\n",
    "                                       rgi_version=\"6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "glacierName = 'GOLDBERG K.'\n",
    "# stakes\n",
    "df_stakes = df_it_at_stakes_dropped_stakes_topo.copy()\n",
    "df_stakes = df_stakes[(df_stakes['GLACIER'] == glacierName)]\n",
    "RGIId = df_stakes.RGIId.unique()[0]\n",
    "print(RGIId)\n",
    "# open OGGM xr for glacier\n",
    "# Get oggm data for that RGI grid\n",
    "ds_oggm = xr.open_dataset(f'{cfg.dataPath + path_OGGM_xrgrids}/{RGIId}.zarr')\n",
    "\n",
    "# Define the coordinate transformation\n",
    "transf = pyproj.Transformer.from_proj(\n",
    "    pyproj.CRS.from_user_input(\"EPSG:4326\"),  # Input CRS (WGS84)\n",
    "    pyproj.CRS.from_user_input(ds_oggm.pyproj_srs),  # Output CRS from dataset\n",
    "    always_xy=True)\n",
    "\n",
    "# Transform all coordinates in the group\n",
    "lon, lat = df_stakes[\"POINT_LON\"].values, df_stakes[\"POINT_LAT\"].values\n",
    "x_stake, y_stake = transf.transform(lon, lat)\n",
    "df_stakes['x'] = x_stake\n",
    "df_stakes['y'] = y_stake\n",
    "\n",
    "# plot stakes\n",
    "plt.figure(figsize=(8, 6))\n",
    "ds_oggm.glacier_mask.plot(cmap='binary')\n",
    "sns.scatterplot(df_stakes,\n",
    "                x='x',\n",
    "                y='y',\n",
    "                hue='within_glacier_shape',\n",
    "                palette=['r', 'b'])\n",
    "plt.title(f'Stakes on {glacierName} (OGGM)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict to within glacier shape\n",
    "df_it_at_stakes_dropped_stakes_topo = df_it_at_stakes_dropped_stakes_topo[df_it_at_stakes_dropped_stakes_topo['within_glacier_shape']]\n",
    "df_it_at_stakes_dropped_stakes_topo = df_it_at_stakes_dropped_stakes_topo.drop(columns=['within_glacier_shape'])\n",
    "\n",
    "print('Number of winter, summer and annual samples:', len(df_it_at_stakes_dropped_stakes_topo))\n",
    "print('Number of annual samples:',\n",
    "      len(df_it_at_stakes_dropped_stakes_topo[df_it_at_stakes_dropped_stakes_topo.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_it_at_stakes_dropped_stakes_topo[df_it_at_stakes_dropped_stakes_topo.PERIOD == 'winter']))\n",
    "print('Number of summer samples:',\n",
    "      len(df_it_at_stakes_dropped_stakes_topo[df_it_at_stakes_dropped_stakes_topo.PERIOD == 'summer']))\n",
    "# Unique glaciers, sorted\n",
    "glacier_list = sorted(df_it_at_stakes_dropped_stakes_topo.GLACIER.unique())\n",
    "print(f\"Number of glaciers: {len(glacier_list)}\")\n",
    "print(f\"Glaciers: {glacier_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN\n",
    "display(df_it_at_stakes_dropped_stakes_topo[df_it_at_stakes_dropped_stakes_topo.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_it_at_stakes_dropped_stakes_topo.head(2))\n",
    "df_it_at_stakes_dropped_stakes_topo.to_csv(cfg.dataPath + path_PMB_WGMS_csv + 'IT_AT_wgms_dataset_all_oggm.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
