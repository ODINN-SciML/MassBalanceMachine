{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import massbalancemachine as mbm\n",
    "import pyproj\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "from cmcrameri import cm\n",
    "from oggm import utils\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.norway_preprocess import *\n",
    "from scripts.config_NOR import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.NorwayConfig(dataPath='/home/mburlet/scratch/data/DATA_MB/WGMS/Norway/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "cmap = cm.devon\n",
    "\n",
    "# For bars and lines:\n",
    "color_diff_xgb = '#4d4d4d'\n",
    "\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_1 = colors[0]\n",
    "color_2 = '#c51b7d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load stakes, fill missing start dates, split into winter and annual and transform to WGMS format\n",
    "\n",
    "###### Dataset acquired from https://doi.org/10.58059/sjse-6w92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stakes = pd.read_csv(cfg.dataPath + path_PMB_WGMS_raw + 'glaciological_point_mass_balance_Norway.csv')\n",
    "\n",
    "df_stakes = df_stakes.rename(columns={'rgiid': 'RGIId'})\n",
    "\n",
    "# Add data modification column to keep track of mannual changes\n",
    "df_stakes['DATA_MODIFICATION'] = ''\n",
    "\n",
    "# FROM_DATE is missing in some glaciers despite having pmb measurements, fill with start of hydr. year\n",
    "df_stakes_filled = fill_missing_dates(df_stakes)\n",
    "\n",
    "# Split into winter and annual measurements\n",
    "df_stakes_split = split_stake_measurements(df_stakes_filled)\n",
    "\n",
    "# Transform to WGMS format\n",
    "df_stakes_split = df_stakes_split.rename(columns={\n",
    "    'lat': 'POINT_LAT', \n",
    "    'lon': 'POINT_LON',\n",
    "    'altitude': 'POINT_ELEVATION',\n",
    "    'breid': 'GLACIER',\n",
    "})\n",
    "# Only keep relevant columns in df\n",
    "df_stakes_split = df_stakes_split[[\n",
    "                                                 'POINT_LAT', \n",
    "                                                 'POINT_LON', \n",
    "                                                 'POINT_ELEVATION', \n",
    "                                                 'FROM_DATE', \n",
    "                                                 'TO_DATE', \n",
    "                                                 'POINT_BALANCE', \n",
    "                                                 'PERIOD', \n",
    "                                                 'RGIId', \n",
    "                                                 'YEAR',\n",
    "                                                 'GLACIER',\n",
    "                                                 'DATA_MODIFICATION',\n",
    "                                                 'approx_loc',\n",
    "                                                 'approx_altitude']]\n",
    "\n",
    "display(df_stakes_split)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### convert datetime to yyyymmdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stakes_split['FROM_DATE'] = pd.to_datetime(df_stakes_split['FROM_DATE'], format='%d.%m.%Y').dt.strftime('%Y%m%d')\n",
    "df_stakes_split['TO_DATE'] = pd.to_datetime(df_stakes_split['TO_DATE'], format='%d.%m.%Y').dt.strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get glacier names from RGIId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize OGGM glacier directories\n",
    "gdirs, rgidf = initialize_oggm_glacier_directories(\n",
    "    working_dir = cfg.dataPath + path_OGGM,\n",
    "    rgi_region=\"08\",\n",
    "    rgi_version=\"6\",\n",
    "    base_url=\n",
    "    \"https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L3-L5_files/2023.1/elev_bands/W5E5_w_data/\",\n",
    "    log_level='WARNING',\n",
    "    task_list=None,\n",
    ")\n",
    "\n",
    "# Create a dictionary mapping from RGIId to glacier name\n",
    "rgi_to_name_dict = dict(zip(rgidf.RGIId, rgidf.Name))\n",
    "df_stakes_split['GLACIER'] = df_stakes_split['RGIId'].map(rgi_to_name_dict)\n",
    "\n",
    "# RGI60-08.02966 has no glacier name in the RGI map so directly give it name BlÃ¥breen\n",
    "df_stakes_split.loc[df_stakes_split['GLACIER'].isna(), 'GLACIER'] = 'Blabreen'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create unique POINT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_stakes_split.head(2))\n",
    "\n",
    "# Create new POINT_ID column\n",
    "df_stakes_split['POINT_ID'] = (\n",
    "    df_stakes_split['GLACIER'] + '_' + \n",
    "    df_stakes_split['YEAR'].astype(str) + '_' + \n",
    "    df_stakes_split['PERIOD'].astype(str) + '_' +\n",
    "    df_stakes_split['POINT_LAT'].astype(str) + '_' +\n",
    "    df_stakes_split['POINT_LON'].astype(str) + '_' +\n",
    "    df_stakes_split['approx_loc'].astype(str) + '_' +\n",
    "    df_stakes_split['approx_altitude'].astype(str) + '_' +\n",
    "    df_stakes_split.index.astype(str)\n",
    ")\n",
    "\n",
    "# Drop columns that are not needed anymore\n",
    "df_stakes_split = df_stakes_split.drop(columns=['approx_loc', 'approx_altitude'])\n",
    "\n",
    "display(df_stakes_split.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix problematic date ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_inconsistent, winter_inconsistent = check_period_consistency(df_stakes_split)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "if len(annual_inconsistent) > 0:\n",
    "    print(\"\\nInconsistent annual periods:\")\n",
    "    display(annual_inconsistent[['GLACIER', 'FROM_DATE', 'TO_DATE', 'MONTH_DIFF', 'PERIOD', 'YEAR', 'RGIId', 'POINT_ID']])\n",
    "\n",
    "if len(winter_inconsistent) > 0:\n",
    "    print(\"\\nInconsistent winter periods:\")\n",
    "    display(winter_inconsistent[['GLACIER', 'FROM_DATE', 'TO_DATE', 'MONTH_DIFF', 'PERIOD', 'YEAR', 'RGIId', 'POINT_ID']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### First fix is to switch all the months that have been wrongfully recorded as 01 instead of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function corrects the dates where 01 (Jan) has been entered as the month instead of 10 (Oct)\n",
    "df_stakes_split_fixed1 = fix_january_to_october_dates(df_stakes_split, annual_inconsistent, winter_inconsistent)\n",
    "\n",
    "annual_inconsistent, winter_inconsistent = check_period_consistency(df_stakes_split_fixed1)\n",
    "\n",
    "if len(annual_inconsistent) > 0:\n",
    "    print(\"\\nInconsistent annual periods:\")\n",
    "    display(annual_inconsistent[['GLACIER', 'FROM_DATE', 'TO_DATE', 'MONTH_DIFF', 'PERIOD', 'YEAR', 'RGIId', 'POINT_ID']])\n",
    "\n",
    "if len(winter_inconsistent) > 0:\n",
    "    print(\"\\nInconsistent winter periods:\")\n",
    "    display(winter_inconsistent[['GLACIER', 'FROM_DATE', 'TO_DATE', 'MONTH_DIFF', 'PERIOD', 'YEAR', 'RGIId', 'POINT_ID']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Second fix is some by hand and the rest are wrong years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix outliers that don't have common explanation by hand\n",
    "# May instead of september\n",
    "df_stakes_split_fixed1.loc[df_stakes_split_fixed1['POINT_ID']=='Svartisheibreen_1994_annual_66.55012_13.72724_N_N_883', \n",
    "                          ['TO_DATE', 'DATA_MODIFICATION']] = ['19940915', 'Changed TO_DATE month from May to September']\n",
    "df_stakes_split_fixed1.loc[df_stakes_split_fixed1['POINT_ID']=='Svartisheibreen_1994_annual_66.54826_13.73128_N_N_884', \n",
    "                          ['TO_DATE', 'DATA_MODIFICATION']] = ['19940915', 'Changed TO_DATE month from May to September']\n",
    "# TO_DATE annual wrong year\n",
    "df_stakes_split_fixed1.loc[df_stakes_split_fixed1['POINT_ID']=='Aalfotbreen_1974_annual_61.74236_5.64623_N_N_1386', \n",
    "                          ['TO_DATE', 'DATA_MODIFICATION']] = ['19740920', 'Changed TO_DATE year from 1975 to 1974']\n",
    "df_stakes_split_fixed1.loc[df_stakes_split_fixed1['POINT_ID']=='Aalfotbreen_1971_annual_61.75213_5.63165_N_N_1493', \n",
    "                          ['TO_DATE', 'DATA_MODIFICATION']] = ['19711124', 'Changed TO_DATE year from 1970 to 1971']\n",
    "df_stakes_split_fixed1.loc[df_stakes_split_fixed1['POINT_ID']=='Graafjellsbrea_2009_annual_60.06923_6.38925_N_N_3545', \n",
    "                          ['TO_DATE', 'DATA_MODIFICATION']] = ['20091013', 'Changed TO_DATE year from 2019 to 2009']\n",
    "df_stakes_split_fixed1.loc[df_stakes_split_fixed1['POINT_ID']=='Bondhusbrea_1981_annual_60.03108_6.31014_N_N_3738', \n",
    "                          ['TO_DATE', 'DATA_MODIFICATION']] = ['19810827', 'Changed TO_DATE year fomr 1980 to 1981']\n",
    "# TO_DATE winter wrong year\n",
    "df_stakes_split_fixed1.loc[df_stakes_split_fixed1['POINT_ID']=='Langfjordjoekulen_2019_winter_70.12528_21.71827_N_N_4019', \n",
    "                          ['TO_DATE', 'DATA_MODIFICATION', 'YEAR', 'POINT_ID']] = ['20200526', 'Changed TO_DATE year fomr 2019 to 2020', '2020', 'Langfjordjoekulen_2020_winter_70.12528_21.71827_N_N_4019']\n",
    "\n",
    "df_stakes_split_fixed1.loc[df_stakes_split_fixed1['POINT_ID']=='Blaaisen_1966_winter_68.33479_17.85005_N_N_4155', \n",
    "                          ['TO_DATE', 'DATA_MODIFICATION', 'YEAR', 'POINT_ID']] = ['19670520', 'Changed TO_DATE year fomr 1966 to 1967', '1967', 'Blaaisen_1967_winter_68.33479_17.85005_N_N_4155']\n",
    "\n",
    "df_stakes_split_fixed1.loc[df_stakes_split_fixed1['POINT_ID']=='Nigardsbreen_1963_winter_61.71461_7.11601_N_N_5802', \n",
    "                          ['TO_DATE', 'DATA_MODIFICATION', 'YEAR', 'POINT_ID']] = ['19640507', 'Changed TO_DATE year fomr 1963 to 1964', '1964', 'Nigardsbreen_1964_winter_61.71461_7.11601_N_N_5802']\n",
    "\n",
    "df_stakes_split_fixed1.loc[df_stakes_split_fixed1['POINT_ID']=='Vesledalsbreen_1967_winter_61.84804_7.25335_N_N_6694', \n",
    "                          ['TO_DATE', 'DATA_MODIFICATION', 'YEAR', 'POINT_ID']] = ['19680418', 'Changed TO_DATE year fomr 1967 to 1968', '1968', 'Vesledalsbreen_1968_winter_61.84804_7.25335_N_N_6694']\n",
    "\n",
    "df_stakes_split_fixed1.loc[df_stakes_split_fixed1['POINT_ID']=='Hellstugubreen_2010_winter_61.57329_8.44438_N_N_6935', \n",
    "                          ['TO_DATE', 'DATA_MODIFICATION', 'YEAR', 'POINT_ID']] = ['20110505', 'Changed TO_DATE year fomr 2010 to 2011', '2011', 'Hellstugubreen_2011_winter_61.57329_8.44438_N_N_6935']\n",
    "                          \n",
    "# These stakes have nonsensical periods, remove them out of df and index list\n",
    "stakes_to_remove = ['Austdalsbreen_2017_annual_61.81113_7.36766_Y_N_3038',\n",
    "                    'Austdalsbreen_2017_annual_61.80888_7.38239_Y_N_3065',\n",
    "                    'Aalfotbreen_1967_winter_61.74294_5.6365_N_N_5379',\n",
    "                    'Hansebreen_2012_winter_61.74307_5.66278_N_N_5625',\n",
    "                    'Austdalsbreen_2017_winter_61.81113_7.36766_Y_N_6792',\n",
    "                    'Austdalsbreen_2017_winter_61.80888_7.38239_Y_N_6819']\n",
    "df_stakes_split_fixed1 = df_stakes_split_fixed1[~df_stakes_split_fixed1['POINT_ID'].isin(stakes_to_remove)]\n",
    "\n",
    "annual_inconsistent, winter_inconsistent = check_period_consistency(df_stakes_split_fixed1)\n",
    "\n",
    "if len(annual_inconsistent) > 0:\n",
    "    print(\"\\nInconsistent annual periods:\")\n",
    "    display(annual_inconsistent[['GLACIER', 'FROM_DATE', 'TO_DATE', 'MONTH_DIFF', 'PERIOD', 'YEAR', 'RGIId', 'POINT_ID']])\n",
    "\n",
    "if len(winter_inconsistent) > 0:\n",
    "    print(\"\\nInconsistent winter periods:\")\n",
    "    display(winter_inconsistent[['GLACIER', 'FROM_DATE', 'TO_DATE', 'MONTH_DIFF', 'PERIOD', 'YEAR', 'RGIId', 'POINT_ID']])\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_colwidth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### remaining inconsistencies are all wrong FROM_DATE year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_indices = list(annual_inconsistent.index) + list(winter_inconsistent.index)\n",
    "\n",
    "# For each remaining inconsistent record, change the year in FROM_DATE to the previous year\n",
    "for idx in remaining_indices:\n",
    "    # Get year from the YEAR column \n",
    "    year = int(df_stakes_split_fixed1.loc[idx, 'YEAR']) - 1\n",
    "    \n",
    "    # Extract month and day part from current FROM_DATE (keeping positions 4-8 which contain MMDD)\n",
    "    month_day = df_stakes_split_fixed1.loc[idx, 'FROM_DATE'][4:8]\n",
    "    \n",
    "    # Create new FROM_DATE by combining YEAR with the extracted month_day\n",
    "    df_stakes_split_fixed1.loc[idx, 'FROM_DATE'] = f\"{year}{month_day}\"\n",
    "\n",
    "annual_inconsistent, winter_inconsistent = check_period_consistency(df_stakes_split_fixed1)\n",
    "\n",
    "# Display the inconsistent records\n",
    "if len(annual_inconsistent) > 0:\n",
    "    print(\"\\nInconsistent annual periods:\")\n",
    "    display(annual_inconsistent[['GLACIER', 'FROM_DATE', 'TO_DATE', 'MONTH_DIFF', 'PERIOD', 'YEAR', 'RGIId', 'POINT_ID']])\n",
    "\n",
    "if len(winter_inconsistent) > 0:\n",
    "    print(\"\\nInconsistent winter periods:\")\n",
    "    display(winter_inconsistent[['GLACIER', 'FROM_DATE', 'TO_DATE', 'MONTH_DIFF', 'PERIOD', 'YEAR', 'RGIId', 'POINT_ID']])\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_colwidth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge close stakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stakes_merged = remove_close_points(df_stakes_split_fixed1)\n",
    "\n",
    "display(find_close_stakes(df_stakes_merged).sort_values('DISTANCE_M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_stakes_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add OGGM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_rgis = df_stakes_merged['RGIId'].unique()\n",
    "\n",
    "run = True\n",
    "if run:\n",
    "    export_oggm_grids(gdirs, subset_rgis=unique_rgis, output_path=cfg.dataPath + path_OGGM_xrgrids)\n",
    "\n",
    "df_stakes_topo = merge_pmb_with_oggm_data(df_pmb=df_stakes_merged,\n",
    "                                       gdirs=gdirs,\n",
    "                                       rgi_region=\"08\",\n",
    "                                       rgi_version=\"6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "glacierName = 'Langfjordjoekulen'\n",
    "# stakes\n",
    "df_stakes_topo_1 = df_stakes_topo.copy()\n",
    "df_stakes_topo_1 = df_stakes_topo_1[(df_stakes_topo_1['GLACIER'] == glacierName)]\n",
    "RGIId = df_stakes_topo_1['RGIId'].unique()[0]\n",
    "print(RGIId)\n",
    "# open OGGM xr for glacier\n",
    "# Get oggm data for that RGI grid\n",
    "ds_oggm = xr.open_dataset(f'{cfg.dataPath + path_OGGM_xrgrids}/{RGIId}.zarr')\n",
    "\n",
    "# Define the coordinate transformation\n",
    "transf = pyproj.Transformer.from_proj(\n",
    "    pyproj.CRS.from_user_input(\"EPSG:4326\"),  # Input CRS (WGS84)\n",
    "    pyproj.CRS.from_user_input(ds_oggm.pyproj_srs),  # Output CRS from dataset\n",
    "    always_xy=True)\n",
    "\n",
    "# Transform all coordinates in the group\n",
    "lon, lat = df_stakes_topo_1[\"POINT_LON\"].values, df_stakes_topo_1[\"POINT_LAT\"].values\n",
    "x_stake, y_stake = transf.transform(lon, lat)\n",
    "df_stakes_topo_1['x'] = x_stake\n",
    "df_stakes_topo_1['y'] = y_stake\n",
    "\n",
    "# plot stakes\n",
    "plt.figure(figsize=(8, 6))\n",
    "ds_oggm.glacier_mask.plot(cmap='binary')\n",
    "sns.scatterplot(df_stakes_topo_1,\n",
    "                x='x',\n",
    "                y='y',\n",
    "                hue='within_glacier_shape',\n",
    "                palette=['r', 'b'])\n",
    "plt.title(f'Stakes on {glacierName} (OGGM)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Only keep glaciers within RGIId shape and drop rows with NaN values anywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict to within glacier shape\n",
    "df_stakes_topo = df_stakes_topo[df_stakes_topo['within_glacier_shape'] == True]\n",
    "df_stakes_topo = df_stakes_topo.drop(columns=['within_glacier_shape'])\n",
    "\n",
    "# Drop rows with NaN in consensus_ice_thickness\n",
    "df_stakes_topo = df_stakes_topo.dropna(subset=['consensus_ice_thickness'])\n",
    "\n",
    "print('Number of winter and annual samples:', len(df_stakes_topo))\n",
    "print('Number of annual samples:',\n",
    "      len(df_stakes_topo[df_stakes_topo.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_stakes_topo[df_stakes_topo.PERIOD == 'winter']))\n",
    "\n",
    "# Unique glaciers, sorted\n",
    "glacier_list = sorted(df_stakes_topo.GLACIER.unique())\n",
    "print(f\"Number of glaciers: {len(glacier_list)}\")\n",
    "print(f\"Glaciers: {glacier_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN\n",
    "display(df_stakes_topo[df_stakes_topo.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df to folder\n",
    "df_stakes_topo.to_csv(cfg.dataPath + path_PMB_WGMS_csv + 'Nor_dataset_all_oggm.csv', index=False)\n",
    "display(df_stakes_topo.head(2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
