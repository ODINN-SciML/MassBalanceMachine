{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# MassBalanceMachine Neural Network Model Training - Iceland Region Example\n",
    "<p style='text-align: justify;'>\n",
    "In this notebook, we will simulate the glacier surface mass balance for the Iceland region using a neural network model. This model is designed with a custom objective function that generates monthly predictions based on aggregated observational data. We will create an instance of <code>CustomNeuralNetRegressor</code> and train it using this custom loss function on the stake data from Iceland, which we have prepared in earlier notebooks. If you haven't already, please review the <a href='https://github.com/ODINN-SciML/MassBalanceMachine/blob/main/notebooks/data_preprocessing.ipynb'>data preprocessing</a> and <a href='https://github.com/ODINN-SciML/MassBalanceMachine/blob/main/notebooks/data_preprocessing.ipynb'>data processing WGMS</a> notebooks for more details.</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "The workflow includes several key steps:\n",
    "</p>\n",
    "\n",
    "<ol style=\"margin-left: 20px; padding-left: 0;\">\n",
    "    <li style=\"margin-bottom: 10px;\">\n",
    "        <p style='text-align: justify;'><strong>Data Loading and Preparation:</strong> A <code>Dataloader</code> object is created to handle the loading of data and the creation of a training and testing split. This object also manages the generation of data splits for cross-validation.</p>\n",
    "    </li>\n",
    "    <li style=\"margin-bottom: 10px;\">\n",
    "        <p style='text-align: justify;'><strong>Cross-Validation and Model Training:</strong> Using Scikit-learn's cross-validation techniques, we explore different hyperparameters and train the model on the prepared data splits. This approach ensures a robust evaluation and helps in selecting suitable parameters.</p>\n",
    "    </li>\n",
    "    <li style=\"margin-bottom: 10px;\">\n",
    "        <p style='text-align: justify;'><strong>Aggregated Predictions:</strong> After training, we will display the aggregated monthly predictions generated by the model to visualize and analyze the results.</p>\n",
    "    </li>\n",
    "    <li style=\"margin-bottom: 10px;\">\n",
    "        <p style='text-align: justify;'><strong>Model Evaluation:</strong> Finally, the model's performance is evaluated on the test set, providing insights into its predictive accuracy for glacier mass balance.</p>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import massbalancemachine as mbm\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from skorch.helper import SliceDataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Load dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed:\n",
    "data = pd.read_csv('./example_data/iceland/files/iceland_monthly_dataset.csv')\n",
    "print('Number of winter and annual samples:', len(data))\n",
    "display(data)\n",
    "\n",
    "cfg = mbm.Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Create the Train and Test Dataset and the Data Splits for Cross Validation\n",
    "<p style='text-align: justify;'>\n",
    "First, we create a <code>DataLoader</code> object, which generates both training and testing datasets, as well as the data splits required for cross-validation. To conserve memory, the <code>set_train_test_split</code> method returns iterators containing indices for the training and testing datasets. These indices are then used to retrieve the corresponding data for training and testing. Next, the <code>get_cv_split</code> method provides a list indicating the number of folds needed for cross-validation.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataLoader object with the monthly stake data measurements.\n",
    "dataloader = mbm.DataLoader(cfg, data=data)\n",
    "# Create a training and testing iterators. The parameters are optional. The default value of test_size is 0.3.\n",
    "train_itr, test_itr = dataloader.set_train_test_split(test_size=0.3)\n",
    "\n",
    "# Get all indices of the training and testing dataset at once from the iterators. Once called, the iterators are empty.\n",
    "train_indices, test_indices = list(train_itr), list(test_itr)\n",
    "\n",
    "# Get the features and targets of the training data for the indices as defined above, that will be used during the cross validation.\n",
    "df_X_train = data.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Get test set\n",
    "df_X_test = data.iloc[test_indices]\n",
    "y_test = df_X_test['POINT_BALANCE'].values\n",
    "\n",
    "# Create the cross validation splits based on the training dataset. The default value for the number of splits is 5.\n",
    "type_fold = 'group-meas-id'  # 'group-rgi' # or 'group-meas-id'\n",
    "splits = dataloader.get_cv_split(n_splits=5, type_fold=type_fold)\n",
    "\n",
    "# Print size of train and test\n",
    "print(f\"Size of training set: {len(train_indices)}\")\n",
    "print(f\"Size of test set: {len(test_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Create a CustomNeuralNetRegressor Model\n",
    "<p style='text-align: justify;'>\n",
    "Next, we define the parameter ranges for each hyper-parameter of the neural network. In the subsequent step, we use cross-validation to explore these parameter ranges and select the combination that yields the lowest loss. Additionally, we create a <code>CustomNeuralNetRegressor</code> object.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = df_X_train.columns.difference(cfg.metaData)\n",
    "feature_columns = feature_columns.drop(cfg.notMetaDataNotFeatures)\n",
    "feature_columns = list(feature_columns)\n",
    "nInp = len(feature_columns)\n",
    "\n",
    "network = nn.Sequential(\n",
    "    nn.Linear(nInp, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4, 1),\n",
    ")\n",
    "\n",
    "# Create a CustomNeuralNetRegressor instance\n",
    "params_init = {\"device\": \"cpu\"}\n",
    "custom_nn = mbm.models.CustomNeuralNetRegressor(\n",
    "    cfg,\n",
    "    network,\n",
    "    nbFeatures=nInp,\n",
    "    train_split=\n",
    "    False,  # train_split is disabled since cross validation is handled by the splits variable hereafter\n",
    "    batch_size=16,\n",
    "    verbose=0,\n",
    "    iterator_train__shuffle=True,\n",
    "    **params_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Normalize features:\n",
    "\n",
    "Normalise features and create input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, metadata = custom_nn._create_features_metadata(df_X_train)\n",
    "\n",
    "bounds_features = {\n",
    "    k: (np.min(data[k].values), np.max(data[k].values))\n",
    "    for k in feature_columns\n",
    "}\n",
    "norm = mbm.data_processing.Normalizer(bounds_features)\n",
    "norm_features = norm.normalize(features)\n",
    "\n",
    "# Define the dataset for the NN\n",
    "dataset = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                features=norm_features,\n",
    "                                                metadata=metadata,\n",
    "                                                targets=y_train)\n",
    "splits = dataset.mapSplitsToDataset(splits)\n",
    "\n",
    "# Use SliceDataset to make the dataset accessible as a numpy array for scikit learn\n",
    "dataset = [SliceDataset(dataset, idx=0), SliceDataset(dataset, idx=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Train the CustomNeuralNetRegressor Model\n",
    "\n",
    "<p style='text-align: justify; margin-bottom: 5px;'>\n",
    "In the following cell, we begin training our model using either <strong>GridSearchCV</strong> or <strong>RandomizedSearchCV</strong>:\n",
    "</p>\n",
    "\n",
    "<ul style=\"margin-left: 20px; padding-left: 0; margin-bottom: 5px;\">\n",
    "  <li style=\"margin-bottom: 10px;\">\n",
    "    <p style='text-align: justify;'><strong>GridSearchCV</strong> performs an exhaustive search across all possible parameter combinations to find the best set for optimal performance using cross-validation. While this method is thorough, it is often time-consuming and computationally expensive.</p>\n",
    "  </li>\n",
    "  <li style=\"margin-bottom: 0px;\">\n",
    "    <p style='text-align: justify;'><strong>RandomizedSearchCV</strong>, on the other hand, samples a fixed number of parameter combinations from the distribution, making it more efficient in terms of time and computational resources, especially with larger hyperparameter spaces. However, this approach may miss some of the best parameter combinations that aren't selected in the random sampling.</p>\n",
    "  </li>\n",
    "</ul>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "You can choose either of the two training methods. Both methods will use all CPU cores by default. If you want to adjust the number of cores used, you can change the <code>num_jobs</code> parameter.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Grid search or train custom model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_GRIDSEARCH = True\n",
    "if RUN_GRIDSEARCH:\n",
    "    # For each of the NN hyper-parameter, define the grid range\n",
    "    parameters = {'lr': [0.001, 0.01], 'max_epochs': [1000, 2000]}\n",
    "    \n",
    "    # GridSearch\n",
    "    # custom_nn.gridsearch(parameters=parameters, splits=splits, dataset=dataset, num_jobs=-1)\n",
    "\n",
    "    # RandomisedSearch, with n_iter the number of parameter settings that are sampled. Trade-off between goodness of the solution\n",
    "    # versus runtime.\n",
    "    custom_nn.randomsearch(\n",
    "        parameters=parameters,\n",
    "        n_iter=20,\n",
    "        splits=splits,\n",
    "        dataset=dataset,\n",
    "    )\n",
    "    best_params = custom_nn.param_search.best_params_\n",
    "    best_estimator = custom_nn.param_search.best_estimator_\n",
    "    print(\"Best parameters:\\n\", best_params)\n",
    "    print(\"Best score:\\n\", custom_nn.param_search.best_score_)\n",
    "\n",
    "    plt.plot(custom_nn.param_search.cv_results_['mean_train_score'])\n",
    "    plt.plot(custom_nn.param_search.cv_results_['mean_test_score'])\n",
    "\n",
    "    # save the best model\n",
    "    best_estimator.save_model('model_iceland_.pkl')\n",
    "\n",
    "    # to load:\n",
    "    # custom_nn.load_model('model_iceland_.pkl')\n",
    "    \n",
    "    \n",
    "else:\n",
    "    custom_nn.set_params(lr=0.01, max_epochs=1000)\n",
    "    custom_nn.fit(dataset[0], dataset[1])\n",
    "    \n",
    "    best_estimator = custom_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Show the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predVSTruth(grouped_ids, mae, rmse, title):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    legend_nn = \"\\n\".join(\n",
    "        (r\"$\\mathrm{MAE_{nn}}=%.3f, \\mathrm{RMSE_{nn}}=%.3f$ \" % (\n",
    "            mae,\n",
    "            rmse,\n",
    "        ), ))\n",
    "\n",
    "    marker_nn = 'o'\n",
    "    sns.scatterplot(grouped_ids,\n",
    "                    x=\"target\",\n",
    "                    y=\"pred\",\n",
    "                    ax=ax,\n",
    "                    alpha=0.5,\n",
    "                    marker=marker_nn)\n",
    "\n",
    "    ax.set_ylabel('Predicted PMB [m w.e.]', fontsize=20)\n",
    "    ax.set_xlabel('Observed PMB [m w.e.]', fontsize=20)\n",
    "\n",
    "    ax.text(0.03,\n",
    "            0.98,\n",
    "            legend_nn,\n",
    "            transform=ax.transAxes,\n",
    "            verticalalignment=\"top\",\n",
    "            fontsize=20)\n",
    "    ax.legend([], [], frameon=False)\n",
    "    # diagonal line\n",
    "    pt = (0, 0)\n",
    "    ax.axline(pt, slope=1, color=\"grey\", linestyle=\"-\", linewidth=0.2)\n",
    "    ax.axvline(0, color=\"grey\", linestyle=\"-\", linewidth=0.2)\n",
    "    ax.axhline(0, color=\"grey\", linestyle=\"-\", linewidth=0.2)\n",
    "    ax.grid()\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to CPU for predictions:\n",
    "nn = best_estimator.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on test\n",
    "features_test, metadata_test = nn._create_features_metadata(df_X_test)\n",
    "norm_features_test = norm.normalize(features_test)\n",
    "\n",
    "dataset_test = mbm.data_processing.AggregatedDataset(\n",
    "    cfg, features=norm_features_test, metadata=metadata_test, targets=y_test)\n",
    "\n",
    "dataset_test = [\n",
    "    SliceDataset(dataset_test, idx=0),\n",
    "    SliceDataset(dataset_test, idx=1)\n",
    "]\n",
    "\n",
    "# Make predictions aggr to meas ID\n",
    "y_pred = nn.predict(dataset_test[0])\n",
    "y_pred_agg = nn.aggrPredict(dataset_test[0])\n",
    "\n",
    "batchIndex = np.arange(len(y_pred_agg))\n",
    "y_true = np.array([e for e in dataset_test[1][batchIndex]])\n",
    "\n",
    "# Calculate scores\n",
    "score = nn.score(dataset_test[0], dataset_test[1])\n",
    "mse, rmse, mae, pearson = nn.evalMetrics(y_pred, y_true)\n",
    "\n",
    "# Aggregate predictions\n",
    "id = dataset_test[0].dataset.indexToId(batchIndex)\n",
    "data = {\n",
    "    'target': [e[0] for e in dataset_test[1]],\n",
    "    'ID': id,\n",
    "    'pred': y_pred_agg\n",
    "}\n",
    "grouped_ids = pd.DataFrame(data)\n",
    "\n",
    "predVSTruth(grouped_ids, mae, rmse, title='NN on test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Make cumulative predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulativePredVSTruth(grouped_ids, title, month_abbr_hydr):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "    marker_nn = 'o'\n",
    "    sns.scatterplot(grouped_ids,\n",
    "                    x=\"monthNb\",\n",
    "                    y=\"cum_pred\",\n",
    "                    hue=\"ID\",\n",
    "                    palette=\"YlOrBr\",\n",
    "                    ax=ax,\n",
    "                    marker=marker_nn)\n",
    "\n",
    "    ax.set_ylabel('Predicted cumulative PMB [m w.e.]', fontsize=15)\n",
    "    ax.set_xlabel('Month', fontsize=15)\n",
    "\n",
    "    plt.xticks(np.arange(1, 13), month_abbr_hydr.keys())\n",
    "\n",
    "    ax.axvline(1, color=\"grey\", linestyle=\"-\", linewidth=0.2)\n",
    "    ax.axhline(0, color=\"grey\", linestyle=\"-\", linewidth=0.2)\n",
    "    ax.grid()\n",
    "    ax.set_title(title, fontsize=18)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cum_pred = nn.cumulative_pred(dataset_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = [\n",
    "    dataset_test[0].dataset.indexToMetadata(index)\n",
    "    [:, cfg.metaData.index('MONTHS')] for index in batchIndex\n",
    "]\n",
    "monthsNb = [[cfg.month_abbr_hydr[e] for e in l] for l in months]\n",
    "\n",
    "ids = dataset_test[0].dataset.indexToId(batchIndex)\n",
    "data = {'ID': [], 'cum_pred': [], 'monthNb': []}\n",
    "for i, (id, mi) in enumerate(zip(ids, monthsNb)):\n",
    "    yi_cum_pred = y_cum_pred[i][~np.isnan(y_cum_pred[i])]\n",
    "    data['monthNb'] += mi\n",
    "    data['cum_pred'] += yi_cum_pred.tolist()\n",
    "    data['ID'] += [id] * len(mi)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "cumulativePredVSTruth(df, 'NN on test', cfg.month_abbr_hydr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MBM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
