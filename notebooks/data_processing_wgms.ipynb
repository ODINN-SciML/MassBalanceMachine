{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "<h1>MassBalanceMachine Data Processing - Example for Retrieving Training Features for the Iceland Region (WGMS)</h1>\n",
    "<p style='text-align: justify;'>\n",
    "In this notebook, we will demonstrate the data processing workflow of the MassBalanceMachine using an example with glaciological data from Icelandic glaciers. This example will guide you through the data processing pipeline, which retrieves topographical and meteorological features for all points with glaciological surface mass balance observations, and transforms the data to a monthly resolution.\n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "We begin by importing some basic libraries along with the <code>massbalancemachine</code> library. Next, we specify the location where we will store the files for the region of interest . The data used in this example is from the <a href='https://icelandicglaciers.is/index.html#/page/map'>Icelandic Glaciers inventory</a>, that was already preprocessed, to align with the WGMS data format we are using, in this <a href='https://github.com/ODINN-SciML/MassBalanceMachine/blob/main/notebooks/data_preprocessing.ipynb'>notebook</a>. You can also download data from the <a href='https://wgms.ch/data_databaseversions/'>WGMS database</a>, as you desire.\n",
    "</p>\n",
    "\n",
    "<b>Note:</b> WGMS data can contain errors or incorrect data, please check the data for correctness before using it.   \n",
    "\n",
    "<b>Note:</b> All data, stake measurements and glaciers outlines, are expected to have the WGS84 CRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import massbalancemachine as mbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "<h2>1. Load your Target Surface Mass Balance Dataset and Retrieve RGI ID per Stake</h2>\n",
    "<p style='text-align: justify;'>\n",
    "In this step, we define and load our glaciological data from a region of interest. The WGMS dataset does not include RGI IDs by default, so we need to retrieve them from a glacier outline shapefile provided by the Randolph Glacier Inventory (v6). Each stake is then matched with an RGI ID. The RGI ID is necessary for the MassBalanceMachine to add additional topographical and meteorological features for training stage.\n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "<b>How to Retrieve the Glacier Outlines:</b> Download the shapefiles for the region of interest from this <a href='https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0770_rgi_v6/'>link</a>. Extract the files and copy the .shp, .prj, .shx, and .dbf files in the correct directory so that you can use it with the Jupyter Notebook. Also, make sure you point to the correct directory and files in the next code cell.\n",
    "</p>\n",
    "\n",
    "<b>Note:</b> Data records that have an invalid FROM_DATE or TO_DATE, where the day is indicated as 99, are deleted from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the filename of the input file with the raw data\n",
    "target_data_fname = './example_data/iceland/files/iceland_wgms_dataset.csv'\n",
    "# Specify the shape filename of the glaciers outline obtained from RGIv6\n",
    "glacier_outline_fname = './example_data/iceland/glacier_outlines/06_rgi60_Iceland.shp'\n",
    "\n",
    "# Load the target data and the glacier outlines\n",
    "data = pd.read_csv(target_data_fname)\n",
    "glacier_outline = gpd.read_file(glacier_outline_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "<h3>1.1 Match the Stake Measurements with a RGI ID</h3>\n",
    "<p style='text-align: justify;'>\n",
    "Based on the location of the stake measurement given by POINT_LAT and POINT_LON, each data record is matched with the RGI ID for the glacier where the stake is located.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the RGI ID for each stake measurement for the region of interest\n",
    "data = mbm.data_processing.utils.get_rgi(data=data, glacier_outlines=glacier_outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "Then, we can create a MassBalanceMachine `Dataset`, by using the loaded dataframe for Norway stake data together with the matched RGI IDs, as such: \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the column name for the column that has the RGI IDs for each of the stakes\n",
    "dataset = mbm.Dataset(data=data, region_name='iceland', data_path='./example_data/iceland/files/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "<h2>2. Get Topographical Features per Stake Measurement</h2>\n",
    "<p style='text-align: justify;'>\n",
    "Once we have created a <code>Dataset</code>, the first thing we can do is to add topographical data in our dataset. This can be done automatically with the MassBalanceMachine (which calls OGGM) by doing the following:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the topographical features of interest \n",
    "# Please see the OGGM documentation what variables are available: https://oggm.org/tutorials/stable/notebooks/10minutes/machine_learning.html ('topo', 'slope_factor', 'dis_from_border')\n",
    "voi_topographical = ['aspect', 'slope']\n",
    "\n",
    "# Retrieve the topographical features for each stake measurement and add them to the dataset\n",
    "dataset.get_topo_features(vois=voi_topographical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "<h2>3. Get Meteorological Features per Stake Measurement</h2>\n",
    "<p style='text-align: justify;'>\n",
    "Once we have the topographical data, we can add the necessary climate data for the dataset. This is done by pulling that from ERA5-Land database for the region of interest. Before the climate data is matched with the stake measurements, we need to manually download the climate data for the region of interest from <a href='https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-land-monthly-means?tab=form'>Climate Copernicus website</a>. Check the following options:\n",
    "</p>\n",
    "\n",
    "\n",
    "<table border=\"1\" style=\"margin-top: 20px; margin-bottom: 20px; border-collapse: collapse; width: 100%;\">\n",
    "    <tr>\n",
    "        <th style=\"padding: 10px; text-align: left;\">Field</th>\n",
    "        <th style=\"padding: 10px; text-align: left;\">Details</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"padding: 10px;\"><strong>Product type:</strong></td>\n",
    "        <td style=\"padding: 10px;\"><em>Monthly averaged reanalysis</em></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"padding: 10px;\"><strong>Variable:</strong></td>\n",
    "        <td style=\"padding: 10px;\">\n",
    "            <ul style=\"margin: 0; padding-left: 20px;\">\n",
    "                <li><em>2m temperature</em> (t2m)</li>\n",
    "                <li><em>Total precipitation</em> (tp)</li>\n",
    "                <li><em>Surface latent heat flux</em> (slhf)</li>\n",
    "                <li><em>Surface sensible heat flux</em> (sshf)</li>\n",
    "                <li><em>Surface solar radiation downwards</em> (ssrd)</li>\n",
    "                <li><em>Forecast albedo</em> (fal)</li>\n",
    "                <li><em>Surface net thermal radiation</em> (str)</li>\n",
    "                <li>...</li>\n",
    "                <li>You can explore additional options as you prefer. In this example, we use the variables listed above.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"padding: 10px;\"><strong>Year:</strong></td>\n",
    "        <td style=\"padding: 10px;\"><em>Select all</em></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"padding: 10px;\"><strong>Month:</strong></td>\n",
    "        <td style=\"padding: 10px;\"><em>Select all</em></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"padding: 10px;\"><strong>Time:</strong></td>\n",
    "        <td style=\"padding: 10px;\"><em>Select all</em></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"padding: 10px;\"><strong>Geographical area:</strong></td>\n",
    "        <td style=\"padding: 10px;\">\n",
    "            <em>Either download for the entire Earth, or specify the coordinates for a bounding box in the region of interest. For the region of interest, provide a North, East, South, and West coordinate, specifying the two corners of the bounding box.</em>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"padding: 10px;\"><strong>Format:</strong></td>\n",
    "        <td style=\"padding: 10px;\"><em>NetCDF-3</em></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Then click <i>Submit Request</i> (after you have registered or logged into your account). Please be aware that downloading this data can take up to several hours, depending on the number of variables, timescales, and the area selected. Once the download is complete, place the netCDF file in the correct directory and rename it accordingly.\n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "Additionally, we need the _geopotential height_ as an extra variable in our dataset. We will calculate a new variable by determining the difference between the geopotential height and the elevation at the stake measurement. The purpose of this height difference is to encourage the model to use it for downscaling the ERA5Land climate data, rather than relying on the lapse rate. The geopotential is downloaded from <a href='https://confluence.ecmwf.int/display/CKB/ERA5-Land%3A+data+documentation#ERA5Land:datadocumentation-parameterlistingParameterlistings'>here</a>, and is already included in this example.\n",
    "</p>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the files of the climate data, that will be matched with the coordinates of the stake data\n",
    "era5_climate_data = './example_data/iceland/climate/era5_monthly_averaged_data.nc'\n",
    "geopotential_data = './example_data/iceland/climate/era5_geopotential_pressure.nc'\n",
    "\n",
    "# Match the climate features, from the ERA5Land netCDF file, for each of the stake measurement dataset\n",
    "dataset.get_climate_features(climate_data=era5_climate_data, geopotential_data=geopotential_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "<h2>4. Transform Data to Monthly Time Resolution</h2>\n",
    "<p style='text-align: justify;'>\n",
    "Finally, we need to transform the dataset into a monthly resolution, accounting for a variable period for the SMB target data. This will be done in order to perform SMB predictions at a monthly time step, which then will be integrated both in time and space to match the available glaciological and geodetic SMB observations for the training. Please specify the climate variables used in the previous step in the list below. Consult the <a href='https://confluence.ecmwf.int/display/CKB/ERA5-Land%3A+data+documentation'>documentation</a> of all short names of the climate variables.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the short names of the climate variables available in the dataset\n",
    "vois_climate = ['t2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str']\n",
    "\n",
    "# For each record, convert to a monthly time resolution\n",
    "dataset.convert_to_monthly(vois_climate=vois_climate, vois_topographical=voi_topographical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Finally, we can take a look at the dataset which will be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dataset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "We have finished preprocessing the training data for our machine learning model. You can either explore the dataset in this <a href='https://github.com/ODINN-SciML/MassBalanceMachine/blob/main/notebooks/date_exploration.ipynb'>notebook</a> or continue with model training in this <a href='https://github.com/ODINN-SciML/MassBalanceMachine/blob/main/notebooks/xgboost_model_training.ipynb'>notebook</a>.\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
